postid|data
3401428|"#!/usr/bin/env python&#xa;# -*- coding: utf-8 -*-&#xa;from wsgiref.simple_server import make_server&#xa;import sys&#xa;import json&#xa;import traceback&#xa;import datetime&#xa;from multiprocessing import Process&#xa;from getopt import getopt, GetoptError&#xa;from jsonrpcbase import JSONRPCService, InvalidParamsError, KeywordError,\&#xa;    JSONRPCError, InvalidRequestError&#xa;from jsonrpcbase import ServerError as JSONServerError&#xa;from os import environ&#xa;from ConfigParser import ConfigParser&#xa;from biokbase import log&#xa;import requests as _requests&#xa;import random as _random&#xa;import os&#xa;from biokbase.RNASeq.authclient import KBaseAuth as _KBaseAuth&#xa;&#xa;DEPLOY = 'KB_DEPLOYMENT_CONFIG'&#xa;SERVICE = 'KB_SERVICE_NAME'&#xa;AUTH = 'auth-server-url'&#xa;&#xa;# Note that the error fields do not match the 2.0 JSONRPC spec&#xa;&#xa;&#xa;def get_config_file():&#xa;    return environ.get(DEPLOY, None)&#xa;&#xa;&#xa;def get_service_name():&#xa;    return environ.get(SERVICE, None)&#xa;&#xa;&#xa;def get_config():&#xa;    if not get_config_file():&#xa;        return None&#xa;    retconfig = {}&#xa;    config = ConfigParser()&#xa;    config.read(get_config_file())&#xa;    for nameval in config.items(get_service_name() or 'KBaseRNASeq'):&#xa;        retconfig[nameval[0]] = nameval[1]&#xa;    return retconfig&#xa;&#xa;config = get_config()&#xa;&#xa;from biokbase.RNASeq.KBaseRNASeqImpl import KBaseRNASeq  # @IgnorePep8&#xa;impl_KBaseRNASeq = KBaseRNASeq(config)&#xa;&#xa;&#xa;class JSONObjectEncoder(json.JSONEncoder):&#xa;&#xa;    def default(self, obj):&#xa;        if isinstance(obj, set):&#xa;            return list(obj)&#xa;        if isinstance(obj, frozenset):&#xa;            return list(obj)&#xa;        if hasattr(obj, 'toJSONable'):&#xa;            return obj.toJSONable()&#xa;        return json.JSONEncoder.default(self, obj)&#xa;&#xa;&#xa;class JSONRPCServiceCustom(JSONRPCService):&#xa;&#xa;    def call(self, ctx, jsondata):&#xa;        """"""&#xa;        Calls jsonrpc service's method and returns its return value in a JSON&#xa;        string or None if there is none.&#xa;&#xa;        Arguments:&#xa;        jsondata -- remote method call in jsonrpc format&#xa;        """"""&#xa;        result = self.call_py(ctx, jsondata)&#xa;        if result is not None:&#xa;            return json.dumps(result, cls=JSONObjectEncoder)&#xa;&#xa;        return None&#xa;&#xa;    def _call_method(self, ctx, request):&#xa;        """"""Calls given method with given params and returns it value.""""""&#xa;        method = self.method_data[request['method']]['method']&#xa;        params = request['params']&#xa;        result = None&#xa;        try:&#xa;            if isinstance(params, list):&#xa;                # Does it have enough arguments?&#xa;                if len(params) < self._man_args(method) - 1:&#xa;                    raise InvalidParamsError('not enough arguments')&#xa;                # Does it have too many arguments?&#xa;                if(not self._vargs(method) and len(params) >&#xa;                        self._max_args(method) - 1):&#xa;                    raise InvalidParamsError('too many arguments')&#xa;&#xa;                result = method(ctx, *params)&#xa;            elif isinstance(params, dict):&#xa;                # Do not accept keyword arguments if the jsonrpc version is&#xa;                # not >=1.1.&#xa;                if request['jsonrpc'] < 11:&#xa;                    raise KeywordError&#xa;&#xa;                result = method(ctx, **params)&#xa;            else:  # No params&#xa;                result = method(ctx)&#xa;        except JSONRPCError:&#xa;            raise&#xa;        except Exception as e:&#xa;            # log.exception('method %s threw an exception' % request['method'])&#xa;            # Exception was raised inside the method.&#xa;            newerr = JSONServerError()&#xa;            newerr.trace = traceback.format_exc()&#xa;            newerr.data = e.message&#xa;            raise newerr&#xa;        return result&#xa;&#xa;    def call_py(self, ctx, jsondata):&#xa;        """"""&#xa;        Calls jsonrpc service's method and returns its return value in python&#xa;        object format or None if there is none.&#xa;&#xa;        This method is same as call() except the return value is a python&#xa;        object instead of JSON string. This method is mainly only useful for&#xa;        debugging purposes.&#xa;        """"""&#xa;        rdata = jsondata&#xa;        # we already deserialize the json string earlier in the server code, no&#xa;        # need to do it again&#xa;#        try:&#xa;#            rdata = json.loads(jsondata)&#xa;#        except ValueError:&#xa;#            raise ParseError&#xa;&#xa;        # set some default values for error handling&#xa;        request = self._get_default_vals()&#xa;&#xa;        if isinstance(rdata, dict) and rdata:&#xa;            # It's a single request.&#xa;            self._fill_request(request, rdata)&#xa;            respond = self._handle_request(ctx, request)&#xa;&#xa;            # Don't respond to notifications&#xa;            if respond is None:&#xa;                return None&#xa;&#xa;            return respond&#xa;        elif isinstance(rdata, list) and rdata:&#xa;            # It's a batch.&#xa;            requests = []&#xa;            responds = []&#xa;&#xa;            for rdata_ in rdata:&#xa;                # set some default values for error handling&#xa;                request_ = self._get_default_vals()&#xa;                self._fill_request(request_, rdata_)&#xa;                requests.append(request_)&#xa;&#xa;            for request_ in requests:&#xa;                respond = self._handle_request(ctx, request_)&#xa;                # Don't respond to notifications&#xa;                if respond is not None:&#xa;                    responds.append(respond)&#xa;&#xa;            if responds:&#xa;                return responds&#xa;&#xa;            # Nothing to respond.&#xa;            return None&#xa;        else:&#xa;            # empty dict, list or wrong type&#xa;            raise InvalidRequestError&#xa;&#xa;    def _handle_request(self, ctx, request):&#xa;        """"""Handles given request and returns its response.""""""&#xa;        if self.method_data[request['method']].has_key('types'): # @IgnorePep8&#xa;            self._validate_params_types(request['method'], request['params'])&#xa;&#xa;        result = self._call_method(ctx, request)&#xa;&#xa;        # Do not respond to notifications.&#xa;        if request['id'] is None:&#xa;            return None&#xa;&#xa;        respond = {}&#xa;        self._fill_ver(request['jsonrpc'], respond)&#xa;        respond['result'] = result&#xa;        respond['id'] = request['id']&#xa;&#xa;        return respond&#xa;&#xa;&#xa;class MethodContext(dict):&#xa;&#xa;    def __init__(self, logger):&#xa;        self['client_ip'] = None&#xa;        self['user_id'] = None&#xa;        self['authenticated'] = None&#xa;        self['token'] = None&#xa;        self['module'] = None&#xa;        self['method'] = None&#xa;        self['call_id'] = None&#xa;        self['rpc_context'] = None&#xa;        self['provenance'] = None&#xa;        self._debug_levels = set([7, 8, 9, 'DEBUG', 'DEBUG2', 'DEBUG3'])&#xa;        self._logger = logger&#xa;&#xa;    def log_err(self, message):&#xa;        self._log(log.ERR, message)&#xa;&#xa;    def log_info(self, message):&#xa;        self._log(log.INFO, message)&#xa;&#xa;    def log_debug(self, message, level=1):&#xa;        if level in self._debug_levels:&#xa;            pass&#xa;        else:&#xa;            level = int(level)&#xa;            if level < 1 or level > 3:&#xa;                raise ValueError(""Illegal log level: "" + str(level))&#xa;            level = level + 6&#xa;        self._log(level, message)&#xa;&#xa;    def set_log_level(self, level):&#xa;        self._logger.set_log_level(level)&#xa;&#xa;    def get_log_level(self):&#xa;        return self._logger.get_log_level()&#xa;&#xa;    def clear_log_level(self):&#xa;        self._logger.clear_user_log_level()&#xa;&#xa;    def _log(self, level, message):&#xa;        self._logger.log_message(level, message, self['client_ip'],&#xa;                                 self['user_id'], self['module'],&#xa;                                 self['method'], self['call_id'])&#xa;&#xa;    def provenance(self):&#xa;        callbackURL = os.environ.get('SDK_CALLBACK_URL')&#xa;        if callbackURL:&#xa;            # OK, there's a callback server from which we can get provenance&#xa;            arg_hash = {'method': 'CallbackServer.get_provenance',&#xa;                        'params': [],&#xa;                        'version': '1.1',&#xa;                        'id': str(_random.random())[2:]&#xa;                        }&#xa;            body = json.dumps(arg_hash)&#xa;            response = _requests.post(callbackURL, data=body,&#xa;                                      timeout=60)&#xa;            response.encoding = 'utf-8'&#xa;            if response.status_code == 500:&#xa;                if ('content-type' in response.headers and&#xa;                        response.headers['content-type'] ==&#xa;                        'application/json'):&#xa;                    err = response.json()&#xa;                    if 'error' in err:&#xa;                        raise ServerError(**err['error'])&#xa;                    else:&#xa;                        raise ServerError('Unknown', 0, response.text)&#xa;                else:&#xa;                    raise ServerError('Unknown', 0, response.text)&#xa;            if not response.ok:&#xa;                response.raise_for_status()&#xa;            resp = response.json()&#xa;            if 'result' not in resp:&#xa;                raise ServerError('Unknown', 0,&#xa;                                  'An unknown server error occurred')&#xa;            return resp['result'][0]&#xa;        else:&#xa;            return self.get('provenance')&#xa;&#xa;&#xa;class ServerError(Exception):&#xa;    '''&#xa;    The call returned an error. Fields:&#xa;    name - the name of the error.&#xa;    code - the error code.&#xa;    message - a human readable error message.&#xa;    data - the server side stacktrace.&#xa;    '''&#xa;&#xa;    def __init__(self, name, code, message, data=None, error=None):&#xa;        super(Exception, self).__init__(message)&#xa;        self.name = name&#xa;        self.code = code&#xa;        self.message = message if message else ''&#xa;        self.data = data or error or ''&#xa;        # data = JSON RPC 2.0, error = 1.1&#xa;&#xa;    def __str__(self):&#xa;        return self.name + ': ' + str(self.code) + '. ' + self.message + \&#xa;            '\n' + self.data&#xa;&#xa;&#xa;def getIPAddress(environ):&#xa;    xFF = environ.get('HTTP_X_FORWARDED_FOR')&#xa;    realIP = environ.get('HTTP_X_REAL_IP')&#xa;    trustXHeaders = config is None or \&#xa;        config.get('dont_trust_x_ip_headers') != 'true'&#xa;&#xa;    if (trustXHeaders):&#xa;        if (xFF):&#xa;            return xFF.split(',')[0].strip()&#xa;        if (realIP):&#xa;            return realIP.strip()&#xa;    return environ.get('REMOTE_ADDR')&#xa;&#xa;&#xa;class Application(object):&#xa;    # Wrap the wsgi handler in a class definition so that we can&#xa;    # do some initialization and avoid regenerating stuff over&#xa;    # and over&#xa;&#xa;    def logcallback(self):&#xa;        self.serverlog.set_log_file(self.userlog.get_log_file())&#xa;&#xa;    def log(self, level, context, message):&#xa;        self.serverlog.log_message(level, message, context['client_ip'],&#xa;                                   context['user_id'], context['module'],&#xa;                                   context['method'], context['call_id'])&#xa;&#xa;    def __init__(self):&#xa;        submod = get_service_name() or 'KBaseRNASeq'&#xa;        self.userlog = log.log(&#xa;            submod, ip_address=True, authuser=True, module=True, method=True,&#xa;            call_id=True, changecallback=self.logcallback,&#xa;            config=get_config_file())&#xa;        self.serverlog = log.log(&#xa;            submod, ip_address=True, authuser=True, module=True, method=True,&#xa;            call_id=True, logfile=self.userlog.get_log_file())&#xa;        self.serverlog.set_log_level(6)&#xa;        self.rpc_service = JSONRPCServiceCustom()&#xa;        self.method_authentication = dict()&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CreateRNASeqSampleSet,&#xa;                             name='KBaseRNASeq.CreateRNASeqSampleSet',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CreateRNASeqSampleSet'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.BuildBowtie2Index,&#xa;                             name='KBaseRNASeq.BuildBowtie2Index',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.BuildBowtie2Index'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.GetFeaturesToGTF,&#xa;                             name='KBaseRNASeq.GetFeaturesToGTF',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.GetFeaturesToGTF'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Bowtie2Call,&#xa;                             name='KBaseRNASeq.Bowtie2Call',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Bowtie2Call'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Hisat2Call,&#xa;                             name='KBaseRNASeq.Hisat2Call',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Hisat2Call'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.TophatCall,&#xa;                             name='KBaseRNASeq.TophatCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.TophatCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.StringTieCall,&#xa;                             name='KBaseRNASeq.StringTieCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.StringTieCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Hisat2StringTieCall,&#xa;                             name='KBaseRNASeq.Hisat2StringTieCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Hisat2StringTieCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CufflinksCall,&#xa;                             name='KBaseRNASeq.CufflinksCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CufflinksCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CuffdiffCall,&#xa;                             name='KBaseRNASeq.CuffdiffCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CuffdiffCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.DiffExpCallforBallgown,&#xa;                             name='KBaseRNASeq.DiffExpCallforBallgown',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.DiffExpCallforBallgown'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.status,&#xa;                             name='KBaseRNASeq.status',&#xa;                             types=[dict])&#xa;        authurl = config.get(AUTH) if config else None&#xa;        self.auth_client = _KBaseAuth(authurl)&#xa;&#xa;    def __call__(self, environ, start_response):&#xa;        # Context object, equivalent to the perl impl CallContext&#xa;        ctx = MethodContext(self.userlog)&#xa;        ctx['client_ip'] = getIPAddress(environ)&#xa;        status = '500 Internal Server Error'&#xa;&#xa;        try:&#xa;            body_size = int(environ.get('CONTENT_LENGTH', 0))&#xa;        except (ValueError):&#xa;            body_size = 0&#xa;        if environ['REQUEST_METHOD'] == 'OPTIONS':&#xa;            # we basically do nothing and just return headers&#xa;            status = '200 OK'&#xa;            rpc_result = """"&#xa;        else:&#xa;            request_body = environ['wsgi.input'].read(body_size)&#xa;            try:&#xa;                req = json.loads(request_body)&#xa;            except ValueError as ve:&#xa;                err = {'error': {'code': -32700,&#xa;                                 'name': ""Parse error"",&#xa;                                 'message': str(ve),&#xa;                                 }&#xa;                       }&#xa;                rpc_result = self.process_error(err, ctx, {'version': '1.1'})&#xa;            else:&#xa;                ctx['module'], ctx['method'] = req['method'].split('.')&#xa;                ctx['call_id'] = req['id']&#xa;                ctx['rpc_context'] = {&#xa;                    'call_stack': [{'time': self.now_in_utc(),&#xa;                                    'method': req['method']}&#xa;                                   ]&#xa;                }&#xa;                prov_action = {'service': ctx['module'],&#xa;                               'method': ctx['method'],&#xa;                               'method_params': req['params']&#xa;                               }&#xa;                ctx['provenance'] = [prov_action]&#xa;                try:&#xa;                    token = environ.get('HTTP_AUTHORIZATION')&#xa;                    # parse out the method being requested and check if it&#xa;                    # has an authentication requirement&#xa;                    method_name = req['method']&#xa;                    auth_req = self.method_authentication.get(&#xa;                        method_name, 'none')&#xa;                    if auth_req != 'none':&#xa;                        if token is None and auth_req == 'required':&#xa;                            err = JSONServerError()&#xa;                            err.data = (&#xa;                                'Authentication required for KBaseRNASeq ' +&#xa;                                'but no authentication header was passed')&#xa;                            raise err&#xa;                        elif token is None and auth_req == 'optional':&#xa;                            pass&#xa;                        else:&#xa;                            try:&#xa;                                user = self.auth_client.get_user(token)&#xa;                                ctx['user_id'] = user&#xa;                                ctx['authenticated'] = 1&#xa;                                ctx['token'] = token&#xa;                            except Exception, e:&#xa;                                if auth_req == 'required':&#xa;                                    err = JSONServerError()&#xa;                                    err.data = \&#xa;                                        ""Token validation failed: %s"" % e&#xa;                                    raise err&#xa;                    if (environ.get('HTTP_X_FORWARDED_FOR')):&#xa;                        self.log(log.INFO, ctx, 'X-Forwarded-For: ' +&#xa;                                 environ.get('HTTP_X_FORWARDED_FOR'))&#xa;                    self.log(log.INFO, ctx, 'start method')&#xa;                    rpc_result = self.rpc_service.call(ctx, req)&#xa;                    self.log(log.INFO, ctx, 'end method')&#xa;                    status = '200 OK'&#xa;                except JSONRPCError as jre:&#xa;                    err = {'error': {'code': jre.code,&#xa;                                     'name': jre.message,&#xa;                                     'message': jre.data&#xa;                                     }&#xa;                           }&#xa;                    trace = jre.trace if hasattr(jre, 'trace') else None&#xa;                    rpc_result = self.process_error(err, ctx, req, trace)&#xa;                except Exception, e:&#xa;                    err = {'error': {'code': 0,&#xa;                                     'name': 'Unexpected Server Error',&#xa;                                     'message': 'An unexpected server error ' +&#xa;                                                'occurred',&#xa;                                     }&#xa;                           }&#xa;                    rpc_result = self.process_error(err, ctx, req,&#xa;                                                    traceback.format_exc())&#xa;&#xa;        # print 'The request method was %s\n' % environ['REQUEST_METHOD']&#xa;        # print 'The environment dictionary is:\n%s\n' % pprint.pformat(environ) @IgnorePep8&#xa;        # print 'The request body was: %s' % request_body&#xa;        # print 'The result from the method call is:\n%s\n' % \&#xa;        #    pprint.pformat(rpc_result)&#xa;&#xa;        if rpc_result:&#xa;            response_body = rpc_result&#xa;        else:&#xa;            response_body = ''&#xa;&#xa;        response_headers = [&#xa;            ('Access-Control-Allow-Origin', '*'),&#xa;            ('Access-Control-Allow-Headers', environ.get(&#xa;                'HTTP_ACCESS_CONTROL_REQUEST_HEADERS', 'authorization')),&#xa;            ('content-type', 'application/json'),&#xa;            ('content-length', str(len(response_body)))]&#xa;        start_response(status, response_headers)&#xa;        return [response_body]&#xa;&#xa;    def process_error(self, error, context, request, trace=None):&#xa;        if trace:&#xa;            self.log(log.ERR, context, trace.split('\n')[0:-1])&#xa;        if 'id' in request:&#xa;            error['id'] = request['id']&#xa;        if 'version' in request:&#xa;            error['version'] = request['version']&#xa;            e = error['error'].get('error')&#xa;            if not e:&#xa;                error['error']['error'] = trace&#xa;        elif 'jsonrpc' in request:&#xa;            error['jsonrpc'] = request['jsonrpc']&#xa;            error['error']['data'] = trace&#xa;        else:&#xa;            error['version'] = '1.0'&#xa;            error['error']['error'] = trace&#xa;        return json.dumps(error)&#xa;&#xa;    def now_in_utc(self):&#xa;        # Taken from http://stackoverflow.com/questions/3401428/how-to-get-an-isoformat-datetime-string-including-the-default-timezone @IgnorePep8&#xa;        dtnow = datetime.datetime.now()&#xa;        dtutcnow = datetime.datetime.utcnow()&#xa;        delta = dtnow - dtutcnow&#xa;        hh, mm = divmod((delta.days * 24*60*60 + delta.seconds + 30) // 60, 60)&#xa;        return ""%s%+02d:%02d"" % (dtnow.isoformat(), hh, mm)&#xa;&#xa;application = Application()&#xa;&#xa;# This is the uwsgi application dictionary. On startup uwsgi will look&#xa;# for this dict and pull its configuration from here.&#xa;# This simply lists where to ""mount"" the application in the URL path&#xa;#&#xa;# This uwsgi module ""magically"" appears when running the app within&#xa;# uwsgi and is not available otherwise, so wrap an exception handler&#xa;# around it&#xa;#&#xa;# To run this server in uwsgi with 4 workers listening on port 9999 use:&#xa;# uwsgi -M -p 4 --http :9999 --wsgi-file _this_file_&#xa;# To run a using the single threaded python BaseHTTP service&#xa;# listening on port 9999 by default execute this file&#xa;#&#xa;try:&#xa;    import uwsgi&#xa;# Before we do anything with the application, see if the&#xa;# configs specify patching all std routines to be asynch&#xa;# *ONLY* use this if you are going to wrap the service in&#xa;# a wsgi container that has enabled gevent, such as&#xa;# uwsgi with the --gevent option&#xa;    if config is not None and config.get('gevent_monkeypatch_all', False):&#xa;        print ""Monkeypatching std libraries for async""&#xa;        from gevent import monkey&#xa;        monkey.patch_all()&#xa;    uwsgi.applications = {&#xa;        '': application&#xa;        }&#xa;except ImportError:&#xa;    # Not available outside of wsgi, ignore&#xa;    pass&#xa;&#xa;_proc = None&#xa;&#xa;&#xa;def start_server(host='localhost', port=0, newprocess=False):&#xa;    '''&#xa;    By default, will start the server on localhost on a system assigned port&#xa;    in the main thread. Excecution of the main thread will stay in the server&#xa;    main loop until interrupted. To run the server in a separate process, and&#xa;    thus allow the stop_server method to be called, set newprocess = True. This&#xa;    will also allow returning of the port number.'''&#xa;&#xa;    global _proc&#xa;    if _proc:&#xa;        raise RuntimeError('server is already running')&#xa;    httpd = make_server(host, port, application)&#xa;    port = httpd.server_address[1]&#xa;    print ""Listening on port %s"" % port&#xa;    if newprocess:&#xa;        _proc = Process(target=httpd.serve_forever)&#xa;        _proc.daemon = True&#xa;        _proc.start()&#xa;    else:&#xa;        httpd.serve_forever()&#xa;    return port&#xa;&#xa;&#xa;def stop_server():&#xa;    global _proc&#xa;    _proc.terminate()&#xa;    _proc = None&#xa;&#xa;&#xa;def process_async_cli(input_file_path, output_file_path, token):&#xa;    exit_code = 0&#xa;    with open(input_file_path) as data_file:&#xa;        req = json.load(data_file)&#xa;    if 'version' not in req:&#xa;        req['version'] = '1.1'&#xa;    if 'id' not in req:&#xa;        req['id'] = str(_random.random())[2:]&#xa;    ctx = MethodContext(application.userlog)&#xa;    if token:&#xa;        user = application.auth_client.get_user(token)&#xa;        ctx['user_id'] = user&#xa;        ctx['authenticated'] = 1&#xa;        ctx['token'] = token&#xa;    if 'context' in req:&#xa;        ctx['rpc_context'] = req['context']&#xa;    ctx['CLI'] = 1&#xa;    ctx['module'], ctx['method'] = req['method'].split('.')&#xa;    prov_action = {'service': ctx['module'], 'method': ctx['method'],&#xa;                   'method_params': req['params']}&#xa;    ctx['provenance'] = [prov_action]&#xa;    resp = None&#xa;    try:&#xa;        resp = application.rpc_service.call_py(ctx, req)&#xa;    except JSONRPCError as jre:&#xa;        trace = jre.trace if hasattr(jre, 'trace') else None&#xa;        resp = {'id': req['id'],&#xa;                'version': req['version'],&#xa;                'error': {'code': jre.code,&#xa;                          'name': jre.message,&#xa;                          'message': jre.data,&#xa;                          'error': trace}&#xa;                }&#xa;    except Exception:&#xa;        trace = traceback.format_exc()&#xa;        resp = {'id': req['id'],&#xa;                'version': req['version'],&#xa;                'error': {'code': 0,&#xa;                          'name': 'Unexpected Server Error',&#xa;                          'message': 'An unexpected server error occurred',&#xa;                          'error': trace}&#xa;                }&#xa;    if 'error' in resp:&#xa;        exit_code = 500&#xa;    with open(output_file_path, ""w"") as f:&#xa;        f.write(json.dumps(resp, cls=JSONObjectEncoder))&#xa;    return exit_code&#xa;&#xa;if __name__ == ""__main__"":&#xa;    if (len(sys.argv) >= 3 and len(sys.argv) <= 4 and&#xa;            os.path.isfile(sys.argv[1])):&#xa;        token = None&#xa;        if len(sys.argv) == 4:&#xa;            if os.path.isfile(sys.argv[3]):&#xa;                with open(sys.argv[3]) as token_file:&#xa;                    token = token_file.read()&#xa;            else:&#xa;                token = sys.argv[3]&#xa;        sys.exit(process_async_cli(sys.argv[1], sys.argv[2], token))&#xa;    try:&#xa;        opts, args = getopt(sys.argv[1:], """", [""port="", ""host=""])&#xa;    except GetoptError as err:&#xa;        # print help information and exit:&#xa;        print str(err)  # will print something like ""option -a not recognized""&#xa;        sys.exit(2)&#xa;    port = 9999&#xa;    host = 'localhost'&#xa;    for o, a in opts:&#xa;        if o == '--port':&#xa;            port = int(a)&#xa;        elif o == '--host':&#xa;            host = a&#xa;            print ""Host set to %s"" % host&#xa;        else:&#xa;            assert False, ""unhandled option""&#xa;&#xa;    start_server(host=host, port=port)&#xa;#    print ""Listening on port %s"" % port&#xa;#    httpd = make_server( host, port, application)&#xa;#&#xa;#    httpd.serve_forever()&#xa;"
7857352|"# Copyright (c) 2015-2016, NVIDIA CORPORATION. All rights reserved.&#xa;#&#xa;# SPDX-License-Identifier: GPL-2.0&#xa;&#xa;# Logic to spawn a sub-process and interact with its stdio.&#xa;&#xa;import os&#xa;import re&#xa;import pty&#xa;import signal&#xa;import select&#xa;import time&#xa;&#xa;class Timeout(Exception):&#xa;    """"""An exception sub-class that indicates that a timeout occurred.""""""&#xa;    pass&#xa;&#xa;class Spawn(object):&#xa;    """"""Represents the stdio of a freshly created sub-process. Commands may be&#xa;    sent to the process, and responses waited for.&#xa;&#xa;    Members:&#xa;        output: accumulated output from expect()&#xa;    """"""&#xa;&#xa;    def __init__(self, args, cwd=None):&#xa;        """"""Spawn (fork/exec) the sub-process.&#xa;&#xa;        Args:&#xa;            args: array of processs arguments. argv[0] is the command to&#xa;              execute.&#xa;            cwd: the directory to run the process in, or None for no change.&#xa;&#xa;        Returns:&#xa;            Nothing.&#xa;        """"""&#xa;&#xa;        self.waited = False&#xa;        self.buf = ''&#xa;        self.output = ''&#xa;        self.logfile_read = None&#xa;        self.before = ''&#xa;        self.after = ''&#xa;        self.timeout = None&#xa;        # http://stackoverflow.com/questions/7857352/python-regex-to-match-vt100-escape-sequences&#xa;        # Note that re.I doesn't seem to work with this regex (or perhaps the&#xa;        # version of Python in Ubuntu 14.04), hence the inclusion of a-z inside&#xa;        # [] instead.&#xa;        self.re_vt100 = re.compile('(\x1b\[|\x9b)[^@-_a-z]*[@-_a-z]|\x1b[@-_a-z]')&#xa;&#xa;        (self.pid, self.fd) = pty.fork()&#xa;        if self.pid == 0:&#xa;            try:&#xa;                # For some reason, SIGHUP is set to SIG_IGN at this point when&#xa;                # run under ""go"" (www.go.cd). Perhaps this happens under any&#xa;                # background (non-interactive) system?&#xa;                signal.signal(signal.SIGHUP, signal.SIG_DFL)&#xa;                if cwd:&#xa;                    os.chdir(cwd)&#xa;                os.execvp(args[0], args)&#xa;            except:&#xa;                print 'CHILD EXECEPTION:'&#xa;                import traceback&#xa;                traceback.print_exc()&#xa;            finally:&#xa;                os._exit(255)&#xa;&#xa;        try:&#xa;            self.poll = select.poll()&#xa;            self.poll.register(self.fd, select.POLLIN | select.POLLPRI | select.POLLERR | select.POLLHUP | select.POLLNVAL)&#xa;        except:&#xa;            self.close()&#xa;            raise&#xa;&#xa;    def kill(self, sig):&#xa;        """"""Send unix signal ""sig"" to the child process.&#xa;&#xa;        Args:&#xa;            sig: The signal number to send.&#xa;&#xa;        Returns:&#xa;            Nothing.&#xa;        """"""&#xa;&#xa;        os.kill(self.pid, sig)&#xa;&#xa;    def isalive(self):&#xa;        """"""Determine whether the child process is still running.&#xa;&#xa;        Args:&#xa;            None.&#xa;&#xa;        Returns:&#xa;            Boolean indicating whether process is alive.&#xa;        """"""&#xa;&#xa;        if self.waited:&#xa;            return False&#xa;&#xa;        w = os.waitpid(self.pid, os.WNOHANG)&#xa;        if w[0] == 0:&#xa;            return True&#xa;&#xa;        self.waited = True&#xa;        return False&#xa;&#xa;    def send(self, data):&#xa;        """"""Send data to the sub-process's stdin.&#xa;&#xa;        Args:&#xa;            data: The data to send to the process.&#xa;&#xa;        Returns:&#xa;            Nothing.&#xa;        """"""&#xa;&#xa;        os.write(self.fd, data)&#xa;&#xa;    def expect(self, patterns):&#xa;        """"""Wait for the sub-process to emit specific data.&#xa;&#xa;        This function waits for the process to emit one pattern from the&#xa;        supplied list of patterns, or for a timeout to occur.&#xa;&#xa;        Args:&#xa;            patterns: A list of strings or regex objects that we expect to&#xa;                see in the sub-process' stdout.&#xa;&#xa;        Returns:&#xa;            The index within the patterns array of the pattern the process&#xa;            emitted.&#xa;&#xa;        Notable exceptions:&#xa;            Timeout, if the process did not emit any of the patterns within&#xa;            the expected time.&#xa;        """"""&#xa;&#xa;        for pi in xrange(len(patterns)):&#xa;            if type(patterns[pi]) == type(''):&#xa;                patterns[pi] = re.compile(patterns[pi])&#xa;&#xa;        tstart_s = time.time()&#xa;        try:&#xa;            while True:&#xa;                earliest_m = None&#xa;                earliest_pi = None&#xa;                for pi in xrange(len(patterns)):&#xa;                    pattern = patterns[pi]&#xa;                    m = pattern.search(self.buf)&#xa;                    if not m:&#xa;                        continue&#xa;                    if earliest_m and m.start() >= earliest_m.start():&#xa;                        continue&#xa;                    earliest_m = m&#xa;                    earliest_pi = pi&#xa;                if earliest_m:&#xa;                    pos = earliest_m.start()&#xa;                    posafter = earliest_m.end()&#xa;                    self.before = self.buf[:pos]&#xa;                    self.after = self.buf[pos:posafter]&#xa;                    self.output += self.buf[:posafter]&#xa;                    self.buf = self.buf[posafter:]&#xa;                    return earliest_pi&#xa;                tnow_s = time.time()&#xa;                if self.timeout:&#xa;                    tdelta_ms = (tnow_s - tstart_s) * 1000&#xa;                    poll_maxwait = self.timeout - tdelta_ms&#xa;                    if tdelta_ms > self.timeout:&#xa;                        raise Timeout()&#xa;                else:&#xa;                    poll_maxwait = None&#xa;                events = self.poll.poll(poll_maxwait)&#xa;                if not events:&#xa;                    raise Timeout()&#xa;                c = os.read(self.fd, 1024)&#xa;                if not c:&#xa;                    raise EOFError()&#xa;                if self.logfile_read:&#xa;                    self.logfile_read.write(c)&#xa;                self.buf += c&#xa;                # count=0 is supposed to be the default, which indicates&#xa;                # unlimited substitutions, but in practice the version of&#xa;                # Python in Ubuntu 14.04 appears to default to count=2!&#xa;                self.buf = self.re_vt100.sub('', self.buf, count=1000000)&#xa;        finally:&#xa;            if self.logfile_read:&#xa;                self.logfile_read.flush()&#xa;&#xa;    def close(self):&#xa;        """"""Close the stdio connection to the sub-process.&#xa;&#xa;        This also waits a reasonable time for the sub-process to stop running.&#xa;&#xa;        Args:&#xa;            None.&#xa;&#xa;        Returns:&#xa;            Nothing.&#xa;        """"""&#xa;&#xa;        os.close(self.fd)&#xa;        for i in xrange(100):&#xa;            if not self.isalive():&#xa;                break&#xa;            time.sleep(0.1)&#xa;&#xa;    def get_expect_output(self):&#xa;        """"""Return the output read by expect()&#xa;&#xa;        Returns:&#xa;            The output processed by expect(), as a string.&#xa;        """"""&#xa;        return self.output&#xa;"
14872829|"from nose.tools import assert_raises&#xa;from sqlalchemy.exc import IntegrityError&#xa;from flask.ext.testing import TestCase&#xa;&#xa;from flask_truss.factory import create_app&#xa;from flask_truss.conf.app import Config&#xa;from flask_truss.models.base import db&#xa;&#xa;&#xa;class LocalhostProxyHack(object):&#xa;    """"""Ensures that a remote_addr is available for testing.&#xa;    From http://stackoverflow.com/questions/14872829/get-ip-address-when-testing-flask-application-through-nosetests&#xa;    """"""&#xa;    def __init__(self, app):&#xa;        self.app = app&#xa;&#xa;    def __call__(self, environ, start_response):&#xa;        environ['REMOTE_ADDR'] = environ.get('REMOTE_ADDR', '127.0.0.1')&#xa;        return self.app(environ, start_response)&#xa;&#xa;&#xa;class BaseTestCase(TestCase):&#xa;    def create_app(self):&#xa;        config = Config()&#xa;        app = create_app(config)&#xa;        app.config['TESTING'] = True&#xa;        app.config['WTF_CSRF_ENABLED'] = False&#xa;        app.wsgi_app = LocalhostProxyHack(app.wsgi_app)&#xa;        test_client = app.test_client()&#xa;        app.post = test_client.post&#xa;        app.get = test_client.get&#xa;        return app&#xa;&#xa;    def setUp(self):&#xa;        """"""Create the testing database, then swap commit for flush""""""&#xa;        db.create_all()&#xa;        self.orig_commit = db.session.commit&#xa;        db.session.commit = db.session.flush&#xa;&#xa;    def tearDown(self):&#xa;        """"""Clean up session, sway flush for commit, drop the testing database""""""&#xa;        db.session.remove()&#xa;        db.session.commit = self.orig_commit&#xa;        db.drop_all()&#xa;&#xa;    def add_to_session_and_flush_expecting_exception(self, obj):&#xa;        db.session.add(obj)&#xa;        assert_raises(IntegrityError, db.session.flush)&#xa;        db.session.rollback()&#xa;&#xa;    def add_to_session_and_flush(self, obj):&#xa;        db.session.add(obj)&#xa;        db.session.flush()&#xa;"
651794|"'''&#xa;Created on Sep 15, 2012&#xa;&#xa;@author: gaprice@lbl.gov&#xa;'''&#xa;&#xa;from collections import defaultdict as _defaultdict&#xa;&#xa;&#xa;class AutoVivifingDict(dict):&#xa;    """"""Implementation of perl's autovivification feature.""""""&#xa;# see http://stackoverflow.com/questions/651794/whats-the-best-way-to-&#xa;# initialize-a-dict-of-dicts-in-python&#xa;&#xa;    def __getitem__(self, item):&#xa;        try:&#xa;            return dict.__getitem__(self, item)&#xa;        except KeyError:&#xa;            value = self[item] = type(self)()&#xa;            return value&#xa;&#xa;&#xa;class DictListWithSortedIterator(object):&#xa;    ''' Implements a dict-like object, the values of which are lists of&#xa;  objects. The iterator returned from the __iter__ method traverses the lists&#xa;  in order of 1) the sorted list of keys and 2) the order the items were added&#xa;  to the list. Once iteration is started, the DLWSI cannot be modified until&#xa;  the iterator is exhausted or the discard() method is called on the&#xa;  iterator.'''&#xa;&#xa;    def __init__(self):&#xa;        self._store = _defaultdict(list)&#xa;        self._itercount = 0&#xa;        self._len = 0&#xa;&#xa;    def __setitem__(self, key, value):&#xa;        '''Add value to the end of the list stored at key'''&#xa;        self._check_iter_ok()&#xa;        self._store[key].append(value)&#xa;        self._len += 1&#xa;&#xa;    def __delitem__(self, key):&#xa;        '''Delete the list stored at key.'''&#xa;        self._check_iter_ok()&#xa;        if key in self._store:&#xa;            l = len(self._store[key])&#xa;            del self._store[key]&#xa;            self._len -= l&#xa;&#xa;    def __getitem__(self, key):&#xa;        '''Returns the list stored at key as a tuple.'''&#xa;        if self._store[key]:&#xa;            return tuple(self._store[key])&#xa;        else:&#xa;            raise KeyError(str(key))&#xa;&#xa;    def get(self, key, default=None):&#xa;        '''Returns the list stored at key as a tuple. The default argument&#xa;    specifies the object to return if key does not exist (default None).'''&#xa;        if self._store[key]:&#xa;            return tuple(self._store[key])&#xa;        return default&#xa;&#xa;    def keys(self):&#xa;        '''Returns an unsorted list of keys.'''&#xa;        return self._store.keys()&#xa;&#xa;    def __len__(self):&#xa;        return self._len&#xa;&#xa;    def clear(self):&#xa;        '''Removes all keys and values from the DLWSI.'''&#xa;        self._check_iter_ok()&#xa;        self._store = _defaultdict(list)&#xa;        self._len = 0&#xa;&#xa;    def merge(self, dictlist):&#xa;        '''Adds all key value pairs of the passed in DLWSI to this DLWSI. Any&#xa;    keys in this DLWSI that have matching names to keys in the passed in DLWSI&#xa;    will be overwritten.'''&#xa;        self._check_iter_ok()&#xa;        for k, v in dictlist:&#xa;            self.__setitem__(k, v)&#xa;&#xa;    def _check_iter_ok(self):&#xa;        if self._itercount:&#xa;            raise RuntimeError('Attempt to modify while iterating')&#xa;&#xa;    def __iter__(self):&#xa;        '''Returns an iterator over this DLWSI. The iterator proceeds through&#xa;    each list in the order of the sorted keys and returns a key / list item&#xa;    pair for each next call. Thus if a particular key has 3 list items that&#xa;    key will be returned 3 times in succession, once with each list item.&#xa;    The DLWSI cannot be modified while iterating. To allow modification without&#xa;    exhausting the iterator call the discard() method on the iterator.'''&#xa;        if not self._itercount:&#xa;            self._sortedKeys = sorted(self._store.keys())&#xa;        self._itercount += 1&#xa;        return self._ObjIter(self)&#xa;&#xa;    class _ObjIter(object):&#xa;&#xa;        def __init__(self, objStore):&#xa;            self._ostore = objStore&#xa;            self._dictindex = 0&#xa;            self._listindex = -1&#xa;            self._notexhausted = True&#xa;&#xa;        def next(self):&#xa;            if self._notexhausted and self._has_next():&#xa;                self._advance_index()&#xa;                return self._get_current_key_val_tuple()&#xa;            else:&#xa;                self._dec_iter_count()&#xa;                raise StopIteration&#xa;&#xa;        def discard(self):&#xa;            self._dec_iter_count()&#xa;&#xa;        def _dec_iter_count(self):&#xa;            if self._notexhausted:&#xa;                self._ostore._itercount -= 1&#xa;            self._notexhausted = False&#xa;&#xa;        def _has_next(self):&#xa;            if len(self._ostore._store) == 0:&#xa;                return False&#xa;            dictI = self._dictindex&#xa;            if self._listindex + 1 >= len(self._get_list(&#xa;                                          self._get_sorted_key(dictI))):&#xa;                dictI += 1&#xa;            return dictI < len(self._ostore._sortedKeys)&#xa;&#xa;        def _advance_index(self):&#xa;            if self._listindex + 1 >= len(self._get_list(self._get_sorted_key(&#xa;                                                         self._dictindex))):&#xa;                self._dictindex += 1&#xa;                self._listindex = 0&#xa;            else:&#xa;                self._listindex += 1&#xa;&#xa;        def _get_current_key_val_tuple(self):&#xa;            key = self._get_sorted_key(self._dictindex)&#xa;            return key, self._get_list(key)[self._listindex]&#xa;&#xa;        def _get_sorted_key(self, index):&#xa;            return self._ostore._sortedKeys[index]&#xa;&#xa;        def _get_list(self, key):&#xa;            return self._ostore._store[key]&#xa;&#xa;        def __next__(self):&#xa;            return self.next()&#xa;"
952302|"# Copyright 2010 Benjamin Dumke&#xa;# &#xa;# This file is part of Unicornify&#xa;# &#xa;# Unicornify is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU Affero General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;# &#xa;# Unicornify is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;# &#xa;# You should have received a copy of the NU Affero General Public License&#xa;# along with Unicornify; see the file COPYING. If not, see&#xa;# <http://www.gnu.org/licenses/>.&#xa;    &#xa;from math import sin, cos, pi, sqrt&#xa;import functools&#xa;from graphics import hls_to_rgb&#xa;&#xa;class Data(object):&#xa;    def __getattribute__(self, attr):&#xa;        try:&#xa;            return super(Data, self).__getattribute__(attr)&#xa;        except AttributeError:&#xa;            if attr in self._data:&#xa;                return self._data[attr]&#xa;            elif attr.endswith(""_col""):&#xa;                part = attr[:-4]&#xa;                func = lambda lightness: hls_to_rgb(self._data[part + ""_hue""], lightness, self._data[part + ""_sat""])&#xa;                self._data[attr] = func&#xa;                return func&#xa;            else:&#xa;                raise KeyError(""Unknown parameter %s"" % attr)&#xa;        &#xa;    def __setattr__(self, attr, value):&#xa;        if attr == ""_data"":&#xa;            super(Data, self).__setattr__(attr, value)&#xa;        elif attr in self._data:&#xa;            self._data[attr] = value&#xa;        else:&#xa;            raise KeyError(""Unknown parameter %s"" % attr)&#xa;&#xa;&#xa;class Rect(object):&#xa;    def __init__(self, left, top, right, bottom):&#xa;        self.coords = left, top, right, bottom&#xa;        self.left = left&#xa;        self.right = right&#xa;        self.top = top&#xa;        self.bottom = bottom&#xa;        &#xa;    def __add__(self, other):&#xa;        if other is None:&#xa;            return self&#xa;        zipped = zip(self.coords, other.coords)&#xa;        lt = map(min, zipped[:2])&#xa;        rb = map(max, zipped[2:])&#xa;        return Rect(*(lt + rb))&#xa;&#xa;    def __radd__(self, other):&#xa;        return self + other&#xa;        &#xa;    def intersects(self, other):&#xa;        hori = other.left <= self.left <= other.right or other.left <= self.right <= other.right or (self.left <= other.left and self.right >= other.right)&#xa;        vert = other.top <= self.top <= other.bottom or other.top <= self.bottom <= other.bottom or (self.top <= other.top and self.bottom >= other.bottom)&#xa;        return hori and vert&#xa;&#xa;class WorldView(object):&#xa;    """"""Note that projecting does not depend on shift, hence unlike&#xa;       the other parameters, shift may be changed without re-projecting.""""""&#xa;    def __init__(self, angle_y, angle_x, rotation_center, shift):&#xa;        self.angle_y = angle_y&#xa;        self.angle_x = angle_x&#xa;        self.rotation_center = rotation_center&#xa;        self.shift = shift&#xa;&#xa;class Ball(object):&#xa;    def __init__(self, center, radius, color):&#xa;        self.center = tuple(map(float, center))&#xa;        self.radius = float(radius)&#xa;        self.color = color&#xa;        &#xa;        self.projection = None&#xa;&#xa;    def project(self, worldview):&#xa;        rad_y = worldview.angle_y * pi / 180&#xa;        rad_x = worldview.angle_x * pi / 180&#xa;        x1, y1, z1 = (self.center[i] - worldview.rotation_center[i] for i in xrange(3))&#xa;        x2 = x1 * cos(rad_y) - z1 * sin(rad_y)&#xa;        y2 = y1&#xa;        z2 = x1 * sin(rad_y) + z1 * cos(rad_y)&#xa;        x3 = x2&#xa;        y3 = y2 * cos(rad_x) - z2 * sin(rad_x)&#xa;        z3 = y2 * sin(rad_x) + z2 * cos(rad_x)&#xa;        self.projection = tuple(c[0] + c[1] for c in zip((x3, y3, z3), worldview.rotation_center))&#xa;&#xa;    def rotate(self, angle, other, axis = 2):&#xa;        """"""Rotate this ball around the ball ""other"", leaving the ""axis"" coordinate&#xa;           as is. (default z, i.e. rotation is in the x-y-plane)""""""&#xa;        rad = angle * pi / 180&#xa;        swap = [(1, 2, 0), (0, 2, 1), (0, 1, 2)][axis]&#xa;        reverse = [(2, 0, 1), (0, 2, 1), (0, 1, 2)][axis]&#xa;        # the letters are the correct ones for the default case axis = 2&#xa;        x1, y1, z1 = (self.center[i] - other.center[i] for i in swap)&#xa;        x2 = x1 * cos(rad) - y1 * sin(rad)&#xa;        y2 = x1 * sin(rad) + y1 * cos(rad)&#xa;        z2 = z1&#xa;        self.center = tuple((x2, y2, z2)[reverse[i]] + other.center[i] for i in xrange(3))&#xa;           &#xa;    def set_distance(self, distance, other):&#xa;        """"""Move this ball to have the given distance to ""other"" while not changing the&#xa;           direction. This is the distance of the ball centers.""""""&#xa;        span = tuple(c[0] - c[1] for c in zip(self.center, other.center))&#xa;        stretch = distance / sqrt(sum(c * c for c in span))&#xa;        self.center = tuple(c[0] + stretch * c[1] for c in zip(other.center, span))&#xa;        &#xa;    def set_gap(self, gap, other):&#xa;        self.set_distance(gap + self.radius + other.radius, other)&#xa;        &#xa;    def move_to_sphere(self, other):&#xa;        self.set_distance(other.radius, other)&#xa;            &#xa;    def twoD(self):&#xa;        return self.projection[:2]            &#xa;            &#xa;    def draw(self, image, worldview):&#xa;        x, y = map(sum, zip(worldview.shift, self.twoD()))&#xa;        r = self.radius&#xa;        image.circle((x, y), r, self.color)&#xa;        &#xa;    def __sub__(self, other):&#xa;        tup1 = self.center&#xa;        if isinstance(other, Ball):&#xa;            tup2 = other.center&#xa;        else:&#xa;            tup2 = other&#xa;        return tuple(c[0] - c[1] for c in zip(tup1, tup2))&#xa;        &#xa;    def bounding(self):&#xa;        x, y, r = self.twoD() + (self.radius, )&#xa;        return Rect(x - r, y - r, x + r, y + r)&#xa;        &#xa;    def balls(self):&#xa;        yield self&#xa;        &#xa;    def sort(self, worldview):&#xa;        return&#xa;        &#xa;def identity(x):&#xa;    return x&#xa;&#xa;class Bone(object):&#xa;    def __init__(self, ball1, ball2):&#xa;        self._balls = [ball1, ball2]&#xa;        &#xa;    def draw(self, image, worldview, xfunc = identity, yfunc = identity):&#xa;        """"""xfunc and / or yfunc should map [0,1] -> [0,1] if the parameter ""step""&#xa;           should not be applied linearly to the coordinates. Note that these x and&#xa;           y are screen, i.e. 2D, coordinates. This is currently used to make the hair&#xa;           wavy.""""""&#xa;&#xa;        calc = lambda c1, c2, factor: c1 + (c2 - c1) * factor&#xa;        x1, y1 = map(sum, zip(self[0].twoD(), worldview.shift))&#xa;        x2, y2 = map(sum, zip(self[1].twoD(), worldview.shift))&#xa;        steps = max(*map(abs, (x2-x1, y2-y1)))&#xa;        # The centers might be very close, but the radii my be more apart,&#xa;        # hence the following step. without it, the eye/iris gradient sometimes&#xa;        # only has two or three steps&#xa;        steps = max(steps, abs(self[0].radius - self[1].radius))&#xa;        colors = zip(self[0].color, self[1].color)&#xa;&#xa;        # the old way is faster for smaller images, the new one for larger ones.&#xa;        # This table shows test measurings. Note that the size is the final size,&#xa;        # i.e. half the drawing size. O means the old one is faster, N the new one.&#xa;        #&#xa;        # zoom    test hash         32  64  128 256 512&#xa;        # ----------------------------------------------&#xa;        # close   21b96dcc68138     O   N   N   N!  N!&#xa;        # medium  18011847b11145af  O!  O   =   N   N&#xa;        # far     1895854ba5a70     O!  O!  O   =   N&#xa;&#xa;        if xfunc is identity and yfunc is identity:&#xa;            if steps > 80: # based on tests, this number seems roughly to be the break-even point&#xa;                image.connect_circles((x1, y1), self[0].radius, self[0].color, (x2, y2), self[1].radius, self[1].color)&#xa;                return &#xa;&#xa;        for step in xrange(int(steps + 1)):&#xa;            factor = float(step) / steps&#xa;            color = tuple(map(int, (calc(c[0], c[1], factor) for c in colors)))&#xa;            x, y, r = calc(x1, x2, xfunc(factor)), calc(y1, y2, yfunc(factor)), calc(self[0].radius, self[1].radius, factor)&#xa;            image.circle((x, y), r, color)&#xa;&#xa;    def __getitem__(self, index):&#xa;        return self._balls[index]&#xa;        &#xa;    def balls(self):&#xa;        return iter(self._balls)&#xa;        &#xa;    def project(self, worldview):&#xa;        for ball in self._balls:&#xa;            ball.project(worldview)&#xa;            &#xa;    def sort(self, worldview):&#xa;        self._balls.sort(key = lambda ball: ball.projection[2], reverse = True)&#xa;        &#xa;    def span(self):&#xa;        return self[1] - self[0]&#xa;        &#xa;    def bounding(self):&#xa;        return self[0].bounding() + self[1].bounding()&#xa;&#xa;def reverse(func):&#xa;    def result(v):&#xa;        return 1 - func(1 - v)&#xa;    return result&#xa;&#xa;class NonLinBone(Bone):        &#xa;    def __init__(self, ball1, ball2, xfunc = identity, yfunc = identity):&#xa;        self._balls = [ball1, ball2]&#xa;        self._xfunc = xfunc&#xa;        self._yfunc = yfunc&#xa;        &#xa;    def draw(self, image, worldview):&#xa;        super(NonLinBone, self).draw(image, worldview, self._xfunc, self._yfunc)&#xa;&#xa;    def sort(self, worldview):&#xa;        previous = self._balls[:]&#xa;        super(NonLinBone, self).sort(worldview)&#xa;        if previous != self._balls:&#xa;            self._xfunc = reverse(self._xfunc)&#xa;            self._yfunc = reverse(self._yfunc)&#xa;&#xa;def compare(worldview, first, second):&#xa;    """"""Compares two objects (balls or bones) to determine which one is behind the other.&#xa;       Note that although the worldview must be given, the objects still must&#xa;       have already been projected.""""""&#xa;       &#xa;    # FIXME: currently, this should be okay most of the time, because the&#xa;    # only subfigure used so far is unicorn.hairs. &#xa;    if isinstance(first, Figure):&#xa;        return 1&#xa;    elif isinstance(second, Figure):&#xa;        return -1&#xa;    &#xa;       &#xa;    if isinstance(first, Ball) and isinstance(second, Ball):&#xa;        return cmp(first.projection[2], second.projection[2])&#xa;    elif isinstance(first, Bone) and isinstance(second, Ball):&#xa;        # see Definition 1.1 at http://en.wikibooks.org/wiki/Linear_Algebra/Orthogonal_Projection_Into_a_Line&#xa;        span = first.span()&#xa;        lensquare = float(sum(c**2 for c in span))&#xa;        factor = sum(c[0] * c[1] for c in zip(second - first[0], span)) / lensquare&#xa;        if factor < 0:&#xa;            return compare(worldview, first[0], second)&#xa;        elif factor > 1:&#xa;            return compare(worldview, first[1], second)&#xa;        else: # the projection is within the bone&#xa;            proj = tuple(c[0] * factor + c[1] for c in zip(span, first[0].center))&#xa;            proj_ball = Ball(proj, 1, None)&#xa;            proj_ball.project(worldview)&#xa;            return compare(worldview, proj_ball, second)&#xa;    elif isinstance(first, Ball) and isinstance(second, Bone):&#xa;        return -compare(worldview, second, first)&#xa;    elif isinstance(first, Bone) and isinstance(second, Bone):&#xa;        set1 = set(first.balls())&#xa;        set2 = set(second.balls())&#xa;        if set1 & set2: # they share a ball&#xa;            # which bone is longer?&#xa;            l1, l2 = (sum((b[1].projection[i] - b[0].projection[i])**2 for i in (0, 1, 2)) for b in (first, second))&#xa;            if l1 > l2:&#xa;                return compare(worldview, first, (set2 - set1).pop())&#xa;            else:&#xa;                return compare(worldview, (set1 - set2).pop(), second)&#xa;        else:&#xa;            # check for the simple case: is there a pair of balls (one from&#xa;            # first, one from second that we can compare instead?&#xa;            for ball1 in first.balls():&#xa;                for ball2 in second.balls():&#xa;                    if ball1.bounding().intersects(ball2.bounding()):&#xa;                        result = compare(worldview, ball1, ball2)&#xa;                        if result != 0:&#xa;                            return result&#xa;        &#xa;            # find the point where the bones intersect *on the screen*. t1&#xa;            # and t2 are the parameters such that, say, ball1_x + t1 * (ball2_x-ball1_x)&#xa;            # is the x-coordinate refereced by t1. t_ < 0 or t_ > 1 means&#xa;            # that the ""intersection"" isn't on the line itself.&#xa;            &#xa;            s1x, s1y, s1z = first[0].projection&#xa;            d1x, d1y, d1z = (first[1].projection[i] - first[0].projection[i] for i in (0, 1, 2))&#xa;            s2x, s2y, s2z = second[0].projection&#xa;            d2x, d2y, d2z = (second[1].projection[i] - second[0].projection[i] for i in (0, 1, 2))&#xa;            &#xa;            # this number is zero if and only if the lines are parallel&#xa;            # (again, their screen projections -- not neccessarily&#xa;            # parallel in 3d space)&#xa;            denom = d1x * d2y - d2x * d1y&#xa;            &#xa;            if abs(denom) < 1e-4:&#xa;                return 0 #FIXME later?&#xa;                &#xa;            t2 = (d1y * (s2x - s1x) - d1x * (s2y - s1y)) / denom&#xa;            if abs(d1x) > 1e-4:&#xa;                t1 = (s2x + t2 * d2x - s1x) / d1x&#xa;            elif abs(d1y) > 1e-4:&#xa;                t1 = (s2y + t2 * d2y - s1y) / d1y&#xa;            else:&#xa;                return 0 #FIXME later?&#xa;            &#xa;                &#xa;            if t1 < -.5 or t1 > 1.5 or t2 < -1 or t2 > 2:&#xa;                return 0&#xa;                &#xa;                &#xa;            return cmp(s1z + t1 * d1z, s2z + t2 * d2z) #FIXME: zero case?&#xa;    else:&#xa;        raise ValueError(""Can't compare %s and %s"" % (first, second))&#xa;&#xa;def two_combinations(l):&#xa;    """"""itertools.combinations was introduced in Python 2.6; the app engine&#xa;       runs 2.5.""""""&#xa;    for i, first in enumerate(l):&#xa;        for second in l[i+1:]:&#xa;            yield (first, second)&#xa;&#xa;# used in Figure.sort() to determine which thing should be drawn next if&#xa;# it's not possible to fullfill all draw_after constraints&#xa;def evilness(thing):&#xa;    z = thing.projection[2] if isinstance(thing, Ball) else max(b.projection[2] for b in thing.balls())&#xa;    return -z&#xa;&#xa;class Figure(object):&#xa;    def __init__(self):&#xa;        self._things = []&#xa;        &#xa;    def add(self, *things):&#xa;        self._things.extend(things)&#xa;        &#xa;    def project(self, worldview):&#xa;        for thing in self._things:&#xa;            thing.project(worldview)&#xa;            &#xa;    def sort(self, worldview):&#xa;        """"""this assumes that projection has already happened!""""""&#xa;        comp = functools.partial(compare, worldview)&#xa;        &#xa;        # values of this dict are lists of all things that have&#xa;        # to be drawn before the corresponding key&#xa;        draw_after = dict((thing, []) for thing in self._things) &#xa;&#xa;        for first, second in two_combinations(self._things):&#xa;            if second not in draw_after[first] and first not in draw_after[second]:&#xa;                if first.bounding().intersects(second.bounding()):&#xa;                    c = comp(first, second)&#xa;                    if c < 0:&#xa;                        # first is in front of second&#xa;                        draw_after[first].append(second)&#xa;                    elif c > 0:&#xa;                        draw_after[second].append(first)&#xa;        &#xa;        # this is pretty much the algorithm from http://stackoverflow.com/questions/952302/&#xa;        sorted_things = []&#xa;        queue = []&#xa;        for thing, deps in draw_after.items():&#xa;            if not deps:&#xa;                queue.append(thing)&#xa;                del draw_after[thing]&#xa;&#xa;        while draw_after:&#xa;            while queue:&#xa;                popped = queue.pop()&#xa;                sorted_things.append(popped)&#xa;                for thing, deps in draw_after.items():&#xa;                    if popped in deps:&#xa;                        deps.remove(popped)&#xa;                        if not deps:&#xa;                            queue.append(thing)&#xa;                            del draw_after[thing]&#xa;&#xa;            if draw_after:&#xa;                # if the sorting couldn't fullfill all ""draw after"" contraints,&#xa;                # we remove the ball / bone which lies farthest in the back&#xa;                # and try again&#xa;&#xa;                least_evil = min((thing for thing in draw_after.iterkeys()),&#xa;                                 key = evilness&#xa;                                )&#xa;                sorted_things.append(least_evil)&#xa;                del draw_after[least_evil]&#xa;                for thing, deps in draw_after.items():&#xa;                    if least_evil in deps:&#xa;                        deps.remove(least_evil)&#xa;                        if not deps:&#xa;                            queue.append(thing)&#xa;                            del draw_after[thing]&#xa;                &#xa;        self._things = sorted_things            &#xa;&#xa;        for thing in self._things:&#xa;            thing.sort(worldview)&#xa;            &#xa;    def draw(self, image, worldview):&#xa;        viewrect = Rect(*(tuple(-c for c in worldview.shift) + tuple(image.size - c for c in worldview.shift)))&#xa;        for thing in self._things:&#xa;            if thing.bounding().intersects(viewrect):&#xa;                thing.draw(image, worldview)&#xa;&#xa;    def balls(self):&#xa;        for thing in self._things:&#xa;            for ball in thing.balls():&#xa;                yield ball&#xa;&#xa;    def ball_set(self):&#xa;        """"""Returns all balls that are either directly in this figure or in&#xa;           on of its bones. It returns a set; i.e. each ball is returned&#xa;           exactly once.""""""&#xa;        return set(self.balls())&#xa;        &#xa;    def scale(self, factor):&#xa;        for ball in self.ball_set():&#xa;            ball.radius *= factor&#xa;            ball.center = tuple(c * factor for c in ball.center)&#xa;            &#xa;    def bounding(self):&#xa;        return sum((thing.bounding() for thing in self._things), None)&#xa;        &#xa;"
2475750|"# -*- coding: utf-8 -*-&#xa;""""""&#xa;GUI for CogStat.&#xa;""""""&#xa;&#xa;import sys&#xa;import os&#xa;import webbrowser&#xa;import gettext&#xa;import logging&#xa;import traceback&#xa;from urllib2 import urlopen&#xa;from distutils.version import LooseVersion&#xa;&#xa;import cogstat&#xa;import cogstat_dialogs&#xa;import cogstat_config as csc&#xa;csc.versions['cogstat'] = cogstat.__version__&#xa;import cogstat_util as cs_util&#xa;&#xa;from PyQt4 import QtGui&#xa;from PyQt4 import QtCore&#xa;&#xa;cs_util.get_versions()&#xa;&#xa;logging.root.setLevel(logging.INFO)&#xa;&#xa;reload(sys)&#xa;sys.setdefaultencoding(""utf-8"")  # TODO Not sure if this will work correctly for most systems.&#xa;&#xa;t = gettext.translation('cogstat', os.path.dirname(os.path.abspath(__file__))+'/locale/', [csc.language], fallback=True)&#xa;_ = t.ugettext&#xa;&#xa;rtl_lang = True if csc.language in ['he', 'fa', 'ar'] else False&#xa;&#xa;broken_analysis = '<default>'+_('%s Oops, something went wrong, CogStat could not run the analysis. You may want to report it.')&#xa;&#xa;class StatMainWindow(QtGui.QMainWindow):&#xa;    """"""&#xa;    CogStat GUI.&#xa;    """"""&#xa;    def __init__(self):&#xa;        super(StatMainWindow, self).__init__()  # TOD do we need super()?&#xa;        self._init_UI()&#xa;&#xa;        # Check if all required components are installed&#xa;        # TODO Maybe all these checking can be removed&#xa;        missing_required_components, missing_recommended_components = self._check_installed_components()&#xa;        if missing_required_components or missing_recommended_components:&#xa;            QtGui.QMessageBox.critical(self, 'Incomplete installation', u'Install missing component(s): ' + ''.join([x+u', ' for x in missing_required_components+missing_recommended_components])[:-2]+u'.<br><br>'+u'<a href = ""https://github.com/cogstat/cogstat/wiki/Installation"">Visit the installation help page</a> to see how to complete the installation.', QtGui.QMessageBox.Ok)&#xa;            if missing_required_components:&#xa;                sys.exit()&#xa;        &#xa;        self.analysis_results = []&#xa;        # analysis_result stores list of GuiResultPackages.&#xa;        # It will be useful when we can rerun all the previous analysis in the GUI output&#xa;        # At the moment no former results can be manipulated later&#xa;&#xa;        cogstat.output_type = 'gui'  # For some GUI specific formatting&#xa;&#xa;        self.check_for_update()&#xa;&#xa;        # Only for testing&#xa;#        self.open_file('sample_data/example2.csv'); #self.compare_groups()&#xa;#        self.open_file('test/data/test_data.csv')&#xa;#        self.open_clipboard()&#xa;#        self.print_data()&#xa;#        self.explore_variable('X')&#xa;#        self.explore_variable(u'a', freq=False)&#xa;#        self.explore_variable_pair(['G', 'H'])&#xa;#        self.pivot([u'X'], row_names=[], col_names=[], page_names=[u'CONDITION', u'TIME3'], function='N')&#xa;#        self.compare_variables(['X', 'Y'])&#xa;#        self.compare_variables(['A', 'B', 'C1'])&#xa;#        self.compare_variables(['D', 'E', 'F'])&#xa;#        self.compare_variables([u'CONDITION', u'CONDITION2', u'CONDITION3'])&#xa;#        self.compare_groups(['slope'], ['group'],  ['slope_SE'], 25)&#xa;#        self.compare_groups(['A'], ['G', 'H'])&#xa;#        self.compare_groups(['X'], ['W'])&#xa;#        self.save_result_as()&#xa;#        self.save_result_as(filename='pdf_test.pdf')&#xa;&#xa;    def check_for_update(self):&#xa;        """"""Check for update, and if update is available, display a message box with the download link.&#xa;&#xa;        The version number is available in a plain text file, at the appropriate web address.""""""&#xa;        try:&#xa;            latest_version = urlopen('http://kognitiv.elte.hu/cogstat/version').read()&#xa;            if LooseVersion(cogstat.__version__) < LooseVersion(latest_version):&#xa;                QtGui.QMessageBox.about(self, _('Update available'),&#xa;                                        _('New version is available.') + '<br><br>' +&#xa;                                        _('You can download the new version<br>from the <a href = ""%s"">CogStat download page</a>.')%'http://www.cogstat.org/download.html')&#xa;        except:&#xa;            print ""Couldn't check for update""&#xa;&#xa;    def _init_UI(self):&#xa;        self.resize(800, 600)&#xa;        self.setWindowTitle('CogStat')&#xa;        self.setWindowIcon(QtGui.QIcon(os.path.dirname(os.path.abspath(__file__)) + u'/resources/CogStat.ico'))&#xa;&#xa;        if rtl_lang:&#xa;            self.setLayoutDirection(QtCore.Qt.RightToLeft)&#xa;&#xa;        # Menus and commands&#xa;        menu_commands = [  # This list will be used to construct the menus&#xa;                            [_('&Data'),&#xa;                                ['', _('&Open data file')+'...', _('Ctrl+O'), _('Open data file (csv text file)'), 'self.open_file'],&#xa;                                ['', _('&Paste data'), _('Ctrl+V'), _('Paste data from clipboard'), 'self.open_clipboard'],&#xa;                                ['separator'],&#xa;                                # ['', _('&Filter outliers'), _('Ctrl+L'), _('Filter cases based on outliers'), 'self.xxx'],&#xa;                                # ['separator'],&#xa;                                ['', _('&Display data'), _('Ctrl+D'), _('Print data to the output'), 'self.print_data'],&#xa;                                ['', _('Display data &briefly'), _('Ctrl+B'), _('Print beginning of the data to the output'), 'self._print_data_brief'],&#xa;                            ],&#xa;                            [_('&Analysis'),&#xa;                                ['', _('&Explore variable')+'...', _('Ctrl+1'), _('Main properties of variables'), 'self.explore_variable'],&#xa;                                ['', _('Explore relation of variable &pair')+'...', _('Ctrl+2'), _('Properties of variable pairs'), 'self.explore_variable_pair'],&#xa;                                ['separator'],&#xa;                                ['', _('Pivot &table')+'...', 'Ctrl+T', _('Build a pivot table'), 'self.pivot'],&#xa;                                ['separator'],&#xa;                                ['', _('Compare repeated measures va&riables')+'...', 'Ctrl+R', _('Compare variables'), 'self.compare_variables'],&#xa;                                ['', _('Compare &groups')+'...', 'Ctrl+G', _('Compare groups'), 'self.compare_groups'],&#xa;                            ],&#xa;                            [_('&Results'),&#xa;                                ['', _('&Clear results'), _('Del'), _('Delete the output window'), 'self.delete_output'],&#xa;                                ['separator'],&#xa;                                ['', _('Save results'), _('Ctrl+P'), _('Save the output to .pdf format'), 'self.save_result'],&#xa;                                ['', _('Save results as')+'...', _('Shift+Ctrl+P'), _('Save the results'), 'self.save_result_as']&#xa;                            ],&#xa;                            [_('&CogStat'),&#xa;                                ['', _('&Help'), _('F1'), _('Read online documentation'), 'self._open_help_webpage'],&#xa;                                ['', _('&Preferences')+'...', '', _('Set the preferences'), 'self._show_preferences'],&#xa;                                ['', _('Request a &feature'), '', _(""Can't find a feature? Ask for it!""), 'self._open_reqfeat_webpage'],&#xa;                                ['separator'],&#xa;                                ['', _('&Report a problem'), '', _('Fill online form to report a problem'), 'self._open_reportbug_webpage'],&#xa;                                ['', _('&Diagnosis information'), '', _('List the version of the components on your system'), 'self.print_versions'],&#xa;                                ['separator'],&#xa;                                ['', _('&About'), '', _('About CogStat'), 'self._show_about'],&#xa;                                ['separator'],&#xa;                                ['', _('&Exit'), _('Ctrl+Q'), _('Exit CogStat'), 'self.closeEvent']&#xa;                            ]&#xa;                        ]&#xa;        # Enable these commands only when active_data is available&#xa;        self.analysis_commands = [_('&Save data'), _('Save data &as')+'...', _('&Display data'), _('Display data &briefly'),&#xa;                                  _('Pivot &table')+'...', _('&Explore variable')+'...',&#xa;                                  _('Explore relation of variable &pair')+'...', _('Compare repeated measures va&riables')+'...', _('Compare &groups')+'...',&#xa;                                  _('&Compare groups and variables')+'...']&#xa;&#xa;        # Create menus and commands&#xa;        self.menubar = self.menuBar()&#xa;        self.menus = []&#xa;        self.menu_commands = {}&#xa;        for menu in menu_commands:&#xa;            self.menus.append(self.menubar.addMenu(menu[0]))&#xa;            for i in range(1, len(menu)):&#xa;                if menu[i][0] == 'separator':&#xa;                    self.menus[-1].addSeparator()&#xa;                else:&#xa;                    self.menu_commands[menu[i][1]] = QtGui.QAction(QtGui.QIcon(menu[i][0]), menu[i][1], self)&#xa;                    self.menu_commands[menu[i][1]].setShortcut(menu[i][2])&#xa;                    self.menu_commands[menu[i][1]].setStatusTip(menu[i][3])&#xa;                    self.menu_commands[menu[i][1]].triggered.connect(eval(menu[i][4]))&#xa;                    self.menus[-1].addAction(self.menu_commands[menu[i][1]])&#xa;        for menu in self.analysis_commands:&#xa;            try:&#xa;                self.menu_commands[menu].setEnabled(False)&#xa;            except KeyError:&#xa;                pass&#xa;        &#xa;        # Prepare Output pane&#xa;        self.output_pane = QtGui.QTextBrowser()  # QTextBrowser can handle links, QTextEdit cannot&#xa;        self.output_pane.setLineWrapMode (QtGui.QTextEdit.NoWrap)&#xa;        self.output_pane.setText('<br><b>%s</b><br>%s<br>%s<br>%s<br>' %&#xa;                                 (_('Welcome to CogStat!'), _('CogStat makes statistical analysis more simple and efficient.'),&#xa;                                  _('To start working open a data file or paste your data from a spreadsheet.'),&#xa;                                  _('Find more information about CogStat on its <a href = ""https://www.cogstat.org"">webpage</a> or read the <a href=""https://github.com/cogstat/cogstat/wiki/Quick-Start-Tutorial"">quick start tutorial.</a>')))&#xa;        self.welcome_text_on = True  # Used for deleting the welcome text at the first analysis&#xa;        self.output_pane.setReadOnly(True)&#xa;        self.output_pane.setOpenExternalLinks(True)&#xa;        self.output_pane.setStyleSheet(""QTextBrowser { background-color: white; }"")&#xa;            # Some styles use non-white background (e.g. Linux Mint 17 Mate uses gray)&#xa;        # Set default font&#xa;        #print self.output_pane.currentFont().toString()&#xa;        # http://stackoverflow.com/questions/2475750/using-qt-css-to-set-own-q-propertyqfont&#xa;        font = QtGui.QFont()&#xa;        font.setFamily(csc.default_font)&#xa;        font.setPointSizeF(csc.default_font_size)&#xa;        self.output_pane.setFont(font)&#xa;        #print self.output_pane.currentFont().toString()&#xa;&#xa;        self.setCentralWidget(self.output_pane)&#xa;        self.setAcceptDrops(True)&#xa;        self.statusBar().showMessage(_('Ready'))&#xa;&#xa;        self.unsaved_output = False  # Do not want to save the output with the welcome message&#xa;        self.output_filename = ''&#xa;        &#xa;        self.show()&#xa;&#xa;    def _show_data_menus(self, on=True):&#xa;        """"""&#xa;        Enable or disable data handling menus depending on whether data is loaded.&#xa;        &#xa;        parameters:&#xa;        on: True to enable menus&#xa;            False to disable&#xa;            default is True&#xa;        """"""&#xa;        for menu in self.analysis_commands:&#xa;            try:&#xa;                self.menu_commands[menu].setEnabled(on)&#xa;            except:&#xa;                pass&#xa;        &#xa;    def dragEnterEvent(self, event):&#xa;        if event.mimeData().hasFormat(""text/uri-list""):&#xa;            event.accept()&#xa;        elif event.mimeData().hasFormat(""text/plain""):&#xa;            event.accept()&#xa;        else:&#xa;            event.ignore()&#xa;&#xa;    def dropEvent(self, event):&#xa;        if event.mimeData().hasFormat(""text/uri-list""):&#xa;            # print 'Dropped URL: ', event.mimeData().urls()[0].toString()[7:]&#xa;            self.open_file(filename=event.mimeData().urls()[0].toString()[7:])&#xa;        elif event.mimeData().hasFormat(""text/plain""):&#xa;            # print 'Dropped Text: ', event.mimeData().text()&#xa;            self._open_data(data=unicode(event.mimeData().text()))&#xa;        &#xa;    def _check_installed_components(self):&#xa;        """"""&#xa;        Check if all required and recommended components are installed.&#xa;        Return the list of missing components as strings.&#xa;        """"""&#xa;        missing_required_components = []&#xa;        missing_recommended_components = []&#xa;&#xa;        # Required components&#xa;        for module in ['pyqt', 'numpy', 'pandas', 'scipy', 'statsmodels']:&#xa;            if csc.versions[module] is None:&#xa;                missing_required_components.append(module)&#xa;&#xa;        # Recommended components&#xa;        for module in []:  # At the moment it's empty&#xa;            if csc.versions[module] is None:&#xa;                missing_recommended_components.append(module)&#xa;        # Check R only on Linux, since Win doesn't have a working rpy at the moment&#xa;        if sys.platform in ['linux2', 'linux']:&#xa;            for module in ['r', 'rpy2', 'car']:&#xa;                if csc.versions[module] is None:&#xa;                    missing_recommended_components.append(module)&#xa;        &#xa;        if missing_required_components:&#xa;            logging.error('Missing required components: %s' % missing_required_components)&#xa;        if missing_recommended_components:&#xa;            logging.error('Missing recommended components: %s' % missing_recommended_components)&#xa;        &#xa;        return missing_required_components, missing_recommended_components&#xa;&#xa;    def _busy_signal(self, on):&#xa;        """"""&#xa;        Changes the mouse, signalling that the system is busy&#xa;        """"""&#xa;        # http://qt-project.org/doc/qt-4.7/qt.html see CursorShape&#xa;        # http://qt-project.org/doc/qt-4.7/qapplication.html#id-19f00dae-ec43-493e-824c-ef07ce96d4c6&#xa;        if on:&#xa;            QtGui.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.WaitCursor))&#xa;            #QtGui.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.BusyCursor))&#xa;        else:&#xa;            while QtGui.QApplication.overrideCursor() is not None:&#xa;                # TODO if for some reason (unhandled exception) the cursor was not set back formerly,&#xa;                # then next time set it back&#xa;                # FIXME exception handling should solve this problem on the long term&#xa;                QtGui.QApplication.restoreOverrideCursor()&#xa;            #QtGui.QApplication.setOverrideCursor(QtGui.QCursor(QtCore.Qt.ArrowCursor))&#xa;        &#xa;    def _print_to_output_pane(self, index=-1):&#xa;        """"""Print a GuiResultPackage to GUI output pane&#xa;        :param index: index of the item in self.analysis_results to be printed&#xa;                      If no index is given, the last item is printed.&#xa;        """"""&#xa;        if self.welcome_text_on:&#xa;            self.output_pane.clear()&#xa;            self.welcome_text_on = False&#xa;        for output in self.analysis_results[index].output:&#xa;            if isinstance(output, basestring):&#xa;                self.output_pane.append(output)&#xa;            elif isinstance(output, QtGui.QImage):&#xa;                self.output_pane.moveCursor(11, 0)  # Moves cursor to the end&#xa;                self.output_pane.textCursor().insertImage(output)&#xa;            elif output is None:&#xa;                pass  # We simply don't do anything with None-s&#xa;            else:&#xa;                logging.error('Unknown output type: %s' % type(output))&#xa;        self.unsaved_output = True&#xa;                        &#xa;    ### Data menu methods ###&#xa;    def open_file(self, filename=''):&#xa;        """"""Open file.&#xa;        :param filename: filename with path&#xa;        """"""&#xa;        if filename in ['', False]:&#xa;            filename = cogstat_dialogs.open_data_file()&#xa;        if filename:&#xa;            self._open_data(unicode(filename))&#xa;&#xa;    def open_clipboard(self):&#xa;        """"""Open data copied to clipboard.""""""&#xa;        clipboard = QtGui.QApplication.clipboard()&#xa;        if clipboard.mimeData().hasFormat(""text/plain""):&#xa;            self._open_data(unicode(clipboard.text(""plain"", QtGui.QClipboard.Clipboard)))&#xa;    &#xa;    def _open_data(self, data):&#xa;        """""" Core of the import process.&#xa;        """"""&#xa;        self._busy_signal(True)&#xa;        try:&#xa;            self.active_data = cogstat.CogStatData(data=data)&#xa;            if self.active_data.import_source == _('Import failed'):&#xa;                QtGui.QMessageBox.warning(self, _('Import error'), _('Data could not be loaded.'), QtGui.QMessageBox.Ok)&#xa;                self._show_data_menus(False)&#xa;            else:&#xa;                self._show_data_menus()&#xa;                self.statusBar().showMessage((_('Data loaded from file: ') if self.active_data.import_source[:9] in ['text file', 'SPSS file'] else _('Data loaded from clipboard: '))&#xa;                                            + _('%s variables and %s cases.') % (len(self.active_data.data_frame.columns),&#xa;                                                                                 len(self.active_data.data_frame.index)))&#xa;                self.print_data(brief=True, display_import_message=True)&#xa;        except:&#xa;            self.analysis_results.append(GuiResultPackage())&#xa;            self.analysis_results[-1].add_command('self._open_data()')  # TODO&#xa;            self.analysis_results[-1].\&#xa;                add_output(cs_util.reformat_output('<default>' +&#xa;                                                   _('Open data. Oops, something went wrong, CogStat could not open the data. You may want to report the issue.')))&#xa;            traceback.print_exc()&#xa;            self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;            &#xa;    def print_data(self, brief=False, display_import_message=False):&#xa;        """"""Print the current data to the output.&#xa;        &#xa;        :param brief (bool): print only the first 10 rows&#xa;        :param display_import_message (bool):&#xa;        """"""&#xa;        self.analysis_results.append(GuiResultPackage())&#xa;        self.analysis_results[-1].add_command('self.print_data')  # TODO commands will be used to rerun the analysis&#xa;        self.analysis_results[-1].add_output(self.active_data.print_data(brief=brief))&#xa;        if self.active_data.import_message and display_import_message:&#xa;            self.analysis_results[-1].add_output(cs_util.reformat_output(self.active_data.import_message))&#xa;        self._print_to_output_pane()&#xa;&#xa;    def _print_data_brief(self):&#xa;        self.print_data(brief=True)&#xa;&#xa;    ### Analysis menu methods ###&#xa;&#xa;    def explore_variable(self, var_names=None, freq=True, dist=True, descr=True, norm=True, loc_test=True,&#xa;                         loc_test_value=0):&#xa;        """"""Computes various properties of variables.&#xa;&#xa;        Arguments:&#xa;        var_names (list): variable names&#xa;        freq (bool): compute frequencies (default True)&#xa;        dist (bool): compute distribution (default True)&#xa;        descr (bool): compute descriptive statistics (default True)&#xa;        norm (bool): check normality (default True)&#xa;        loc_test (bool): test location (e.g. t-test) (default True)&#xa;        loc_test_value (numeric): test location against this value (default 0.0)&#xa;        """"""&#xa;        if not var_names:&#xa;            try:&#xa;                self.dial_var_prop&#xa;            except:&#xa;                self.dial_var_prop = cogstat_dialogs.explore_var_dialog(names=self.active_data.data_frame.columns)&#xa;            else:  # TODO is it not necessary anymore? For all dialogs&#xa;                self.dial_var_prop.init_vars(names=self.active_data.data_frame.columns)&#xa;            if self.dial_var_prop.exec_():&#xa;                var_names, freq, loc_test_value = self.dial_var_prop.read_parameters()&#xa;            else:&#xa;                return&#xa;        self._busy_signal(True)&#xa;        try:&#xa;            for var_name in var_names:&#xa;                self.analysis_results.append(GuiResultPackage())&#xa;                self.analysis_results[-1].add_command('self.explore_variable()')  # TODO&#xa;                result = self.active_data.explore_variable(var_name, frequencies=freq, central_value=loc_test_value)&#xa;                self.analysis_results[-1].add_output(result)&#xa;                self._print_to_output_pane()&#xa;        except:&#xa;            self.analysis_results[-1].add_output(cs_util.reformat_output(broken_analysis % _('Explore variable.')))&#xa;            traceback.print_exc()&#xa;            self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;&#xa;    def explore_variable_pair(self, var_names=None):&#xa;        """"""Explore variable pairs.&#xa;        &#xa;        Arguments:&#xa;        var_names (list): variable names&#xa;        """"""&#xa;        if not var_names:&#xa;            try:&#xa;                self.dial_var_pair&#xa;            except:&#xa;                self.dial_var_pair = cogstat_dialogs.explore_var_pairs_dialog(names=self.active_data.data_frame.columns)&#xa;            else:&#xa;                self.dial_var_pair.init_vars(names=self.active_data.data_frame.columns)&#xa;            if self.dial_var_pair.exec_():&#xa;                var_names = self.dial_var_pair.read_parameters()&#xa;            else:&#xa;                return&#xa;        self._busy_signal(True)&#xa;        if len(var_names) < 2:  # TODO this check should go to the appropriate dialog&#xa;            self.analysis_results.append(GuiResultPackage())&#xa;            text_result = cs_util.reformat_output('<default> %s %s'%(_('Explore variable pair.'), _(u'At least two variables should be set.')))&#xa;            self.analysis_results[-1].add_output(text_result)&#xa;        else:&#xa;            try:&#xa;                for x in var_names:&#xa;                    pass_diag = False&#xa;                    for y in var_names:&#xa;                        if pass_diag:&#xa;                            self.analysis_results.append(GuiResultPackage())&#xa;                            self.analysis_results[-1].add_command('self.explore_variable_pair')  # TODO&#xa;                            result_list = self.active_data.explore_variable_pair(x, y)&#xa;                            self.analysis_results[-1].add_output(result_list)&#xa;                            self._print_to_output_pane()&#xa;                        if x == y:&#xa;                            pass_diag = True&#xa;            except:&#xa;                self.analysis_results[-1].add_output(cs_util.reformat_output(broken_analysis % _('Explore variable pair.')))&#xa;                traceback.print_exc()&#xa;                self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;            &#xa;    def pivot(self, depend_names=None, row_names=[], col_names=[], page_names=[], function='Mean'):&#xa;        """"""Build a pivot table.&#xa;        &#xa;        Arguments:&#xa;        depend_names (str): name of the dependent variable&#xa;        row_names, col_names, page_names (lists of str): name of the independent variables&#xa;        function (str): available functions: N,Sum, Mean, Median, Standard Deviation, Variance (default Mean)&#xa;        """"""&#xa;        if not depend_names:&#xa;            try:&#xa;                self.dial_pivot&#xa;            except:&#xa;                self.dial_pivot = cogstat_dialogs.pivot_dialog(names=self.active_data.data_frame.columns)&#xa;            else:&#xa;                self.dial_pivot.init_vars(names=self.active_data.data_frame.columns)&#xa;            if self.dial_pivot.exec_():&#xa;                row_names, col_names, page_names, depend_names, function = self.dial_pivot.read_parameters()&#xa;            else:&#xa;                return&#xa;        self._busy_signal(True)&#xa;        self.analysis_results.append(GuiResultPackage())&#xa;        if not depend_names or not (row_names or col_names or page_names):  # TODO this check should go to the dialog&#xa;            text_result = cs_util.reformat_output('<default>%s %s'%(_('Pivot table.'), _('The dependent variable and at least one grouping variable should be given.')))&#xa;        else:&#xa;            try:&#xa;                text_result = self.active_data.pivot(depend_names, row_names, col_names, page_names, function)&#xa;            except:&#xa;                text_result = cs_util.reformat_output(broken_analysis % _('Pivot table.'))&#xa;                traceback.print_exc()&#xa;        self.analysis_results[-1].add_output(text_result)&#xa;        self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;&#xa;    def compare_variables(self, var_names=None):&#xa;        """"""Compare variables.&#xa;        &#xa;        Arguments:&#xa;        var_names (list): variable names&#xa;        """"""&#xa;        if not var_names:&#xa;            try:&#xa;                self.dial_comp_var&#xa;            except:&#xa;                self.dial_comp_var = cogstat_dialogs.compare_vars_dialog(names=self.active_data.data_frame.columns)&#xa;            else:&#xa;                self.dial_comp_var.init_vars(names=self.active_data.data_frame.columns)&#xa;            if self.dial_comp_var.exec_():&#xa;                var_names = self.dial_comp_var.read_parameters()  # TODO check if settings are appropriate&#xa;            else:&#xa;                return&#xa;        self._busy_signal(True)&#xa;        self.analysis_results.append(GuiResultPackage())&#xa;        self.analysis_results[-1].add_command('self.compare_variables()')  # TODO&#xa;        if len(var_names) < 2:&#xa;            text_result = cs_util.reformat_output('<default>%s %s'%(_('Compare variables.'), _(u'At least two variables should be set.')))&#xa;            self.analysis_results[-1].add_output(text_result)&#xa;        else:&#xa;            try:&#xa;                result_list = self.active_data.compare_variables(var_names)&#xa;                for result in result_list:  # TODO is this a list of lists? Can we remove the loop?&#xa;                    self.analysis_results[-1].add_output(result)&#xa;            except:&#xa;                self.analysis_results[-1].add_output(cs_util.reformat_output(broken_analysis % _('Compare variables.')))&#xa;                traceback.print_exc()&#xa;        self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;        &#xa;    def compare_groups(self, var_names=None, groups=None, single_case_slope_SEs=None, single_case_slope_trial_n=None):&#xa;        """"""Compare groups.&#xa;        &#xa;        Arguments:&#xa;        var_names (list): dependent variable names&#xa;        groups (list): grouping variable names&#xa;        """"""&#xa;        if not var_names:&#xa;            try:&#xa;                self.dial_comp_grp&#xa;            except:&#xa;                self.dial_comp_grp = cogstat_dialogs.compare_groups_dialog(names=self.active_data.data_frame.columns)&#xa;            else:&#xa;                self.dial_comp_grp.init_vars(names=self.active_data.data_frame.columns)&#xa;            if self.dial_comp_grp.exec_():&#xa;                var_names, groups, single_case_slope_SEs, single_case_slope_trial_n = self.dial_comp_grp.read_parameters()  # TODO check if settings are appropriate&#xa;            else:&#xa;                return&#xa;        self._busy_signal(True)&#xa;        if not var_names or not groups:&#xa;            self.analysis_results.append(GuiResultPackage())&#xa;            self.analysis_results[-1].add_command('self.compare_groups()')  # TODO&#xa;            text_result = cs_util.reformat_output('<default>%s %s' % (_('Compare groups.'), _(u'Both the dependent and the grouping variables should be set.')))&#xa;            self.analysis_results[-1].add_output(text_result)&#xa;        else:&#xa;            for var_name in var_names:&#xa;                try:&#xa;                    self.analysis_results.append(GuiResultPackage())&#xa;                    self.analysis_results[-1].add_command('self.compare_groups()')  # TODO&#xa;                    result_list = self.active_data.compare_groups(var_name, groups,  single_case_slope_SEs, single_case_slope_trial_n)&#xa;                    self.analysis_results[-1].add_output(result_list)&#xa;                    self._print_to_output_pane()&#xa;                except:&#xa;                    self.analysis_results[-1].add_output(cs_util.reformat_output(broken_analysis % _('Compare groups.')))&#xa;                    traceback.print_exc()&#xa;                    self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;&#xa;    ### Result menu methods ###&#xa;    def delete_output(self):&#xa;        reply = QtGui.QMessageBox.question(self, _('Clear output'),&#xa;            _('Are you sure you want to delete the output?'), QtGui.QMessageBox.Yes | &#xa;            QtGui.QMessageBox.No, QtGui.QMessageBox.No)&#xa;        if reply == QtGui.QMessageBox.Yes:&#xa;            self.output_pane.clear()&#xa;            self.analysis_results = []&#xa;            self.unsaved_output = False  # Not necessary to save the empty output&#xa;&#xa;    def save_result(self):&#xa;        """"""Save the output pane to pdf file.""""""&#xa;        if self.output_filename == '':&#xa;            self.save_result_as()&#xa;        else:&#xa;            pdf_printer = QtGui.QPrinter()&#xa;            pdf_printer.setOutputFormat(QtGui.QPrinter.PdfFormat)&#xa;            pdf_printer.setColorMode(QtGui.QPrinter.Color)&#xa;            pdf_printer.setOutputFileName(self.output_filename)&#xa;            self.output_pane.print_(pdf_printer)&#xa;            self.unsaved_output = False&#xa;            &#xa;    def save_result_as(self, filename=None):&#xa;        """"""Save the output pane to pdf file.&#xa;        &#xa;        Arguments:&#xa;        filename (str): name of the file to save to&#xa;        """"""&#xa;        if not filename:&#xa;            filename = cogstat_dialogs.save_output()&#xa;        self.output_filename = filename&#xa;        if filename:&#xa;            # self.output_pane.setLineWrapMode (QtGui.QTextEdit.FixedPixelWidth)  # TODO&#xa;            pdf_printer = QtGui.QPrinter()&#xa;            pdf_printer.setOutputFormat(QtGui.QPrinter.PdfFormat)&#xa;            pdf_printer.setOutputFileName(self.output_filename)&#xa;            self.output_pane.print_(pdf_printer)&#xa;            # self.output_pane.setLineWrapMode (QtGui.QTextEdit.NoWrap)&#xa;            self.unsaved_output = False&#xa;&#xa;    ### Cogstat menu  methods ###&#xa;    def _open_help_webpage(self):&#xa;        webbrowser.open('https://github.com/cogstat/cogstat/wiki')&#xa;        &#xa;    def _show_preferences(self):&#xa;        try:&#xa;            self.dial_pref&#xa;        except:&#xa;            self.dial_pref = cogstat_dialogs.preferences_dialog()&#xa;        self.dial_pref.exec_()&#xa;    &#xa;    def _open_reqfeat_webpage(self):&#xa;        webbrowser.open('https://github.com/cogstat/cogstat/wiki/Suggest-a-new-feature')&#xa;        &#xa;    def _open_reportbug_webpage(self):&#xa;        webbrowser.open('https://github.com/cogstat/cogstat/wiki/Report-a-bug')&#xa;        &#xa;    def _show_about(self):&#xa;        QtGui.QMessageBox.about(self, _('About CogStat ')+csc.versions['cogstat'], u'CogStat '+csc.versions['cogstat']+(u'<br>%s<br><br>Copyright © %s-%s Attila Krajcsi<br><br><a href = ""http://www.cogstat.org"">%s</a>'%(_('Simple automatic data analysis software'), 2012, 2018, _('Visit CogStat website'))))&#xa;&#xa;    def print_versions(self):&#xa;        """"""Print the versions of the software components CogStat uses.""""""&#xa;        # Intentionally not localized.&#xa;        self._busy_signal(True)&#xa;        &#xa;        text_output = cs_util.reformat_output(cs_util.print_versions())&#xa;        &#xa;        self.analysis_results.append(GuiResultPackage())&#xa;        self.analysis_results[-1].add_output(csc.heading_style_begin + _('System components') + csc.heading_style_end)&#xa;        self.analysis_results[-1].add_output(text_output)&#xa;        self._print_to_output_pane()&#xa;        self._busy_signal(False)&#xa;&#xa;    def closeEvent(self, event):&#xa;        # Override the close behavior, otherwise alt+F4 quits unconditionally.&#xa;        # http://stackoverflow.com/questions/1414781/prompt-on-exit-in-pyqt-application&#xa;        &#xa;        # Check if everything is saved&#xa;        tosave = True&#xa;        while self.unsaved_output and tosave:&#xa;            reply = QtGui.QMessageBox.question(self, _('Save output'),&#xa;                _('Output has unsaved results. Do you want to save it?'), QtGui.QMessageBox.Yes | &#xa;                QtGui.QMessageBox.No, QtGui.QMessageBox.Yes)&#xa;            if reply == QtGui.QMessageBox.Yes:&#xa;                self.save_result()&#xa;            else:&#xa;                tosave=False&#xa;&#xa;        """"""&#xa;        reply = QtGui.QMessageBox.question(self, _('Confirm exit'), &#xa;            _('Are you sure you want to exit the program?'), QtGui.QMessageBox.Yes, QtGui.QMessageBox.No)&#xa;        if reply == QtGui.QMessageBox.Yes:&#xa;            QtGui.qApp.quit()&#xa;        else:&#xa;            event.ignore()&#xa;        """"""&#xa;&#xa;# -*- coding: utf-8 -*-&#xa;&#xa;class GuiResultPackage():&#xa;    """""" A class for storing a package of results.&#xa;&#xa;    Result object includes:&#xa;    - self.command: Command to run (python code) - not used yet&#xa;    - self.output:&#xa;        - list of strings (html) or figures (QImages)&#xa;        - the first item is recommended to be the title line&#xa;    """"""&#xa;&#xa;    def __init__(self):&#xa;        self.command = []&#xa;        self.output = []&#xa;&#xa;    def add_command(self, command):&#xa;        self.command.append(command)&#xa;&#xa;    def add_output(self, output):&#xa;        """"""Add output to the self.output&#xa;&#xa;        :param output: item or list of items to add&#xa;        """"""&#xa;        if isinstance(output, list):&#xa;            for outp in output:&#xa;                self.output.append(outp)&#xa;        else:&#xa;            self.output.append(output)&#xa;&#xa;def main():&#xa;    app = QtGui.QApplication(sys.argv)&#xa;    ex = StatMainWindow()&#xa;    sys.exit(app.exec_())"
2801882|"""""""&#xa;This file is part of VDISCOVER.&#xa;&#xa;VDISCOVER is free software: you can redistribute it and/or modify&#xa;it under the terms of the GNU General Public License as published by&#xa;the Free Software Foundation, either version 3 of the License, or&#xa;(at your option) any later version.&#xa;&#xa;VDISCOVER is distributed in the hope that it will be useful,&#xa;but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the&#xa;GNU General Public License for more details.&#xa;&#xa;You should have received a copy of the GNU General Public License&#xa;along with VDISCOVER. If not, see <http://www.gnu.org/licenses/>.&#xa;&#xa;Copyright 2014 by G.Grieco&#xa;""""""&#xa;&#xa;import random&#xa;import gzip&#xa;import sys&#xa;import csv&#xa;import subprocess&#xa;import pickle&#xa;import numpy as np&#xa;import matplotlib as mpl&#xa;&#xa;# hack from https://stackoverflow.com/questions/2801882/generating-a-png-with-matplotlib-when-display-is-undefined to avoid using X&#xa;# mpl.use('Agg')&#xa;import matplotlib.pyplot as plt&#xa;&#xa;from Utils import *&#xa;from Pipeline import *&#xa;&#xa;&#xa;# def Cluster(X, labels)&#xa;""""""&#xa;  assert(len(X_red) == len(labels))&#xa;&#xa;  from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;  bandwidth = estimate_bandwidth(X, quantile=0.2)&#xa;  print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;  af = MeanShift(bandwidth=bandwidth/1).fit(X_red)&#xa;&#xa;  cluster_centers = af.cluster_centers_&#xa;  cluster_labels = af.labels_&#xa;  n_clusters = len(cluster_centers)&#xa;&#xa;  plt.figure()&#xa;&#xa;  for ([x,y],label, cluster_label) in zip(X_red,labels, cluster_labels):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    plt.scatter(x, y, c = colors[cluster_label % ncolors])&#xa;    #plt.text(x-0.05, y+0.01, label.split(""/"")[-1])&#xa;&#xa;  for i,[x,y] in enumerate(cluster_centers):&#xa;    plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;             markeredgecolor='k', markersize=7)&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters)&#xa;""""""&#xa;# return zip(labels, cluster_labels)&#xa;&#xa;&#xa;batch_size = 25&#xa;window_size = 32&#xa;maxlen = window_size&#xa;&#xa;embedding_dims = 5&#xa;nb_filters = 50&#xa;filter_length = 3&#xa;hidden_dims = 50&#xa;nb_epoch = 3&#xa;&#xa;&#xa;def ClusterCnn(model_file, train_file, valid_file, ftype, nsamples, outdir):&#xa;&#xa;    f = open(model_file + "".pre"")&#xa;    preprocessor = pickle.load(f)&#xa;&#xa;    import h5py&#xa;    f = h5py.File(model_file + "".wei"")&#xa;&#xa;    layers = []&#xa;    for k in range(f.attrs['nb_layers']):&#xa;        g = f['layer_{}'.format(k)]&#xa;        layers.append([g['param_{}'.format(p)]&#xa;                       for p in range(g.attrs['nb_params'])])&#xa;&#xa;    max_features = len(preprocessor.tokenizer.word_counts)&#xa;&#xa;    print ""Reading and sampling data to train..""&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples, cut=None)&#xa;    train_size = len(train_features)&#xa;&#xa;    #y = train_programs&#xa;    X_train, y_train, labels = preprocessor.preprocess_traces(&#xa;        train_features, y_data=train_classes, labels=train_programs)&#xa;    new_model = make_cluster_cnn(&#xa;        ""test"",&#xa;        max_features,&#xa;        maxlen,&#xa;        embedding_dims,&#xa;        nb_filters,&#xa;        filter_length,&#xa;        hidden_dims,&#xa;        None,&#xa;        weights=layers)&#xa;&#xa;    train_dict = dict()&#xa;    train_dict[ftype] = new_model.predict(X_train)&#xa;&#xa;    model = make_cluster_pipeline_subtraces(ftype)&#xa;    X_red_comp = model.fit_transform(train_dict)&#xa;    explained_var = np.var(X_red_comp, axis=0)&#xa;    print explained_var&#xa;&#xa;    X_red = X_red_comp[:, 0:2]&#xa;    X_red_next = X_red_comp[:, 2:4]&#xa;&#xa;    colors = mpl.colors.cnames.keys()&#xa;    progs = list(set(labels))&#xa;    ncolors = len(colors)&#xa;    size = len(labels)&#xa;    print ""Plotting..""&#xa;&#xa;    for prog, [x, y] in zip(labels, X_red):&#xa;        # for prog,[x,y] in sample(zip(labels, X_red), min(size, 1000)):&#xa;        x = gauss(0, 0.05) + x&#xa;        y = gauss(0, 0.05) + y&#xa;        color = 'r'&#xa;        plt.scatter(x, y, c=color)&#xa;&#xa;    """"""&#xa;  if valid_file is not None:&#xa;    valid_programs, valid_features, valid_classes = read_traces(valid_file, None, cut=None, maxsize=window_size) #None)&#xa;    valid_dict = dict()&#xa;&#xa;    X_valid, _, valid_labels = preprocessor.preprocess_traces(valid_features, y_data=None, labels=valid_programs)&#xa;    valid_dict[ftype] = new_model.predict(X_valid)&#xa;    X_red_valid_comp = model.transform(valid_dict)&#xa;&#xa;    X_red_valid = X_red_valid_comp[:,0:2]&#xa;    X_red_valid_next = X_red_valid_comp[:,2:4]&#xa;&#xa;    for prog,[x,y] in zip(valid_labels, X_red_valid):&#xa;      x = gauss(0,0.05) + x&#xa;      y = gauss(0,0.05) + y&#xa;      plt.scatter(x, y, c='b')&#xa;      plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;  plt.show()&#xa;  """"""&#xa;    plt.savefig(train_file.replace("".gz"", """") + "".png"")&#xa;    print ""Bandwidth estimation..""&#xa;    from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;    X_red_sample = X_red[:min(size, 1000)]&#xa;    bandwidth = estimate_bandwidth(X_red_sample, quantile=0.2)&#xa;    print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;    #X_red = np.vstack((X_red,X_red_valid))&#xa;    #X_red_next = np.vstack((X_red_next,X_red_valid_next))&#xa;    #labels = labels + valid_labels&#xa;&#xa;    print X_red.shape, len(X_red), len(labels)&#xa;    # print valid_labels&#xa;&#xa;    af = MeanShift(bandwidth=bandwidth / 1).fit(X_red)&#xa;&#xa;    cluster_centers = af.cluster_centers_&#xa;    cluster_labels = af.labels_&#xa;    n_clusters = len(cluster_centers)&#xa;&#xa;    plt.figure()&#xa;    for ([x, y], label, cluster_label) in zip(X_red, labels, cluster_labels):&#xa;        # for ([x,y],label, cluster_label) in sample(zip(X_red,labels,&#xa;        # cluster_labels), min(size, 1000)):&#xa;        x = gauss(0, 0.1) + x&#xa;        y = gauss(0, 0.1) + y&#xa;        plt.scatter(x, y, c=colors[cluster_label % ncolors])&#xa;        # print label&#xa;        # if label in valid_labels:&#xa;        #  plt.text(x-0.05, y+0.01, label.split(""/"")[-1])&#xa;&#xa;    for i, [x, y] in enumerate(cluster_centers):&#xa;        plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;                 markeredgecolor='k', markersize=7)&#xa;&#xa;    """"""&#xa;  #for prog,[x,y] in zip(valid_labels, X_red_valid):&#xa;    #x = gauss(0,0.1) + x&#xa;    #y = gauss(0,0.1) + y&#xa;    #plt.scatter(x, y, c='black')&#xa;    #plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters)&#xa;&#xa;  #plt.savefig(""clusters.png"")&#xa;  plt.show()&#xa;  """"""&#xa;    plt.savefig(train_file.replace("".gz"", """") + "".clusters.png"")&#xa;&#xa;    clustered_traces = zip(labels, cluster_labels)&#xa;    writer = open_csv(train_file.replace("".gz"", """") + "".clusters"")&#xa;    for label, cluster in clustered_traces:&#xa;        writer.writerow([label, cluster])&#xa;&#xa;    """"""&#xa;&#xa;  clusters = dict()&#xa;  for label, cluster in clustered_traces:&#xa;    clusters[cluster] = clusters.get(cluster, []) + [label]&#xa;&#xa;  for cluster, traces in clusters.items():&#xa;    plt.figure()&#xa;    plt.title('Cluster %d' % cluster)&#xa;    #X_clus = []&#xa;&#xa;    #for prog in traces:&#xa;    #  i = labels.index(prog)&#xa;    #  X_clus.append(X_train[i])&#xa;&#xa;    #train_dict = dict()&#xa;    #train_dict[ftype] = X_clus&#xa;&#xa;    #model = make_cluster_pipeline_subtraces(ftype)&#xa;    #X_red = model.fit_transform(train_dict)&#xa;&#xa;    #for [x,y],prog in zip(X_red,traces):&#xa;    for prog in traces:&#xa;&#xa;      i = labels.index(prog)&#xa;      assert(i>=0)&#xa;      [x,y] = X_red_next[i]&#xa;      x = gauss(0,0.1) + x&#xa;      y = gauss(0,0.1) + y&#xa;      plt.scatter(x, y, c='r')&#xa;&#xa;      #if prog in valid_labels:&#xa;      plt.text(x-0.05, y+0.01, prog.split(""/"")[-1])&#xa;&#xa;      #plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;    plt.show()&#xa;    #plt.savefig('cluster-%d.png' % cluster)&#xa;  """"""&#xa;&#xa;    # return clustered_traces&#xa;&#xa;&#xa;def TrainCnn(model_file, train_file, valid_file, ftype, nsamples):&#xa;&#xa;    csvreader = open_csv(train_file)&#xa;&#xa;    train_features = []&#xa;    train_programs = []&#xa;    train_classes = []&#xa;&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples, cut=None)&#xa;    train_size = len(train_features)&#xa;&#xa;    from keras.preprocessing.text import Tokenizer&#xa;&#xa;    tokenizer = Tokenizer(nb_words=None, filters="""", lower=False, split="" "")&#xa;    # print type(train_features[0])&#xa;    tokenizer.fit_on_texts(train_features)&#xa;    max_features = len(tokenizer.word_counts)&#xa;&#xa;    preprocessor = DeepReprPreprocessor(tokenizer, window_size, batch_size)&#xa;    X_train, y_train = preprocessor.preprocess(train_features, 10000)&#xa;    nb_classes = len(preprocessor.classes)&#xa;    print preprocessor.classes&#xa;&#xa;    model = make_cluster_cnn(&#xa;        ""train"",&#xa;        max_features,&#xa;        maxlen,&#xa;        embedding_dims,&#xa;        nb_filters,&#xa;        filter_length,&#xa;        hidden_dims,&#xa;        nb_classes)&#xa;    model.fit(X_train, y_train, validation_split=0.1,&#xa;              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True)&#xa;&#xa;    model.mypreprocessor = preprocessor&#xa;    #model_file = model_file + "".wei""&#xa;    #modelfile = open_model(model_file)&#xa;    print ""Saving model to"", model_file + "".wei""&#xa;    model.save_weights(model_file + "".wei"")&#xa;&#xa;    #model_file = model_file + "".pre""&#xa;    modelfile = open_model(model_file + "".pre"")&#xa;    print ""Saving preprocessor to"", model_file + "".pre""&#xa;    # model.save_weights(model_file)&#xa;    modelfile.write(pickle.dumps(preprocessor, protocol=2))&#xa;&#xa;""""""&#xa;def ClusterDoc2Vec(model_file, train_file, valid_file, ftype, nsamples, param):&#xa;&#xa;  train_programs, train_features, train_classes = read_traces(train_file, nsamples)&#xa;  train_size = len(train_programs)&#xa;&#xa;  print ""using"", train_size,""examples to train.""&#xa;&#xa;  from gensim.models.doc2vec import TaggedDocument&#xa;  from gensim.models import Doc2Vec&#xa;&#xa;  print ""Vectorizing traces..""&#xa;  sentences = []&#xa;&#xa;  for (prog,trace) in zip(train_programs,train_features):&#xa;     sentences.append(TaggedDocument(trace.split("" ""), [prog]))&#xa;&#xa;  model = Doc2Vec(dm=2, min_count=1, window=5, size=100, sample=1e-4, negative=5, workers=8, iter=1)&#xa;  model.build_vocab(sentences)&#xa;&#xa;  for epoch in range(20):&#xa;    #print model&#xa;    model.train(sentences)&#xa;    shuffle(sentences)&#xa;&#xa;  train_dict = dict()&#xa;&#xa;  vec_train_features = []&#xa;  for prog in train_programs:&#xa;    #print prog, model.docvecs[prog]&#xa;    vec_train_features.append(model.docvecs[prog])&#xa;&#xa;  train_dict[ftype] = vec_train_features&#xa;&#xa;  print ""Transforming data and fitting model..""&#xa;  model = make_cluster_pipeline_doc2vec(ftype)&#xa;  X_red = model.fit_transform(train_dict)&#xa;&#xa;  #mpl.rcParams.update({'font.size': 10})&#xa;  plt.figure()&#xa;  colors = 'brgcmykbgrcmykbgrcmykbgrcmyk'&#xa;  ncolors = len(colors)&#xa;&#xa;  for prog,[x,y],cl in zip(train_programs, X_red, train_classes):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    try:&#xa;        plt.scatter(x, y, c=colors[int(cl)])&#xa;        plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;    except ValueError:&#xa;        plt.text(x, y+0.02, cl)&#xa;&#xa;  #plt.show()&#xa;  plt.savefig(train_file.replace("".gz"","""")+"".png"")&#xa;&#xa;  from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;  bandwidth = estimate_bandwidth(X_red, quantile=0.2)&#xa;  print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;  af = MeanShift(bandwidth=bandwidth*param).fit(X_red)&#xa;&#xa;  cluster_centers = af.cluster_centers_&#xa;  labels = af.labels_&#xa;  n_clusters_ = len(cluster_centers)&#xa;&#xa;  plt.close('all')&#xa;  plt.figure(1)&#xa;  plt.clf()&#xa;&#xa;  for ([x,y],label, cluster_label) in zip(X_red,train_programs, labels):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    plt.scatter(x, y, c = colors[cluster_label % ncolors])&#xa;&#xa;  for i,[x,y] in enumerate(cluster_centers):&#xa;    plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;             markeredgecolor='k', markersize=7)&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters_)&#xa;  plt.savefig(train_file.replace("".gz"","""")+"".clusters.png"")&#xa;&#xa;  #plt.show()&#xa;&#xa;  clustered_traces = zip(train_programs, labels)&#xa;  writer = write_csv(train_file.replace("".gz"","""")+"".clusters"")&#xa;  for label, cluster in clustered_traces:&#xa;     writer.writerow([label.split(""/"")[-1], cluster])&#xa;&#xa;""""""&#xa;&#xa;&#xa;def ClusterScikit(&#xa;        model_file,&#xa;        train_file,&#xa;        valid_file,&#xa;        ftype,&#xa;        nsamples,&#xa;        vectorizer,&#xa;        reducer,&#xa;        param):&#xa;&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples)&#xa;    train_size = len(train_programs)&#xa;    print ""using"", train_size, ""examples to train.""&#xa;&#xa;    if vectorizer == ""bow"":&#xa;&#xa;        train_dict = dict()&#xa;        train_dict[ftype] = train_features&#xa;        #batch_size = 16&#xa;        #window_size = 20&#xa;&#xa;        print ""Transforming data and fitting model..""&#xa;        model = make_cluster_pipeline_bow(ftype, reducer)&#xa;        X_red = model.fit_transform(train_dict)&#xa;&#xa;    elif vectorizer == ""doc2vec"":&#xa;&#xa;        from gensim.models.doc2vec import TaggedDocument&#xa;        from gensim.models import Doc2Vec&#xa;&#xa;        print ""Vectorizing traces..""&#xa;        sentences = []&#xa;&#xa;        for (prog, trace) in zip(train_programs, train_features):&#xa;            sentences.append(TaggedDocument(trace.split("" ""), [prog]))&#xa;&#xa;        model = Doc2Vec(dm=2, min_count=1, window=5, size=100,&#xa;                        sample=1e-4, negative=5, workers=8, iter=1)&#xa;        model.build_vocab(sentences)&#xa;&#xa;        for epoch in range(20):&#xa;            # print model&#xa;            model.train(sentences)&#xa;            shuffle(sentences)&#xa;&#xa;        train_dict = dict()&#xa;&#xa;        vec_train_features = []&#xa;        for prog in train_programs:&#xa;            # print prog, model.docvecs[prog]&#xa;            vec_train_features.append(model.docvecs[prog])&#xa;&#xa;        train_dict[ftype] = vec_train_features&#xa;&#xa;        print ""Transforming data and fitting model..""&#xa;        model = make_cluster_pipeline_doc2vec(ftype, reducer)&#xa;        X_red = model.fit_transform(train_dict)&#xa;&#xa;    #pl.rcParams.update({'font.size': 10})&#xa;    if isinstance(X_red, list):&#xa;        X_red = np.vstack(X_red)&#xa;        print X_red.shape&#xa;&#xa;    if X_red.shape[1] == 2:&#xa;&#xa;        plt.figure()&#xa;        colors = 'brgcmykbgrcmykbgrcmykbgrcmyk'&#xa;        ncolors = len(colors)&#xa;&#xa;        for prog, [x, y], cl in zip(train_programs, X_red, train_classes):&#xa;            x = gauss(0, 0.1) + x&#xa;            y = gauss(0, 0.1) + y&#xa;            try:&#xa;                plt.scatter(x, y, c=colors[int(cl)])&#xa;                plt.text(x, y + 0.02, prog.split(""/"")[-1])&#xa;            except ValueError:&#xa;                plt.text(x, y + 0.02, cl)&#xa;&#xa;        if valid_file is not None:&#xa;            valid_programs, valid_features, valid_classes = read_traces(&#xa;                valid_file, None)&#xa;            valid_dict = dict()&#xa;            valid_dict[ftype] = valid_features&#xa;&#xa;            X_red = model.transform(valid_dict)&#xa;            for prog, [x, y], cl in zip(valid_programs, X_red, valid_classes):&#xa;                x = gauss(0, 0.1) + x&#xa;                y = gauss(0, 0.1) + y&#xa;                plt.scatter(x, y, c=colors[cl + 1])&#xa;                plt.text(x, y + 0.02, prog.split(""/"")[-1])&#xa;&#xa;        # plt.show()&#xa;        plt.savefig(train_file.replace("".gz"", """") + "".png"")&#xa;&#xa;    from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;    bandwidth = estimate_bandwidth(X_red, quantile=0.2)&#xa;    print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;    af = MeanShift(bandwidth=bandwidth * param).fit(X_red)&#xa;&#xa;    cluster_centers = af.cluster_centers_&#xa;    labels = af.labels_&#xa;    n_clusters_ = len(cluster_centers)&#xa;&#xa;    if X_red.shape[1] == 2:&#xa;&#xa;        plt.close('all')&#xa;        plt.figure(1)&#xa;        plt.clf()&#xa;&#xa;        for ([x, y], label, cluster_label) in zip(&#xa;                X_red, train_programs, labels):&#xa;            x = gauss(0, 0.1) + x&#xa;            y = gauss(0, 0.1) + y&#xa;            plt.scatter(x, y, c=colors[cluster_label % ncolors])&#xa;&#xa;        for i, [x, y] in enumerate(cluster_centers):&#xa;            plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;                     markeredgecolor='k', markersize=7)&#xa;&#xa;        plt.title('Estimated number of clusters: %d' % n_clusters_)&#xa;        plt.savefig(train_file.replace("".gz"", """") + "".clusters.png"")&#xa;&#xa;    # plt.show()&#xa;&#xa;    clustered_traces = zip(train_programs, labels)&#xa;    writer = write_csv(train_file.replace("".gz"", """") + "".clusters"")&#xa;    for label, cluster in clustered_traces:&#xa;        writer.writerow([label.split(""/"")[-1], cluster])&#xa;"
12322210|"import sys, getpass, os, psutil, time, requests, errno, threading, inspect&#xa;import h2o_args&#xa;import h2o_os_util, h2o_print as h2p&#xa;import h2o_nodes&#xa;from h2o_test import \&#xa;    tmp_dir, tmp_file, flatfile_pathname, spawn_cmd, find_file, verboseprint, \&#xa;    dump_json, log, log_rest, check_sandbox_for_errors&#xa;&#xa;# print ""h2o_objects""&#xa;&#xa;# used to drain stdout on the h2o objects below (before terminating a node)&#xa;def __drain(src, dst):&#xa;    for l in src:&#xa;        if type(dst) == type(0):&#xa;            # got this with random data to parse.. why? it shows up in our stdout?&#xa;            # UnicodeEncodeError: 'ascii' codec can't encode character u'\x86' in position 60:&#xa;            #  ordinal not in range(128)&#xa;            # could we be getting unicode object?&#xa;            try:&#xa;                os.write(dst, l)&#xa;            except:&#xa;                # os.write(dst,""kbn: non-ascii char in the next line?"")&#xa;                os.write(dst,l.encode('utf8'))&#xa;        else:&#xa;            # FIX! this case probably can have the same issue?&#xa;            dst.write(l)&#xa;            dst.flush()&#xa;    src.close()&#xa;    if type(dst) == type(0):&#xa;        os.close(dst)&#xa;&#xa;&#xa;def drain(src, dst):&#xa;    t = threading.Thread(target=__drain, args=(src, dst))&#xa;    t.daemon = True&#xa;    t.start()&#xa;&#xa;#*****************************************************************&#xa;class H2O(object):&#xa;    def __init__(self,&#xa;        use_this_ip_addr=None, port=54321, capture_output=True,&#xa;        force_ip=False, network=None,&#xa;        use_debugger=None, classpath=None,&#xa;        use_hdfs=False, use_maprfs=False,&#xa;        hdfs_version=None, hdfs_name_node=None, hdfs_config=None,&#xa;        aws_credentials=None,&#xa;        use_flatfile=False, java_heap_GB=None, java_heap_MB=None, java_extra_args=None,&#xa;        use_home_for_ice=False, node_id=None, username=None,&#xa;        random_udp_drop=False, force_tcp=False,&#xa;        redirect_import_folder_to_s3_path=None,&#xa;        redirect_import_folder_to_s3n_path=None,&#xa;        disable_h2o_log=False,&#xa;        enable_benchmark_log=False,&#xa;        h2o_remote_buckets_root=None,&#xa;        delete_keys_at_teardown=False,&#xa;        cloud_name=None,&#xa;        disable_assertions=None,&#xa;        sandbox_ignore_errors=False,&#xa;        ):&#xa;&#xa;        if use_hdfs:&#xa;            # see if we can touch a 0xdata machine&#xa;            try:&#xa;                # long timeout in ec2...bad&#xa;                a = requests.get('http://172.16.2.176:80', timeout=1)&#xa;                hdfs_0xdata_visible = True&#xa;            except:&#xa;                hdfs_0xdata_visible = False&#xa;&#xa;            # different defaults, depending on where we're running&#xa;            if hdfs_name_node is None:&#xa;                if hdfs_0xdata_visible:&#xa;                    hdfs_name_node = ""172.16.2.176""&#xa;                else: # ec2&#xa;                    hdfs_name_node = ""10.78.14.235:9000""&#xa;&#xa;            if hdfs_version is None:&#xa;                if hdfs_0xdata_visible:&#xa;                    hdfs_version = ""cdh4""&#xa;                else: # ec2&#xa;                    hdfs_version = ""0.20.2""&#xa;&#xa;        self.redirect_import_folder_to_s3_path = redirect_import_folder_to_s3_path&#xa;        self.redirect_import_folder_to_s3n_path = redirect_import_folder_to_s3n_path&#xa;&#xa;        self.aws_credentials = aws_credentials&#xa;        self.port = port&#xa;        # None is legal for self.h2o_addr.&#xa;        # means we won't give an ip to the jar when we start.&#xa;        # Or we can say use use_this_ip_addr=127.0.0.1, or the known address&#xa;        # if use_this_addr is None, use 127.0.0.1 for urls and json&#xa;        # Command line arg 'ip_from_cmd_line' dominates:&#xa;&#xa;        # ip_from_cmd_line and use_this_ip_addr shouldn't be used for mutli-node&#xa;        if h2o_args.ip_from_cmd_line:&#xa;            self.h2o_addr = h2o_args.ip_from_cmd_line&#xa;        else:&#xa;            self.h2o_addr = use_this_ip_addr&#xa;&#xa;        self.force_ip = force_ip or (self.h2o_addr!=None)&#xa;&#xa;        if self.h2o_addr:&#xa;            self.http_addr = self.h2o_addr&#xa;        else:&#xa;            self.http_addr = h2o_args.python_cmd_ip&#xa;&#xa;        if h2o_args.network_from_cmd_line:&#xa;            self.network = h2o_args.network_from_cmd_line&#xa;        else:&#xa;            self.network = network&#xa;        &#xa;        # command line should always dominate for enabling&#xa;        if h2o_args.debugger: use_debugger = True&#xa;        self.use_debugger = use_debugger&#xa;&#xa;        self.classpath = classpath&#xa;        self.capture_output = capture_output&#xa;&#xa;        self.use_hdfs = use_hdfs&#xa;        self.use_maprfs = use_maprfs&#xa;        self.hdfs_name_node = hdfs_name_node&#xa;        self.hdfs_version = hdfs_version&#xa;        self.hdfs_config = hdfs_config&#xa;&#xa;        self.use_flatfile = use_flatfile&#xa;        self.java_heap_GB = java_heap_GB&#xa;        self.java_heap_MB = java_heap_MB&#xa;        self.java_extra_args = java_extra_args&#xa;&#xa;        self.use_home_for_ice = use_home_for_ice&#xa;        self.node_id = node_id&#xa;&#xa;        if username:&#xa;            self.username = username&#xa;        else:&#xa;            self.username = getpass.getuser()&#xa;&#xa;        # don't want multiple reports from tearDown and tearDownClass&#xa;        # have nodes[0] remember (0 always exists)&#xa;        self.sandbox_error_was_reported = False&#xa;        self.sandbox_ignore_errors = sandbox_ignore_errors&#xa;&#xa;        self.random_udp_drop = random_udp_drop&#xa;        self.force_tcp = force_tcp&#xa;        self.disable_h2o_log = disable_h2o_log&#xa;&#xa;        # this dumps stats from tests, and perf stats while polling to benchmark.log&#xa;        self.enable_benchmark_log = enable_benchmark_log&#xa;        self.h2o_remote_buckets_root = h2o_remote_buckets_root&#xa;        self.delete_keys_at_teardown = delete_keys_at_teardown&#xa;        self.disable_assertions = disable_assertions&#xa;&#xa;        if cloud_name:&#xa;            self.cloud_name = cloud_name&#xa;        else:&#xa;            self.cloud_name = 'pytest-%s-%s' % (getpass.getuser(), os.getpid())&#xa;&#xa;    def __str__(self):&#xa;        return '%s - http://%s:%d/' % (type(self), self.http_addr, self.port)&#xa;&#xa;    def url(self, loc, port=None):&#xa;        # always use the new api port&#xa;        if port is None: port = self.port&#xa;        if loc.startswith('/'):&#xa;            delim = ''&#xa;        else:&#xa;            delim = '/'&#xa;        u = 'http://%s:%d%s%s' % (self.http_addr, port, delim, loc)&#xa;        return u&#xa;&#xa;&#xa;    def do_json_request(self, jsonRequest=None, fullUrl=None, timeout=10, params=None, returnFast=False,&#xa;        cmd='get', extraComment=None, ignoreH2oError=False, noExtraErrorCheck=False, **kwargs):&#xa;        # if url param is used, use it as full url. otherwise crate from the jsonRequest&#xa;        if fullUrl:&#xa;            url = fullUrl&#xa;        else:&#xa;            url = self.url(jsonRequest)&#xa;&#xa;        # remove any params that are 'None'&#xa;        # need to copy dictionary, since can't delete while iterating&#xa;        if params is not None:&#xa;            params2 = params.copy()&#xa;            for k in params2:&#xa;                if params2[k] is None:&#xa;                    del params[k]&#xa;            paramsStr = '?' + '&'.join(['%s=%s' % (k, v) for (k, v) in params.items()])&#xa;        else:&#xa;            paramsStr = ''&#xa;&#xa;        if extraComment:&#xa;            log('Start ' + url + paramsStr, comment=extraComment)&#xa;        else:&#xa;            log('Start ' + url + paramsStr)&#xa;&#xa;        log_rest("""")&#xa;        log_rest(""----------------------------------------------------------------------\n"")&#xa;        if extraComment:&#xa;            log_rest(""# Extra comment info about this request: "" + extraComment)&#xa;        if cmd == 'get':&#xa;            log_rest(""GET"")&#xa;        else:&#xa;            log_rest(""POST"")&#xa;        log_rest(url + paramsStr)&#xa;&#xa;        # file get passed thru kwargs here&#xa;        try:&#xa;            if cmd == 'post':&#xa;                r = requests.post(url, timeout=timeout, params=params, **kwargs)&#xa;            else:&#xa;                r = requests.get(url, timeout=timeout, params=params, **kwargs)&#xa;&#xa;        except Exception, e:&#xa;            # rethrow the exception after we've checked for stack trace from h2o&#xa;            # out of memory errors maybe don't show up right away? so we should wait for h2o&#xa;            # to get it out to h2o stdout. We don't want to rely on cloud teardown to check&#xa;            # because there's no delay, and we don't want to delay all cloud teardowns by waiting.&#xa;            exc_info = sys.exc_info()&#xa;            # use this to ignore the initial connection errors during build cloud when h2o is coming up&#xa;            if not noExtraErrorCheck: &#xa;                h2p.red_print(&#xa;                    ""ERROR: got exception on %s to h2o. \nGoing to check sandbox, then rethrow.."" % (url + paramsStr))&#xa;                time.sleep(2)&#xa;                check_sandbox_for_errors(python_test_name=h2o_args.python_test_name);&#xa;            log_rest("""")&#xa;            log_rest(""EXCEPTION CAUGHT DOING REQUEST: "" + str(e.message))&#xa;            raise exc_info[1], None, exc_info[2]&#xa;&#xa;        log_rest("""")&#xa;        try:&#xa;            if r is None:&#xa;                log_rest(""r is None"")&#xa;            else:&#xa;                log_rest(""HTTP status code: "" + str(r.status_code))&#xa;                if hasattr(r, 'text'):&#xa;                    if r.text is None:&#xa;                        log_rest(""r.text is None"")&#xa;                    else:&#xa;                        log_rest(r.text)&#xa;                else:&#xa;                    log_rest(""r does not have attr text"")&#xa;        except Exception, e:&#xa;            # Paranoid exception catch.  &#xa;            log('WARNING: ignoring unexpected exception on %s' + url + paramsStr)&#xa;            # Ignore logging exceptions in the case that the above error checking isn't sufficient.&#xa;            pass&#xa;&#xa;        # fatal if no response&#xa;        if not r:&#xa;            raise Exception(""Maybe bad url? no r in __do_json_request in %s:"" % inspect.stack()[1][3])&#xa;&#xa;        # this is used to open a browser on results, or to redo the operation in the browser&#xa;        # we don't' have that may urls flying around, so let's keep them all&#xa;        h2o_nodes.json_url_history.append(r.url)&#xa;        # if r.json():&#xa;        #     raise Exception(""Maybe bad url? no r.json in __do_json_request in %s:"" % inspect.stack()[1][3])&#xa;&#xa;        rjson = None&#xa;        if returnFast:&#xa;            return&#xa;        try:&#xa;            rjson = r.json()&#xa;        except:&#xa;            print dump_json(r.text)&#xa;            if not isinstance(r, (list, dict)):&#xa;                raise Exception(""h2o json responses should always be lists or dicts, see previous for text"")&#xa;&#xa;            raise Exception(""Could not decode any json from the request."")&#xa;&#xa;        # TODO: we should really only look in the response object.  This check&#xa;        # prevents us from having a field called ""error"" (e.g., for a scoring result).&#xa;        for e in ['error', 'Error', 'errors', 'Errors']:&#xa;            # error can be null (python None). This happens in exec2&#xa;            if e in rjson and rjson[e]:&#xa;                print ""rjson:"", dump_json(rjson)&#xa;                emsg = 'rjson %s in %s: %s' % (e, inspect.stack()[1][3], rjson[e])&#xa;                if ignoreH2oError:&#xa;                    # well, we print it..so not totally ignore. test can look at rjson returned&#xa;                    print emsg&#xa;                else:&#xa;                    print emsg&#xa;                    raise Exception(emsg)&#xa;&#xa;        for w in ['warning', 'Warning', 'warnings', 'Warnings']:&#xa;            # warning can be null (python None).&#xa;            if w in rjson and rjson[w]:&#xa;                verboseprint(dump_json(rjson))&#xa;                print 'rjson %s in %s: %s' % (w, inspect.stack()[1][3], rjson[w])&#xa;&#xa;        return rjson&#xa;&#xa;&#xa;&#xa;    def stabilize(self, test_func, error, timeoutSecs=10, retryDelaySecs=0.5):&#xa;        '''Repeatedly test a function waiting for it to return True.&#xa;&#xa;        Arguments:&#xa;        test_func      -- A function that will be run repeatedly&#xa;        error          -- A function that will be run to produce an error message&#xa;                          it will be called with (node, timeTakenSecs, numberOfRetries)&#xa;                    OR&#xa;                       -- A string that will be interpolated with a dictionary of&#xa;                          { 'timeTakenSecs', 'numberOfRetries' }&#xa;        timeoutSecs    -- How long in seconds to keep trying before declaring a failure&#xa;        retryDelaySecs -- How long to wait between retry attempts&#xa;        '''&#xa;        start = time.time()&#xa;        numberOfRetries = 0&#xa;        while time.time() - start < timeoutSecs:&#xa;            if test_func(self, tries=numberOfRetries, timeoutSecs=timeoutSecs):&#xa;                break&#xa;            time.sleep(retryDelaySecs)&#xa;            numberOfRetries += 1&#xa;            # hey, check the sandbox if we've been waiting a long time...rather than wait for timeout&#xa;            # to find the badness?. can check_sandbox_for_errors at any time&#xa;            if ((numberOfRetries % 50) == 0):&#xa;                check_sandbox_for_errors(python_test_name=h2o_args.python_test_name)&#xa;&#xa;        else:&#xa;            timeTakenSecs = time.time() - start&#xa;            if isinstance(error, type('')):&#xa;                raise Exception('%s failed after %.2f seconds having retried %d times' % (&#xa;                    error, timeTakenSecs, numberOfRetries))&#xa;            else:&#xa;                msg = error(self, timeTakenSecs, numberOfRetries)&#xa;                raise Exception(msg)&#xa;&#xa;    def wait_for_node_to_accept_connections(self, nodeList, timeoutSecs=15, noExtraErrorCheck=False):&#xa;        verboseprint(""wait_for_node_to_accept_connections"")&#xa;&#xa;        def test(n, tries=None, timeoutSecs=timeoutSecs):&#xa;            try:&#xa;                n.get_cloud(noExtraErrorCheck=noExtraErrorCheck, timeoutSecs=timeoutSecs)&#xa;                return True&#xa;            except requests.ConnectionError, e:&#xa;                # Now using: requests 1.1.0 (easy_install --upgrade requests) 2/5/13&#xa;                # Now: assume all requests.ConnectionErrors are H2O legal connection errors.&#xa;                # Have trouble finding where the errno is, fine to assume all are good ones.&#xa;                # Timeout check will kick in if continued H2O badness.&#xa;                return False&#xa;&#xa;        # get their http addr to represent the nodes&#xa;        expectedCloudStr = "","".join([str(n) for n in nodeList])&#xa;        self.stabilize(test, error=('waiting for initial connection: Expected cloud %s' % expectedCloudStr),&#xa;            timeoutSecs=timeoutSecs, # with cold cache's this can be quite slow&#xa;            retryDelaySecs=0.1) # but normally it is very fast&#xa;&#xa;    def sandbox_error_report(self, done=None):&#xa;        # not clearable..just or in new value&#xa;        if done:&#xa;            self.sandbox_error_was_reported = True&#xa;        return (self.sandbox_error_was_reported)&#xa;&#xa;    def get_args(self):&#xa;        args = ['java']&#xa;&#xa;        # I guess it doesn't matter if we use flatfile for both now&#xa;        # defaults to not specifying&#xa;        # FIX! we need to check that it's not outside the limits of the dram of the machine it's running on?&#xa;        if self.java_heap_GB is not None:&#xa;            if not (1 <= self.java_heap_GB <= 256):&#xa;                raise Exception('java_heap_GB <1 or >256  (GB): %s' % (self.java_heap_GB))&#xa;            args += ['-Xms%dG' % self.java_heap_GB]&#xa;            args += ['-Xmx%dG' % self.java_heap_GB]&#xa;&#xa;        if self.java_heap_MB is not None:&#xa;            if not (1 <= self.java_heap_MB <= 256000):&#xa;                raise Exception('java_heap_MB <1 or >256000  (MB): %s' % (self.java_heap_MB))&#xa;            args += ['-Xms%dm' % self.java_heap_MB]&#xa;            args += ['-Xmx%dm' % self.java_heap_MB]&#xa;&#xa;        if self.java_extra_args is not None:&#xa;            args += ['%s' % self.java_extra_args]&#xa;&#xa;        if self.use_debugger:&#xa;            # currently hardwire the base port for debugger to 8000&#xa;            # increment by one for every node we add&#xa;            # sence this order is different than h2o cluster order, print out the ip and port for the user&#xa;            # we could save debugger_port state per node, but not really necessary (but would be more consistent)&#xa;            debuggerBasePort = 8000&#xa;            if self.node_id is None:&#xa;                debuggerPort = debuggerBasePort&#xa;            else:&#xa;                debuggerPort = debuggerBasePort + self.node_id&#xa;&#xa;            if self.http_addr:&#xa;                a = self.http_addr&#xa;            else:&#xa;                a = ""localhost""&#xa;&#xa;            if self.port:&#xa;                b = str(self.port)&#xa;            else:&#xa;                b = ""h2o determined""&#xa;&#xa;            # I guess we always specify port?&#xa;            print ""You can attach debugger at port %s for jvm at %s:%s"" % (debuggerPort, a, b)&#xa;            args += ['-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=%s' % debuggerPort]&#xa;&#xa;        if self.disable_assertions:&#xa;            print ""WARNING: h2o is running with assertions disabled""&#xa;        else:&#xa;            args += [""-ea""]&#xa;            &#xa;&#xa;        if self.use_maprfs:&#xa;            args += [""-Djava.library.path=/opt/mapr/lib""]&#xa;&#xa;        if self.classpath:&#xa;            entries = [find_file('build/classes'), find_file('lib/javassist.jar')]&#xa;            entries += glob.glob(find_file('lib') + '/*/*.jar')&#xa;            entries += glob.glob(find_file('lib') + '/*/*/*.jar')&#xa;            args += ['-classpath', os.pathsep.join(entries), 'water.Boot']&#xa;        else:&#xa;            args += [""-jar"", self.get_h2o_jar()]&#xa;&#xa;        if 1==1:&#xa;            if self.hdfs_config:&#xa;                args += [&#xa;                    '-hdfs_config=' + self.hdfs_config&#xa;                ]&#xa;&#xa;        if h2o_args.beta_features:&#xa;            args += [""-beta""]&#xa;&#xa;        if self.network:&#xa;            args += [""-network="" + self.network]&#xa;&#xa;        # H2O should figure it out, if not specified&#xa;        # DON""T EVER USE on multi-machine...h2o should always get it right, to be able to run on hadoop &#xa;        # where it's not told&#xa;        # new 10/22/14. Allow forcing the ip when we do remote, for networks with bridges, where&#xa;        # h2o can't self identify (does -network work?)&#xa;        if self.force_ip and self.h2o_addr: # should always have an addr if force_ip...but..&#xa;            args += [&#xa;                '--ip=%s' % self.h2o_addr,&#xa;            ]&#xa;&#xa;        # Need to specify port, since there can be multiple ports for an ip in the flatfile&#xa;        if self.port is not None:&#xa;            args += [&#xa;                ""--port=%d"" % self.port,&#xa;            ]&#xa;&#xa;        if self.use_flatfile:&#xa;            args += [&#xa;                '--flatfile=' + self.flatfile,&#xa;            ]&#xa;&#xa;        args += [&#xa;            '--ice_root=%s' % self.get_ice_dir(),&#xa;            # if I have multiple jenkins projects doing different h2o clouds, I need&#xa;            # I need different ports and different cloud name.&#xa;            # does different cloud name prevent them from joining up&#xa;            # (even if same multicast ports?)&#xa;            # I suppose I can force a base address. or run on another machine?&#xa;        ]&#xa;        args += [&#xa;            '--name=' + self.cloud_name&#xa;        ]&#xa;&#xa;        # ignore the other -hdfs args if the config is used?&#xa;        if 1==0:&#xa;            if self.hdfs_config:&#xa;                args += [&#xa;                    '-hdfs_config=' + self.hdfs_config&#xa;                ]&#xa;&#xa;        if self.use_hdfs:&#xa;            args += [&#xa;                # it's fine if hdfs_name has a "":9000"" port or something too&#xa;                '-hdfs hdfs://' + self.hdfs_name_node,&#xa;                '-hdfs_version=' + self.hdfs_version,&#xa;            ]&#xa;&#xa;        if self.use_maprfs:&#xa;            args += [&#xa;                # 3 slashes?&#xa;                '-hdfs maprfs:///' + self.hdfs_name_node,&#xa;                '-hdfs_version=' + self.hdfs_version,&#xa;            ]&#xa;&#xa;        if self.aws_credentials:&#xa;            args += ['--aws_credentials=' + self.aws_credentials]&#xa;&#xa;        # passed thru build_cloud in test, or global from commandline arg&#xa;        if self.random_udp_drop or h2o_args.random_udp_drop:&#xa;            args += ['--random_udp_drop']&#xa;&#xa;        if self.force_tcp:&#xa;            args += ['--force_tcp']&#xa;&#xa;        if self.disable_h2o_log:&#xa;            args += ['--nolog']&#xa;&#xa;        # disable logging of requests, as some contain ""error"", which fails the test&#xa;        ## FIXED. better escape in check_sandbox_for_errors&#xa;        ## args += ['--no_requests_log']&#xa;        return args&#xa;&#xa;&#xa;#*****************************************************************&#xa;import h2o_methods&#xa;&#xa;class LocalH2O(H2O):&#xa;    '''An H2O instance launched by the python framework on the local host using psutil'''&#xa;&#xa;    def __init__(self, *args, **kwargs):&#xa;        super(LocalH2O, self).__init__(*args, **kwargs)&#xa;        self.rc = None&#xa;        # FIX! no option for local /home/username ..always the sandbox (LOG_DIR)&#xa;        self.ice = tmp_dir('ice.')&#xa;        self.flatfile = flatfile_pathname()&#xa;        # so we can tell if we're remote or local. Apparently used in h2o_import.py&#xa;        self.remoteH2O = False &#xa;&#xa;        h2o_os_util.check_port_group(self.port)&#xa;        h2o_os_util.show_h2o_processes()&#xa;&#xa;        if self.node_id is not None:&#xa;            logPrefix = 'local-h2o-' + str(self.node_id)&#xa;        else:&#xa;            logPrefix = 'local-h2o'&#xa;&#xa;        spawn = spawn_cmd(logPrefix, cmd=self.get_args(), capture_output=self.capture_output)&#xa;        self.ps = spawn[0]&#xa;&#xa;    def get_h2o_jar(self):&#xa;        return find_file('target/h2o.jar')&#xa;&#xa;    def get_flatfile(self):&#xa;        return self.flatfile&#xa;        # return find_file(flatfile_pathname())&#xa;&#xa;    def get_ice_dir(self):&#xa;        return self.ice&#xa;&#xa;    def is_alive(self):&#xa;        verboseprint(""Doing is_alive check for LocalH2O"", self.wait(0))&#xa;        return self.wait(0) is None&#xa;&#xa;    def terminate_self_only(self):&#xa;        def on_terminate(proc):&#xa;            print(""process {} terminated"".format(proc))&#xa;&#xa;        waitingForKill = False&#xa;        try:&#xa;            # we already sent h2o shutdown and waited a second. Don't bother checking if alive still.&#xa;            # send terminate...wait up to 3 secs, then send kill&#xa;            self.ps.terminate()&#xa;            gone, alive = wait_procs(procs=[self.ps], timeout=3, callback=on_terminate)&#xa;            if alive:&#xa;                self.ps.kill()&#xa;            # from http://code.google.com/p/psutil/wiki/Documentation: wait(timeout=None) Wait for process termination &#xa;            # If the process is already terminated does not raise NoSuchProcess exception but just return None immediately. &#xa;            # If timeout is specified and process is still alive raises TimeoutExpired exception. &#xa;            # hmm. maybe we're hitting the timeout&#xa;            waitingForKill = True&#xa;            return self.wait(timeout=3)&#xa;&#xa;        except psutil.NoSuchProcess:&#xa;            return -1&#xa;        except:&#xa;            if waitingForKill:&#xa;                # this means we must have got the exception on the self.wait()&#xa;                # just print a message&#xa;                print ""\nUsed psutil to kill h2o process...but""&#xa;                print ""It didn't die within 2 secs. Maybe will die soon. Maybe not! At: %s"" % self.http_addr&#xa;            else:&#xa;                print ""Unexpected exception in terminate_self_only: ignoring""&#xa;            # hack. &#xa;            # psutil 2.x needs function reference&#xa;            # psutil 1.x needs object reference&#xa;            if hasattr(self.ps.cmdline, '__call__'):&#xa;                pcmdline = self.ps.cmdline()&#xa;            else:&#xa;                pcmdline = self.ps.cmdline&#xa;            print ""process cmdline:"", pcmdline&#xa;            return -1&#xa;&#xa;    def terminate(self):&#xa;        # send a shutdown request first.&#xa;        # since local is used for a lot of buggy new code, also do the ps kill.&#xa;        # try/except inside shutdown_all now&#xa;        # new: moved this out..anyone using this should do h2o.nodes[0].shutdown_all first&#xa;        if 1==0:&#xa;            self.shutdown_all()&#xa;        if self.is_alive():&#xa;            print ""\nShutdown didn't work fast enough for local node? : %s. Will kill though"" % self&#xa;        self.terminate_self_only()&#xa;&#xa;    def wait(self, timeout=0):&#xa;        if self.rc is not None:&#xa;            return self.rc&#xa;        try:&#xa;            self.rc = self.ps.wait(timeout)&#xa;            return self.rc&#xa;        except psutil.TimeoutExpired:&#xa;            return None&#xa;&#xa;    def stack_dump(self):&#xa;        self.ps.send_signal(signal.SIGQUIT)&#xa;&#xa;    # to see all the methods&#xa;    # print dump_json(dir(LocalH2O))&#xa;&#xa;#*****************************************************************&#xa;class RemoteH2O(H2O):&#xa;    '''An H2O instance launched by the python framework on a specified host using openssh'''&#xa;&#xa;    def __init__(self, host, *args, **kwargs):&#xa;        super(RemoteH2O, self).__init__(*args, **kwargs)&#xa;&#xa;        # it gets set True if an address is specified for LocalH2o init. Override.&#xa;        if 'force_ip' in kwargs:&#xa;            self.force_ip = kwargs['force_ip']&#xa;&#xa;        self.remoteH2O = True # so we can tell if we're remote or local&#xa;        self.jar = host.upload_file('target/h2o.jar')&#xa;        # need to copy the flatfile. We don't always use it (depends on h2o args)&#xa;        self.flatfile = host.upload_file(flatfile_pathname())&#xa;        # distribute AWS credentials&#xa;        if self.aws_credentials:&#xa;            self.aws_credentials = host.upload_file(self.aws_credentials)&#xa;&#xa;        if self.hdfs_config:&#xa;            self.hdfs_config = host.upload_file(self.hdfs_config)&#xa;&#xa;        if self.use_home_for_ice:&#xa;            # this will be the username used to ssh to the host&#xa;            self.ice = ""/home/"" + host.username + '/ice.%d.%s' % (self.port, time.time())&#xa;        else:&#xa;            self.ice = '/tmp/ice.%d.%s' % (self.port, time.time())&#xa;&#xa;        self.channel = host.open_channel()&#xa;        ### FIX! TODO...we don't check on remote hosts yet&#xa;&#xa;        # this fires up h2o over there&#xa;        cmd = ' '.join(self.get_args())&#xa;        # UPDATE: somehow java -jar on cygwin target (xp) can't handle /tmp/h2o*jar&#xa;        # because it's a windows executable and expects windows style path names.&#xa;        # but if we cd into /tmp, it can do java -jar h2o*jar.&#xa;        # So just split out the /tmp (pretend we don't know) and the h2o jar file name&#xa;        # Newer windows may not have this problem? Do the ls (this goes into the local stdout&#xa;        # files) so we can see the file is really where we expect.&#xa;        # This hack only works when the dest is /tmp/h2o*jar. It's okay to execute&#xa;        # with pwd = /tmp. If /tmp/ isn't in the jar path, I guess things will be the same as&#xa;        # normal.&#xa;        if 1 == 0: # enable if you want windows remote machines&#xa;            cmdList = [""cd /tmp""] # separate by ;<space> when we join&#xa;            cmdList += [""ls -ltr "" + self.jar]&#xa;            cmdList += [re.sub(""/tmp/"", """", cmd)]&#xa;            self.channel.exec_command(""; "".join(cmdList))&#xa;        else:&#xa;            self.channel.exec_command(cmd)&#xa;&#xa;        if self.capture_output:&#xa;            if self.node_id is not None:&#xa;                logPrefix = 'remote-h2o-' + str(self.node_id)&#xa;            else:&#xa;                logPrefix = 'remote-h2o'&#xa;&#xa;            logPrefix += '-' + host.h2o_addr&#xa;&#xa;            outfd, outpath = tmp_file(logPrefix + '.stdout.', '.log')&#xa;            errfd, errpath = tmp_file(logPrefix + '.stderr.', '.log')&#xa;&#xa;            drain(self.channel.makefile(), outfd)&#xa;            drain(self.channel.makefile_stderr(), errfd)&#xa;            comment = 'Remote on %s, stdout %s, stderr %s' % (&#xa;                self.h2o_addr, os.path.basename(outpath), os.path.basename(errpath))&#xa;        else:&#xa;            drain(self.channel.makefile(), sys.stdout)&#xa;            drain(self.channel.makefile_stderr(), sys.stderr)&#xa;            comment = 'Remote on %s' % self.h2o_addr&#xa;&#xa;        log(cmd, comment=comment)&#xa;&#xa;    def get_h2o_jar(self):&#xa;        return self.jar&#xa;&#xa;    def get_flatfile(self):&#xa;        return self.flatfile&#xa;&#xa;    def get_ice_dir(self):&#xa;        return self.ice&#xa;&#xa;    def is_alive(self):&#xa;        verboseprint(""Doing is_alive check for RemoteH2O"")&#xa;        if self.channel.closed: return False&#xa;        if self.channel.exit_status_ready(): return False&#xa;        try:&#xa;            self.get_cloud(noExtraErrorCheck=True)&#xa;            return True&#xa;        except:&#xa;            return False&#xa;&#xa;    def terminate_self_only(self):&#xa;        self.channel.close()&#xa;&#xa;        # Don't check afterwards. api watchdog in h2o might complain&#xa;        if 1==0:&#xa;            time.sleep(1) # a little delay needed?&#xa;            # kbn: it should be dead now? want to make sure we don't have zombies&#xa;            # we should get a connection error. doing a is_alive subset.&#xa;            try:&#xa;                gc_output = self.get_cloud(noExtraErrorCheck=True)&#xa;                raise Exception(""get_cloud() should fail after we terminate a node. It isn't. %s %s"" % (self, gc_output))&#xa;            except:&#xa;                return True&#xa;&#xa;    def terminate(self):&#xa;        # new, moved this out. anyone using terminate should send h2o shutdown once before this&#xa;        if 1==0:&#xa;            self.shutdown_all()&#xa;        self.terminate_self_only()&#xa;&#xa;#*****************************************************************&#xa;class ExternalH2O(H2O):&#xa;    '''A cloned H2O instance assumed to be created by others, that we can interact with via json requests (urls)&#xa;       Gets initialized with state from json created by another build_cloud, so all methods should work 'as-if""&#xa;       the cloud was built by the test (normally).&#xa;       The normal build_cloud() parameters aren't passed here, the final node state is! (and used for init)&#xa;       The list should be complete, as long as created by build_cloud(create_json=True) or&#xa;       build_cloud_with_hosts(create_json=True)&#xa;       Obviously, no psutil or paramiko work done here.&#xa;    '''&#xa;&#xa;    def __init__(self, nodeState):&#xa;        for k, v in nodeState.iteritems():&#xa;            verboseprint(""init:"", k, v)&#xa;            # hack because it looks like the json is currently created with ""None"" for values of None&#xa;            # rather than worrying about that, just translate ""None"" to None here. ""None"" shouldn't exist&#xa;            # for any other reason.&#xa;            if v == ""None"":&#xa;                v = None&#xa;            elif v == ""false"":&#xa;                v = False&#xa;            elif v == ""true"":&#xa;                v = True&#xa;                # leave ""null"" as-is (string) for now?&#xa;&#xa;            setattr(self, k, v) # achieves self.k = v&#xa;            ## print ""Cloned"", len(nodeState), ""things for a h2o node""&#xa;&#xa;    def is_alive(self):&#xa;        verboseprint(""Doing is_alive check for ExternalH2O"")&#xa;        try:&#xa;            self.get_cloud()&#xa;            return True&#xa;        except:&#xa;            return False&#xa;&#xa;    # no terminate_self_only method&#xa;    def terminate_self_only(self):&#xa;        raise Exception(""terminate_self_only() not supported for ExternalH2O"")&#xa;&#xa;    def terminate(self):&#xa;        self.shutdown_all()&#xa;&#xa;&#xa;#*****************************************************************&#xa;class RemoteHost(object):&#xa;    def upload_file(self, f, progress=None):&#xa;        # FIX! we won't find it here if it's hdfs://172.16.2.151/ file&#xa;        f = find_file(f)&#xa;        if f not in self.uploaded:&#xa;            start = time.time()&#xa;            import md5&#xa;&#xa;            m = md5.new()&#xa;            m.update(open(f).read())&#xa;            m.update(getpass.getuser())&#xa;            dest = '/tmp/' + m.hexdigest() + ""-"" + os.path.basename(f)&#xa;&#xa;            # sigh. we rm/create sandbox in build_cloud now&#xa;            # (because nosetests doesn't exec h2o_main and we&#xa;            # don't want to code ""clean_sandbox()"" in all the tests.&#xa;            # So: we don't have a sandbox here, or if we do, we're going to delete it.&#xa;            # Just don't log anything until build_cloud()? that should be okay?&#xa;            # we were just logging this upload message..not needed.&#xa;            # log('Uploading to %s: %s -> %s' % (self.http_addr, f, dest))&#xa;            sftp = self.ssh.open_sftp()&#xa;            # check if file exists on remote side&#xa;            # does paramiko have issues with big files? (>1GB, or 650MB?). maybe we don't care.&#xa;            # This would arise (as mentioned in the source, line no 667, &#xa;            # http://www.lag.net/paramiko/docs/paramiko.sftp_client-pysrc.html) when there is &#xa;            # any error reading the packet or when there is EOFError&#xa;&#xa;            # but I'm getting sftp close here randomly at sm.&#xa;            # http://stackoverflow.com/questions/22708942/python-paramiko-module-error-with-callback&#xa;            # http://stackoverflow.com/questions/15010540/paramiko-sftp-server-connection-dropped&#xa;            # http://stackoverflow.com/questions/12322210/handling-paramiko-sshexception-server-connection-dropped&#xa;            try:&#xa;                # note we don't do a md5 compare. so if a corrupted file was uploaded we won't re-upload &#xa;                # until we do another build.&#xa;                sftp.stat(dest)&#xa;                print ""{0} Skipping upload of file {1}. File {2} exists on remote side!"".format(self, f, dest)&#xa;            except IOError, e:&#xa;                # if self.channel.closed or self.channel.exit_status_ready():&#xa;                #     raise Exception(""something bad happened to our %s being used for sftp. keepalive? %s %s"" % \&#xa;                #         (self, self.channel.closed, self.channel.exit_status_ready()))&#xa;&#xa;                if e.errno == errno.ENOENT: # no such file or directory&#xa;                    verboseprint(""{0} uploading file {1}"".format(self, f))&#xa;                    sftp.put(f, dest, callback=progress)&#xa;                    # if you want to track upload times&#xa;                    ### print ""\n{0:.3f} seconds"".format(time.time() - start)&#xa;                elif e.errno == errno.EEXIST: # File Exists&#xa;                    pass&#xa;                else:&#xa;                    print ""Got unexpected errno: %s on paramiko sftp."" % e.errno&#xa;                    print ""Lookup here: https://docs.python.org/2/library/errno.html""&#xa;                    # throw the exception again, if not what we expected&#xa;                    exc_info = sys.exc_info()&#xa;                    raise exc_info[1], None, exc_info[2]&#xa;            finally:&#xa;                sftp.close()&#xa;            self.uploaded[f] = dest&#xa;        sys.stdout.flush()&#xa;        return self.uploaded[f]&#xa;&#xa;    def record_file(self, f, dest):&#xa;        '''Record a file as having been uploaded by external means'''&#xa;        self.uploaded[f] = dest&#xa;&#xa;    def run_cmd(self, cmd):&#xa;        log('Running `%s` on %s' % (cmd, self))&#xa;        (stdin, stdout, stderr) = self.ssh.exec_command(cmd)&#xa;        stdin.close()&#xa;&#xa;        sys.stdout.write(stdout.read())&#xa;        sys.stdout.flush()&#xa;        stdout.close()&#xa;&#xa;        sys.stderr.write(stderr.read())&#xa;        sys.stderr.flush()&#xa;        stderr.close()&#xa;&#xa;    def push_file_to_remotes(self, f, hosts):&#xa;        dest = self.uploaded[f]&#xa;        for h in hosts:&#xa;            if h == self: continue&#xa;            self.run_cmd('scp %s %s@%s:%s' % (dest, h.username, h.h2o_addr, dest))&#xa;            h.record_file(f, dest)&#xa;&#xa;    def __init__(self, addr, username=None, password=None, **kwargs):&#xa;&#xa;        import paramiko&#xa;        # To debug paramiko you can use the following code:&#xa;        #paramiko.util.log_to_file('/tmp/paramiko.log')&#xa;        #paramiko.common.logging.basicConfig(level=paramiko.common.DEBUG)&#xa;&#xa;        # kbn. trying 9/23/13. Never specify -ip on java command line for multi-node&#xa;        # but self.h2o_addr is used elsewhere. so look at self.remoteH2O to disable in get_args()&#xa;&#xa;        # by definition, this must be the publicly visible addrs, otherwise we can't ssh or browse!&#xa;        self.h2o_addr = addr&#xa;        self.http_addr = addr&#xa;&#xa;        self.username = username # this works, but it's host state&#xa;        self.ssh = paramiko.SSHClient()&#xa;&#xa;        # don't require keys. If no password, assume passwordless setup was done&#xa;        policy = paramiko.AutoAddPolicy()&#xa;        self.ssh.set_missing_host_key_policy(policy)&#xa;        self.ssh.load_system_host_keys()&#xa;        if password is None:&#xa;            self.ssh.connect(self.h2o_addr, username=username, **kwargs)&#xa;        else:&#xa;            self.ssh.connect(self.h2o_addr, username=username, password=password, **kwargs)&#xa;&#xa;        # keep connection - send keepalive packet evety 5minutes&#xa;        self.ssh.get_transport().set_keepalive(300)&#xa;        self.uploaded = {}&#xa;&#xa;    def remote_h2o(self, *args, **kwargs):&#xa;        return RemoteH2O(self, self.h2o_addr, *args, **kwargs)&#xa;&#xa;    def open_channel(self):&#xa;        ch = self.ssh.get_transport().open_session()&#xa;        ch.get_pty() # force the process to die without the connection&#xa;        return ch&#xa;&#xa;    def __str__(self):&#xa;        return 'ssh://%s@%s' % (self.username, self.h2o_addr)&#xa;&#xa;&#xa;"
963965|"# http://stackoverflow.com/questions/963965/how-is-this-strategy-pattern&#xa;# -written-in-python-the-sample-in-wikipedia&#xa;""""""&#xa;In most of other languages Strategy pattern is implemented via creating some&#xa;base strategy interface/abstract class and subclassing it with a number of&#xa;concrete strategies (as we can see at&#xa;http://en.wikipedia.org/wiki/Strategy_pattern), however Python supports&#xa;higher-order functions and allows us to have only one class and inject&#xa;functions into it's instances, as shown in this example.&#xa;""""""&#xa;import types&#xa;&#xa;&#xa;class StrategyExample:&#xa;    def __init__(self, func=None):&#xa;        self.name = 'Strategy Example 0'&#xa;        if func is not None:&#xa;            self.execute = types.MethodType(func, self)&#xa;&#xa;    def execute(self):&#xa;        print(self.name)&#xa;&#xa;&#xa;def execute_replacement1(self):&#xa;    print(self.name + ' from execute 1')&#xa;&#xa;&#xa;def execute_replacement2(self):&#xa;    print(self.name + ' from execute 2')&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    strat0 = StrategyExample()&#xa;&#xa;    strat1 = StrategyExample(execute_replacement1)&#xa;    strat1.name = 'Strategy Example 1'&#xa;&#xa;    strat2 = StrategyExample(execute_replacement2)&#xa;    strat2.name = 'Strategy Example 2'&#xa;&#xa;    strat0.execute()&#xa;    strat1.execute()&#xa;    strat2.execute()&#xa;"
24717027|"""""""&#xa;pytest local configuration plug-in&#xa;""""""&#xa;&#xa;import gc&#xa;import warnings&#xa;&#xa;import pytest&#xa;&#xa;@pytest.yield_fixture(scope='function', autouse=True)&#xa;def error_on_ResourceWarning():&#xa;    """"""This fixture captures ResourceWarning's and reports an ""error""&#xa;    describing the file handles left open.&#xa;    &#xa;    This is shown regardless of how successful the test was, if a test fails&#xa;    and leaves files open then those files will be reported.  Ideally, even&#xa;    those files should be closed properly after a test failure or exception.&#xa;&#xa;    Since only Python 3 and PyPy3 have ResourceWarning's, this context will&#xa;    have no effect when running tests on Python 2 or PyPy.&#xa;&#xa;    Because of autouse=True, this function will be automatically enabled for&#xa;    all test_* functions in this module.&#xa;&#xa;    This code is primarily based on the examples found here:&#xa;    https://stackoverflow.com/questions/24717027/convert-python-3-resourcewarnings-into-exception&#xa;    """"""&#xa;    try:&#xa;        ResourceWarning&#xa;    except NameError:&#xa;        # Python 2, PyPy&#xa;        yield&#xa;        return&#xa;    # Python 3, PyPy3&#xa;    with warnings.catch_warnings(record=True) as caught:&#xa;        warnings.resetwarnings() # clear all filters&#xa;        warnings.simplefilter('ignore') # ignore all&#xa;        warnings.simplefilter('always', ResourceWarning) # add filter&#xa;        yield # run tests in this context&#xa;        gc.collect() # run garbage collection (for pypy3)&#xa;        if not caught:&#xa;            return&#xa;        pytest.fail('The following file descriptors were not closed properly:\n' +&#xa;                    '\n'.join((str(warning.message) for warning in caught)),&#xa;                    pytrace=False)&#xa;"
21083571|# vim:fileencoding=utf-8:noet&#xa;from __future__ import (unicode_literals, division, absolute_import, print_function)&#xa;&#xa;import os&#xa;import sys&#xa;import re&#xa;&#xa;from powerline.lib.shell import run_cmd&#xa;&#xa;&#xa;def _fetch_battery_info(pl):&#xa;	try:&#xa;		import dbus&#xa;	except ImportError:&#xa;		pl.debug('Not using DBUS+UPower as dbus is not available')&#xa;	else:&#xa;		try:&#xa;			bus = dbus.SystemBus()&#xa;		except Exception as e:&#xa;			pl.exception('Failed to connect to system bus: {0}', str(e))&#xa;		else:&#xa;			interface = 'org.freedesktop.UPower'&#xa;			try:&#xa;				up = bus.get_object(interface, '/org/freedesktop/UPower')&#xa;			except dbus.exceptions.DBusException as e:&#xa;				if getattr(e, '_dbus_error_name', '').endswith('ServiceUnknown'):&#xa;					pl.debug('Not using DBUS+UPower as UPower is not available via dbus')&#xa;				else:&#xa;					pl.exception('Failed to get UPower service with dbus: {0}', str(e))&#xa;			else:&#xa;				devinterface = 'org.freedesktop.DBus.Properties'&#xa;				devtype_name = interface + '.Device'&#xa;				for devpath in up.EnumerateDevices(dbus_interface=interface):&#xa;					dev = bus.get_object(interface, devpath)&#xa;					devget = lambda what: dev.Get(&#xa;						devtype_name,&#xa;						what,&#xa;						dbus_interface=devinterface&#xa;					)&#xa;					if int(devget('Type')) != 2:&#xa;						pl.debug('Not using DBUS+UPower with {0}: invalid type', devpath)&#xa;						continue&#xa;					if not bool(devget('IsPresent')):&#xa;						pl.debug('Not using DBUS+UPower with {0}: not present', devpath)&#xa;						continue&#xa;					if not bool(devget('PowerSupply')):&#xa;						pl.debug('Not using DBUS+UPower with {0}: not a power supply', devpath)&#xa;						continue&#xa;					pl.debug('Using DBUS+UPower with {0}', devpath)&#xa;					return lambda pl: (&#xa;						float(&#xa;							dbus.Interface(dev, dbus_interface=devinterface).Get(&#xa;								devtype_name,&#xa;								'Percentage'&#xa;							),&#xa;						),&#xa;						dbus.Interface(dev, dbus_interface=devinterface).Get(&#xa;							devtype_name,&#xa;							'State'&#xa;						) == 1&#xa;					)&#xa;				pl.debug('Not using DBUS+UPower as no batteries were found')&#xa;&#xa;	if os.path.isdir('/sys/class/power_supply'):&#xa;		linux_bat_fmt = '/sys/class/power_supply/{0}/capacity'&#xa;		linux_ac_fmt = '/sys/class/power_supply/{0}/online'&#xa;		for linux_bat in os.listdir('/sys/class/power_supply'):&#xa;			cap_path = linux_bat_fmt.format(linux_bat)&#xa;			online_path = linux_ac_fmt.format(linux_bat)&#xa;			if linux_bat.startswith('BAT') and os.path.exists(cap_path):&#xa;				pl.debug('Using /sys/class/power_supply with battery {0}', linux_bat)&#xa;&#xa;				def _get_battery_status(pl):&#xa;					with open(cap_path, 'r') as f:&#xa;						_capacity = int(float(f.readline().split()[0]))&#xa;					with open(online_path, 'r') as f:&#xa;						_ac_powered = f.readline() == 1&#xa;					return _capacity, _ac_powered&#xa;				return _get_battery_status&#xa;			pl.debug('Not using /sys/class/power_supply as no batteries were found')&#xa;	else:&#xa;		pl.debug('Not using /sys/class/power_supply: no directory')&#xa;&#xa;	try:&#xa;		from shutil import which  # Python-3.3 and later&#xa;	except ImportError:&#xa;		pl.info('Using dumb “which” which only checks for file in /usr/bin')&#xa;		which = lambda f: (lambda fp: os.path.exists(fp) and fp)(os.path.join('/usr/bin', f))&#xa;&#xa;	if which('pmset'):&#xa;		pl.debug('Using pmset')&#xa;&#xa;		BATTERY_PERCENT_RE = re.compile(r'(\d+)%')&#xa;&#xa;		def _get_battery_status(pl):&#xa;			battery_summary = run_cmd(pl, ['pmset', '-g', 'batt'])&#xa;			battery_percent = BATTERY_PERCENT_RE.search(battery_summary).group(1)&#xa;			ac_charging = 'AC' in battery_summary&#xa;			return int(battery_percent), ac_charging&#xa;		return _get_battery_status&#xa;	else:&#xa;		pl.debug('Not using pmset: executable not found')&#xa;&#xa;	if sys.platform.startswith('win') or sys.platform == 'cygwin':&#xa;		# From http://stackoverflow.com/a/21083571/273566, reworked&#xa;		try:&#xa;			from win32com.client import GetObject&#xa;		except ImportError:&#xa;			pl.debug('Not using win32com.client as it is not available')&#xa;		else:&#xa;			try:&#xa;				wmi = GetObject('winmgmts:')&#xa;			except Exception as e:&#xa;				pl.exception('Failed to run GetObject from win32com.client: {0}', str(e))&#xa;			else:&#xa;				for battery in wmi.InstancesOf('Win32_Battery'):&#xa;					pl.debug('Using win32com.client with Win32_Battery')&#xa;&#xa;					def _get_battery_status(pl):&#xa;						# http://msdn.microsoft.com/en-us/library/aa394074(v=vs.85).aspx&#xa;						return battery.EstimatedChargeRemaining, battery.BatteryStatus == 6&#xa;&#xa;					return _get_battery_status&#xa;				pl.debug('Not using win32com.client as no batteries were found')&#xa;		from ctypes import Structure, c_byte, c_ulong, byref&#xa;		if sys.platform == 'cygwin':&#xa;			pl.debug('Using cdll to communicate with kernel32 (Cygwin)')&#xa;			from ctypes import cdll&#xa;			library_loader = cdll&#xa;		else:&#xa;			pl.debug('Using windll to communicate with kernel32 (Windows)')&#xa;			from ctypes import windll&#xa;			library_loader = windll&#xa;&#xa;		class PowerClass(Structure):&#xa;			_fields_ = [&#xa;				('ACLineStatus', c_byte),&#xa;				('BatteryFlag', c_byte),&#xa;				('BatteryLifePercent', c_byte),&#xa;				('Reserved1', c_byte),&#xa;				('BatteryLifeTime', c_ulong),&#xa;				('BatteryFullLifeTime', c_ulong)&#xa;			]&#xa;&#xa;		def _get_battery_status(pl):&#xa;			powerclass = PowerClass()&#xa;			result = library_loader.kernel32.GetSystemPowerStatus(byref(powerclass))&#xa;			# http://msdn.microsoft.com/en-us/library/windows/desktop/aa372693(v=vs.85).aspx&#xa;			if result:&#xa;				return None&#xa;			return powerclass.BatteryLifePercent, powerclass.ACLineStatus == 1&#xa;&#xa;		if _get_battery_status() is None:&#xa;			pl.debug('Not using GetSystemPowerStatus because it failed')&#xa;		else:&#xa;			pl.debug('Using GetSystemPowerStatus')&#xa;&#xa;		return _get_battery_status&#xa;&#xa;	raise NotImplementedError&#xa;&#xa;&#xa;def _get_battery_status(pl):&#xa;	global _get_battery_status&#xa;&#xa;	def _failing_get_status(pl):&#xa;		raise NotImplementedError&#xa;&#xa;	try:&#xa;		_get_battery_status = _fetch_battery_info(pl)&#xa;	except NotImplementedError:&#xa;		_get_battery_status = _failing_get_status&#xa;	except Exception as e:&#xa;		pl.exception('Exception while obtaining battery status: {0}', str(e))&#xa;		_get_battery_status = _failing_get_status&#xa;	return _get_battery_status(pl)&#xa;&#xa;&#xa;def battery(pl, format='{ac_state} {capacity:3.0%}', steps=5, gamify=False, full_heart='O', empty_heart='O', online='C', offline=' '):&#xa;	'''Return battery charge status.&#xa;&#xa;	:param str format:&#xa;		Percent format in case gamify is False. Format arguments: ``ac_state`` &#xa;		which is equal to either ``online`` or ``offline`` string arguments and &#xa;		``capacity`` which is equal to current battery capacity in interval [0, &#xa;		100].&#xa;	:param int steps:&#xa;		Number of discrete steps to show between 0% and 100% capacity if gamify&#xa;		is True.&#xa;	:param bool gamify:&#xa;		Measure in hearts (♥) instead of percentages. For full hearts &#xa;		``battery_full`` highlighting group is preferred, for empty hearts there &#xa;		is ``battery_empty``. ``battery_online`` or ``battery_offline`` group &#xa;		will be used for leading segment containing ``online`` or ``offline`` &#xa;		argument contents.&#xa;	:param str full_heart:&#xa;		Heart displayed for “full” part of battery.&#xa;	:param str empty_heart:&#xa;		Heart displayed for “used” part of battery. It is also displayed using&#xa;		another gradient level and highlighting group, so it is OK for it to be &#xa;		the same as full_heart as long as necessary highlighting groups are &#xa;		defined.&#xa;	:param str online:&#xa;		Symbol used if computer is connected to a power supply.&#xa;	:param str offline:&#xa;		Symbol used if computer is not connected to a power supply.&#xa;&#xa;	``battery_gradient`` and ``battery`` groups are used in any case, first is &#xa;	preferred.&#xa;&#xa;	Highlight groups used: ``battery_full`` or ``battery_gradient`` (gradient) or ``battery``, ``battery_empty`` or ``battery_gradient`` (gradient) or ``battery``, ``battery_online`` or ``battery_ac_state`` or ``battery_gradient`` (gradient) or ``battery``, ``battery_offline`` or ``battery_ac_state`` or ``battery_gradient`` (gradient) or ``battery``.&#xa;	'''&#xa;	try:&#xa;		capacity, ac_powered = _get_battery_status(pl)&#xa;	except NotImplementedError:&#xa;		pl.info('Unable to get battery status.')&#xa;		return None&#xa;&#xa;	ret = []&#xa;	if gamify:&#xa;		denom = int(steps)&#xa;		numer = int(denom * capacity / 100)&#xa;		ret.append({&#xa;			'contents': online if ac_powered else offline,&#xa;			'draw_inner_divider': False,&#xa;			'highlight_groups': ['battery_online' if ac_powered else 'battery_offline', 'battery_ac_state', 'battery_gradient', 'battery'],&#xa;			'gradient_level': 0,&#xa;		})&#xa;		ret.append({&#xa;			'contents': full_heart * numer,&#xa;			'draw_inner_divider': False,&#xa;			'highlight_groups': ['battery_full', 'battery_gradient', 'battery'],&#xa;			# Using zero as “nothing to worry about”: it is least alert color.&#xa;			'gradient_level': 0,&#xa;		})&#xa;		ret.append({&#xa;			'contents': empty_heart * (denom - numer),&#xa;			'draw_inner_divider': False,&#xa;			'highlight_groups': ['battery_empty', 'battery_gradient', 'battery'],&#xa;			# Using a hundred as it is most alert color.&#xa;			'gradient_level': 100,&#xa;		})&#xa;	else:&#xa;		ret.append({&#xa;			'contents': format.format(ac_state=(online if ac_powered else offline), capacity=(capacity / 100.0)),&#xa;			'highlight_groups': ['battery_gradient', 'battery'],&#xa;			# Gradients are “least alert – most alert” by default, capacity has &#xa;			# the opposite semantics.&#xa;			'gradient_level': 100 - capacity,&#xa;		})&#xa;	return ret&#xa;
1838699|"""""""Miscellaneous utility functions.""""""&#xa;&#xa;from __future__ import absolute_import, division, with_statement&#xa;&#xa;import zlib&#xa;&#xa;&#xa;class ObjectDict(dict):&#xa;    """"""Makes a dictionary behave like an object.""""""&#xa;    def __getattr__(self, name):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            raise AttributeError(name)&#xa;&#xa;    def __setattr__(self, name, value):&#xa;        self[name] = value&#xa;&#xa;&#xa;class GzipDecompressor(object):&#xa;    """"""Streaming gzip decompressor.&#xa;&#xa;    The interface is like that of `zlib.decompressobj` (without the&#xa;    optional arguments, but it understands gzip headers and checksums.&#xa;    """"""&#xa;    def __init__(self):&#xa;        # Magic parameter makes zlib module understand gzip header&#xa;        # http://stackoverflow.com/questions/1838699/how-can-i-decompress-a-gzip-stream-with-zlib&#xa;        # This works on cpython and pypy, but not jython.&#xa;        self.decompressobj = zlib.decompressobj(16 + zlib.MAX_WBITS)&#xa;&#xa;    def decompress(self, value):&#xa;        """"""Decompress a chunk, returning newly-available data.&#xa;&#xa;        Some data may be buffered for later processing; `flush` must&#xa;        be called when there is no more input data to ensure that&#xa;        all data was processed.&#xa;        """"""&#xa;        return self.decompressobj.decompress(value)&#xa;&#xa;    def flush(self):&#xa;        """"""Return any remaining buffered data not yet returned by decompress.&#xa;&#xa;        Also checks for errors such as truncated input.&#xa;        No other methods may be called on this object after `flush`.&#xa;        """"""&#xa;        return self.decompressobj.flush()&#xa;&#xa;&#xa;def import_object(name):&#xa;    """"""Imports an object by name.&#xa;&#xa;    import_object('x.y.z') is equivalent to 'from x.y import z'.&#xa;&#xa;    >>> import tornado.escape&#xa;    >>> import_object('tornado.escape') is tornado.escape&#xa;    True&#xa;    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8&#xa;    True&#xa;    """"""&#xa;    parts = name.split('.')&#xa;    obj = __import__('.'.join(parts[:-1]), None, None, [parts[-1]], 0)&#xa;    return getattr(obj, parts[-1])&#xa;&#xa;# Fake byte literal support:  In python 2.6+, you can say b""foo"" to get&#xa;# a byte literal (str in 2.x, bytes in 3.x).  There's no way to do this&#xa;# in a way that supports 2.5, though, so we need a function wrapper&#xa;# to convert our string literals.  b() should only be applied to literal&#xa;# latin1 strings.  Once we drop support for 2.5, we can remove this function&#xa;# and just use byte literals.&#xa;if str is unicode:&#xa;    def b(s):&#xa;        return s.encode('latin1')&#xa;    bytes_type = bytes&#xa;else:&#xa;    def b(s):&#xa;        return s&#xa;    bytes_type = str&#xa;&#xa;&#xa;def raise_exc_info(exc_info):&#xa;    """"""Re-raise an exception (with original traceback) from an exc_info tuple.&#xa;&#xa;    The argument is a ``(type, value, traceback)`` tuple as returned by&#xa;    `sys.exc_info`.&#xa;    """"""&#xa;    # 2to3 isn't smart enough to convert three-argument raise&#xa;    # statements correctly in some cases.&#xa;    if isinstance(exc_info[1], exc_info[0]):&#xa;        raise exc_info[1], None, exc_info[2]&#xa;        # After 2to3: raise exc_info[1].with_traceback(exc_info[2])&#xa;    else:&#xa;        # I think this branch is only taken for string exceptions,&#xa;        # which were removed in Python 2.6.&#xa;        raise exc_info[0], exc_info[1], exc_info[2]&#xa;        # After 2to3: raise exc_info[0](exc_info[1]).with_traceback(exc_info[2])&#xa;&#xa;&#xa;def doctests():&#xa;    import doctest&#xa;    return doctest.DocTestSuite()&#xa;"
16327037|"#&#xa;# Author:  Travis Oliphant, 2002&#xa;#&#xa;&#xa;from __future__ import division, print_function, absolute_import&#xa;&#xa;import warnings&#xa;&#xa;import numpy as np&#xa;import math&#xa;from scipy._lib.six import xrange&#xa;from numpy import (pi, asarray, floor, isscalar, iscomplex, real, imag, sqrt,&#xa;                   where, mgrid, sin, place, issubdtype, extract,&#xa;                   less, inexact, nan, zeros, atleast_1d, sinc)&#xa;from ._ufuncs import (ellipkm1, mathieu_a, mathieu_b, iv, jv, gamma,&#xa;                      psi, _zeta, hankel1, hankel2, yv, kv, _gammaln,&#xa;                      ndtri, errprint, poch, binom, hyp0f1)&#xa;from . import specfun&#xa;from . import orthogonal&#xa;from ._comb import _comb_int&#xa;&#xa;__all__ = ['agm', 'ai_zeros', 'assoc_laguerre', 'bei_zeros', 'beip_zeros',&#xa;           'ber_zeros', 'bernoulli', 'berp_zeros', 'bessel_diff_formula',&#xa;           'bi_zeros', 'clpmn', 'comb', 'digamma', 'diric', 'ellipk',&#xa;           'erf_zeros', 'erfcinv', 'erfinv', 'errprint', 'euler', 'factorial',&#xa;           'factorialk', 'factorial2', 'fresnel_zeros',&#xa;           'fresnelc_zeros', 'fresnels_zeros', 'gamma', 'gammaln', 'h1vp',&#xa;           'h2vp', 'hankel1', 'hankel2', 'hyp0f1', 'iv', 'ivp', 'jn_zeros',&#xa;           'jnjnp_zeros', 'jnp_zeros', 'jnyn_zeros', 'jv', 'jvp', 'kei_zeros',&#xa;           'keip_zeros', 'kelvin_zeros', 'ker_zeros', 'kerp_zeros', 'kv',&#xa;           'kvp', 'lmbda', 'lpmn', 'lpn', 'lqmn', 'lqn', 'mathieu_a',&#xa;           'mathieu_b', 'mathieu_even_coef', 'mathieu_odd_coef', 'ndtri',&#xa;           'obl_cv_seq', 'pbdn_seq', 'pbdv_seq', 'pbvv_seq', 'perm',&#xa;           'polygamma', 'pro_cv_seq', 'psi', 'riccati_jn', 'riccati_yn',&#xa;           'sinc', 'sph_in', 'sph_inkn',&#xa;           'sph_jn', 'sph_jnyn', 'sph_kn', 'sph_yn', 'y0_zeros', 'y1_zeros',&#xa;           'y1p_zeros', 'yn_zeros', 'ynp_zeros', 'yv', 'yvp', 'zeta',&#xa;           'SpecialFunctionWarning']&#xa;&#xa;&#xa;class SpecialFunctionWarning(Warning):&#xa;    """"""Warning that can be issued with ``errprint(True)``""""""&#xa;    pass&#xa;warnings.simplefilter(""always"", category=SpecialFunctionWarning)&#xa;&#xa;&#xa;def diric(x, n):&#xa;    """"""Periodic sinc function, also called the Dirichlet function.&#xa;&#xa;    The Dirichlet function is defined as::&#xa;&#xa;        diric(x) = sin(x * n/2) / (n * sin(x / 2)),&#xa;&#xa;    where `n` is a positive integer.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Input data&#xa;    n : int&#xa;        Integer defining the periodicity.&#xa;&#xa;    Returns&#xa;    -------&#xa;    diric : ndarray&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import special&#xa;    >>> import matplotlib.pyplot as plt&#xa;&#xa;    >>> x = np.linspace(-8*np.pi, 8*np.pi, num=201)&#xa;    >>> plt.figure(figsize=(8, 8));&#xa;    >>> for idx, n in enumerate([2, 3, 4, 9]):&#xa;    ...     plt.subplot(2, 2, idx+1)&#xa;    ...     plt.plot(x, special.diric(x, n))&#xa;    ...     plt.title('diric, n={}'.format(n))&#xa;    >>> plt.show()&#xa;&#xa;    The following example demonstrates that `diric` gives the magnitudes&#xa;    (modulo the sign and scaling) of the Fourier coefficients of a&#xa;    rectangular pulse.&#xa;&#xa;    Suppress output of values that are effectively 0:&#xa;&#xa;    >>> np.set_printoptions(suppress=True)&#xa;&#xa;    Create a signal `x` of length `m` with `k` ones:&#xa;&#xa;    >>> m = 8&#xa;    >>> k = 3&#xa;    >>> x = np.zeros(m)&#xa;    >>> x[:k] = 1&#xa;&#xa;    Use the FFT to compute the Fourier transform of `x`, and&#xa;    inspect the magnitudes of the coefficients:&#xa;&#xa;    >>> np.abs(np.fft.fft(x))&#xa;    array([ 3.        ,  2.41421356,  1.        ,  0.41421356,  1.        ,&#xa;            0.41421356,  1.        ,  2.41421356])&#xa;&#xa;    Now find the same values (up to sign) using `diric`.  We multiply&#xa;    by `k` to account for the different scaling conventions of&#xa;    `numpy.fft.fft` and `diric`:&#xa;&#xa;    >>> theta = np.linspace(0, 2*np.pi, m, endpoint=False)&#xa;    >>> k * special.diric(theta, k)&#xa;    array([ 3.        ,  2.41421356,  1.        , -0.41421356, -1.        ,&#xa;           -0.41421356,  1.        ,  2.41421356])&#xa;    """"""&#xa;    x, n = asarray(x), asarray(n)&#xa;    n = asarray(n + (x-x))&#xa;    x = asarray(x + (n-n))&#xa;    if issubdtype(x.dtype, inexact):&#xa;        ytype = x.dtype&#xa;    else:&#xa;        ytype = float&#xa;    y = zeros(x.shape, ytype)&#xa;&#xa;    # empirical minval for 32, 64 or 128 bit float computations&#xa;    # where sin(x/2) < minval, result is fixed at +1 or -1&#xa;    if np.finfo(ytype).eps < 1e-18:&#xa;        minval = 1e-11&#xa;    elif np.finfo(ytype).eps < 1e-15:&#xa;        minval = 1e-7&#xa;    else:&#xa;        minval = 1e-3&#xa;&#xa;    mask1 = (n <= 0) | (n != floor(n))&#xa;    place(y, mask1, nan)&#xa;&#xa;    x = x / 2&#xa;    denom = sin(x)&#xa;    mask2 = (1-mask1) & (abs(denom) < minval)&#xa;    xsub = extract(mask2, x)&#xa;    nsub = extract(mask2, n)&#xa;    zsub = xsub / pi&#xa;    place(y, mask2, pow(-1, np.round(zsub)*(nsub-1)))&#xa;&#xa;    mask = (1-mask1) & (1-mask2)&#xa;    xsub = extract(mask, x)&#xa;    nsub = extract(mask, n)&#xa;    dsub = extract(mask, denom)&#xa;    place(y, mask, sin(nsub*xsub)/(nsub*dsub))&#xa;    return y&#xa;&#xa;&#xa;def gammaln(x):&#xa;    """"""&#xa;    Logarithm of the absolute value of the Gamma function for real inputs.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array-like&#xa;        Values on the real line at which to compute ``gammaln``&#xa;&#xa;    Returns&#xa;    -------&#xa;    gammaln : ndarray&#xa;        Values of ``gammaln`` at x.&#xa;&#xa;    See Also&#xa;    --------&#xa;    gammasgn : sign of the gamma function&#xa;    loggamma : principal branch of the logarithm of the gamma function&#xa;&#xa;    Notes&#xa;    -----&#xa;    When used in conjunction with `gammasgn`, this function is useful&#xa;    for working in logspace on the real axis without having to deal with&#xa;    complex numbers, via the relation ``exp(gammaln(x)) = gammasgn(x)*gamma(x)``.&#xa;&#xa;    Note that `gammaln` currently accepts complex-valued inputs, but it is not&#xa;    the same function as for real-valued inputs, and the branch is not&#xa;    well-defined --- using `gammaln` with complex is deprecated and will be&#xa;    disallowed in future Scipy versions.&#xa;&#xa;    For complex-valued log-gamma, use `loggamma` instead of `gammaln`.&#xa;&#xa;    """"""&#xa;    if np.iscomplexobj(x):&#xa;        warnings.warn((""Use of gammaln for complex arguments is ""&#xa;                       ""deprecated as of scipy 0.18.0. Use ""&#xa;                       ""scipy.special.loggamma instead.""),&#xa;                      DeprecationWarning)&#xa;    return _gammaln(x)&#xa;&#xa;&#xa;def jnjnp_zeros(nt):&#xa;    """"""Compute zeros of integer-order Bessel functions Jn and Jn'.&#xa;&#xa;    Results are arranged in order of the magnitudes of the zeros.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number (<=1200) of zeros to compute&#xa;&#xa;    Returns&#xa;    -------&#xa;    zo[l-1] : ndarray&#xa;        Value of the lth zero of Jn(x) and Jn'(x). Of length `nt`.&#xa;    n[l-1] : ndarray&#xa;        Order of the Jn(x) or Jn'(x) associated with lth zero. Of length `nt`.&#xa;    m[l-1] : ndarray&#xa;        Serial number of the zeros of Jn(x) or Jn'(x) associated&#xa;        with lth zero. Of length `nt`.&#xa;    t[l-1] : ndarray&#xa;        0 if lth zero in zo is zero of Jn(x), 1 if it is a zero of Jn'(x). Of&#xa;        length `nt`.&#xa;&#xa;    See Also&#xa;    --------&#xa;    jn_zeros, jnp_zeros : to get separated arrays of zeros.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt > 1200):&#xa;        raise ValueError(""Number must be integer <= 1200."")&#xa;    nt = int(nt)&#xa;    n, m, t, zo = specfun.jdzo(nt)&#xa;    return zo[1:nt+1], n[:nt], m[:nt], t[:nt]&#xa;&#xa;&#xa;def jnyn_zeros(n, nt):&#xa;    """"""Compute nt zeros of Bessel functions Jn(x), Jn'(x), Yn(x), and Yn'(x).&#xa;&#xa;    Returns 4 arrays of length `nt`, corresponding to the first `nt` zeros of&#xa;    Jn(x), Jn'(x), Yn(x), and Yn'(x), respectively.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of the Bessel functions&#xa;    nt : int&#xa;        Number (<=1200) of zeros to compute&#xa;&#xa;    See jn_zeros, jnp_zeros, yn_zeros, ynp_zeros to get separate arrays.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(nt) and isscalar(n)):&#xa;        raise ValueError(""Arguments must be scalars."")&#xa;    if (floor(n) != n) or (floor(nt) != nt):&#xa;        raise ValueError(""Arguments must be integers."")&#xa;    if (nt <= 0):&#xa;        raise ValueError(""nt > 0"")&#xa;    return specfun.jyzo(abs(n), nt)&#xa;&#xa;&#xa;def jn_zeros(n, nt):&#xa;    """"""Compute zeros of integer-order Bessel function Jn(x).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of Bessel function&#xa;    nt : int&#xa;        Number of zeros to return&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    return jnyn_zeros(n, nt)[0]&#xa;&#xa;&#xa;def jnp_zeros(n, nt):&#xa;    """"""Compute zeros of integer-order Bessel function derivative Jn'(x).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of Bessel function&#xa;    nt : int&#xa;        Number of zeros to return&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    return jnyn_zeros(n, nt)[1]&#xa;&#xa;&#xa;def yn_zeros(n, nt):&#xa;    """"""Compute zeros of integer-order Bessel function Yn(x).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of Bessel function&#xa;    nt : int&#xa;        Number of zeros to return&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    return jnyn_zeros(n, nt)[2]&#xa;&#xa;&#xa;def ynp_zeros(n, nt):&#xa;    """"""Compute zeros of integer-order Bessel function derivative Yn'(x).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of Bessel function&#xa;    nt : int&#xa;        Number of zeros to return&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    return jnyn_zeros(n, nt)[3]&#xa;&#xa;&#xa;def y0_zeros(nt, complex=False):&#xa;    """"""Compute nt zeros of Bessel function Y0(z), and derivative at each zero.&#xa;&#xa;    The derivatives are given by Y0'(z0) = -Y1(z0) at each zero z0.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number of zeros to return&#xa;    complex : bool, default False&#xa;        Set to False to return only the real zeros; set to True to return only&#xa;        the complex zeros with negative real part and positive imaginary part.&#xa;        Note that the complex conjugates of the latter are also zeros of the&#xa;        function, but are not returned by this routine.&#xa;&#xa;    Returns&#xa;    -------&#xa;    z0n : ndarray&#xa;        Location of nth zero of Y0(z)&#xa;    y0pz0n : ndarray&#xa;        Value of derivative Y0'(z0) for nth zero&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""Arguments must be scalar positive integer."")&#xa;    kf = 0&#xa;    kc = not complex&#xa;    return specfun.cyzo(nt, kf, kc)&#xa;&#xa;&#xa;def y1_zeros(nt, complex=False):&#xa;    """"""Compute nt zeros of Bessel function Y1(z), and derivative at each zero.&#xa;&#xa;    The derivatives are given by Y1'(z1) = Y0(z1) at each zero z1.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number of zeros to return&#xa;    complex : bool, default False&#xa;        Set to False to return only the real zeros; set to True to return only&#xa;        the complex zeros with negative real part and positive imaginary part.&#xa;        Note that the complex conjugates of the latter are also zeros of the&#xa;        function, but are not returned by this routine.&#xa;&#xa;    Returns&#xa;    -------&#xa;    z1n : ndarray&#xa;        Location of nth zero of Y1(z)&#xa;    y1pz1n : ndarray&#xa;        Value of derivative Y1'(z1) for nth zero&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""Arguments must be scalar positive integer."")&#xa;    kf = 1&#xa;    kc = not complex&#xa;    return specfun.cyzo(nt, kf, kc)&#xa;&#xa;&#xa;def y1p_zeros(nt, complex=False):&#xa;    """"""Compute nt zeros of Bessel derivative Y1'(z), and value at each zero.&#xa;&#xa;    The values are given by Y1(z1) at each z1 where Y1'(z1)=0.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number of zeros to return&#xa;    complex : bool, default False&#xa;        Set to False to return only the real zeros; set to True to return only&#xa;        the complex zeros with negative real part and positive imaginary part.&#xa;        Note that the complex conjugates of the latter are also zeros of the&#xa;        function, but are not returned by this routine.&#xa;&#xa;    Returns&#xa;    -------&#xa;    z1pn : ndarray&#xa;        Location of nth zero of Y1'(z)&#xa;    y1z1pn : ndarray&#xa;        Value of derivative Y1(z1) for nth zero&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""Arguments must be scalar positive integer."")&#xa;    kf = 2&#xa;    kc = not complex&#xa;    return specfun.cyzo(nt, kf, kc)&#xa;&#xa;&#xa;def _bessel_diff_formula(v, z, n, L, phase):&#xa;    # from AMS55.&#xa;    # L(v, z) = J(v, z), Y(v, z), H1(v, z), H2(v, z), phase = -1&#xa;    # L(v, z) = I(v, z) or exp(v*pi*i)K(v, z), phase = 1&#xa;    # For K, you can pull out the exp((v-k)*pi*i) into the caller&#xa;    v = asarray(v)&#xa;    p = 1.0&#xa;    s = L(v-n, z)&#xa;    for i in xrange(1, n+1):&#xa;        p = phase * (p * (n-i+1)) / i   # = choose(k, i)&#xa;        s += p*L(v-n + i*2, z)&#xa;    return s / (2.**n)&#xa;&#xa;&#xa;bessel_diff_formula = np.deprecate(_bessel_diff_formula,&#xa;    message=""bessel_diff_formula is a private function, do not use it!"")&#xa;&#xa;&#xa;def jvp(v, z, n=1):&#xa;    """"""Compute nth derivative of Bessel function Jv(z) with respect to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of Bessel function&#xa;    z : complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int, default 1&#xa;        Order of derivative&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.6.7 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.6.E7&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return jv(v, z)&#xa;    else:&#xa;        return _bessel_diff_formula(v, z, n, jv, -1)&#xa;&#xa;&#xa;def yvp(v, z, n=1):&#xa;    """"""Compute nth derivative of Bessel function Yv(z) with respect to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of Bessel function&#xa;    z : complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int, default 1&#xa;        Order of derivative&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.6.7 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.6.E7&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return yv(v, z)&#xa;    else:&#xa;        return _bessel_diff_formula(v, z, n, yv, -1)&#xa;&#xa;&#xa;def kvp(v, z, n=1):&#xa;    """"""Compute nth derivative of real-order modified Bessel function Kv(z)&#xa;&#xa;    Kv(z) is the modified Bessel function of the second kind.&#xa;    Derivative is calculated with respect to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : array_like of float&#xa;        Order of Bessel function&#xa;    z : array_like of complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int&#xa;        Order of derivative.  Default is first derivative.&#xa;&#xa;    Returns&#xa;    -------&#xa;    out : ndarray&#xa;        The results&#xa;&#xa;    Examples&#xa;    --------&#xa;    Calculate multiple values at order 5:&#xa;&#xa;    >>> from scipy.special import kvp&#xa;    >>> kvp(5, (1, 2, 3+5j))&#xa;    array([-1849.0354+0.j    ,   -25.7735+0.j    ,    -0.0307+0.0875j])&#xa;&#xa;    Calculate for a single value at multiple orders:&#xa;&#xa;    >>> kvp((4, 4.5, 5), 1)&#xa;    array([ -184.0309,  -568.9585, -1849.0354])&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.29.5 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 6.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.29.E5&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return kv(v, z)&#xa;    else:&#xa;        return (-1)**n * _bessel_diff_formula(v, z, n, kv, 1)&#xa;&#xa;&#xa;def ivp(v, z, n=1):&#xa;    """"""Compute nth derivative of modified Bessel function Iv(z) with respect&#xa;    to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : array_like of float&#xa;        Order of Bessel function&#xa;    z : array_like of complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int, default 1&#xa;        Order of derivative&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.29.5 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 6.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.29.E5&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return iv(v, z)&#xa;    else:&#xa;        return _bessel_diff_formula(v, z, n, iv, 1)&#xa;&#xa;&#xa;def h1vp(v, z, n=1):&#xa;    """"""Compute nth derivative of Hankel function H1v(z) with respect to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of Hankel function&#xa;    z : complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int, default 1&#xa;        Order of derivative&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.6.7 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.6.E7&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return hankel1(v, z)&#xa;    else:&#xa;        return _bessel_diff_formula(v, z, n, hankel1, -1)&#xa;&#xa;&#xa;def h2vp(v, z, n=1):&#xa;    """"""Compute nth derivative of Hankel function H2v(z) with respect to `z`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of Hankel function&#xa;    z : complex&#xa;        Argument at which to evaluate the derivative&#xa;    n : int, default 1&#xa;        Order of derivative&#xa;&#xa;    Notes&#xa;    -----&#xa;    The derivative is computed using the relation DLFM 10.6.7 [2]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 5.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.6.E7&#xa;&#xa;    """"""&#xa;    if not isinstance(n, int) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if n == 0:&#xa;        return hankel2(v, z)&#xa;    else:&#xa;        return _bessel_diff_formula(v, z, n, hankel2, -1)&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_jn is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_jn instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_jn(n, z):&#xa;    """"""Compute spherical Bessel function jn(z) and derivative.&#xa;&#xa;    This function computes the value and first derivative of jn(z) for all&#xa;    orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of jn to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    jn : ndarray&#xa;        Value of j0(z), ..., jn(z)&#xa;    jnp : ndarray&#xa;        First derivative j0'(z), ..., jn'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_jn&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z):&#xa;        nm, jn, jnp, yn, ynp = specfun.csphjy(n1, z)&#xa;    else:&#xa;        nm, jn, jnp = specfun.sphj(n1, z)&#xa;    return jn[:(n+1)], jnp[:(n+1)]&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_yn is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_yn instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_yn(n, z):&#xa;    """"""Compute spherical Bessel function yn(z) and derivative.&#xa;&#xa;    This function computes the value and first derivative of yn(z) for all&#xa;    orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of yn to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    yn : ndarray&#xa;        Value of y0(z), ..., yn(z)&#xa;    ynp : ndarray&#xa;        First derivative y0'(z), ..., yn'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_yn&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z) or less(z, 0):&#xa;        nm, jn, jnp, yn, ynp = specfun.csphjy(n1, z)&#xa;    else:&#xa;        nm, yn, ynp = specfun.sphy(n1, z)&#xa;    return yn[:(n+1)], ynp[:(n+1)]&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_jnyn is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_jn and ""&#xa;                      ""scipy.special.spherical_yn instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_jnyn(n, z):&#xa;    """"""Compute spherical Bessel functions jn(z) and yn(z) and derivatives.&#xa;&#xa;    This function computes the value and first derivative of jn(z) and yn(z)&#xa;    for all orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of jn and yn to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    jn : ndarray&#xa;        Value of j0(z), ..., jn(z)&#xa;    jnp : ndarray&#xa;        First derivative j0'(z), ..., jn'(z)&#xa;    yn : ndarray&#xa;        Value of y0(z), ..., yn(z)&#xa;    ynp : ndarray&#xa;        First derivative y0'(z), ..., yn'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_jn&#xa;    spherical_yn&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z) or less(z, 0):&#xa;        nm, jn, jnp, yn, ynp = specfun.csphjy(n1, z)&#xa;    else:&#xa;        nm, yn, ynp = specfun.sphy(n1, z)&#xa;        nm, jn, jnp = specfun.sphj(n1, z)&#xa;    return jn[:(n+1)], jnp[:(n+1)], yn[:(n+1)], ynp[:(n+1)]&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_in is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_in instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_in(n, z):&#xa;    """"""Compute spherical Bessel function in(z) and derivative.&#xa;&#xa;    This function computes the value and first derivative of in(z) for all&#xa;    orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of in to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    in : ndarray&#xa;        Value of i0(z), ..., in(z)&#xa;    inp : ndarray&#xa;        First derivative i0'(z), ..., in'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_in&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z):&#xa;        nm, In, Inp, kn, knp = specfun.csphik(n1, z)&#xa;    else:&#xa;        nm, In, Inp = specfun.sphi(n1, z)&#xa;    return In[:(n+1)], Inp[:(n+1)]&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_kn is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_kn instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_kn(n, z):&#xa;    """"""Compute spherical Bessel function kn(z) and derivative.&#xa;&#xa;    This function computes the value and first derivative of kn(z) for all&#xa;    orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of kn to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    kn : ndarray&#xa;        Value of k0(z), ..., kn(z)&#xa;    knp : ndarray&#xa;        First derivative k0'(z), ..., kn'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_kn&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z) or less(z, 0):&#xa;        nm, In, Inp, kn, knp = specfun.csphik(n1, z)&#xa;    else:&#xa;        nm, kn, knp = specfun.sphk(n1, z)&#xa;    return kn[:(n+1)], knp[:(n+1)]&#xa;&#xa;&#xa;@np.deprecate(message=""scipy.special.sph_inkn is deprecated in scipy 0.18.0. ""&#xa;                      ""Use scipy.special.spherical_in and ""&#xa;                      ""scipy.special.spherical_kn instead. ""&#xa;                      ""Note that the new function has a different signature."")&#xa;def sph_inkn(n, z):&#xa;    """"""Compute spherical Bessel functions in(z), kn(z), and derivatives.&#xa;&#xa;    This function computes the value and first derivative of in(z) and kn(z)&#xa;    for all orders up to and including n.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of in and kn to compute&#xa;    z : complex&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    in : ndarray&#xa;        Value of i0(z), ..., in(z)&#xa;    inp : ndarray&#xa;        First derivative i0'(z), ..., in'(z)&#xa;    kn : ndarray&#xa;        Value of k0(z), ..., kn(z)&#xa;    knp : ndarray&#xa;        First derivative k0'(z), ..., kn'(z)&#xa;&#xa;    See also&#xa;    --------&#xa;    spherical_in&#xa;    spherical_kn&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 8.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z) or less(z, 0):&#xa;        nm, In, Inp, kn, knp = specfun.csphik(n1, z)&#xa;    else:&#xa;        nm, In, Inp = specfun.sphi(n1, z)&#xa;        nm, kn, knp = specfun.sphk(n1, z)&#xa;    return In[:(n+1)], Inp[:(n+1)], kn[:(n+1)], knp[:(n+1)]&#xa;&#xa;&#xa;def riccati_jn(n, x):&#xa;    r""""""Compute Ricatti-Bessel function of the first kind and its derivative.&#xa;&#xa;    The Ricatti-Bessel function of the first kind is defined as :math:`x&#xa;    j_n(x)`, where :math:`j_n` is the spherical Bessel function of the first&#xa;    kind of order :math:`n`.&#xa;&#xa;    This function computes the value and first derivative of the&#xa;    Ricatti-Bessel function for all orders up to and including `n`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of function to compute&#xa;    x : float&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    jn : ndarray&#xa;        Value of j0(x), ..., jn(x)&#xa;    jnp : ndarray&#xa;        First derivative j0'(x), ..., jn'(x)&#xa;&#xa;    Notes&#xa;    -----&#xa;    The computation is carried out via backward recurrence, using the&#xa;    relation DLMF 10.51.1 [2]_.&#xa;&#xa;    Wrapper for a Fortran routine created by Shanjie Zhang and Jianming&#xa;    Jin [1]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.51.E1&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(x)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n == 0):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    nm, jn, jnp = specfun.rctj(n1, x)&#xa;    return jn[:(n+1)], jnp[:(n+1)]&#xa;&#xa;&#xa;def riccati_yn(n, x):&#xa;    """"""Compute Ricatti-Bessel function of the second kind and its derivative.&#xa;&#xa;    The Ricatti-Bessel function of the second kind is defined as :math:`x&#xa;    y_n(x)`, where :math:`y_n` is the spherical Bessel function of the second&#xa;    kind of order :math:`n`.&#xa;&#xa;    This function computes the value and first derivative of the function for&#xa;    all orders up to and including `n`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Maximum order of function to compute&#xa;    x : float&#xa;        Argument at which to evaluate&#xa;&#xa;    Returns&#xa;    -------&#xa;    yn : ndarray&#xa;        Value of y0(x), ..., yn(x)&#xa;    ynp : ndarray&#xa;        First derivative y0'(x), ..., yn'(x)&#xa;&#xa;    Notes&#xa;    -----&#xa;    The computation is carried out via ascending recurrence, using the&#xa;    relation DLMF 10.51.1 [2]_.&#xa;&#xa;    Wrapper for a Fortran routine created by Shanjie Zhang and Jianming&#xa;    Jin [1]_.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions.&#xa;           http://dlmf.nist.gov/10.51.E1&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(x)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n == 0):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    nm, jn, jnp = specfun.rcty(n1, x)&#xa;    return jn[:(n+1)], jnp[:(n+1)]&#xa;&#xa;&#xa;def erfinv(y):&#xa;    """"""Inverse function for erf.&#xa;    """"""&#xa;    return ndtri((y+1)/2.0)/sqrt(2)&#xa;&#xa;&#xa;def erfcinv(y):&#xa;    """"""Inverse function for erfc.&#xa;    """"""&#xa;    return -ndtri(0.5*y)/sqrt(2)&#xa;&#xa;&#xa;def erf_zeros(nt):&#xa;    """"""Compute nt complex zeros of error function erf(z).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if (floor(nt) != nt) or (nt <= 0) or not isscalar(nt):&#xa;        raise ValueError(""Argument must be positive scalar integer."")&#xa;    return specfun.cerzo(nt)&#xa;&#xa;&#xa;def fresnelc_zeros(nt):&#xa;    """"""Compute nt complex zeros of cosine Fresnel integral C(z).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if (floor(nt) != nt) or (nt <= 0) or not isscalar(nt):&#xa;        raise ValueError(""Argument must be positive scalar integer."")&#xa;    return specfun.fcszo(1, nt)&#xa;&#xa;&#xa;def fresnels_zeros(nt):&#xa;    """"""Compute nt complex zeros of sine Fresnel integral S(z).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if (floor(nt) != nt) or (nt <= 0) or not isscalar(nt):&#xa;        raise ValueError(""Argument must be positive scalar integer."")&#xa;    return specfun.fcszo(2, nt)&#xa;&#xa;&#xa;def fresnel_zeros(nt):&#xa;    """"""Compute nt complex zeros of sine and cosine Fresnel integrals S(z) and C(z).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if (floor(nt) != nt) or (nt <= 0) or not isscalar(nt):&#xa;        raise ValueError(""Argument must be positive scalar integer."")&#xa;    return specfun.fcszo(2, nt), specfun.fcszo(1, nt)&#xa;&#xa;&#xa;def assoc_laguerre(x, n, k=0.0):&#xa;    """"""Compute the generalized (associated) Laguerre polynomial of degree n and order k.&#xa;&#xa;    The polynomial :math:`L^{(k)}_n(x)` is orthogonal over ``[0, inf)``,&#xa;    with weighting function ``exp(-x) * x**k`` with ``k > -1``.&#xa;&#xa;    Notes&#xa;    -----&#xa;    `assoc_laguerre` is a simple wrapper around `eval_genlaguerre`, with&#xa;    reversed argument order ``(x, n, k=0.0) --> (n, k, x)``.&#xa;&#xa;    """"""&#xa;    return orthogonal.eval_genlaguerre(n, k, x)&#xa;&#xa;digamma = psi&#xa;&#xa;&#xa;def polygamma(n, x):&#xa;    """"""Polygamma function n.&#xa;&#xa;    This is the nth derivative of the digamma (psi) function.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : array_like of int&#xa;        The order of the derivative of `psi`.&#xa;    x : array_like&#xa;        Where to evaluate the polygamma function.&#xa;&#xa;    Returns&#xa;    -------&#xa;    polygamma : ndarray&#xa;        The result.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import special&#xa;    >>> x = [2, 3, 25.5]&#xa;    >>> special.polygamma(1, x)&#xa;    array([ 0.64493407,  0.39493407,  0.03999467])&#xa;    >>> special.polygamma(0, x) == special.psi(x)&#xa;    array([ True,  True,  True], dtype=bool)&#xa;&#xa;    """"""&#xa;    n, x = asarray(n), asarray(x)&#xa;    fac2 = (-1.0)**(n+1) * gamma(n+1.0) * zeta(n+1, x)&#xa;    return where(n == 0, psi(x), fac2)&#xa;&#xa;&#xa;def mathieu_even_coef(m, q):&#xa;    r""""""Fourier coefficients for even Mathieu and modified Mathieu functions.&#xa;&#xa;    The Fourier series of the even solutions of the Mathieu differential&#xa;    equation are of the form&#xa;&#xa;    .. math:: \mathrm{ce}_{2n}(z, q) = \sum_{k=0}^{\infty} A_{(2n)}^{(2k)} \cos 2kz&#xa;&#xa;    .. math:: \mathrm{ce}_{2n+1}(z, q) = \sum_{k=0}^{\infty} A_{(2n+1)}^{(2k+1)} \cos (2k+1)z&#xa;&#xa;    This function returns the coefficients :math:`A_{(2n)}^{(2k)}` for even&#xa;    input m=2n, and the coefficients :math:`A_{(2n+1)}^{(2k+1)}` for odd input&#xa;    m=2n+1.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : int&#xa;        Order of Mathieu functions.  Must be non-negative.&#xa;    q : float (>=0)&#xa;        Parameter of Mathieu functions.  Must be non-negative.&#xa;&#xa;    Returns&#xa;    -------&#xa;    Ak : ndarray&#xa;        Even or odd Fourier coefficients, corresponding to even or odd m.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions&#xa;           http://dlmf.nist.gov/28.4#i&#xa;&#xa;    """"""&#xa;    if not (isscalar(m) and isscalar(q)):&#xa;        raise ValueError(""m and q must be scalars."")&#xa;    if (q < 0):&#xa;        raise ValueError(""q >=0"")&#xa;    if (m != floor(m)) or (m < 0):&#xa;        raise ValueError(""m must be an integer >=0."")&#xa;&#xa;    if (q <= 1):&#xa;        qm = 7.5 + 56.1*sqrt(q) - 134.7*q + 90.7*sqrt(q)*q&#xa;    else:&#xa;        qm = 17.0 + 3.1*sqrt(q) - .126*q + .0037*sqrt(q)*q&#xa;    km = int(qm + 0.5*m)&#xa;    if km > 251:&#xa;        print(""Warning, too many predicted coefficients."")&#xa;    kd = 1&#xa;    m = int(floor(m))&#xa;    if m % 2:&#xa;        kd = 2&#xa;&#xa;    a = mathieu_a(m, q)&#xa;    fc = specfun.fcoef(kd, m, q, a)&#xa;    return fc[:km]&#xa;&#xa;&#xa;def mathieu_odd_coef(m, q):&#xa;    r""""""Fourier coefficients for even Mathieu and modified Mathieu functions.&#xa;&#xa;    The Fourier series of the odd solutions of the Mathieu differential&#xa;    equation are of the form&#xa;&#xa;    .. math:: \mathrm{se}_{2n+1}(z, q) = \sum_{k=0}^{\infty} B_{(2n+1)}^{(2k+1)} \sin (2k+1)z&#xa;&#xa;    .. math:: \mathrm{se}_{2n+2}(z, q) = \sum_{k=0}^{\infty} B_{(2n+2)}^{(2k+2)} \sin (2k+2)z&#xa;&#xa;    This function returns the coefficients :math:`B_{(2n+2)}^{(2k+2)}` for even&#xa;    input m=2n+2, and the coefficients :math:`B_{(2n+1)}^{(2k+1)}` for odd&#xa;    input m=2n+1.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : int&#xa;        Order of Mathieu functions.  Must be non-negative.&#xa;    q : float (>=0)&#xa;        Parameter of Mathieu functions.  Must be non-negative.&#xa;&#xa;    Returns&#xa;    -------&#xa;    Bk : ndarray&#xa;        Even or odd Fourier coefficients, corresponding to even or odd m.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(m) and isscalar(q)):&#xa;        raise ValueError(""m and q must be scalars."")&#xa;    if (q < 0):&#xa;        raise ValueError(""q >=0"")&#xa;    if (m != floor(m)) or (m <= 0):&#xa;        raise ValueError(""m must be an integer > 0"")&#xa;&#xa;    if (q <= 1):&#xa;        qm = 7.5 + 56.1*sqrt(q) - 134.7*q + 90.7*sqrt(q)*q&#xa;    else:&#xa;        qm = 17.0 + 3.1*sqrt(q) - .126*q + .0037*sqrt(q)*q&#xa;    km = int(qm + 0.5*m)&#xa;    if km > 251:&#xa;        print(""Warning, too many predicted coefficients."")&#xa;    kd = 4&#xa;    m = int(floor(m))&#xa;    if m % 2:&#xa;        kd = 3&#xa;&#xa;    b = mathieu_b(m, q)&#xa;    fc = specfun.fcoef(kd, m, q, b)&#xa;    return fc[:km]&#xa;&#xa;&#xa;def lpmn(m, n, z):&#xa;    """"""Associated Legendre function of the first kind, Pmn(z).&#xa;&#xa;    Computes the associated Legendre function of the first kind of order m and&#xa;    degree n, ``Pmn(z)`` = :math:`P_n^m(z)`, and its derivative, ``Pmn'(z)``.&#xa;    Returns two arrays of size ``(m+1, n+1)`` containing ``Pmn(z)`` and&#xa;    ``Pmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.&#xa;&#xa;    This function takes a real argument ``z``. For complex arguments ``z``&#xa;    use clpmn instead.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : int&#xa;       ``|m| <= n``; the order of the Legendre function.&#xa;    n : int&#xa;       where ``n >= 0``; the degree of the Legendre function.  Often&#xa;       called ``l`` (lower case L) in descriptions of the associated&#xa;       Legendre function&#xa;    z : float&#xa;        Input value.&#xa;&#xa;    Returns&#xa;    -------&#xa;    Pmn_z : (m+1, n+1) array&#xa;       Values for all orders 0..m and degrees 0..n&#xa;    Pmn_d_z : (m+1, n+1) array&#xa;       Derivatives for all orders 0..m and degrees 0..n&#xa;&#xa;    See Also&#xa;    --------&#xa;    clpmn: associated Legendre functions of the first kind for complex z&#xa;&#xa;    Notes&#xa;    -----&#xa;    In the interval (-1, 1), Ferrer's function of the first kind is&#xa;    returned. The phase convention used for the intervals (1, inf)&#xa;    and (-inf, -1) is such that the result is always real.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions&#xa;           http://dlmf.nist.gov/14.3&#xa;&#xa;    """"""&#xa;    if not isscalar(m) or (abs(m) > n):&#xa;        raise ValueError(""m must be <= n."")&#xa;    if not isscalar(n) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if not isscalar(z):&#xa;        raise ValueError(""z must be scalar."")&#xa;    if iscomplex(z):&#xa;        raise ValueError(""Argument must be real. Use clpmn instead."")&#xa;    if (m < 0):&#xa;        mp = -m&#xa;        mf, nf = mgrid[0:mp+1, 0:n+1]&#xa;        sv = errprint(0)&#xa;        if abs(z) < 1:&#xa;            # Ferrer function; DLMF 14.9.3&#xa;            fixarr = where(mf > nf, 0.0,&#xa;                           (-1)**mf * gamma(nf-mf+1) / gamma(nf+mf+1))&#xa;        else:&#xa;            # Match to clpmn; DLMF 14.9.13&#xa;            fixarr = where(mf > nf, 0.0, gamma(nf-mf+1) / gamma(nf+mf+1))&#xa;        sv = errprint(sv)&#xa;    else:&#xa;        mp = m&#xa;    p, pd = specfun.lpmn(mp, n, z)&#xa;    if (m < 0):&#xa;        p = p * fixarr&#xa;        pd = pd * fixarr&#xa;    return p, pd&#xa;&#xa;&#xa;def clpmn(m, n, z, type=3):&#xa;    """"""Associated Legendre function of the first kind, Pmn(z).&#xa;&#xa;    Computes the associated Legendre function of the first kind of order m and&#xa;    degree n, ``Pmn(z)`` = :math:`P_n^m(z)`, and its derivative, ``Pmn'(z)``.&#xa;    Returns two arrays of size ``(m+1, n+1)`` containing ``Pmn(z)`` and&#xa;    ``Pmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : int&#xa;       ``|m| <= n``; the order of the Legendre function.&#xa;    n : int&#xa;       where ``n >= 0``; the degree of the Legendre function.  Often&#xa;       called ``l`` (lower case L) in descriptions of the associated&#xa;       Legendre function&#xa;    z : float or complex&#xa;        Input value.&#xa;    type : int, optional&#xa;       takes values 2 or 3&#xa;       2: cut on the real axis ``|x| > 1``&#xa;       3: cut on the real axis ``-1 < x < 1`` (default)&#xa;&#xa;    Returns&#xa;    -------&#xa;    Pmn_z : (m+1, n+1) array&#xa;       Values for all orders ``0..m`` and degrees ``0..n``&#xa;    Pmn_d_z : (m+1, n+1) array&#xa;       Derivatives for all orders ``0..m`` and degrees ``0..n``&#xa;&#xa;    See Also&#xa;    --------&#xa;    lpmn: associated Legendre functions of the first kind for real z&#xa;&#xa;    Notes&#xa;    -----&#xa;    By default, i.e. for ``type=3``, phase conventions are chosen according&#xa;    to [1]_ such that the function is analytic. The cut lies on the interval&#xa;    (-1, 1). Approaching the cut from above or below in general yields a phase&#xa;    factor with respect to Ferrer's function of the first kind&#xa;    (cf. `lpmn`).&#xa;&#xa;    For ``type=2`` a cut at ``|x| > 1`` is chosen. Approaching the real values&#xa;    on the interval (-1, 1) in the complex plane yields Ferrer's function&#xa;    of the first kind.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] NIST Digital Library of Mathematical Functions&#xa;           http://dlmf.nist.gov/14.21&#xa;&#xa;    """"""&#xa;    if not isscalar(m) or (abs(m) > n):&#xa;        raise ValueError(""m must be <= n."")&#xa;    if not isscalar(n) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if not isscalar(z):&#xa;        raise ValueError(""z must be scalar."")&#xa;    if not(type == 2 or type == 3):&#xa;        raise ValueError(""type must be either 2 or 3."")&#xa;    if (m < 0):&#xa;        mp = -m&#xa;        mf, nf = mgrid[0:mp+1, 0:n+1]&#xa;        sv = errprint(0)&#xa;        if type == 2:&#xa;            fixarr = where(mf > nf, 0.0,&#xa;                           (-1)**mf * gamma(nf-mf+1) / gamma(nf+mf+1))&#xa;        else:&#xa;            fixarr = where(mf > nf, 0.0, gamma(nf-mf+1) / gamma(nf+mf+1))&#xa;        sv = errprint(sv)&#xa;    else:&#xa;        mp = m&#xa;    p, pd = specfun.clpmn(mp, n, real(z), imag(z), type)&#xa;    if (m < 0):&#xa;        p = p * fixarr&#xa;        pd = pd * fixarr&#xa;    return p, pd&#xa;&#xa;&#xa;def lqmn(m, n, z):&#xa;    """"""Associated Legendre function of the second kind, Qmn(z).&#xa;&#xa;    Computes the associated Legendre function of the second kind of order m and&#xa;    degree n, ``Qmn(z)`` = :math:`Q_n^m(z)`, and its derivative, ``Qmn'(z)``.&#xa;    Returns two arrays of size ``(m+1, n+1)`` containing ``Qmn(z)`` and&#xa;    ``Qmn'(z)`` for all orders from ``0..m`` and degrees from ``0..n``.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : int&#xa;       ``|m| <= n``; the order of the Legendre function.&#xa;    n : int&#xa;       where ``n >= 0``; the degree of the Legendre function.  Often&#xa;       called ``l`` (lower case L) in descriptions of the associated&#xa;       Legendre function&#xa;    z : complex&#xa;        Input value.&#xa;&#xa;    Returns&#xa;    -------&#xa;    Qmn_z : (m+1, n+1) array&#xa;       Values for all orders 0..m and degrees 0..n&#xa;    Qmn_d_z : (m+1, n+1) array&#xa;       Derivatives for all orders 0..m and degrees 0..n&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(m) or (m < 0):&#xa;        raise ValueError(""m must be a non-negative integer."")&#xa;    if not isscalar(n) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if not isscalar(z):&#xa;        raise ValueError(""z must be scalar."")&#xa;    m = int(m)&#xa;    n = int(n)&#xa;&#xa;    # Ensure neither m nor n == 0&#xa;    mm = max(1, m)&#xa;    nn = max(1, n)&#xa;&#xa;    if iscomplex(z):&#xa;        q, qd = specfun.clqmn(mm, nn, z)&#xa;    else:&#xa;        q, qd = specfun.lqmn(mm, nn, z)&#xa;    return q[:(m+1), :(n+1)], qd[:(m+1), :(n+1)]&#xa;&#xa;&#xa;def bernoulli(n):&#xa;    """"""Bernoulli numbers B0..Bn (inclusive).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(n) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    n = int(n)&#xa;    if (n < 2):&#xa;        n1 = 2&#xa;    else:&#xa;        n1 = n&#xa;    return specfun.bernob(int(n1))[:(n+1)]&#xa;&#xa;&#xa;def euler(n):&#xa;    """"""Euler numbers E0..En (inclusive).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(n) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    n = int(n)&#xa;    if (n < 2):&#xa;        n1 = 2&#xa;    else:&#xa;        n1 = n&#xa;    return specfun.eulerb(n1)[:(n+1)]&#xa;&#xa;&#xa;def lpn(n, z):&#xa;    """"""Legendre functions of the first kind, Pn(z).&#xa;&#xa;    Compute sequence of Legendre functions of the first kind (polynomials),&#xa;    Pn(z) and derivatives for all degrees from 0 to n (inclusive).&#xa;&#xa;    See also special.legendre for polynomial class.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z):&#xa;        pn, pd = specfun.clpn(n1, z)&#xa;    else:&#xa;        pn, pd = specfun.lpn(n1, z)&#xa;    return pn[:(n+1)], pd[:(n+1)]&#xa;&#xa;&#xa;def lqn(n, z):&#xa;    """"""Legendre functions of the second kind, Qn(z).&#xa;&#xa;    Compute sequence of Legendre functions of the second kind, Qn(z) and&#xa;    derivatives for all degrees from 0 to n (inclusive).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (n != floor(n)) or (n < 0):&#xa;        raise ValueError(""n must be a non-negative integer."")&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    if iscomplex(z):&#xa;        qn, qd = specfun.clqn(n1, z)&#xa;    else:&#xa;        qn, qd = specfun.lqnb(n1, z)&#xa;    return qn[:(n+1)], qd[:(n+1)]&#xa;&#xa;&#xa;def ai_zeros(nt):&#xa;    """"""&#xa;    Compute `nt` zeros and values of the Airy function Ai and its derivative.&#xa;&#xa;    Computes the first `nt` zeros, `a`, of the Airy function Ai(x);&#xa;    first `nt` zeros, `ap`, of the derivative of the Airy function Ai'(x);&#xa;    the corresponding values Ai(a');&#xa;    and the corresponding values Ai'(a).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number of zeros to compute&#xa;&#xa;    Returns&#xa;    -------&#xa;    a : ndarray&#xa;        First `nt` zeros of Ai(x)&#xa;    ap : ndarray&#xa;        First `nt` zeros of Ai'(x)&#xa;    ai : ndarray&#xa;        Values of Ai(x) evaluated at first `nt` zeros of Ai'(x)&#xa;    aip : ndarray&#xa;        Values of Ai'(x) evaluated at first `nt` zeros of Ai(x)&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    kf = 1&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be a positive integer scalar."")&#xa;    return specfun.airyzo(nt, kf)&#xa;&#xa;&#xa;def bi_zeros(nt):&#xa;    """"""&#xa;    Compute `nt` zeros and values of the Airy function Bi and its derivative.&#xa;&#xa;    Computes the first `nt` zeros, b, of the Airy function Bi(x);&#xa;    first `nt` zeros, b', of the derivative of the Airy function Bi'(x);&#xa;    the corresponding values Bi(b');&#xa;    and the corresponding values Bi'(b).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    nt : int&#xa;        Number of zeros to compute&#xa;&#xa;    Returns&#xa;    -------&#xa;    b : ndarray&#xa;        First `nt` zeros of Bi(x)&#xa;    bp : ndarray&#xa;        First `nt` zeros of Bi'(x)&#xa;    bi : ndarray&#xa;        Values of Bi(x) evaluated at first `nt` zeros of Bi'(x)&#xa;    bip : ndarray&#xa;        Values of Bi'(x) evaluated at first `nt` zeros of Bi(x)&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    kf = 2&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be a positive integer scalar."")&#xa;    return specfun.airyzo(nt, kf)&#xa;&#xa;&#xa;def lmbda(v, x):&#xa;    r""""""Jahnke-Emden Lambda function, Lambdav(x).&#xa;&#xa;    This function is defined as [2]_,&#xa;&#xa;    .. math:: \Lambda_v(x) = \Gamma(v+1) \frac{J_v(x)}{(x/2)^v},&#xa;&#xa;    where :math:`\Gamma` is the gamma function and :math:`J_v` is the&#xa;    Bessel function of the first kind.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of the Lambda function&#xa;    x : float&#xa;        Value at which to evaluate the function and derivatives&#xa;&#xa;    Returns&#xa;    -------&#xa;    vl : ndarray&#xa;        Values of Lambda_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;    dl : ndarray&#xa;        Derivatives Lambda_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;    .. [2] Jahnke, E. and Emde, F. ""Tables of Functions with Formulae and&#xa;           Curves"" (4th ed.), Dover, 1945&#xa;    """"""&#xa;    if not (isscalar(v) and isscalar(x)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (v < 0):&#xa;        raise ValueError(""argument must be > 0."")&#xa;    n = int(v)&#xa;    v0 = v - n&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    v1 = n1 + v0&#xa;    if (v != floor(v)):&#xa;        vm, vl, dl = specfun.lamv(v1, x)&#xa;    else:&#xa;        vm, vl, dl = specfun.lamn(v1, x)&#xa;    return vl[:(n+1)], dl[:(n+1)]&#xa;&#xa;&#xa;def pbdv_seq(v, x):&#xa;    """"""Parabolic cylinder functions Dv(x) and derivatives.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of the parabolic cylinder function&#xa;    x : float&#xa;        Value at which to evaluate the function and derivatives&#xa;&#xa;    Returns&#xa;    -------&#xa;    dv : ndarray&#xa;        Values of D_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;    dp : ndarray&#xa;        Derivatives D_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 13.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(v) and isscalar(x)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    n = int(v)&#xa;    v0 = v-n&#xa;    if (n < 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    v1 = n1 + v0&#xa;    dv, dp, pdf, pdd = specfun.pbdv(v1, x)&#xa;    return dv[:n1+1], dp[:n1+1]&#xa;&#xa;&#xa;def pbvv_seq(v, x):&#xa;    """"""Parabolic cylinder functions Vv(x) and derivatives.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    v : float&#xa;        Order of the parabolic cylinder function&#xa;    x : float&#xa;        Value at which to evaluate the function and derivatives&#xa;&#xa;    Returns&#xa;    -------&#xa;    dv : ndarray&#xa;        Values of V_vi(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;    dp : ndarray&#xa;        Derivatives V_vi'(x), for vi=v-int(v), vi=1+v-int(v), ..., vi=v.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 13.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(v) and isscalar(x)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    n = int(v)&#xa;    v0 = v-n&#xa;    if (n <= 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    v1 = n1 + v0&#xa;    dv, dp, pdf, pdd = specfun.pbvv(v1, x)&#xa;    return dv[:n1+1], dp[:n1+1]&#xa;&#xa;&#xa;def pbdn_seq(n, z):&#xa;    """"""Parabolic cylinder functions Dn(z) and derivatives.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Order of the parabolic cylinder function&#xa;    z : complex&#xa;        Value at which to evaluate the function and derivatives&#xa;&#xa;    Returns&#xa;    -------&#xa;    dv : ndarray&#xa;        Values of D_i(z), for i=0, ..., i=n.&#xa;    dp : ndarray&#xa;        Derivatives D_i'(z), for i=0, ..., i=n.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996, chapter 13.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(n) and isscalar(z)):&#xa;        raise ValueError(""arguments must be scalars."")&#xa;    if (floor(n) != n):&#xa;        raise ValueError(""n must be an integer."")&#xa;    if (abs(n) <= 1):&#xa;        n1 = 1&#xa;    else:&#xa;        n1 = n&#xa;    cpb, cpd = specfun.cpbdn(n1, z)&#xa;    return cpb[:n1+1], cpd[:n1+1]&#xa;&#xa;&#xa;def ber_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function ber(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 1)&#xa;&#xa;&#xa;def bei_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function bei(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 2)&#xa;&#xa;&#xa;def ker_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function ker(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 3)&#xa;&#xa;&#xa;def kei_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function kei(x).&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 4)&#xa;&#xa;&#xa;def berp_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function ber'(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 5)&#xa;&#xa;&#xa;def beip_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function bei'(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 6)&#xa;&#xa;&#xa;def kerp_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function ker'(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 7)&#xa;&#xa;&#xa;def keip_zeros(nt):&#xa;    """"""Compute nt zeros of the Kelvin function kei'(x).&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return specfun.klvnzo(nt, 8)&#xa;&#xa;&#xa;def kelvin_zeros(nt):&#xa;    """"""Compute nt zeros of all Kelvin functions.&#xa;&#xa;    Returned in a length-8 tuple of arrays of length nt.  The tuple contains&#xa;    the arrays of zeros of (ber, bei, ker, kei, ber', bei', ker', kei').&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not isscalar(nt) or (floor(nt) != nt) or (nt <= 0):&#xa;        raise ValueError(""nt must be positive integer scalar."")&#xa;    return (specfun.klvnzo(nt, 1),&#xa;            specfun.klvnzo(nt, 2),&#xa;            specfun.klvnzo(nt, 3),&#xa;            specfun.klvnzo(nt, 4),&#xa;            specfun.klvnzo(nt, 5),&#xa;            specfun.klvnzo(nt, 6),&#xa;            specfun.klvnzo(nt, 7),&#xa;            specfun.klvnzo(nt, 8))&#xa;&#xa;&#xa;def pro_cv_seq(m, n, c):&#xa;    """"""Characteristic values for prolate spheroidal wave functions.&#xa;&#xa;    Compute a sequence of characteristic values for the prolate&#xa;    spheroidal wave functions for mode m and n'=m..n and spheroidal&#xa;    parameter c.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(m) and isscalar(n) and isscalar(c)):&#xa;        raise ValueError(""Arguments must be scalars."")&#xa;    if (n != floor(n)) or (m != floor(m)):&#xa;        raise ValueError(""Modes must be integers."")&#xa;    if (n-m > 199):&#xa;        raise ValueError(""Difference between n and m is too large."")&#xa;    maxL = n-m+1&#xa;    return specfun.segv(m, n, c, 1)[1][:maxL]&#xa;&#xa;&#xa;def obl_cv_seq(m, n, c):&#xa;    """"""Characteristic values for oblate spheroidal wave functions.&#xa;&#xa;    Compute a sequence of characteristic values for the oblate&#xa;    spheroidal wave functions for mode m and n'=m..n and spheroidal&#xa;    parameter c.&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] Zhang, Shanjie and Jin, Jianming. ""Computation of Special&#xa;           Functions"", John Wiley and Sons, 1996.&#xa;           http://jin.ece.illinois.edu/specfunc.html&#xa;&#xa;    """"""&#xa;    if not (isscalar(m) and isscalar(n) and isscalar(c)):&#xa;        raise ValueError(""Arguments must be scalars."")&#xa;    if (n != floor(n)) or (m != floor(m)):&#xa;        raise ValueError(""Modes must be integers."")&#xa;    if (n-m > 199):&#xa;        raise ValueError(""Difference between n and m is too large."")&#xa;    maxL = n-m+1&#xa;    return specfun.segv(m, n, c, -1)[1][:maxL]&#xa;&#xa;&#xa;def ellipk(m):&#xa;    """"""Complete elliptic integral of the first kind.&#xa;&#xa;    This function is defined as&#xa;&#xa;    .. math:: K(m) = \\int_0^{\\pi/2} [1 - m \\sin(t)^2]^{-1/2} dt&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    m : array_like&#xa;        The parameter of the elliptic integral.&#xa;&#xa;    Returns&#xa;    -------&#xa;    K : array_like&#xa;        Value of the elliptic integral.&#xa;&#xa;    Notes&#xa;    -----&#xa;    For more precision around point m = 1, use `ellipkm1`, which this&#xa;    function calls.&#xa;&#xa;    See Also&#xa;    --------&#xa;    ellipkm1 : Complete elliptic integral of the first kind around m = 1&#xa;    ellipkinc : Incomplete elliptic integral of the first kind&#xa;    ellipe : Complete elliptic integral of the second kind&#xa;    ellipeinc : Incomplete elliptic integral of the second kind&#xa;&#xa;&#xa;    """"""&#xa;    return ellipkm1(1 - asarray(m))&#xa;&#xa;&#xa;def agm(a, b):&#xa;    """"""Arithmetic, Geometric Mean.&#xa;&#xa;    Start with a_0=a and b_0=b and iteratively compute&#xa;&#xa;    a_{n+1} = (a_n+b_n)/2&#xa;    b_{n+1} = sqrt(a_n*b_n)&#xa;&#xa;    until a_n=b_n.   The result is agm(a, b)&#xa;&#xa;    agm(a, b)=agm(b, a)&#xa;    agm(a, a) = a&#xa;    min(a, b) < agm(a, b) < max(a, b)&#xa;    """"""&#xa;    s = a + b + 0.0&#xa;    return (pi / 4) * s / ellipkm1(4 * a * b / s ** 2)&#xa;&#xa;&#xa;def comb(N, k, exact=False, repetition=False):&#xa;    """"""The number of combinations of N things taken k at a time.&#xa;&#xa;    This is often expressed as ""N choose k"".&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    N : int, ndarray&#xa;        Number of things.&#xa;    k : int, ndarray&#xa;        Number of elements taken.&#xa;    exact : bool, optional&#xa;        If `exact` is False, then floating point precision is used, otherwise&#xa;        exact long integer is computed.&#xa;    repetition : bool, optional&#xa;        If `repetition` is True, then the number of combinations with&#xa;        repetition is computed.&#xa;&#xa;    Returns&#xa;    -------&#xa;    val : int, ndarray&#xa;        The total number of combinations.&#xa;&#xa;    Notes&#xa;    -----&#xa;    - Array arguments accepted only for exact=False case.&#xa;    - If k > N, N < 0, or k < 0, then a 0 is returned.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy.special import comb&#xa;    >>> k = np.array([3, 4])&#xa;    >>> n = np.array([10, 10])&#xa;    >>> comb(n, k, exact=False)&#xa;    array([ 120.,  210.])&#xa;    >>> comb(10, 3, exact=True)&#xa;    120L&#xa;    >>> comb(10, 3, exact=True, repetition=True)&#xa;    220L&#xa;&#xa;    """"""&#xa;    if repetition:&#xa;        return comb(N + k - 1, k, exact)&#xa;    if exact:&#xa;        return _comb_int(N, k)&#xa;    else:&#xa;        k, N = asarray(k), asarray(N)&#xa;        cond = (k <= N) & (N >= 0) & (k >= 0)&#xa;        vals = binom(N, k)&#xa;        if isinstance(vals, np.ndarray):&#xa;            vals[~cond] = 0&#xa;        elif not cond:&#xa;            vals = np.float64(0)&#xa;        return vals&#xa;&#xa;&#xa;def perm(N, k, exact=False):&#xa;    """"""Permutations of N things taken k at a time, i.e., k-permutations of N.&#xa;&#xa;    It's also known as ""partial permutations"".&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    N : int, ndarray&#xa;        Number of things.&#xa;    k : int, ndarray&#xa;        Number of elements taken.&#xa;    exact : bool, optional&#xa;        If `exact` is False, then floating point precision is used, otherwise&#xa;        exact long integer is computed.&#xa;&#xa;    Returns&#xa;    -------&#xa;    val : int, ndarray&#xa;        The number of k-permutations of N.&#xa;&#xa;    Notes&#xa;    -----&#xa;    - Array arguments accepted only for exact=False case.&#xa;    - If k > N, N < 0, or k < 0, then a 0 is returned.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy.special import perm&#xa;    >>> k = np.array([3, 4])&#xa;    >>> n = np.array([10, 10])&#xa;    >>> perm(n, k)&#xa;    array([  720.,  5040.])&#xa;    >>> perm(10, 3, exact=True)&#xa;    720&#xa;&#xa;    """"""&#xa;    if exact:&#xa;        if (k > N) or (N < 0) or (k < 0):&#xa;            return 0&#xa;        val = 1&#xa;        for i in xrange(N - k + 1, N + 1):&#xa;            val *= i&#xa;        return val&#xa;    else:&#xa;        k, N = asarray(k), asarray(N)&#xa;        cond = (k <= N) & (N >= 0) & (k >= 0)&#xa;        vals = poch(N - k + 1, k)&#xa;        if isinstance(vals, np.ndarray):&#xa;            vals[~cond] = 0&#xa;        elif not cond:&#xa;            vals = np.float64(0)&#xa;        return vals&#xa;&#xa;&#xa;# http://stackoverflow.com/a/16327037/125507&#xa;def _range_prod(lo, hi):&#xa;    """"""&#xa;    Product of a range of numbers.&#xa;&#xa;    Returns the product of&#xa;    lo * (lo+1) * (lo+2) * ... * (hi-2) * (hi-1) * hi&#xa;    = hi! / (lo-1)!&#xa;&#xa;    Breaks into smaller products first for speed:&#xa;    _range_prod(2, 9) = ((2*3)*(4*5))*((6*7)*(8*9))&#xa;    """"""&#xa;    if lo + 1 < hi:&#xa;        mid = (hi + lo) // 2&#xa;        return _range_prod(lo, mid) * _range_prod(mid + 1, hi)&#xa;    if lo == hi:&#xa;        return lo&#xa;    return lo * hi&#xa;&#xa;&#xa;def factorial(n, exact=False):&#xa;    """"""&#xa;    The factorial of a number or array of numbers.&#xa;&#xa;    The factorial of non-negative integer `n` is the product of all&#xa;    positive integers less than or equal to `n`::&#xa;&#xa;        n! = n * (n - 1) * (n - 2) * ... * 1&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int or array_like of ints&#xa;        Input values.  If ``n < 0``, the return value is 0.&#xa;    exact : bool, optional&#xa;        If True, calculate the answer exactly using long integer arithmetic.&#xa;        If False, result is approximated in floating point rapidly using the&#xa;        `gamma` function.&#xa;        Default is False.&#xa;&#xa;    Returns&#xa;    -------&#xa;    nf : float or int or ndarray&#xa;        Factorial of `n`, as integer or float depending on `exact`.&#xa;&#xa;    Notes&#xa;    -----&#xa;    For arrays with ``exact=True``, the factorial is computed only once, for&#xa;    the largest input, with each other result computed in the process.&#xa;    The output dtype is increased to ``int64`` or ``object`` if necessary.&#xa;&#xa;    With ``exact=False`` the factorial is approximated using the gamma&#xa;    function:&#xa;&#xa;    .. math:: n! = \Gamma(n+1)&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy.special import factorial&#xa;    >>> arr = np.array([3, 4, 5])&#xa;    >>> factorial(arr, exact=False)&#xa;    array([   6.,   24.,  120.])&#xa;    >>> factorial(arr, exact=True)&#xa;    array([  6,  24, 120])&#xa;    >>> factorial(5, exact=True)&#xa;    120L&#xa;&#xa;    """"""&#xa;    if exact:&#xa;        if np.ndim(n) == 0:&#xa;            return 0 if n < 0 else math.factorial(n)&#xa;        else:&#xa;            n = asarray(n)&#xa;            un = np.unique(n).astype(object)&#xa;&#xa;            # Convert to object array of long ints if np.int can't handle size&#xa;            if un[-1] > 20:&#xa;                dt = object&#xa;            elif un[-1] > 12:&#xa;                dt = np.int64&#xa;            else:&#xa;                dt = np.int&#xa;&#xa;            out = np.empty_like(n, dtype=dt)&#xa;&#xa;            # Handle invalid/trivial values&#xa;            un = un[un > 1]&#xa;            out[n < 2] = 1&#xa;            out[n < 0] = 0&#xa;&#xa;            # Calculate products of each range of numbers&#xa;            if un.size:&#xa;                val = math.factorial(un[0])&#xa;                out[n == un[0]] = val&#xa;                for i in xrange(len(un) - 1):&#xa;                    prev = un[i] + 1&#xa;                    current = un[i + 1]&#xa;                    val *= _range_prod(prev, current)&#xa;                    out[n == current] = val&#xa;            return out&#xa;    else:&#xa;        n = asarray(n)&#xa;        vals = gamma(n + 1)&#xa;        return where(n >= 0, vals, 0)&#xa;&#xa;&#xa;def factorial2(n, exact=False):&#xa;    """"""Double factorial.&#xa;&#xa;    This is the factorial with every second value skipped.  E.g., ``7!! = 7 * 5&#xa;    * 3 * 1``.  It can be approximated numerically as::&#xa;&#xa;      n!! = special.gamma(n/2+1)*2**((m+1)/2)/sqrt(pi)  n odd&#xa;          = 2**(n/2) * (n/2)!                           n even&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int or array_like&#xa;        Calculate ``n!!``.  Arrays are only supported with `exact` set&#xa;        to False.  If ``n < 0``, the return value is 0.&#xa;    exact : bool, optional&#xa;        The result can be approximated rapidly using the gamma-formula&#xa;        above (default).  If `exact` is set to True, calculate the&#xa;        answer exactly using integer arithmetic.&#xa;&#xa;    Returns&#xa;    -------&#xa;    nff : float or int&#xa;        Double factorial of `n`, as an int or a float depending on&#xa;        `exact`.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy.special import factorial2&#xa;    >>> factorial2(7, exact=False)&#xa;    array(105.00000000000001)&#xa;    >>> factorial2(7, exact=True)&#xa;    105L&#xa;&#xa;    """"""&#xa;    if exact:&#xa;        if n < -1:&#xa;            return 0&#xa;        if n <= 0:&#xa;            return 1&#xa;        val = 1&#xa;        for k in xrange(n, 0, -2):&#xa;            val *= k&#xa;        return val&#xa;    else:&#xa;        n = asarray(n)&#xa;        vals = zeros(n.shape, 'd')&#xa;        cond1 = (n % 2) & (n >= -1)&#xa;        cond2 = (1-(n % 2)) & (n >= -1)&#xa;        oddn = extract(cond1, n)&#xa;        evenn = extract(cond2, n)&#xa;        nd2o = oddn / 2.0&#xa;        nd2e = evenn / 2.0&#xa;        place(vals, cond1, gamma(nd2o + 1) / sqrt(pi) * pow(2.0, nd2o + 0.5))&#xa;        place(vals, cond2, gamma(nd2e + 1) * pow(2.0, nd2e))&#xa;        return vals&#xa;&#xa;&#xa;def factorialk(n, k, exact=True):&#xa;    """"""Multifactorial of n of order k, n(!!...!).&#xa;&#xa;    This is the multifactorial of n skipping k values.  For example,&#xa;&#xa;      factorialk(17, 4) = 17!!!! = 17 * 13 * 9 * 5 * 1&#xa;&#xa;    In particular, for any integer ``n``, we have&#xa;&#xa;      factorialk(n, 1) = factorial(n)&#xa;&#xa;      factorialk(n, 2) = factorial2(n)&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    n : int&#xa;        Calculate multifactorial. If `n` < 0, the return value is 0.&#xa;    k : int&#xa;        Order of multifactorial.&#xa;    exact : bool, optional&#xa;        If exact is set to True, calculate the answer exactly using&#xa;        integer arithmetic.&#xa;&#xa;    Returns&#xa;    -------&#xa;    val : int&#xa;        Multifactorial of `n`.&#xa;&#xa;    Raises&#xa;    ------&#xa;    NotImplementedError&#xa;        Raises when exact is False&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy.special import factorialk&#xa;    >>> factorialk(5, 1, exact=True)&#xa;    120L&#xa;    >>> factorialk(5, 3, exact=True)&#xa;    10L&#xa;&#xa;    """"""&#xa;    if exact:&#xa;        if n < 1-k:&#xa;            return 0&#xa;        if n <= 0:&#xa;            return 1&#xa;        val = 1&#xa;        for j in xrange(n, 0, -k):&#xa;            val = val*j&#xa;        return val&#xa;    else:&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;def zeta(x, q=None, out=None):&#xa;    r""""""&#xa;    Riemann zeta function.&#xa;&#xa;    The two-argument version is the Hurwitz zeta function:&#xa;&#xa;    .. math:: \zeta(x, q) = \sum_{k=0}^{\infty} \frac{1}{(k + q)^x},&#xa;&#xa;    Riemann zeta function corresponds to ``q = 1``.&#xa;&#xa;    See also&#xa;    --------&#xa;    zetac&#xa;&#xa;    """"""&#xa;    if q is None:&#xa;        q = 1&#xa;    return _zeta(x, q, out)&#xa;"
7205107|"#&#xa;# @BEGIN LICENSE&#xa;#&#xa;# Psi4: an open-source quantum chemistry software package&#xa;#&#xa;# Copyright (c) 2007-2018 The Psi4 Developers.&#xa;#&#xa;# The copyrights for code used from other parties are included in&#xa;# the corresponding files.&#xa;#&#xa;# This file is part of Psi4.&#xa;#&#xa;# Psi4 is free software; you can redistribute it and/or modify&#xa;# it under the terms of the GNU Lesser General Public License as published by&#xa;# the Free Software Foundation, version 3.&#xa;#&#xa;# Psi4 is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU Lesser General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU Lesser General Public License along&#xa;# with Psi4; if not, write to the Free Software Foundation, Inc.,&#xa;# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.&#xa;#&#xa;# @END LICENSE&#xa;#&#xa;&#xa;import re&#xa;import sys&#xa;import math&#xa;&#xa;import numpy as np&#xa;&#xa;from ..physconst import psi_bohr2angstroms&#xa;&#xa;if sys.version_info >= (3, 0):&#xa;    basestring = str&#xa;&#xa;&#xa;def distance_matrix(a, b):&#xa;    """"""Euclidean distance matrix between rows of arrays `a` and `b`. Equivalent to&#xa;    `scipy.spatial.distance.cdist(a, b, 'euclidean')`. Returns a.shape[0] x b.shape[0] array.&#xa;&#xa;    """"""&#xa;    assert a.shape[1] == b.shape[1], """"""Inner dimensions do not match""""""&#xa;    distm = np.zeros([a.shape[0], b.shape[0]])&#xa;    for i in range(a.shape[0]):&#xa;        distm[i] = np.linalg.norm(a[i] - b, axis=1)&#xa;    return distm&#xa;&#xa;&#xa;def update_with_error(a, b, path=None):&#xa;    """"""Merges `b` into `a` like dict.update; however, raises KeyError if values of a&#xa;    key shared by `a` and `b` conflict.&#xa;&#xa;    Adapted from: https://stackoverflow.com/a/7205107&#xa;&#xa;    """"""&#xa;    if path is None:&#xa;        path = []&#xa;    for key in b:&#xa;        if key in a:&#xa;            if isinstance(a[key], dict) and isinstance(b[key], dict):&#xa;                update_with_error(a[key], b[key], path + [str(key)])&#xa;            elif a[key] == b[key]:&#xa;                pass  # same leaf value&#xa;            elif a[key] is None:&#xa;                a[key] = b[key]&#xa;            elif (isinstance(a[key], (list, tuple)) and&#xa;                  not isinstance(a[key], basestring) and&#xa;                  isinstance(b[key], (list, tuple)) and&#xa;                  not isinstance(b[key], basestring) and&#xa;                  len(a[key]) == len(b[key]) and&#xa;                  all((av is None or av == bv) for av, bv in zip(a[key], b[key]))):  # yapf: disable&#xa;                a[key] = b[key]&#xa;            else:&#xa;                raise KeyError('Conflict at {}: {} vs. {}'.format('.'.join(path + [str(key)]), a[key], b[key]))&#xa;        else:&#xa;            a[key] = b[key]&#xa;    return a&#xa;&#xa;&#xa;def standardize_efp_angles_units(units, geom_hints):&#xa;    """"""Applies to the pre-validated xyzabc or points hints in `geom_hints`&#xa;    the libefp default (1) units of [a0] and (2) radian angle range of&#xa;    (-pi, pi]. The latter is handy since this is how libefp returns hints&#xa;&#xa;    """"""&#xa;&#xa;    def radrge(radang):&#xa;        """"""Adjust `radang` by 2pi into (-pi, pi] range.""""""&#xa;        if radang > math.pi:&#xa;            return radang - 2 * math.pi&#xa;        elif radang <= -math.pi:&#xa;            return radang + 2 * math.pi&#xa;        else:&#xa;            return radang&#xa;&#xa;    if units == 'Angstrom':&#xa;        iutau = 1. / psi_bohr2angstroms&#xa;    else:&#xa;        iutau = 1.&#xa;&#xa;    hints = []&#xa;    for hint in geom_hints:&#xa;        if len(hint) == 6:&#xa;            x, y, z = [i * iutau for i in hint[:3]]&#xa;            a, b, c = [radrge(i) for i in hint[3:]]&#xa;            hints.append([x, y, z, a, b, c])&#xa;        if len(hint) == 9:&#xa;            points = [i * iutau for i in hint]&#xa;            hints.append(points)&#xa;&#xa;    return hints&#xa;&#xa;&#xa;def filter_comments(string):&#xa;    """"""Remove from `string` any Python-style comments ('#' to end of line).""""""&#xa;&#xa;    comment = re.compile(r'(^|[^\\])#.*')&#xa;    string = re.sub(comment, '', string)&#xa;    return string&#xa;&#xa;&#xa;def unnp(dicary):&#xa;    """"""Return `dicary` with any ndarray values replaced by lists.""""""&#xa;&#xa;    ndicary = {}&#xa;    for k, v in dicary.items():&#xa;        try:&#xa;            v.shape&#xa;        except AttributeError:&#xa;            ndicary[k] = v&#xa;        else:&#xa;            ndicary[k] = v.tolist()&#xa;    return ndicary&#xa;"
13564851|"# Windows implementation of PyAutoGUI functions.&#xa;# BSD license&#xa;# Al Sweigart al@inventwithpython.com&#xa;&#xa;import ctypes&#xa;import ctypes.wintypes&#xa;import pyautogui&#xa;&#xa;import sys&#xa;if sys.platform !=  'win32':&#xa;    raise Exception('The pyautogui_win module should only be loaded on a Windows system.')&#xa;&#xa;&#xa;# Fixes the scaling issues where PyAutoGUI was reporting the wrong resolution:&#xa;try:&#xa;   ctypes.windll.user32.SetProcessDPIAware()&#xa;except AttributeError:&#xa;    pass # Windows XP doesn't support this, so just do nothing.&#xa;&#xa;&#xa;""""""&#xa;A lot of this code is probably repeated from win32 extensions module, but I didn't want to have that dependency.&#xa;&#xa;Note: According to http://msdn.microsoft.com/en-us/library/windows/desktop/ms646260(v=vs.85).aspx&#xa;the ctypes.windll.user32.mouse_event() function has been superceded by SendInput.&#xa;&#xa;SendInput() is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646310(v=vs.85).aspx&#xa;&#xa;UPDATE: SendInput() doesn't seem to be working for me. I've switched back to mouse_event().""""""&#xa;&#xa;&#xa;# Event codes to be passed to the mouse_event() win32 function.&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646273(v=vs.85).aspx&#xa;MOUSEEVENTF_LEFTDOWN = 0x0002&#xa;MOUSEEVENTF_LEFTUP = 0x0004&#xa;MOUSEEVENTF_LEFTCLICK = MOUSEEVENTF_LEFTDOWN + MOUSEEVENTF_LEFTUP&#xa;MOUSEEVENTF_RIGHTDOWN = 0x0008&#xa;MOUSEEVENTF_RIGHTUP = 0x0010&#xa;MOUSEEVENTF_RIGHTCLICK = MOUSEEVENTF_RIGHTDOWN + MOUSEEVENTF_RIGHTUP&#xa;MOUSEEVENTF_MIDDLEDOWN = 0x0020&#xa;MOUSEEVENTF_MIDDLEUP = 0x0040&#xa;MOUSEEVENTF_MIDDLECLICK = MOUSEEVENTF_MIDDLEDOWN + MOUSEEVENTF_MIDDLEUP&#xa;&#xa;MOUSEEVENTF_WHEEL = 0x0800&#xa;MOUSEEVENTF_HWHEEL = 0x01000&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646304(v=vs.85).aspx&#xa;KEYEVENTF_KEYUP = 0x0002&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;INPUT_MOUSE = 0&#xa;INPUT_KEYBOARD = 1&#xa;&#xa;&#xa;# This ctypes structure is for a Win32 POINT structure,&#xa;# which is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/dd162805(v=vs.85).aspx&#xa;# The POINT structure is used by GetCursorPos().&#xa;class POINT(ctypes.Structure):&#xa;    _fields_ = [(""x"", ctypes.c_ulong),&#xa;                (""y"", ctypes.c_ulong)]&#xa;&#xa;# These ctypes structures are for Win32 INPUT, MOUSEINPUT, KEYBDINPUT, and HARDWAREINPUT structures,&#xa;# used by SendInput and documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;# Thanks to BSH for this StackOverflow answer: https://stackoverflow.com/questions/18566289/how-would-you-recreate-this-windows-api-structure-with-ctypes&#xa;class MOUSEINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('dx', ctypes.wintypes.LONG),&#xa;        ('dy', ctypes.wintypes.LONG),&#xa;        ('mouseData', ctypes.wintypes.DWORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class KEYBDINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('wVk', ctypes.wintypes.WORD),&#xa;        ('wScan', ctypes.wintypes.WORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class HARDWAREINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('uMsg', ctypes.wintypes.DWORD),&#xa;        ('wParamL', ctypes.wintypes.WORD),&#xa;        ('wParamH', ctypes.wintypes.DWORD)&#xa;    ]&#xa;&#xa;class INPUT(ctypes.Structure):&#xa;    class _I(ctypes.Union):&#xa;        _fields_ = [&#xa;            ('mi', MOUSEINPUT),&#xa;            ('ki', KEYBDINPUT),&#xa;            ('hi', HARDWAREINPUT),&#xa;        ]&#xa;&#xa;    _anonymous_ = ('i', )&#xa;    _fields_ = [&#xa;        ('type', ctypes.wintypes.DWORD),&#xa;        ('i', _I),&#xa;    ]&#xa;# End of the SendInput win32 data structures.&#xa;&#xa;&#xa;&#xa;"""""" Keyboard key mapping for pyautogui:&#xa;Documented at http://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx&#xa;&#xa;The *KB dictionaries in pyautogui map a string that can be passed to keyDown(),&#xa;keyUp(), or press() into the code used for the OS-specific keyboard function.&#xa;&#xa;They should always be lowercase, and the same keys should be used across all OSes.""""""&#xa;keyboardMapping = dict([(key, None) for key in pyautogui.KEY_NAMES])&#xa;keyboardMapping.update({&#xa;    'backspace': 0x08, # VK_BACK&#xa;    '\b': 0x08, # VK_BACK&#xa;    'super': 0x5B, #VK_LWIN&#xa;    'tab': 0x09, # VK_TAB&#xa;    '\t': 0x09, # VK_TAB&#xa;    'clear': 0x0c, # VK_CLEAR&#xa;    'enter': 0x0d, # VK_RETURN&#xa;    '\n': 0x0d, # VK_RETURN&#xa;    'return': 0x0d, # VK_RETURN&#xa;    'shift': 0x10, # VK_SHIFT&#xa;    'ctrl': 0x11, # VK_CONTROL&#xa;    'alt': 0x12, # VK_MENU&#xa;    'pause': 0x13, # VK_PAUSE&#xa;    'capslock': 0x14, # VK_CAPITAL&#xa;    'kana': 0x15, # VK_KANA&#xa;    'hanguel': 0x15, # VK_HANGUEL&#xa;    'hangul': 0x15, # VK_HANGUL&#xa;    'junja': 0x17, # VK_JUNJA&#xa;    'final': 0x18, # VK_FINAL&#xa;    'hanja': 0x19, # VK_HANJA&#xa;    'kanji': 0x19, # VK_KANJI&#xa;    'esc': 0x1b, # VK_ESCAPE&#xa;    'escape': 0x1b, # VK_ESCAPE&#xa;    'convert': 0x1c, # VK_CONVERT&#xa;    'nonconvert': 0x1d, # VK_NONCONVERT&#xa;    'accept': 0x1e, # VK_ACCEPT&#xa;    'modechange': 0x1f, # VK_MODECHANGE&#xa;    ' ': 0x20, # VK_SPACE&#xa;    'space': 0x20,&#xa;    'pgup': 0x21, # VK_PRIOR&#xa;    'pgdn': 0x22, # VK_NEXT&#xa;    'pageup': 0x21, # VK_PRIOR&#xa;    'pagedown': 0x22, # VK_NEXT&#xa;    'end': 0x23, # VK_END&#xa;    'home': 0x24, # VK_HOME&#xa;    'left': 0x25, # VK_LEFT&#xa;    'up': 0x26, # VK_UP&#xa;    'right': 0x27, # VK_RIGHT&#xa;    'down': 0x28, # VK_DOWN&#xa;    'select': 0x29, # VK_SELECT&#xa;    'print': 0x2a, # VK_PRINT&#xa;    'execute': 0x2b, # VK_EXECUTE&#xa;    'prtsc': 0x2c, # VK_SNAPSHOT&#xa;    'prtscr': 0x2c, # VK_SNAPSHOT&#xa;    'prntscrn': 0x2c, # VK_SNAPSHOT&#xa;    'printscreen': 0x2c, # VK_SNAPSHOT&#xa;    'insert': 0x2d, # VK_INSERT&#xa;    'del': 0x2e, # VK_DELETE&#xa;    'delete': 0x2e, # VK_DELETE&#xa;    'help': 0x2f, # VK_HELP&#xa;    'win': 0x5b, # VK_LWIN&#xa;    'winleft': 0x5b, # VK_LWIN&#xa;    'winright': 0x5c, # VK_RWIN&#xa;    'apps': 0x5d, # VK_APPS&#xa;    'sleep': 0x5f, # VK_SLEEP&#xa;    'num0': 0x60, # VK_NUMPAD0&#xa;    'num1': 0x61, # VK_NUMPAD1&#xa;    'num2': 0x62, # VK_NUMPAD2&#xa;    'num3': 0x63, # VK_NUMPAD3&#xa;    'num4': 0x64, # VK_NUMPAD4&#xa;    'num5': 0x65, # VK_NUMPAD5&#xa;    'num6': 0x66, # VK_NUMPAD6&#xa;    'num7': 0x67, # VK_NUMPAD7&#xa;    'num8': 0x68, # VK_NUMPAD8&#xa;    'num9': 0x69, # VK_NUMPAD9&#xa;    'multiply': 0x6a, # VK_MULTIPLY  ??? Is this the numpad *?&#xa;    'add': 0x6b, # VK_ADD  ??? Is this the numpad +?&#xa;    'separator': 0x6c, # VK_SEPARATOR  ??? Is this the numpad enter?&#xa;    'subtract': 0x6d, # VK_SUBTRACT  ??? Is this the numpad -?&#xa;    'decimal': 0x6e, # VK_DECIMAL&#xa;    'divide': 0x6f, # VK_DIVIDE&#xa;    'f1': 0x70, # VK_F1&#xa;    'f2': 0x71, # VK_F2&#xa;    'f3': 0x72, # VK_F3&#xa;    'f4': 0x73, # VK_F4&#xa;    'f5': 0x74, # VK_F5&#xa;    'f6': 0x75, # VK_F6&#xa;    'f7': 0x76, # VK_F7&#xa;    'f8': 0x77, # VK_F8&#xa;    'f9': 0x78, # VK_F9&#xa;    'f10': 0x79, # VK_F10&#xa;    'f11': 0x7a, # VK_F11&#xa;    'f12': 0x7b, # VK_F12&#xa;    'f13': 0x7c, # VK_F13&#xa;    'f14': 0x7d, # VK_F14&#xa;    'f15': 0x7e, # VK_F15&#xa;    'f16': 0x7f, # VK_F16&#xa;    'f17': 0x80, # VK_F17&#xa;    'f18': 0x81, # VK_F18&#xa;    'f19': 0x82, # VK_F19&#xa;    'f20': 0x83, # VK_F20&#xa;    'f21': 0x84, # VK_F21&#xa;    'f22': 0x85, # VK_F22&#xa;    'f23': 0x86, # VK_F23&#xa;    'f24': 0x87, # VK_F24&#xa;    'numlock': 0x90, # VK_NUMLOCK&#xa;    'scrolllock': 0x91, # VK_SCROLL&#xa;    'shiftleft': 0xa0, # VK_LSHIFT&#xa;    'shiftright': 0xa1, # VK_RSHIFT&#xa;    'ctrlleft': 0xa2, # VK_LCONTROL&#xa;    'ctrlright': 0xa3, # VK_RCONTROL&#xa;    'altleft': 0xa4, # VK_LMENU&#xa;    'altright': 0xa5, # VK_RMENU&#xa;    'browserback': 0xa6, # VK_BROWSER_BACK&#xa;    'browserforward': 0xa7, # VK_BROWSER_FORWARD&#xa;    'browserrefresh': 0xa8, # VK_BROWSER_REFRESH&#xa;    'browserstop': 0xa9, # VK_BROWSER_STOP&#xa;    'browsersearch': 0xaa, # VK_BROWSER_SEARCH&#xa;    'browserfavorites': 0xab, # VK_BROWSER_FAVORITES&#xa;    'browserhome': 0xac, # VK_BROWSER_HOME&#xa;    'volumemute': 0xad, # VK_VOLUME_MUTE&#xa;    'volumedown': 0xae, # VK_VOLUME_DOWN&#xa;    'volumeup': 0xaf, # VK_VOLUME_UP&#xa;    'nexttrack': 0xb0, # VK_MEDIA_NEXT_TRACK&#xa;    'prevtrack': 0xb1, # VK_MEDIA_PREV_TRACK&#xa;    'stop': 0xb2, # VK_MEDIA_STOP&#xa;    'playpause': 0xb3, # VK_MEDIA_PLAY_PAUSE&#xa;    'launchmail': 0xb4, # VK_LAUNCH_MAIL&#xa;    'launchmediaselect': 0xb5, # VK_LAUNCH_MEDIA_SELECT&#xa;    'launchapp1': 0xb6, # VK_LAUNCH_APP1&#xa;    'launchapp2': 0xb7, # VK_LAUNCH_APP2&#xa;    #';': 0xba, # VK_OEM_1&#xa;    #'+': 0xbb, # VK_OEM_PLUS&#xa;    #',': 0xbc, # VK_OEM_COMMA&#xa;    #'-': 0xbd, # VK_OEM_MINUS&#xa;    #'.': 0xbe, # VK_OEM_PERIOD&#xa;    #'/': 0xbf, # VK_OEM_2&#xa;    #'~': 0xc0, # VK_OEM_3&#xa;    #'[': 0xdb, # VK_OEM_4&#xa;    #'|': 0xdc, # VK_OEM_5&#xa;    #']': 0xdd, # VK_OEM_6&#xa;    #""'"": 0xde, # VK_OEM_7&#xa;    #'': 0xdf, # VK_OEM_8&#xa;    #'': 0xe7, # VK_PACKET&#xa;    #'': 0xf6, # VK_ATTN&#xa;    #'': 0xf7, # VK_CRSEL&#xa;    #'': 0xf8, # VK_EXSEL&#xa;    #'': 0xf9, # VK_EREOF&#xa;    #'': 0xfa, # VK_PLAY&#xa;    #'': 0xfb, # VK_ZOOM&#xa;    #'': 0xfc, # VK_NONAME&#xa;    #'': 0xfd, # VK_PA1&#xa;    #'': 0xfe, # VK_OEM_CLEAR&#xa;})&#xa;&#xa;# Populate the basic printable ascii characters.&#xa;for c in range(32, 128):&#xa;    keyboardMapping[chr(c)] = ctypes.windll.user32.VkKeyScanA(ctypes.wintypes.WCHAR(chr(c)))&#xa;&#xa;&#xa;def _keyDown(key):&#xa;    """"""Performs a keyboard key press without the release. This will put that&#xa;    key in a held down state.&#xa;&#xa;    NOTE: For some reason, this does not seem to cause key repeats like would&#xa;    happen if a keyboard key was held down on a text field.&#xa;&#xa;    Args:&#xa;      key (str): The key to be pressed down. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;    mods, vkCode = divmod(keyboardMapping[key], 0x100)&#xa;&#xa;    for apply_mod, vk_mod in [(mods & 4, 0x12), (mods & 2, 0x11),&#xa;        (mods & 1 or needsShift, 0x10)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, 0, 0) #&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, 0, 0)&#xa;    for apply_mod, vk_mod in [(mods & 1 or needsShift, 0x10), (mods & 2, 0x11),&#xa;        (mods & 4, 0x12)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, KEYEVENTF_KEYUP, 0) #&#xa;&#xa;&#xa;def _keyUp(key):&#xa;    """"""Performs a keyboard key release (without the press down beforehand).&#xa;&#xa;    Args:&#xa;      key (str): The key to be released up. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;    mods, vkCode = divmod(keyboardMapping[key], 0x100)&#xa;&#xa;    for apply_mod, vk_mod in [(mods & 4, 0x12), (mods & 2, 0x11),&#xa;        (mods & 1 or needsShift, 0x10)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, 0, 0) #&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, KEYEVENTF_KEYUP, 0)&#xa;    for apply_mod, vk_mod in [(mods & 1 or needsShift, 0x10), (mods & 2, 0x11),&#xa;        (mods & 4, 0x12)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, KEYEVENTF_KEYUP, 0) #&#xa;&#xa;&#xa;def _position():&#xa;    """"""Returns the current xy coordinates of the mouse cursor as a two-integer&#xa;    tuple by calling the GetCursorPos() win32 function.&#xa;&#xa;    Returns:&#xa;      (x, y) tuple of the current xy coordinates of the mouse cursor.&#xa;    """"""&#xa;&#xa;    cursor = POINT()&#xa;    ctypes.windll.user32.GetCursorPos(ctypes.byref(cursor))&#xa;    return (cursor.x, cursor.y)&#xa;&#xa;&#xa;def _size():&#xa;    """"""Returns the width and height of the screen as a two-integer tuple.&#xa;&#xa;    Returns:&#xa;      (width, height) tuple of the screen size, in pixels.&#xa;    """"""&#xa;    return (ctypes.windll.user32.GetSystemMetrics(0), ctypes.windll.user32.GetSystemMetrics(1))&#xa;&#xa;&#xa;def _moveTo(x, y):&#xa;    """"""Send the mouse move event to Windows by calling SetCursorPos() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    ctypes.windll.user32.SetCursorPos(x, y)&#xa;&#xa;&#xa;def _mouseDown(x, y, button):&#xa;    """"""Send the mouse down event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLEDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _mouseUp(x, y, button):&#xa;    """"""Send the mouse up event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLEUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _click(x, y, button):&#xa;    """"""Send the mouse click event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTCLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLECLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTCLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _sendMouseEvent(ev, x, y, dwData=0):&#xa;    """"""The helper function that actually makes the call to the mouse_event()&#xa;    win32 function.&#xa;&#xa;    Args:&#xa;      ev (int): The win32 code for the mouse event. Use one of the MOUSEEVENTF_*&#xa;      constants for this argument.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      dwData (int): The argument for mouse_event()'s dwData parameter. So far&#xa;        this is only used by mouse scrolling.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    assert x != None and y != None, 'x and y cannot be set to None'&#xa;    # TODO: ARG! For some reason, SendInput isn't working for mouse events. I'm switching to using the older mouse_event win32 function.&#xa;    #mouseStruct = MOUSEINPUT()&#xa;    #mouseStruct.dx = x&#xa;    #mouseStruct.dy = y&#xa;    #mouseStruct.mouseData = ev&#xa;    #mouseStruct.time = 0&#xa;    #mouseStruct.dwExtraInfo = ctypes.pointer(ctypes.c_ulong(0)) # according to https://stackoverflow.com/questions/13564851/generate-keyboard-events I can just set this. I don't really care about this value.&#xa;    #inputStruct = INPUT()&#xa;    #inputStruct.mi = mouseStruct&#xa;    #inputStruct.type = INPUT_MOUSE&#xa;    #ctypes.windll.user32.SendInput(1, ctypes.pointer(inputStruct), ctypes.sizeof(inputStruct))&#xa;&#xa;    width, height = _size()&#xa;    convertedX = 65536 * x // width + 1&#xa;    convertedY = 65536 * y // height + 1&#xa;    ctypes.windll.user32.mouse_event(ev, ctypes.c_long(convertedX), ctypes.c_long(convertedY), dwData, 0)&#xa;&#xa;    # TODO: Too many false positives with this code: See: https://github.com/asweigart/pyautogui/issues/108&#xa;    #if ctypes.windll.kernel32.GetLastError() != 0:&#xa;    #    raise ctypes.WinError()&#xa;&#xa;&#xa;def _scroll(clicks, x=None, y=None):&#xa;    """"""Send the mouse vertical scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    startx, starty = _position()&#xa;    width, height = _size()&#xa;&#xa;    if x is None:&#xa;        x = startx&#xa;    else:&#xa;        if x < 0:&#xa;            x = 0&#xa;        elif x >= width:&#xa;            x = width - 1&#xa;    if y is None:&#xa;        y = starty&#xa;    else:&#xa;        if y < 0:&#xa;            y = 0&#xa;        elif y >= height:&#xa;            y = height - 1&#xa;&#xa;    try:&#xa;        _sendMouseEvent(MOUSEEVENTF_WHEEL, x, y, dwData=clicks)&#xa;    except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;&#xa;&#xa;def _hscroll(clicks, x, y):&#xa;    """"""Send the mouse horizontal scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving right, a negative value is moving left.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;&#xa;def _vscroll(clicks, x, y):&#xa;    """"""A wrapper for _scroll(), which does vertical scrolling.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;"
13436167|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
33601043|"from graphics_util import alpha_blend&#xa;&#xa;&#xa;def getANSIcolor_for_rgb(rgb):&#xa;    # Convert to web-safe color since that's what terminals can handle in ""256 color mode"" &#xa;    #   https://en.wikipedia.org/wiki/ANSI_escape_code&#xa;    #   http://misc.flogisoft.com/bash/tip_colors_and_formatting#bash_tipscolors_and_formatting_ansivt100_control_sequences&#xa;    #   http://superuser.com/questions/270214/how-can-i-change-the-colors-of-my-xterm-using-ansi-escape-sequences&#xa;    websafe_r = int(round((rgb[0] / 255.0) * 5) )&#xa;    websafe_g = int(round((rgb[1] / 255.0) * 5) )&#xa;    websafe_b = int(round((rgb[2] / 255.0) * 5) )&#xa;&#xa;    # Return ANSI color - only using 216 colors since those are the only ones we can reliably map to   &#xa;    #   https://en.wikipedia.org/wiki/ANSI_escape_code (see 256 color mode section)    &#xa;    return int(((websafe_r * 36) + (websafe_g * 6) + websafe_b) + 16)   &#xa;&#xa;&#xa;def getANSIfgarray_for_ANSIcolor(ANSIcolor):&#xa;    ""Return array of color codes to be used in composing an SGR escape sequence. Using array form lets us compose multiple color updates without putting out additional escapes""&#xa;    # We are using ""256 color mode"" which is available in xterm but not necessarily all terminals&#xa;    return ['38', '5', str(ANSIcolor)]      # To set FG in 256 color you use a code like ESC[38;5;###m&#xa;&#xa;&#xa;def getANSIbgarray_for_ANSIcolor(ANSIcolor):&#xa;    ""Return array of color codes to be used in composing an SGR escape sequence. Using array form lets us compose multiple color updates without putting out additional escapes""&#xa;    # We are using ""256 color mode"" which is available in xterm but not necessarily all terminals&#xa;    return ['48', '5', str(ANSIcolor)]      # To set BG in 256 color you use a code like ESC[48;5;###m&#xa;    &#xa;&#xa;def getANSIbgstring_for_ANSIcolor(ANSIcolor):&#xa;    # Get the array of color code info, prefix it with ESCAPE code and terminate it with ""m""&#xa;    return ""\x1b["" + "";"".join(getANSIbgarray_for_ANSIcolor(ANSIcolor)) + ""m""&#xa;    &#xa;&#xa;def generate_ANSI_to_set_fg_bg_colors(cur_fg_color, cur_bg_color, new_fg_color, new_bg_color):&#xa;             &#xa;    # This code assumes that ESC[49m and ESC[39m work for resetting bg and fg &#xa;    # This may not work on all terminals in which case we would have to use ESC[0m &#xa;    # to reset both at once, and then put back fg or bg that we actually want&#xa; &#xa;    # We don't change colors that are already the way we want them - saves lots of file size&#xa;&#xa;    color_array = []        # use array mechanism to avoid multiple escape sequences if we need to change fg AND bg&#xa;&#xa;    if new_bg_color != cur_bg_color:&#xa;        if new_bg_color is None:&#xa;            color_array.append('49')        # reset to default&#xa;        else:&#xa;            color_array += getANSIbgarray_for_ANSIcolor(new_bg_color)&#xa;                &#xa;    if new_fg_color != cur_fg_color:&#xa;        if new_fg_color is None:&#xa;            color_array.append('39')        # reset to default&#xa;        else:&#xa;            color_array += getANSIfgarray_for_ANSIcolor(new_fg_color)&#xa;&#xa;    if len(color_array) > 0:&#xa;        return ""\x1b["" + "";"".join(color_array) + ""m""&#xa;    else:&#xa;        return """"&#xa;&#xa;&#xa;def generate_optimized_y_move_down_x_SOL(y_dist):&#xa;    """""" move down y_dist, set x=0 """"""&#xa;&#xa;    # Optimization to move N lines and go to SOL in one command. Note that some terminals &#xa;    # may not support this so we might have to remove this optimization or make it optional &#xa;    # if that winds up mattering for terminals we care about. If we had to remove we'd&#xa;    # want to rework things such that we used ""\x1b[{0}B"" but also we would want to change&#xa;    # our interface to this function so we didn't guarantee x=0 since caller might ultimate &#xa;    # want it in a different place and we don't want to output two x moves. Could pass in &#xa;    # desired x, or return current x from here. &#xa;          &#xa;    string = ""\x1b[{0}E"".format(y_dist)  # ANSI code to move down N lines and move x to SOL&#xa;&#xa;    # Would a sequence of 1 or more \n chars be cheaper? If so we'll output that instead&#xa;    if y_dist < len(string):&#xa;        string = '\n' * y_dist&#xa;        &#xa;    return string&#xa;    &#xa;&#xa;def generate_ANSI_to_move_cursor(cur_x, cur_y, target_x, target_y):&#xa;    """"""&#xa;        Note that x positions are absolute (0=SOL) while y positions are relative. That is,&#xa;        we move the y position the relative distance between cur_y and target_y. It doesn't&#xa;        mean that cur_y=0 means we are on the first line of the screen. We have no way of&#xa;        knowing how tall the screen is, etc. at draw-time so we can't know this. &#xa;    """"""&#xa;&#xa;&#xa;    """"""&#xa;        **SIZE - this code (in concert with its caller) implements what I would call ""local optimizations""&#xa;        to try to minimize the number and size of cursor movements outputted. It does not attempt ""global&#xa;        optimizations"" which I think are rarely going to be worthwhile. See the DESIGN NOTE on global&#xa;        optimizations in this file for more details &#xa;    """"""        &#xa;&#xa;&#xa;    string = """"&#xa;&#xa;    if cur_y < target_y:    # MOVE DOWN&#xa;        y_dist = target_y - cur_y&#xa;&#xa;        # See if we can optimize moving x and y together&#xa;        if cur_x == target_x: &#xa;        &#xa;            # Need to move in y only&#xa;            if target_x != 0: &#xa;                # Already in correct x position which is NOT SOL. Just output code to move cursor &#xa;                # down. No special optimization is possible since \n would take us to SOL and then &#xa;                # we'd also need to output a move for x. &#xa;                return ""\x1b[{0}B"".format(y_dist)  # ANSI code to move down N lines&#xa;            else:&#xa;                # Already in correct x position which is SOL. Output efficient code to move down.&#xa;                return generate_optimized_y_move_down_x_SOL(y_dist)&#xa;        else:&#xa;        &#xa;            # Need to move in x and y&#xa;            if target_x != 0: &#xa;                # x move is going to be required so we'll move y efficiently and as a side&#xa;                # effect, x will become 0. Code below will move x to the right place&#xa;                string += generate_optimized_y_move_down_x_SOL(y_dist)&#xa;                cur_x = 0&#xa;            else:&#xa;                # Output move down that brings x to SOL. Then we're done.&#xa;                return generate_optimized_y_move_down_x_SOL(y_dist)&#xa;                &#xa;    elif cur_y > target_y:  # MOVE UP&#xa;        if target_x == 0:        &#xa;            # We want to move up and be at the SOL. That can be achieved with one command so we're&#xa;            # done and we return it. However note that some terminals may not support this so we&#xa;            # might have to remove this optimization or make it optional if that winds up mattering for terminals we care about.  &#xa;            return ""\x1b[{0}F"".format(cur_y - target_y)     # ANSI code to move up N lines and move x to SOL&#xa;        else:&#xa;            string += ""\x1b[{0}A"".format(cur_y - target_y)  # ANSI code to move up N lines &#xa;&#xa;    if cur_x < target_x:    # MOVE RIGHT&#xa;        # **SIZE - Note that when the bgcolor is specified (not None) and not overdrawing another drawing (as in an animation case)&#xa;        # an optimization could be performed to draw spaces rather than output cursor advances. This would use less&#xa;        # size when advancing less than 3 columns since the min escape sequence here is len 4. Not implementing this now&#xa;        # \t (tab) could also be a cheap way to move forward, but not clear we can determine how far it goes or if that would&#xa;        # be consistent, nor whether it is ever destructive.&#xa;        string += ""\x1b[{0}C"".format(target_x - cur_x)  # ANSI code to move cursor right N columns&#xa;    elif cur_x > target_x:  # MOVE LEFT&#xa;        # **SIZE - potential optimizations: \b (backspace) could be a cheaper way to move backwards when there is only a short&#xa;        # way to go. However, not sure if it is ever destructive so not bothering with it now.    &#xa;        # If we need to move to x=0, \r could be a cheap way to get there. However not entirely clear whether some terminals&#xa;        # will move to next line as well, and might sometimes be destructive. Not going to research this so not doing it now. &#xa;        string += ""\x1b[{0}D"".format(cur_x - target_x)  # ANSI code to move cursor left N columns &#xa;&#xa;    return string&#xa;&#xa;&#xa;def generate_ANSI_from_pixels(pixels, width, height, bgcolor_rgba, current_ansi_colors = None, current_cursor_pos = None, get_pixel_func = None, is_overdraw = False, x_offset = 0):&#xa;    """"""&#xa;    Generate ANSI codes for passed pixels&#xa;       &#xa;    Does not include a final newline or a reset to any particular colors at end of returned output string.&#xa;    Caller should take care of that if desired.&#xa;     &#xa;    :param pixels: if get_pixel_func is None, 2D array of RGBA tuples indexed by [x,y]. &#xa;       Otherwise given to get_pixel_func as param.&#xa;    :param width: number of pixels to output on each row&#xa;    :param height: number of rows to output &#xa;    :param bgcolor_rgba: Optional background color used to fill new lines (produced when is_ovedraw is False)&#xa;       and a net new line to the terminal (as opposed to drawing on a current line - e.g. if the cursor was moved&#xa;       up) is produced. Also used as background color for any characters we output that don't fill the entire &#xa;       character area (e.g. a space fills the entire area, while X does not). Non-space only used if get_pixel_func&#xa;       returns it. If bgcolor_rgba is None, then the background is treated as the terminal's default background color&#xa;       which also means that partially transparent pixels will be treated as non-transparent (since we don't know &#xa;       bg color to blend them with).   &#xa;    :param current_ansi_colors: Optional dict holding ""current"" ANSI colors - allows optimization where &#xa;       we don't switch to these colors if already set. See info on return values for format of dict.&#xa;    :param current_cursor_pos: Optional dict holding current cursor position - allows optimization where &#xa;       we don't output extra moves to get to the right place to draw. Consider the passed position relative&#xa;       to where we want to draw the top/left for the current call. Note that a negative value for &#xa;       current_cursor_pos['y'] can be used to start drawing futher down the screen. Don't use ['x'] similarly &#xa;       since x is reset for each line. Use the x_offset param instead. &#xa;    :param get_pixel_func: Optional function that allows using custom ""pixel"" formats. If not None, function &#xa;       that will be passed pixels and a current x,y value and must return character to draw and RGBA to draw it in. &#xa;    :param is_overdraw: if True, drawing code can assume that all lines are being drawn on lines that were already&#xa;       established in the terminal. This allows for optimizations (e.g. not needing to output \n to fill blank lines).&#xa;    :param x_offset: If not zero, allows drawing each line starting at a particular X offset. Useful if &#xa;       you don't want it drawn at x=0. Must be >=0 &#xa;&#xa;    Returns tuple:&#xa;      string containing ANSI codes&#xa;      dict of form {'fg': (r,g,b,a), 'bg': (r,g,b,a)} holding current fg/bg color - suitable for passing as current_ansi_colors param&#xa;      dict of form {'x': <integer>, 'y': <integer>} holding final x,y cursor positions - x is absolute since \n sends it to 0. y is relative to incoming y (or 0 if none). Suitable for passing as current_cursor_pos param&#xa;    """"""&#xa;&#xa;    if get_pixel_func is None:&#xa;        get_pixel_func = lambda pixels, x, y: ("" "", pixels[x, y])      # just treat pixels as 2D array &#xa;&#xa;    # Compute ANSI bg color and strings we'll use to reset colors when moving to next line   &#xa;    if bgcolor_rgba is not None:&#xa;        bgcolor_ANSI = getANSIcolor_for_rgb(bgcolor_rgba)&#xa;        # Reset cur bg color to bgcolor because \n will fill the new line with this color&#xa;        bgcolor_ANSI_string = getANSIbgstring_for_ANSIcolor(bgcolor_ANSI)&#xa;    else:&#xa;        bgcolor_ANSI = None        &#xa;        # Reset cur bg color default because \n will fill the new line with this color (possibly only if BCE supported by terminal)&#xa;        bgcolor_ANSI_string = ""\x1b[49m""     # reset bg to default (if we want to support terminals that can't handle this will need to instead use 0m which clears fg too and then when using this reset prior_fg_color to None too&#xa;    &#xa;    # Do we know the current ANSI colors that have been set? &#xa;    if current_ansi_colors is not None:    &#xa;        string = """"&#xa;        prior_fg_color = current_ansi_colors['fg']       # Value of None is OK - means default&#xa;        prior_bg_color = current_ansi_colors['bg']       # Value of None is OK - means default&#xa;    else:&#xa;        # We don't know the current colors so output a reset to terminal defaults - we want to be in a known state&#xa;        # **SIZE - could suppress outputting this here, and remember that we have unknown (not same as default)&#xa;        # colors. Then when we need to output we can take this into account. If we wind up setting both fg and bg colors&#xa;        # for output (as for a non-space) then we'd never need to output the reset.  &#xa;        # I'm not going to implement this now since the better thing to do for repeated calls is to pass current_ansi_colors&#xa;        # so we'd never get to this case.&#xa;        string = ""\x1b[0m""          # removes all attributes (formatting and colors) to start in a known state&#xa;        prior_fg_color = None       # this is an ANSI color not rgba. None means default.&#xa;        prior_bg_color = None       # this is an ANSI color not rgba. None means default.&#xa;    &#xa;    # Do we know the cursor pos?&#xa;    if current_cursor_pos is not None:&#xa;        cursor_x = current_cursor_pos['x']&#xa;        cursor_y = current_cursor_pos['y']&#xa;    else:&#xa;        cursor_x = 0&#xa;        cursor_y = 0&#xa;&#xa;    for h in range(height):  &#xa;        for w in range(width):&#xa;&#xa;            draw_char, rgba = get_pixel_func(pixels, w, h)&#xa;&#xa;            # Handle fully or partially transparent pixels - but not if it is the special ""erase"" character (None)&#xa;            skip_pixel = False&#xa;            if draw_char is not None:&#xa;                alpha = rgba[3]&#xa;                if alpha == 0:&#xa;                    skip_pixel = True       # skip any full transparent pixel. Note that we don't output a bgcolor space (in specified or default cases). Why? In overdraw mode, that would be wrong since whatever is already drawn should show through. In non-overdraw, assumption is that any line we're drawing on has already been filled with bgcolor so lets not do extra output. If this was an issue in practice, could make it an option.                        &#xa;                elif alpha != 255 and bgcolor_rgba is not None:&#xa;                    rgba = alpha_blend(rgba, bgcolor_rgba)  # non-opaque so blend with specified bgcolor&#xa;                        &#xa;            if not skip_pixel:&#xa;&#xa;                this_pixel_str = """"&#xa;&#xa;                # Throw away alpha channel - can still have non-fully-opaque alpha value here if&#xa;                # bgcolor was partially transparent or if no bgcolor and not fully transparent&#xa;                # Could make argument to use threshold to decide if throw away (e.g. >50% transparent) &#xa;                # vs. consider opaque (e.g. <50% transparent) but at least for now we just throw it away &#xa;                # which means we treat the pixel as fully opaque.&#xa;                rgb = rgba[:3]   &#xa;                &#xa;                # If we've got the special ""erase"" character turn it into outputting a space using the bgcolor&#xa;                # which if None will just be a reset to default bg which is what we want&#xa;                if draw_char is None:&#xa;                    draw_char = "" ""&#xa;                    color = bgcolor_ANSI&#xa;                else:&#xa;                    # Convert from RGB to ansi color, using closest color. Conceivably we could optionally support&#xa;                    # dithering to spread the color error. Problematic when dealing with transparency (see cmt in dither_image_to_web_palette())&#xa;                    # or unknown/default bgcolor, and currently not worthwhile since either easy (img2txt) or more correct (graphics) to do &#xa;                    # dithering upstream. &#xa;                    color = getANSIcolor_for_rgb(rgb)&#xa;                &#xa;                    # Optimization - if we're drawing a space and the color is the same as a specified bg color&#xa;                    # then just skip this pixel. We need to make this check here because the conversion to ANSI above can &#xa;                    # cause colors that didn't match to now match&#xa;                    # We cannot do this optimization in overdraw mode because we cannot assume that the bg color&#xa;                    # is already drawn at this location. We could presumably pass in the known state of the screen&#xa;                    # and thus have this knoweldge if the optimization was worthwhile. &#xa;                    if not is_overdraw and (draw_char == "" "") and (color == bgcolor_ANSI):&#xa;                        skip_pixel = True&#xa;&#xa;                if not skip_pixel:&#xa;      &#xa;                    if len(draw_char) > 1:&#xa;                        raise ValueError(""Not allowing multicharacter draw strings"")&#xa;&#xa;                    # If we are not at the cursor location where we need to draw (happens if we skip pixels or lines)&#xa;                    # then output ANSI sequence to move cursor there.&#xa;                    # This is how we implement transparency - we don't draw spaces, we skip via cursor moves&#xa;                    # We take the x_offset (if any) into account here&#xa;                    ofsetted_w = x_offset + w&#xa;                    if (cursor_x != ofsetted_w) or (cursor_y != h):&#xa;                        string += generate_ANSI_to_move_cursor(cursor_x, cursor_y, ofsetted_w, h)&#xa;                        cursor_x = ofsetted_w&#xa;                        cursor_y = h&#xa;          &#xa;                    # Generate the ANSI sequences to set the colors the way we want them&#xa;                    if draw_char == "" "":&#xa;                &#xa;                        # **SIZE - If we are willing to assume terminals that support ECH (Erase Character) as specified&#xa;                        #   in here http://vt100.net/docs/vt220-rm/chapter4.html we could replace long runs of same-color&#xa;                        #   spaces with single ECH codes. Seems like it is only correct to do this if BCE is supported&#xa;                        #   though (http://superuser.com/questions/249898/how-can-i-prevent-os-x-terminal-app-from-overriding-vim-colours-on-a-remote-syst)&#xa;                        #   else ""erase"" would draw the _default_ background color not the currently set background color&#xa;                        #   Note that if we implement this by accumulating spaces (as opposed to lookahead), need to output that &#xa;                        #   before any different output be that a color change, or if we need to output a \n (if line ended &#xa;                        #   in same-color spaces in non-overdraw)&#xa;                                        &#xa;                        # We are supposed to output a space, so we're going to need to change the background color.&#xa;                        # No, we can't output an ""upper ascii"" character that fills the entire foreground - all terminals&#xa;                        # don't display such characters the same way, if at all. e.g. Mac terminal outputs ? for ""upper ascii"" chars&#xa;                        # Since we're outputting a space we can leave the prior fg color intact as it won't be used      &#xa;                        string += generate_ANSI_to_set_fg_bg_colors(prior_fg_color, prior_bg_color, prior_fg_color, color)&#xa;                        prior_bg_color = color&#xa;                    &#xa;                    else:&#xa;                        # We're supposed to output a non-space character, so we're going to need to change the foreground color&#xa;                        # and make sure the bg is set appropriately&#xa;                        string += generate_ANSI_to_set_fg_bg_colors(prior_fg_color, prior_bg_color, color, bgcolor_ANSI)&#xa;                        prior_fg_color = color&#xa;                        prior_bg_color = bgcolor_ANSI&#xa;&#xa;                    # Actually output the character&#xa;                    string += draw_char                               &#xa;&#xa;                    cursor_x = cursor_x + 1&#xa; &#xa;        # Handle end of line - unless last line which is NOP because we don't want to do anything to the _line after_ our drawing&#xa;        # and outputting \n would establish it and fill it&#xa;        if (h + 1) != height:&#xa;                      &#xa;            # Move to next line. If this establishes a new line in the terminal then it fills the _newly established line_&#xa;            # up to EOL with current bg color. Filling with the current bg color vs. default might be dependent on terminal's&#xa;            # support for BCE (Background Color Erase) - I'm not sure.&#xa;            # If cursor had been moved up and this just goes back down to an existing line, no filling occurs&#xa;            # In overdraw mode, we are going to assume we don't need to establish/fill a new line (which could be untrue&#xa;            # if we are overdrawing some lines but going further down too - if that becomes important can allow passing&#xa;            # in how many lines we can go down before hitting that). Next time we need to draw in overdraw mode we'll&#xa;            # move the cursor down as needed.&#xa;            if not is_overdraw:&#xa;&#xa;                # If not already desired color, reset bg color so \n fills with it&#xa;                # NOTE: it would be ideal to optionally dither the background color if it is not perfectly resolvable&#xa;                # in the palette we have to work with. However, we can't actually do this in the general case because&#xa;                # we don't know the width of the terminal (which can be different at display-time) and because we&#xa;                # don't always know the bg color (""default"" is not known by us, and not known by anybody until display-time) &#xa;                if prior_bg_color != bgcolor_ANSI:&#xa;                    string += bgcolor_ANSI_string;    &#xa;                    prior_bg_color = bgcolor_ANSI       &#xa;         &#xa;                # If the cursor is not at the correct y, move it there before outputting the newline&#xa;                # In current use this will only occur if current_cursor_pos includes a y offset and&#xa;                # the first line was entirely transparent. We pass 0/0 for cur/target x because no need&#xa;                # to adjust x as it will be changed by the \n &#xa;                if (cursor_y != h):&#xa;                    string += generate_ANSI_to_move_cursor(0, cursor_y, 0, h)&#xa;                    cursor_y = h&#xa;              &#xa;                string += ""\n""      &#xa;                cursor_y += 1&#xa;                cursor_x = 0        # we are assuming UNIX-style \n behavior - if it were windows we'd have to output \r to get cursor_x to 0&#xa;&#xa;    return string, {'fg': prior_fg_color, 'bg': prior_bg_color}, { 'x': cursor_x, 'y': cursor_y }&#xa;&#xa;&#xa;""""""&#xa;DESIGN NOTE (Global Optimization)&#xa;&#xa;The code in this file currently implements ""local optimization"" to minimize the cost of moving&#xa;the cursor around and changing colors. However, it always follows a top-to-bottom left-to-right&#xa;path. There are scenarios where choosing a different path would yield a more optimal result&#xa;(smaller output size). I have not bothered to implement any global optimization because I&#xa;think it will rarely produce a better output.&#xa;&#xa;Here's an example of a scenario where a global optimization of cursor movements that didn't just&#xa; go scanline by scanline top to bottom left to right would be a win:&#xa;&#xa;For example, assume this pattern is to be drawn, beginning at x=0 (SOL)&#xa;XXX      XXX&#xa;   XXX      XXX&#xa;      XXX      XXX              &#xa;Drawing it top down/left to right we must do 13 operations: &#xa;    XXX, move right, XXX, \n, move right, XXX, move right, XXX, \n, move right, XXX, move right, XXX  &#xa;Drawing it in an optimal sequence we can do 11 operations:&#xa;    XXX, move down, XXX, move down, XXX, move up, XXX, move down, XXX, move down, XXX&#xa;However, since \n is cheaper than move down, we actually would need blank lines between the XXX lines&#xa;to really make the second case smaller in terms of bytes (vs operations).&#xa;&#xa;The discussion above covers cursor changes, but of cours color changes play a role as well. If we were&#xa;to assume the XXX on the left were one color while the XXX on the right were another, we'd also save four&#xa;color change operations.&#xa;&#xa;To perfectly implement global optimization, you essentially need to solve a variant of the Traveling &#xa;Salesman Path Problem (TSPP) as I discuss here: http://stackoverflow.com/questions/20032768/graph-traversal-with-shortest-path-with-any-source/33601043#33601043&#xa;We could use the single fixed endpoint variant (P*s) from the Hoogeveen paper. Note that each character&#xa;we want to output is essentially a node in the graph, and the graph is fully connected (can move from&#xa;any character to any other via cursor moves, changing color as needed). Some edges are free (moving right&#xa;while outputting character of same color). It is actually an asymmetric TSP because there are cases&#xa;where e.g. moving right is free and moving left is not, and moving down to SOL via \n costs 1 while moving&#xa;back up to the x pos could be several bytes. Can solve asymmetric TSP via conversion to symmetric.&#xa;Solving a TSPP is generally computationally infeasible, so approximation algorithms such as Hoogeveen's are used.&#xa;Hoogeveen run O(n^3) so it too may be too slow. Can reduce n by combining runs of same color - I haven't bothered&#xa;to prove it but I believe that this does not harm the optimality of the result. Note that this does not reduce&#xa;the worst case n - you can a case where there are no such runs. I believe that there are faster algorithms&#xa;that provide worse (or zero) optimality guarantees - e.g. Lin Kernighan or nearest neighbor. These might be geared&#xa;to solve TSP vs TSPP - though a solution to TSP is also a solution to TSPP, just with the cycle completed and &#xa;no prescribed starting location. We would remove the cycle completing hop, and output a move to the chosen start&#xa;location as needed. The algorithms might also be adaptable to TSPP directly. &#xa;If TSPP solvers can never be made fast enough, heuristics can likely be employed to good effect.&#xa;Solutions from a TSPP solver might be a good way to find such heuristics.&#xa;&#xa;ANSI codes to save/restore cursor pos could open new vistas of global optimization since you can&#xa;restore x/y in only 3 bytes but they are seemingly not supported in Mac xterm so I don't use them.&#xa;""""""&#xa;"
279237|"#!/usr/bin/env python&#xa;&#xa;# Test whether a client produces a correct connect with a will, username and password.&#xa;&#xa;# The client should connect to port 1888 with keepalive=60, clean session set,&#xa;# client id 01-will-unpwd-set , will topic set to ""will-topic"", will payload&#xa;# set to ""will message"", will qos=2, will retain not set, username set to&#xa;# ""oibvvwqw"" and password set to ""#'^2hg9a&nm38*us"".&#xa;&#xa;import inspect&#xa;import os&#xa;import socket&#xa;import sys&#xa;&#xa;# From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder&#xa;cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],"".."")))&#xa;if cmd_subfolder not in sys.path:&#xa;    sys.path.insert(0, cmd_subfolder)&#xa;&#xa;import mosq_test&#xa;&#xa;rc = 1&#xa;keepalive = 60&#xa;connect_packet = mosq_test.gen_connect(""01-will-unpwd-set"",&#xa;        keepalive=keepalive, username=""oibvvwqw"", password=""#'^2hg9a&nm38*us"",&#xa;        will_topic=""will-topic"", will_qos=2, will_payload=""will message"")&#xa;&#xa;sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&#xa;sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)&#xa;sock.settimeout(10)&#xa;sock.bind(('', 1888))&#xa;sock.listen(5)&#xa;&#xa;client_args = sys.argv[1:]&#xa;env = dict(os.environ)&#xa;env['LD_LIBRARY_PATH'] = '../../lib:../../lib/cpp'&#xa;try:&#xa;    pp = env['PYTHONPATH']&#xa;except KeyError:&#xa;    pp = ''&#xa;env['PYTHONPATH'] = '../../lib/python:'+pp&#xa;client = mosq_test.start_client(filename=sys.argv[1].replace('/', '-'), cmd=client_args, env=env)&#xa;&#xa;try:&#xa;    (conn, address) = sock.accept()&#xa;    conn.settimeout(10)&#xa;&#xa;    if mosq_test.expect_packet(conn, ""connect"", connect_packet):&#xa;        rc = 0&#xa;&#xa;    conn.close()&#xa;finally:&#xa;    client.terminate()&#xa;    client.wait()&#xa;    sock.close()&#xa;&#xa;exit(rc)&#xa;&#xa;"
264224|"#!/usr/bin/env python&#xa;&#xa;""""""&#xa;Given an input path and an output path, will put&#xa;Gzipped versions of all files from the input path&#xa;to the output path.&#xa;&#xa;If the file is not gzippable it will be copied&#xa;uncompressed.&#xa;""""""&#xa;&#xa;from fnmatch import fnmatch&#xa;import gzip&#xa;import os&#xa;import shutil&#xa;import sys&#xa;&#xa;class FakeTime:&#xa;    def time(self):&#xa;        return 1261130520.0&#xa;&#xa;# Hack to override gzip's time implementation&#xa;# See: http://stackoverflow.com/questions/264224/setting-the-gzip-timestamp-from-python&#xa;gzip.time = FakeTime()&#xa;&#xa;def is_compressable(filename, gzip_globs):&#xa;    """"""&#xa;    Determine if a filename is a gzippable type&#xa;    by comparing to a known list.&#xa;    """"""&#xa;    return any([fnmatch(filename, glob) for glob in gzip_globs])&#xa;&#xa;def compress(file_path):&#xa;    """"""&#xa;    Gzip a single file in place.&#xa;    """"""&#xa;    f_in = open(file_path, 'rb')&#xa;    contents = f_in.readlines()&#xa;    f_in.close()&#xa;    f_out = gzip.open(file_path, 'wb')&#xa;    f_out.writelines(contents)&#xa;    f_out.close()&#xa;&#xa;def main():&#xa;    in_path = sys.argv[1]&#xa;    out_path = sys.argv[2]&#xa;&#xa;    with open('gzip_types.txt') as f:&#xa;        gzip_globs = [glob.strip() for glob in f]&#xa;&#xa;    # Folders&#xa;    if os.path.isdir(in_path):&#xa;        shutil.rmtree(out_path, ignore_errors=True)&#xa;        shutil.copytree(in_path, out_path)&#xa;&#xa;        for path, dirs, files in os.walk(sys.argv[2]):&#xa;            for filename in files:&#xa;                # Is it a gzippable file type?&#xa;                if not is_compressable(filename, gzip_globs):&#xa;                    continue&#xa;&#xa;                file_path = os.path.join(path, filename)&#xa;&#xa;                compress(file_path)&#xa;    # Single files&#xa;    else:&#xa;        filename = os.path.split(in_path)[-1]&#xa;&#xa;        try:&#xa;            os.remove(out_path)&#xa;        except OSError:&#xa;            pass&#xa;&#xa;        shutil.copy(in_path, out_path)&#xa;&#xa;        if not is_compressable(filename, gzip_globs):&#xa;            return &#xa;&#xa;        compress(out_path)&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    main()&#xa;"
13436167|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
3828723|"""""""&#xa;Python part of radio playout (pypo)&#xa;""""""&#xa;&#xa;from optparse import OptionParser&#xa;from datetime import datetime&#xa;&#xa;import telnetlib&#xa;&#xa;import time&#xa;import sys&#xa;import signal&#xa;import logging&#xa;import locale&#xa;import os&#xa;import re&#xa;&#xa;from Queue import Queue&#xa;from threading import Lock&#xa;&#xa;from pypopush import PypoPush&#xa;from pypofetch import PypoFetch&#xa;from pypofile import PypoFile&#xa;from recorder import Recorder&#xa;from listenerstat import ListenerStat&#xa;from pypomessagehandler import PypoMessageHandler&#xa;from pypoliquidsoap import PypoLiquidsoap&#xa;from timeout import ls_timeout&#xa;&#xa;from pypo.media.update.replaygainupdater import ReplayGainUpdater&#xa;from pypo.media.update.silananalyzer import SilanAnalyzer&#xa;&#xa;from configobj import ConfigObj&#xa;&#xa;# custom imports&#xa;from api_clients import api_client&#xa;#from std_err_override import LogWriter&#xa;import pure&#xa;&#xa;LOG_PATH = '/var/log/airtime/pypo/pypo.log'&#xa;LOG_LEVEL = logging.INFO&#xa;&#xa;# Set up command-line options&#xa;parser = OptionParser()&#xa;&#xa;# help screen / info&#xa;usage = ""%prog [options]"" + "" - python playout system""&#xa;parser = OptionParser(usage=usage)&#xa;&#xa;# Options&#xa;parser.add_option(""-v"", ""--compat"",&#xa;        help=""Check compatibility with server API version"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""check_compat"")&#xa;&#xa;parser.add_option(""-t"", ""--test"",&#xa;        help=""Do a test to make sure everything is working properly."",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""test"")&#xa;&#xa;parser.add_option(""-b"",&#xa;        ""--cleanup"",&#xa;        help=""Cleanup"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""cleanup"")&#xa;&#xa;parser.add_option(""-c"",&#xa;        ""--check"",&#xa;        help=""Check the cached schedule and exit"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""check"")&#xa;&#xa;# parse options&#xa;(options, args) = parser.parse_args()&#xa;&#xa;LIQUIDSOAP_MIN_VERSION = ""1.1.1""&#xa;&#xa;PYPO_HOME='/var/tmp/airtime/pypo/'&#xa;&#xa;def configure_environment():&#xa;    os.environ[""HOME""] = PYPO_HOME&#xa;    os.environ[""TERM""] = 'xterm'&#xa;&#xa;configure_environment()&#xa;&#xa;# need to wait for Python 2.7 for this..&#xa;logging.captureWarnings(True)&#xa;&#xa;# configure logging&#xa;try:&#xa;    # Set up logging&#xa;    logFormatter = logging.Formatter(""%(asctime)s [%(module)s] [%(levelname)-5.5s]  %(message)s"")&#xa;    rootLogger = logging.getLogger()&#xa;    rootLogger.setLevel(LOG_LEVEL)&#xa;    logger = rootLogger&#xa;&#xa;    consoleHandler = logging.StreamHandler()&#xa;    consoleHandler.setFormatter(logFormatter)&#xa;    rootLogger.addHandler(consoleHandler)&#xa;except Exception, e:&#xa;    print ""Couldn't configure logging"", e&#xa;    sys.exit(1)&#xa;&#xa;def configure_locale():&#xa;    """"""&#xa;    Silly hacks to force Python 2.x to run in UTF-8 mode. Not portable at all,&#xa;    however serves our purpose at the moment.&#xa;&#xa;    More information available here:&#xa;    http://stackoverflow.com/questions/3828723/why-we-need-sys-setdefaultencodingutf-8-in-a-py-script&#xa;    """"""&#xa;    logger.debug(""Before %s"", locale.nl_langinfo(locale.CODESET))&#xa;    current_locale = locale.getlocale()&#xa;&#xa;    if current_locale[1] is None:&#xa;        logger.debug(""No locale currently set. Attempting to get default locale."")&#xa;        default_locale = locale.getdefaultlocale()&#xa;&#xa;        if default_locale[1] is None:&#xa;            logger.debug(""No default locale exists. Let's try loading from \&#xa;                    /etc/default/locale"")&#xa;            if os.path.exists(""/etc/default/locale""):&#xa;                locale_config = ConfigObj('/etc/default/locale')&#xa;                lang = locale_config.get('LANG')&#xa;                new_locale = lang&#xa;            else:&#xa;                logger.error(""/etc/default/locale could not be found! Please \&#xa;                        run 'sudo update-locale' from command-line."")&#xa;                sys.exit(1)&#xa;        else:&#xa;            new_locale = default_locale&#xa;&#xa;        logger.info(""New locale set to: %s"", \&#xa;                locale.setlocale(locale.LC_ALL, new_locale))&#xa;&#xa;    reload(sys)&#xa;    sys.setdefaultencoding(""UTF-8"")&#xa;    current_locale_encoding = locale.getlocale()[1].lower()&#xa;    logger.debug(""sys default encoding %s"", sys.getdefaultencoding())&#xa;    logger.debug(""After %s"", locale.nl_langinfo(locale.CODESET))&#xa;&#xa;    if current_locale_encoding not in ['utf-8', 'utf8']:&#xa;        logger.error(""Need a UTF-8 locale. Currently '%s'. Exiting..."" % \&#xa;                current_locale_encoding)&#xa;        sys.exit(1)&#xa;&#xa;&#xa;configure_locale()&#xa;&#xa;# loading config file&#xa;try:&#xa;    config = ConfigObj('/etc/airtime/airtime.conf')&#xa;except Exception, e:&#xa;    logger.error('Error loading config file: %s', e)&#xa;    sys.exit(1)&#xa;&#xa;class Global:&#xa;    def __init__(self, api_client):&#xa;        self.api_client = api_client&#xa;&#xa;    def selfcheck(self):&#xa;        return self.api_client.is_server_compatible()&#xa;&#xa;    def test_api(self):&#xa;        self.api_client.test()&#xa;&#xa;def keyboardInterruptHandler(signum, frame):&#xa;    logger = logging.getLogger()&#xa;    logger.info('\nKeyboard Interrupt\n')&#xa;    sys.exit(0)&#xa;&#xa;@ls_timeout&#xa;def liquidsoap_get_info(telnet_lock, host, port, logger):&#xa;    logger.debug(""Checking to see if Liquidsoap is running"")&#xa;    try:&#xa;        telnet_lock.acquire()&#xa;        tn = telnetlib.Telnet(host, port)&#xa;        msg = ""version\n""&#xa;        tn.write(msg)&#xa;        tn.write(""exit\n"")&#xa;        response = tn.read_all()&#xa;    except Exception, e:&#xa;        logger.error(str(e))&#xa;        return None&#xa;    finally:&#xa;        telnet_lock.release()&#xa;&#xa;    return get_liquidsoap_version(response)&#xa;&#xa;def get_liquidsoap_version(version_string):&#xa;    m = re.match(r""Liquidsoap (\d+.\d+.\d+)"", version_string)&#xa;&#xa;    if m:&#xa;        return m.group(1)&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;    if m:&#xa;        current_version = m.group(1)&#xa;        return pure.version_cmp(current_version, LIQUIDSOAP_MIN_VERSION) >= 0&#xa;    return False&#xa;&#xa;def liquidsoap_startup_test():&#xa;&#xa;    liquidsoap_version_string = \&#xa;            liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;    while not liquidsoap_version_string:&#xa;        logger.warning(""Liquidsoap doesn't appear to be running!, "" + \&#xa;               ""Sleeping and trying again"")&#xa;        time.sleep(1)&#xa;        liquidsoap_version_string = \&#xa;                liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;&#xa;    while pure.version_cmp(liquidsoap_version_string, LIQUIDSOAP_MIN_VERSION) < 0:&#xa;        logger.warning(""Liquidsoap is running but in incorrect version! "" + \&#xa;                ""Make sure you have at least Liquidsoap %s installed"" % LIQUIDSOAP_MIN_VERSION)&#xa;        time.sleep(1)&#xa;        liquidsoap_version_string = \&#xa;                liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;&#xa;    logger.info(""Liquidsoap version string found %s"" % liquidsoap_version_string)&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    logger.info('###########################################')&#xa;    logger.info('#             *** pypo  ***               #')&#xa;    logger.info('#   Liquidsoap Scheduled Playout System   #')&#xa;    logger.info('###########################################')&#xa;&#xa;    #Although all of our calculations are in UTC, it is useful to know what timezone&#xa;    #the local machine is, so that we have a reference for what time the actual&#xa;    #log entries were made&#xa;    logger.info(""Timezone: %s"" % str(time.tzname))&#xa;    logger.info(""UTC time: %s"" % str(datetime.utcnow()))&#xa;&#xa;    signal.signal(signal.SIGINT, keyboardInterruptHandler)&#xa;&#xa;    api_client = api_client.AirtimeApiClient()&#xa;    g = Global(api_client)&#xa;&#xa;    while not g.selfcheck():&#xa;        time.sleep(5)&#xa;&#xa;    success = False&#xa;    while not success:&#xa;        try:&#xa;            api_client.register_component('pypo')&#xa;            success = True&#xa;        except Exception, e:&#xa;            logger.error(str(e))&#xa;            time.sleep(10)&#xa;&#xa;    telnet_lock = Lock()&#xa;&#xa;    ls_host = config['pypo']['ls_host']&#xa;    ls_port = config['pypo']['ls_port']&#xa;&#xa;    liquidsoap_startup_test()&#xa;&#xa;    if options.test:&#xa;        g.test_api()&#xa;        sys.exit(0)&#xa;&#xa;&#xa;    ReplayGainUpdater.start_reply_gain(api_client)&#xa;    SilanAnalyzer.start_silan(api_client, logger)&#xa;&#xa;    pypoFetch_q = Queue()&#xa;    recorder_q = Queue()&#xa;    pypoPush_q = Queue()&#xa;&#xa;    pypo_liquidsoap = PypoLiquidsoap(logger, telnet_lock,\&#xa;            ls_host, ls_port)&#xa;&#xa;    """"""&#xa;    This queue is shared between pypo-fetch and pypo-file, where pypo-file&#xa;    is the consumer. Pypo-fetch will send every schedule it gets to pypo-file&#xa;    and pypo will parse this schedule to determine which file has the highest&#xa;    priority, and retrieve it.&#xa;    """"""&#xa;    media_q = Queue()&#xa;&#xa;    # Pass only the configuration sections needed; PypoMessageHandler only needs rabbitmq settings&#xa;    pmh = PypoMessageHandler(pypoFetch_q, recorder_q, config['rabbitmq'])&#xa;    pmh.daemon = True&#xa;    pmh.start()&#xa;&#xa;    pfile = PypoFile(media_q, config['pypo'])&#xa;    pfile.daemon = True&#xa;    pfile.start()&#xa;&#xa;    pf = PypoFetch(pypoFetch_q, pypoPush_q, media_q, telnet_lock, pypo_liquidsoap, config['pypo'])&#xa;    pf.daemon = True&#xa;    pf.start()&#xa;&#xa;    pp = PypoPush(pypoPush_q, telnet_lock, pypo_liquidsoap, config['pypo'])&#xa;    pp.daemon = True&#xa;    pp.start()&#xa;&#xa;    recorder = Recorder(recorder_q)&#xa;    recorder.daemon = True&#xa;    recorder.start()&#xa;&#xa;    stat = ListenerStat()&#xa;    stat.daemon = True&#xa;    stat.start()&#xa;&#xa;    # Just sleep the main thread, instead of blocking on pf.join().&#xa;    # This allows CTRL-C to work!&#xa;    while True:&#xa;        time.sleep(1)&#xa;&#xa;    logger.info(""System exit"")&#xa;"
1160227|import os&#xa;&#xa;&#xa;# from https://stackoverflow.com/a/1160227&#xa;def touch(fname, times=None):&#xa;    with open(fname, 'a'):&#xa;        os.utime(fname, times)&#xa;
14381940|"""""""&#xa;Module to exercize virsh attach-device command with various devices/options&#xa;""""""&#xa;&#xa;import os&#xa;import os.path&#xa;import logging&#xa;import aexpect&#xa;&#xa;from string import ascii_lowercase&#xa;&#xa;from autotest.client.shared import error&#xa;&#xa;from virttest import virt_vm, virsh, remote, utils_misc, data_dir&#xa;from virttest.libvirt_xml.vm_xml import VMXML&#xa;from virttest.staging.backports import itertools&#xa;&#xa;from provider import libvirt_version&#xa;&#xa;# TODO: Move all these helper classes someplace else&#xa;&#xa;&#xa;class TestParams(object):&#xa;&#xa;    """"""&#xa;    Organize test parameters and decouple from params names&#xa;    """"""&#xa;&#xa;    def __init__(self, params, env, test, test_prefix='vadu_dev_obj_'):&#xa;        self.test_prefix = test_prefix&#xa;        self.test = test&#xa;        self.vmxml = None  # Can't be known yet&#xa;        self.virsh = None  # Can't be known yet&#xa;        self._e = env&#xa;        self._p = params&#xa;&#xa;    @property&#xa;    def start_vm(self):&#xa;        # Required parameter&#xa;        return bool('yes' == self._p['start_vm'])&#xa;&#xa;    @property&#xa;    def main_vm(self):&#xa;        # Required parameter&#xa;        return self._e.get_vm(self._p[""main_vm""])&#xa;&#xa;    @property&#xa;    def file_ref(self):&#xa;        default = ""normal""&#xa;        return self._p.get('vadu_file_ref', default)&#xa;&#xa;    @property&#xa;    def dom_ref(self):&#xa;        default = ""name""&#xa;        return self._p.get('vadu_dom_ref', default)&#xa;&#xa;    @property&#xa;    def dom_value(self):&#xa;        default = None&#xa;        return self._p.get('vadu_dom_value', default)&#xa;&#xa;    @property&#xa;    def extra(self):&#xa;        default = None&#xa;        return self._p.get('vadu_extra', default)&#xa;&#xa;    @property&#xa;    def status_error(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('status_error', default))&#xa;&#xa;    @property&#xa;    def mmconfig(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_config_option', default))&#xa;&#xa;    @property&#xa;    def preboot_function_error(self):&#xa;        return bool(""yes"" == self._p['vadu_preboot_function_error'])&#xa;&#xa;    @property&#xa;    def pstboot_function_error(self):&#xa;        return bool(""yes"" == self._p['vadu_pstboot_function_error'])&#xa;&#xa;    @property&#xa;    def domain_positional(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_domain_positional', default))&#xa;&#xa;    @property&#xa;    def file_positional(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_file_positional', default))&#xa;&#xa;    @property&#xa;    def devs(self):&#xa;        return self._p.objects('vadu_dev_objs')  # mandatory parameter&#xa;&#xa;    def dev_params(self, class_name):&#xa;        """"""&#xa;        Return Dictionary after parsing out prefix + class name postfix&#xa;&#xa;        e.g. vadu_dev_obj_meg_VirtualDisk = 100&#xa;             ^^^^^^^^^^^^^ ^  ^^^^^^^^^^    ^&#xa;        strip   prefix     |  classname     |&#xa;                           |                |&#xa;                           |                |&#xa;        Return        key--+         value--+&#xa;        """"""&#xa;&#xa;        # Roll up all keys with '_class_name' into top-level&#xa;        # keys with same name and no '_class_name' postfix.&#xa;        #       See Params.object_params() docstring&#xa;        object_params = self._p.object_params(class_name)&#xa;        # Return variable to hold modified key names&#xa;        device_params = {}&#xa;        for prefixed_key, original_value in object_params.items():&#xa;            # They get unrolled, but originals always left behind, skip them&#xa;            if prefixed_key.count(class_name):&#xa;                continue&#xa;            if prefixed_key.startswith(self.test_prefix):&#xa;                stripped_key = prefixed_key[len(self.test_prefix):]&#xa;                device_params[stripped_key] = original_value&#xa;        # The 'count' key required by all VADU AttachDeviceBase subclasses&#xa;        if 'count' not in device_params.keys():&#xa;            # stick prefix back on so error message has meaning&#xa;            raise error.TestError('%scount is a required parameter'&#xa;                                  % (self.test_prefix))&#xa;        return device_params&#xa;&#xa;    @staticmethod&#xa;    def cleanup(test_dev_list):&#xa;        xcpt_list = []&#xa;        for device in test_dev_list:&#xa;            try:&#xa;                device.cleanup()&#xa;                # Attempt to finish entire list before raising&#xa;                # any exceptions that occurred&#xa;            # ignore pylint W0703 - exception acumulated and raised below&#xa;            except Exception, xcept_obj:&#xa;                xcpt_list.append(xcept_obj)&#xa;        if xcpt_list:&#xa;            raise RuntimeError(""One or more exceptions occurred during ""&#xa;                               ""cleanup: %s"" % str(xcpt_list))&#xa;&#xa;&#xa;class TestDeviceBase(object):&#xa;&#xa;    """"""&#xa;    Base class for test devices creator and verification subclasses&#xa;    """"""&#xa;&#xa;    # Class-specific unique string&#xa;    identifier = None&#xa;    # Parameters that come in from Cartesian:&#xa;    count = 0  # number of devices to make/test&#xa;    # flag for use in test and by operate() & function() methods&#xa;    booted = False&#xa;&#xa;    def __init__(self, test_params):&#xa;        """"""&#xa;        Setup one or more device xml for a device based on TestParams instance&#xa;        """"""&#xa;        if self.__class__.identifier is None:&#xa;            identifier = utils_misc.generate_random_string(4)&#xa;            self.__class__.identifier = identifier&#xa;        # how many of this type of device to make&#xa;        self.test_params = test_params&#xa;        # Copy params for this class into attributes&#xa;        cls_name = self.__class__.__name__&#xa;        # Already have test-prefix stripped off&#xa;        for key, value in self.test_params.dev_params(cls_name).items():&#xa;            # Any keys with _anything are not used by this class&#xa;            if key.count('_') > 0:&#xa;                logging.debug(""Removing key: %s from params for class %s"",&#xa;                              test_params.test_prefix + key, cls_name)&#xa;                continue&#xa;            # Attempt to convert numbers&#xa;            try:&#xa;                setattr(self, key, int(value))&#xa;            except ValueError:&#xa;                setattr(self, key, value)&#xa;        if self.count < 1:&#xa;            raise error.TestError(""Configuration for class %s count must ""&#xa;                                  ""be specified and greater than zero"")&#xa;        logging.info(""Setting up %d %s device(s)"", self.count, cls_name)&#xa;        # Setup each device_xml instance&#xa;        self._device_xml_list = [self.init_device(index)&#xa;                                 # test_params.dev_params() enforces count&#xa;                                 for index in xrange(0, self.count)]&#xa;&#xa;    def cleanup(self):&#xa;        """"""&#xa;        Remove any temporary files or processes created for testing&#xa;        """"""&#xa;        pass&#xa;&#xa;    @property&#xa;    def device_xmls(self):&#xa;        """"""&#xa;        Return list of device_xml instances&#xa;        """"""&#xa;        return self._device_xml_list&#xa;&#xa;    @property&#xa;    def operation_results(self):&#xa;        """"""&#xa;        Return a list of True/False lists for operation state per device&#xa;        """"""&#xa;        return [self.operate(index) for index in xrange(self.count)]&#xa;&#xa;    @property&#xa;    def function_results(self):&#xa;        """"""&#xa;        Return a list of True/False lists for functional state per device&#xa;        """"""&#xa;        return [self.function(index) for index in xrange(self.count)]&#xa;&#xa;    @staticmethod&#xa;    def good_results(results_list):&#xa;        """"""&#xa;        Return True if all member lists contain only True values&#xa;        """"""&#xa;        for outer in results_list:&#xa;            for inner in outer:&#xa;                if inner is False:&#xa;                    return False&#xa;        return True&#xa;&#xa;    @staticmethod&#xa;    def bad_results(results_list):&#xa;        """"""&#xa;        Return True if all member lists contain only False values&#xa;        """"""&#xa;        for outer in results_list:&#xa;            for inner in outer:&#xa;                if inner is True:&#xa;                    return False&#xa;        return True&#xa;&#xa;    # These should be overridden in subclasses&#xa;    def init_device(self, index):&#xa;        """"""&#xa;        Initialize and return instance of device xml for index&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;    def operate(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of operating on a device&#xa;        """"""&#xa;        # N/B: Take care of self.started&#xa;        raise NotImplementedError&#xa;&#xa;    def function(self, index):&#xa;        """"""&#xa;        Return True/False device functioning&#xa;        """"""&#xa;        # N/B: Take care of self.test_params.start_vm&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;def make_vadu_dargs(test_params, xml_filepath):&#xa;    """"""&#xa;    Return keyword argument dict for virsh attach, detach, update functions&#xa;&#xa;    @param: test_params: a TestParams object&#xa;    @param: xml_filepath: Full path to device XML file (may not exist)&#xa;    """"""&#xa;    dargs = {}&#xa;    # Params value for domain reference (i.e. specific name, number, etc).&#xa;    if test_params.dom_value is None:  # No specific value set&#xa;        if test_params.dom_ref == ""name"":  # reference by runtime name&#xa;            domain = test_params.main_vm.name&#xa;        elif test_params.dom_ref == ""id"":&#xa;            domain = test_params.main_vm.get_id()&#xa;        elif test_params.dom_ref == ""uuid"":&#xa;            domain = test_params.main_vm.get_uuid()&#xa;        elif test_params.dom_ref == ""bad_domain_hex"":&#xa;            domain = ""0x%x"" % int(test_params.main_vm.get_id())&#xa;        elif test_params.dom_ref == ""none"":&#xa;            domain = None&#xa;        else:&#xa;            raise error.TestError(""Parameter vadu_dom_ref or ""&#xa;                                  ""vadu_dom_value are required"")&#xa;    else:  # Config. specified a vadu_dom_value&#xa;        domain = test_params.dom_value&#xa;&#xa;    if test_params.file_ref == ""normal"":  # The default&#xa;        file_value = xml_filepath  # Use un-altered path&#xa;    elif test_params.file_ref == ""empty"":  # empty string&#xa;        file_value = """"  # Empty string argument will be passed!&#xa;    elif test_params.file_ref == ""missing"":&#xa;        file_value = os.path.join(""path"", ""does"", ""not"", ""exist"")&#xa;    elif test_params.file_ref == ""none"":&#xa;        file_value = None  # No file specified&#xa;    else:&#xa;        raise error.TestError(""Parameter vadu_file_ref is reuqired"")&#xa;&#xa;    if test_params.domain_positional:  # boolean&#xa;        dargs['domainarg'] = domain&#xa;    else:&#xa;        dargs['domain_opt'] = domain&#xa;&#xa;    if test_params.file_positional:&#xa;        dargs['filearg'] = file_value&#xa;    else:&#xa;        dargs['file_opt'] = file_value&#xa;&#xa;    if test_params.mmconfig:&#xa;        dargs['flagstr'] = ""--config""&#xa;    else:&#xa;        dargs['flagstr'] = """"&#xa;&#xa;    if test_params.extra is not None:&#xa;        dargs['flagstr'] += "" %s"" % test_params.extra&#xa;    return dargs&#xa;&#xa;&#xa;class AttachDeviceBase(TestDeviceBase):&#xa;&#xa;    """"""&#xa;    All operation behavior is same  all device types in this module&#xa;    """"""&#xa;&#xa;    def operate(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of operating on a device&#xa;        """"""&#xa;        vadu_dargs = make_vadu_dargs(self.test_params,&#xa;                                     self.device_xmls[index].xml)&#xa;        # Acts as a dict for it's own API params&#xa;        self.test_params.virsh['debug'] = True&#xa;        vadu_dargs.update(self.test_params.virsh)&#xa;        options = vadu_dargs.get('flagstr')&#xa;        if options:&#xa;            opt_list = options.split()&#xa;            for opt in opt_list:&#xa;                if not virsh.has_command_help_match(""attach-device"", opt) and\&#xa;                   not self.test_params.status_error:&#xa;                    raise error.TestNAError(""Current libvirt version doesn't ""&#xa;                                            ""support '%s' for attach-device""&#xa;                                            "" command"" % opt)&#xa;        cmdresult = self.test_params.virsh.attach_device(**vadu_dargs)&#xa;        self.test_params.virsh['debug'] = False&#xa;        # Command success is not enough, must also confirm activity worked&#xa;&#xa;        # output XML no matter attach pass or not&#xa;        logging.debug(""Attached XML:"")&#xa;        for line in str(self.device_xmls[index]).splitlines():&#xa;            logging.debug(""%s"", line)&#xa;&#xa;        if (cmdresult.exit_status == 0):&#xa;            if (cmdresult.stdout.count('attached successfully') or&#xa;                    cmdresult.stderr.count('attached successfully')):&#xa;                return True&#xa;        else:&#xa;            # See analyze_negative_results - expects return of true&#xa;            if self.test_params.status_error:&#xa;                return True&#xa;            else:&#xa;                return False&#xa;&#xa;    # Overridden in classes below&#xa;    def init_device(self, index):&#xa;        raise NotImplementedError&#xa;&#xa;    def function(self, index):&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;class SerialFile(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simplistic File-backed isa-serial device test helper&#xa;&#xa;    Consumes Cartesian object parameters:&#xa;        count - number of devices to make&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    type_name = ""file""&#xa;&#xa;    def make_filepath(self, index):&#xa;        """"""Return full path to unique filename per device index""""""&#xa;        # auto-cleaned at end of test&#xa;        return os.path.join(data_dir.get_tmp_dir(), 'serial_%s_%s-%d.log'&#xa;                            % (self.type_name, self.identifier, index))&#xa;&#xa;    @staticmethod&#xa;    def make_source(filepath):&#xa;        """"""Create filepath on disk""""""&#xa;        open(filepath, ""wb"")&#xa;&#xa;    def init_device(self, index):&#xa;        filepath = self.make_filepath(index)&#xa;        self.make_source(filepath)&#xa;        serialclass = self.test_params.vmxml.get_device_class('serial')&#xa;        serial_device = serialclass(type_name=self.type_name,&#xa;                                    virsh_instance=self.test_params.virsh)&#xa;        serial_device.add_source(path=filepath)&#xa;        # Assume default domain serial device on port 0 and index starts at 0&#xa;        serial_device.add_target(port=str(index + 1))&#xa;        return serial_device&#xa;&#xa;    def cleanup(self):&#xa;        for index in xrange(0, self.count):&#xa;            try:&#xa;                os.unlink(self.make_filepath(index))&#xa;            except OSError:&#xa;                pass  # Don't care if not there&#xa;&#xa;    def function(self, index):&#xa;        # TODO: Try to read/write some serial data&#xa;        # Just a stub for now&#xa;        logging.info(""STUB: Serial device functional test passed: %s"",&#xa;                     str(not self.test_params.status_error))&#xa;        # Return an error if an error is expected&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class SerialPipe(SerialFile):&#xa;&#xa;    """"""&#xa;    Simplistic pipe-backed isa-serial device&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    type_name = ""pipe""&#xa;&#xa;    @staticmethod&#xa;    def make_source(filepath):&#xa;        try:&#xa;            os.unlink(filepath)&#xa;        except OSError:&#xa;            pass&#xa;        os.mkfifo(filepath)&#xa;&#xa;    def init_device(self, index):&#xa;        return super(SerialPipe, self).init_device(index)  # stub for now&#xa;&#xa;&#xa;class Console(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple console device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        consoleclass = self.test_params.vmxml.get_device_class('console')&#xa;        console_device = consoleclass(type_name=self.type,&#xa;                                      virsh_instance=self.test_params.virsh)&#xa;        # Assume default domain console device on port 0 and index starts at 0&#xa;        console_device.add_target(type=self.targettype, port=str(index + 1))&#xa;        return console_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class Channel(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple channel device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        channelclass = self.test_params.vmxml.get_device_class('channel')&#xa;        channel_device = channelclass(type_name=self.type,&#xa;                                      virsh_instance=self.test_params.virsh)&#xa;        if hasattr(self, 'sourcemode') and hasattr(self, 'sourcepath'):&#xa;            channel_device.add_source(mode=self.sourcemode,&#xa;                                      path=self.sourcepath)&#xa;        if hasattr(self, 'targettype') and hasattr(self, 'targetname'):&#xa;            channel_device.add_target(type=self.targettype,&#xa;                                      name=self.targetname)&#xa;        return channel_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class Controller(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple controller device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        controllerclass = self.test_params.vmxml.get_device_class('controller')&#xa;        controller_device = controllerclass(type_name=self.type,&#xa;                                            virsh_instance=self.test_params.virsh)&#xa;        controller_device.model = self.model&#xa;        return controller_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class VirtualDiskBasic(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple File-backed virtio/usb/sata/scsi/ide raw disk device&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    count = 0  # number of devices to make&#xa;    meg = 0  # size of device in megabytes (1024**2)&#xa;    devidx = 1  # devnode name index to start at (0 = vda/sda, 1 = vdb/sdb, etc)&#xa;    devtype = 'file'&#xa;    targetbus = ""virtio""&#xa;&#xa;    @staticmethod&#xa;    def devname_suffix(index):&#xa;        """"""&#xa;        Return letter code for index position, a, b, c...aa, ab, ac...&#xa;        """"""&#xa;        # http://stackoverflow.com/questions/14381940/&#xa;        # python-pair-alphabets-after-loop-is-completed/14382997#14382997&#xa;        def multiletters():&#xa;            """"""Generator of count-by-letter strings""""""&#xa;            for num in itertools.count(1):&#xa;                for prod in itertools.product(ascii_lowercase, repeat=num):&#xa;                    yield ''.join(prod)&#xa;        return itertools.islice(multiletters(), index, index + 1).next()&#xa;&#xa;    def devname(self, index):&#xa;        """"""&#xa;        Return disk target name&#xa;        """"""&#xa;        if self.targetbus in ['usb', 'scsi', 'sata']:&#xa;            devname_prefix = ""sd""&#xa;        elif self.targetbus == ""virtio"":&#xa;            devname_prefix = ""vd""&#xa;        elif self.targetbus == ""ide"":&#xa;            devname_prefix = ""hd""&#xa;        else:&#xa;            raise error.TestNAError(""Unsupport bus '%s' in this test"" %&#xa;                                    self.targetbus)&#xa;        return devname_prefix + self.devname_suffix(self.devidx + index)&#xa;&#xa;    def make_image_file_path(self, index):&#xa;        """"""Create backing file for test disk device""""""&#xa;        return os.path.join(data_dir.get_tmp_dir(),&#xa;                            'disk_%s_%s_%d.raw'&#xa;                            % (self.__class__.__name__,&#xa;                               self.identifier,&#xa;                               index))&#xa;&#xa;    def make_image_file(self, index):&#xa;        """"""Create sparse backing file by writing it's full path at it's end""""""&#xa;        # Truncate file&#xa;        image_file_path = self.make_image_file_path(index)&#xa;        image_file = open(image_file_path, 'wb')&#xa;        byte_size = self.meg * 1024 * 1024&#xa;        # Make sparse file byte_size long (starting from 0)&#xa;        image_file.truncate(byte_size)&#xa;        # Write simple unique data to file before end&#xa;        image_file.seek(byte_size - len(image_file_path) - 1)&#xa;        # newline required by aexpect in function()&#xa;        image_file.write(image_file_path + '\n')&#xa;        image_file.close()&#xa;&#xa;    def init_device(self, index):&#xa;        """"""&#xa;        Initialize and return instance of device xml for index&#xa;        """"""&#xa;        self.make_image_file(index)&#xa;        disk_class = self.test_params.vmxml.get_device_class('disk')&#xa;        disk_device = disk_class(type_name=self.devtype,&#xa;                                 virsh_instance=self.test_params.virsh)&#xa;        disk_device.driver = {'name': 'qemu', 'type': 'raw'}&#xa;        # No source elements by default&#xa;        source_properties = {'attrs':&#xa;                             {'file': self.make_image_file_path(index)}}&#xa;        source = disk_device.new_disk_source(**source_properties)&#xa;        disk_device.source = source  # Modified copy, not original&#xa;        dev_name = self.devname(index)&#xa;        disk_device.target = {'dev': dev_name, 'bus': self.targetbus}&#xa;        # libvirt will automatically add <address> element&#xa;        return disk_device&#xa;&#xa;    def cleanup(self):&#xa;        for index in xrange(0, self.count):&#xa;            try:&#xa;                os.unlink(self.make_image_file_path(index))&#xa;            except OSError:&#xa;                pass  # Don't care if not there&#xa;&#xa;    def function(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of a device functioning&#xa;        """"""&#xa;        dev_name = '/dev/' + self.devname(index)&#xa;        # Host image path is static known value&#xa;        test_data = self.make_image_file_path(index)&#xa;        byte_size = self.meg * 1024 * 1024&#xa;        # Place test data at end of device to also confirm sizing&#xa;        offset = byte_size - len(test_data)&#xa;        logging.info('Trying to read test data, %dth device %s, '&#xa;                     'at offset %d.', index + 1, dev_name, offset)&#xa;        session = None&#xa;&#xa;        # Since we know we're going to fail, no sense waiting for the&#xa;        # default timeout to expire or login()'s code to get_address&#xa;        # (currently 300 seconds) or any other timeout code.  With the&#xa;        # guest not alive, just return failure. Not doing so caused a&#xa;        # a 96 minute pause per test with 16 devices all waiting for&#xa;        # the timeouts to occur and probably a days worth of log messages&#xa;        # waiting for something to happen that can't.&#xa;        if not self.test_params.main_vm.is_alive():&#xa;            logging.debug(""VirtualDiskBasic functional test skipping login ""&#xa;                          ""vm is not alive."")&#xa;            return False&#xa;&#xa;        try:&#xa;            session = self.test_params.main_vm.login()&#xa;&#xa;            # The device may not be ready on guest,&#xa;            # just wait at most 5 seconds here&#xa;            utils_misc.wait_for(lambda:&#xa;                                not session.cmd_status('ls %s' % dev_name), 5)&#xa;&#xa;            # aexpect combines stdout + stderr, throw away stderr&#xa;            output = session.cmd_output('tail -c %d %s'&#xa;                                        % (len(test_data) + 1, dev_name))&#xa;            session.close()&#xa;        except (virt_vm.VMAddressError, remote.LoginError,&#xa;                aexpect.ExpectError, aexpect.ShellError):&#xa;            try:&#xa;                session.close()&#xa;            except AttributeError:&#xa;                pass   # session == None&#xa;            logging.debug(""VirtualDiskBasic functional test raised an exception"")&#xa;            return False&#xa;        else:&#xa;            gotit = bool(output.count(test_data))&#xa;            logging.info(""Test data detected in device: %s"",&#xa;                         gotit)&#xa;            if not gotit:&#xa;                logging.debug(""Expecting: '%s'"", test_data)&#xa;                logging.debug(""Received: '%s'"", output)&#xa;            return gotit&#xa;&#xa;&#xa;def operational_action(test_params, test_devices, operational_results):&#xa;    """"""&#xa;    Call & store result list from operate() method on every device&#xa;    """"""&#xa;    if test_params.status_error:&#xa;        logging.info(""Running operational tests: Failure is expected!"")&#xa;    else:&#xa;        logging.info(""Running operational tests"")&#xa;    for device in test_devices:&#xa;        operational_results.append(device.operation_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        operational_results = [False] * device.count&#xa;    #    else:&#xa;    #        operational_results = [True] * device.count&#xa;&#xa;&#xa;def preboot_action(test_params, test_devices, preboot_results):&#xa;    """"""&#xa;    Call & store result of function() method on every device&#xa;    """"""&#xa;    if test_params.preboot_function_error:&#xa;        logging.info(""Running pre-reboot functional tests: Failure expected!"")&#xa;    else:&#xa;        logging.info(""Running pre-reboot functional tests"")&#xa;    for device in test_devices:&#xa;        preboot_results.append(device.function_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        preboot_results = [False] * device.count&#xa;    #    else:&#xa;    #        preboot_results = [True] * device.count&#xa;&#xa;&#xa;def postboot_action(test_params, test_devices, pstboot_results):&#xa;    """"""&#xa;    Call & store result of function() method on every device&#xa;    """"""&#xa;    if test_params.pstboot_function_error:&#xa;        logging.info(""Running post-reboot functional tests: Failure expected!"")&#xa;    else:&#xa;        logging.info(""Running post-reboot functional tests"")&#xa;    for device in test_devices:&#xa;        pstboot_results.append(device.function_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        pstboot_results = [False] * device.count&#xa;    #    else:&#xa;    #        pstboot_results = [True] * device.count&#xa;&#xa;&#xa;# Save a little typing&#xa;all_true = TestDeviceBase.good_results&#xa;all_false = TestDeviceBase.bad_results&#xa;&#xa;&#xa;def analyze_negative_results(test_params, operational_results,&#xa;                             preboot_results, pstboot_results):&#xa;    """"""&#xa;    Analyze available results, return error message if fail&#xa;    """"""&#xa;    if operational_results:&#xa;        if not all_true(operational_results):&#xa;            return (""Negative testing operational test failed"")&#xa;    if preboot_results and test_params.start_vm:&#xa;        if not all_false(preboot_results):&#xa;            return (""Negative testing pre-boot functionality test passed"")&#xa;    if pstboot_results:&#xa;        if not all_false(pstboot_results):&#xa;            return (""Negative testing post-boot functionality ""&#xa;                    ""test passed"")&#xa;&#xa;&#xa;def analyze_positive_results(test_params, operational_results,&#xa;                             preboot_results, pstboot_results):&#xa;    """"""&#xa;    Analyze available results, return error message if fail&#xa;    """"""&#xa;    if operational_results:&#xa;        if not all_true(operational_results):&#xa;            return (""Positive operational test failed"")&#xa;    if preboot_results and test_params.start_vm:&#xa;        if not all_true(preboot_results):&#xa;            if not test_params.preboot_function_error:&#xa;                return (""Positive pre-boot functionality test failed"")&#xa;            # else: An error was expected&#xa;    if pstboot_results:&#xa;        if not all_true(pstboot_results):&#xa;            if not test_params.pstboot_function_error:&#xa;                return (""Positive post-boot functionality test failed"")&#xa;&#xa;&#xa;def analyze_results(test_params, operational_results=None,&#xa;                    preboot_results=None, pstboot_results=None):&#xa;    """"""&#xa;    Analyze available results, raise error message if fail&#xa;    """"""&#xa;    fail_msg = None  # Pass: None, Fail: failure reason&#xa;    if test_params.status_error:  # Negative testing&#xa;        fail_msg = analyze_negative_results(test_params, operational_results,&#xa;                                            preboot_results, pstboot_results)&#xa;    else:  # Positive testing&#xa;        fail_msg = analyze_positive_results(test_params, operational_results,&#xa;                                            preboot_results, pstboot_results)&#xa;    if fail_msg is not None:&#xa;        raise error.TestFail(fail_msg)&#xa;&#xa;&#xa;def run(test, params, env):&#xa;    """"""&#xa;    Test virsh {at|de}tach-device command.&#xa;&#xa;    1) Prepare test environment and its parameters&#xa;    2) Operate virsh on one or more devices&#xa;    3) Check functionality of each device&#xa;    4) Check functionality of mmconfig option&#xa;    5) Restore domain&#xa;    6) Handle results&#xa;    """"""&#xa;&#xa;    dev_obj = params.get(""vadu_dev_objs"")&#xa;    # Skip chardev hotplug on rhel6 host as it is not supported&#xa;    if ""Serial"" in dev_obj:&#xa;        if not libvirt_version.version_compare(1, 1, 0):&#xa;            raise error.TestNAError(""You libvirt version not supported""&#xa;                                    "" attach/detach Serial devices"")&#xa;&#xa;    logging.info(""Preparing initial VM state"")&#xa;    # Prepare test environment and its parameters&#xa;    test_params = TestParams(params, env, test)&#xa;    if test_params.start_vm:&#xa;        # Make sure VM is working&#xa;        test_params.main_vm.verify_alive()&#xa;        test_params.main_vm.wait_for_login().close()&#xa;    else:  # VM not suppose to be started&#xa;        if test_params.main_vm.is_alive():&#xa;            test_params.main_vm.destroy(gracefully=True)&#xa;    # Capture backup of original XML early in test&#xa;    test_params.vmxml = VMXML.new_from_inactive_dumpxml(&#xa;        test_params.main_vm.name)&#xa;    # All devices should share same access state&#xa;    test_params.virsh = virsh.Virsh(ignore_status=True)&#xa;    logging.info(""Creating %d test device instances"", len(test_params.devs))&#xa;    # Create test objects from cfg. class names via subclasses above&#xa;    test_devices = [globals()[class_name](test_params)  # instantiate&#xa;                    for class_name in test_params.devs]  # vadu_dev_objs&#xa;    operational_results = []&#xa;    preboot_results = []&#xa;    pstboot_results = []&#xa;    try:&#xa;        operational_action(test_params, test_devices, operational_results)&#xa;        # Fail early if attach-device return value is not expected&#xa;        analyze_results(test_params=test_params,&#xa;                        operational_results=operational_results)&#xa;&#xa;        #  Can't do functional testing with a cold VM, only test hot-attach&#xa;        preboot_action(test_params, test_devices, preboot_results)&#xa;&#xa;        logging.info(""Preparing test VM state for post-boot functional testing"")&#xa;        if test_params.start_vm:&#xa;            # Hard-reboot required&#xa;            test_params.main_vm.destroy(gracefully=True,&#xa;                                        free_mac_addresses=False)&#xa;        try:&#xa;            test_params.main_vm.start()&#xa;        except virt_vm.VMStartError:&#xa;            raise error.TestFail('VM Failed to start for some reason!')&#xa;        # Signal devices reboot is finished&#xa;        for test_device in test_devices:&#xa;            test_device.booted = True&#xa;        test_params.main_vm.wait_for_login().close()&#xa;        postboot_action(test_params, test_devices, pstboot_results)&#xa;        analyze_results(test_params=test_params,&#xa;                        preboot_results=preboot_results,&#xa;                        pstboot_results=pstboot_results)&#xa;    finally:&#xa;        logging.info(""Restoring VM from backup, then checking results"")&#xa;        test_params.main_vm.destroy(gracefully=False,&#xa;                                    free_mac_addresses=False)&#xa;        test_params.vmxml.undefine()&#xa;        test_params.vmxml.restore()  # Recover the original XML&#xa;        test_params.vmxml.define()&#xa;        if not test_params.start_vm:&#xa;            # Test began with not start_vm, shut it down.&#xa;            test_params.main_vm.destroy(gracefully=True)&#xa;        # Device cleanup can raise multiple exceptions, do it last:&#xa;        logging.info(""Cleaning up test devices"")&#xa;        test_params.cleanup(test_devices)&#xa;"
3876886|"# -*- coding: utf-8 -*-&#xa;##&#xa;## This file is part of Invenio.&#xa;## Copyright (C) 2008, 2009, 2010, 2011 CERN.&#xa;##&#xa;## Invenio is free software; you can redistribute it and/or&#xa;## modify it under the terms of the GNU General Public License as&#xa;## published by the Free Software Foundation; either version 2 of the&#xa;## License, or (at your option) any later version.&#xa;##&#xa;## Invenio is distributed in the hope that it will be useful, but&#xa;## WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;## General Public License for more details.&#xa;##&#xa;## You should have received a copy of the GNU General Public License&#xa;## along with Invenio; if not, write to the Free Software Foundation, Inc.,&#xa;## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.&#xa;&#xa;""""""&#xa;The shellutils module contains helper functions useful for interacting&#xa;with the operating system shell.&#xa;&#xa;The main API functions are:&#xa;   - run_shell_command()&#xa;""""""&#xa;&#xa;import os&#xa;import fcntl&#xa;import tempfile&#xa;import time&#xa;import signal&#xa;import select&#xa;from itertools import chain&#xa;from cStringIO import StringIO&#xa;import subprocess&#xa;&#xa;from invenio.config import CFG_MISCUTIL_DEFAULT_PROCESS_TIMEOUT&#xa;&#xa;__all__ = ['run_shell_command',&#xa;           'run_process_with_timeout',&#xa;           'Timeout',&#xa;           'split_cli_ids_arg']&#xa;&#xa;""""""&#xa;This module implements two functions:&#xa;    - L{run_shell_command}&#xa;    - L{run_process_with_timeout}&#xa;&#xa;L{run_shell_command} will run a command through a shell, capturing its&#xa;standard output and standard error.&#xa;&#xa;L{run_process_with_timeout} will run a process on its own allowing to&#xa;specify a input file, capturing the standard output and standard error and&#xa;killing the process after a given timeout.&#xa;""""""&#xa;&#xa;&#xa;class Timeout(Exception):&#xa;    """"""Exception raised by with_timeout() when the operation takes too long.&#xa;    """"""&#xa;    pass&#xa;&#xa;&#xa;def run_shell_command(cmd, args=None, filename_out=None, filename_err=None):&#xa;    """"""Run operating system command cmd with arguments from the args&#xa;    tuple in a sub-shell and return tuple (exit status code, stdout&#xa;    info, stderr info).&#xa;&#xa;    @param cmd: Command to execute in a shell; may contain %s&#xa;        placeholders for arguments that will be expanded from the args&#xa;        tuple. Example: cmd='echo %s', args = ('hello',).&#xa;    @type cmd: string&#xa;&#xa;    @param args: Arguments to be escaped and substituted for %s&#xa;        placeholders in cmd.&#xa;    @type args: tuple of strings&#xa;&#xa;    @param filename_out: Desired filename for stdout output&#xa;        (optional; see below).&#xa;    @type filename_out: string&#xa;&#xa;    @param filename_err: Desired filename for stderr output&#xa;        (optional; see below).&#xa;    @type filename_err: string&#xa;&#xa;    @return: Tuple (exit code, string containing stdout output buffer,&#xa;        string containing stderr output buffer).&#xa;&#xa;        However, if either filename_out or filename_err are defined,&#xa;        then the output buffers are not passed back but rather written&#xa;        into filename_out/filename_err pathnames.  This is useful for&#xa;        commands that produce big files, for which it is not practical&#xa;        to pass results back to the callers in a Python text buffer.&#xa;        Note that it is the client's responsibility to name these&#xa;        files in the proper fashion (e.g. to be unique) and to close&#xa;        these files after use.&#xa;    @rtype: (number, string, string)&#xa;&#xa;    @raise TypeError: if the number of args does not correspond to the&#xa;       number of placeholders in cmd.&#xa;&#xa;    @note: Uses temporary files to store out/err output, not pipes due&#xa;        to potential pipe race condition on some systems.  If either&#xa;        filename_out or filename_err are defined, then do not create&#xa;        temporary files, but store stdout or stderr output directly in&#xa;        these files instead, and do not delete them after execution.&#xa;    """"""&#xa;    # wash args value:&#xa;    if args:&#xa;        args = tuple(args)&#xa;    else:&#xa;        args = ()&#xa;    # construct command with argument substitution:&#xa;    try:&#xa;        cmd = cmd % tuple([escape_shell_arg(x) for x in args])&#xa;    except TypeError:&#xa;        # there were problems with %s and args substitution, so raise an error:&#xa;        raise&#xa;    cmd_out = ''&#xa;    cmd_err = ''&#xa;    # create files:&#xa;    if filename_out:&#xa;        cmd_out_fd = os.open(filename_out, os.O_CREAT, 0644)&#xa;        file_cmd_out = filename_out&#xa;    else:&#xa;        cmd_out_fd, file_cmd_out = \&#xa;                    tempfile.mkstemp(""invenio-shellutils-cmd-out"")&#xa;    if filename_err:&#xa;        cmd_err_fd = os.open(filename_err, os.O_CREAT, 0644)&#xa;        file_cmd_err = filename_err&#xa;    else:&#xa;        cmd_err_fd, file_cmd_err = \&#xa;                    tempfile.mkstemp(""invenio-shellutils-cmd-err"")&#xa;    # run command:&#xa;    cmd_exit_code = os.system(""%s > %s 2> %s"" % (cmd,&#xa;                                                 file_cmd_out,&#xa;                                                 file_cmd_err))&#xa;    # delete temporary files: (if applicable)&#xa;    if not filename_out:&#xa;        if os.path.exists(file_cmd_out):&#xa;            cmd_out_fo = open(file_cmd_out)&#xa;            cmd_out = cmd_out_fo.read()&#xa;            cmd_out_fo.close()&#xa;            os.remove(file_cmd_out)&#xa;    if not filename_err:&#xa;        if os.path.exists(file_cmd_err):&#xa;            cmd_err_fo = open(file_cmd_err)&#xa;            cmd_err = cmd_err_fo.read()&#xa;            cmd_err_fo.close()&#xa;            os.remove(file_cmd_err)&#xa;    os.close(cmd_out_fd)&#xa;    os.close(cmd_err_fd)&#xa;    # return results:&#xa;    return cmd_exit_code, cmd_out, cmd_err&#xa;&#xa;&#xa;def run_process_with_timeout(args, filename_in=None, filename_out=None, filename_err=None, cwd=None, timeout=CFG_MISCUTIL_DEFAULT_PROCESS_TIMEOUT, sudo=None):&#xa;    """"""Execute the specified process but within a certain timeout.&#xa;&#xa;    @param args: the actuall process. This should be a list of string as in:&#xa;         ['/usr/bin/foo', '--bar', 'baz']&#xa;    @type args: list of string&#xa;&#xa;    @param filename_in: the path to a file that should be provided as standard&#xa;        input to the process. If None this will default to /dev/null&#xa;    @type filename_in: string&#xa;&#xa;    @param filename_out: Desired filename for stdout output&#xa;        (optional; see below).&#xa;    @type filename_out: string&#xa;&#xa;    @param filename_err: Desired filename for stderr output&#xa;        (optional; see below).&#xa;    @type filename_err: string&#xa;&#xa;    @param cwd: the path from where to execute the process&#xa;    @type cwd: string&#xa;&#xa;    @param timeout: the timeout in seconds after which to consider the&#xa;        process execution as failed. a Timeout exception will be raised&#xa;    @type timeout: int&#xa;&#xa;    @param sudo: the optional name of the user under which to execute the&#xa;        process (by using sudo, without prompting for a password)&#xa;    @type sudo: string&#xa;&#xa;    @return: Tuple (exit code, string containing stdout output buffer,&#xa;        string containing stderr output buffer).&#xa;&#xa;        However, if either filename_out or filename_err are defined,&#xa;        then the output buffers are not passed back but rather written&#xa;        into filename_out/filename_err pathnames.  This is useful for&#xa;        commands that produce big files, for which it is not practical&#xa;        to pass results back to the callers in a Python text buffer.&#xa;        Note that it is the client's responsibility to name these&#xa;        files in the proper fashion (e.g. to be unique) and to close&#xa;        these files after use.&#xa;    @rtype: (number, string, string)&#xa;&#xa;    @raise Timeout: if the process does not terminate within the timeout&#xa;    """"""&#xa;    stdout = stderr = None&#xa;    if filename_in is not None:&#xa;        stdin = open(filename_in)&#xa;    else:&#xa;        ## FIXME: should use NUL on Windows&#xa;        stdin = open('/dev/null', 'r')&#xa;    if filename_out:&#xa;        stdout = open(filename_out, 'w')&#xa;    if filename_err:&#xa;        stderr = open(filename_err, 'w')&#xa;    tmp_stdout = StringIO()&#xa;    tmp_stderr = StringIO()&#xa;    if sudo is not None:&#xa;        args = ['sudo', '-u', sudo, '-S'] + list(args)&#xa;    ## See: <http://stackoverflow.com/questions/3876886/timeout-a-subprocess>&#xa;    process = subprocess.Popen(args, stdin=stdin, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, cwd=cwd, preexec_fn=os.setpgrp)&#xa;&#xa;    ## See: <http://stackoverflow.com/questions/375427/non-blocking-read-on-a-stream-in-python>&#xa;    fd = process.stdout.fileno()&#xa;    fl = fcntl.fcntl(fd, fcntl.F_GETFL)&#xa;    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)&#xa;    fd = process.stderr.fileno()&#xa;    fl = fcntl.fcntl(fd, fcntl.F_GETFL)&#xa;    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)&#xa;    fd_to_poll = [process.stdout, process.stderr]&#xa;    select_timeout = 0.5&#xa;    t1 = time.time()&#xa;    try:&#xa;        while process.poll() is None:&#xa;            if time.time() - t1 >= timeout:&#xa;                if process.stdin is not None:&#xa;                    process.stdin.close()&#xa;                time.sleep(1)&#xa;                if process.poll() is None:&#xa;                    ## See: <http://stackoverflow.com/questions/3876886/timeout-a-subprocess>&#xa;                    os.killpg(process.pid, signal.SIGTERM)&#xa;                    time.sleep(1)&#xa;                if process.poll() is None:&#xa;                    os.killpg(process.pid, signal.SIGKILL)&#xa;                try:&#xa;                    os.waitpid(process.pid, 0)&#xa;                except OSError:&#xa;                    pass&#xa;                raise Timeout()&#xa;            for fd in select.select(fd_to_poll, [], [], select_timeout)[0]:&#xa;                if fd == process.stdout:&#xa;                    buf = process.stdout.read(65536)&#xa;                    if stdout is None:&#xa;                        tmp_stdout.write(buf)&#xa;                    else:&#xa;                        stdout.write(buf)&#xa;                elif fd == process.stderr:&#xa;                    buf = process.stderr.read(65536)&#xa;                    if stderr is None:&#xa;                        tmp_stderr.write(buf)&#xa;                    else:&#xa;                        stderr.write(buf)&#xa;                else:&#xa;                    raise OSError(""fd %s is not a valid file descriptor"" % fd)&#xa;    finally:&#xa;        while True:&#xa;            ## Let's just read what is remaining to read.&#xa;            for fd in select.select(fd_to_poll, [], [], select_timeout)[0]:&#xa;                if fd == process.stdout:&#xa;                    buf = process.stdout.read(65536)&#xa;                    tmp_stdout.write(buf)&#xa;                    if stdout is not None:&#xa;                        stdout.write(buf)&#xa;                elif fd == process.stderr:&#xa;                    buf = process.stderr.read(65536)&#xa;                    tmp_stderr.write(buf)&#xa;                    if stderr is not None:&#xa;                        stderr.write(buf)&#xa;                else:&#xa;                    raise OSError(""fd %s is not a valid file descriptor"" % fd)&#xa;            else:&#xa;                break&#xa;    return process.poll(), tmp_stdout.getvalue(), tmp_stderr.getvalue()&#xa;&#xa;&#xa;def escape_shell_arg(shell_arg):&#xa;    """"""Escape shell argument shell_arg by placing it within&#xa;    single-quotes.  Any single quotes found within the shell argument&#xa;    string will be escaped.&#xa;&#xa;    @param shell_arg: The shell argument to be escaped.&#xa;    @type shell_arg: string&#xa;    @return: The single-quote-escaped value of the shell argument.&#xa;    @rtype: string&#xa;    @raise TypeError: if shell_arg is not a string.&#xa;    @see: U{http://mail.python.org/pipermail/python-list/2005-October/346957.html}&#xa;    """"""&#xa;    if type(shell_arg) is not str:&#xa;        msg = ""ERROR: escape_shell_arg() expected string argument but "" \&#xa;              ""got '%s' of type '%s'."" % (repr(shell_arg), type(shell_arg))&#xa;        raise TypeError(msg)&#xa;&#xa;    return ""'%s'"" % shell_arg.replace(""'"", r""'\''"")&#xa;&#xa;&#xa;def mymkdir(newdir, mode=0777):&#xa;    """"""works the way a good mkdir should :)&#xa;        - already exists, silently complete&#xa;        - regular file in the way, raise an exception&#xa;        - parent directory(ies) does not exist, make them as well&#xa;    """"""&#xa;    if os.path.isdir(newdir):&#xa;        pass&#xa;    elif os.path.isfile(newdir):&#xa;        raise OSError(""a file with the same name as the desired "" \&#xa;                      ""dir, '%s', already exists."" % newdir)&#xa;    else:&#xa;        head, tail = os.path.split(newdir)&#xa;        if head and not os.path.isdir(head):&#xa;            mymkdir(head, mode)&#xa;        if tail:&#xa;            os.umask(022)&#xa;            os.mkdir(newdir, mode)&#xa;&#xa;&#xa;def s(t):&#xa;    ## De-comment this to have lots of debugging information&#xa;    #print time.time(), t&#xa;    pass&#xa;&#xa;&#xa;def split_cli_ids_arg(value):&#xa;    """"""&#xa;    Split ids given in the command line&#xa;    Possible formats are:&#xa;    * 1&#xa;    * 1,2,3,4&#xa;    * 1-5,20,30,40&#xa;    Returns respectively&#xa;    * set([1])&#xa;    * set([1,2,3,4])&#xa;    * set([1,2,3,4,5,20,30,40])&#xa;    """"""&#xa;    def parse(el):&#xa;        el = el.strip()&#xa;        if not el:&#xa;            ret = []&#xa;        elif '-' in el:&#xa;            start, end = el.split('-', 1)&#xa;            ret = xrange(int(start), int(end) + 1)&#xa;        else:&#xa;            ret = [int(el)]&#xa;        return ret&#xa;    return set(chain(*(parse(c) for c in value.split(',') if c.strip())))&#xa;"
33175763|"#!/usr/bin/env python&#xa;# -*- coding: utf-8 -*-#&#xa;#&#xa;# Copyright (C) 2016, 2018 University of Zurich. All rights reserved.&#xa;#&#xa;#&#xa;# This program is free software; you can redistribute it and/or modify it&#xa;# under the terms of the GNU General Public License as published by the&#xa;# Free Software Foundation; either version 3 of the License, or (at your&#xa;# option) any later version.&#xa;#&#xa;# This program is distributed in the hope that it will be useful, but&#xa;# WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;# General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License along&#xa;# with this program; if not,&#xa;&#xa;__docformat__ = 'reStructuredText'&#xa;__author__ = 'Riccardo Murri <riccardo.murri@gmail.com>'&#xa;&#xa;# stdlib imports&#xa;from contextlib import contextmanager&#xa;import functools&#xa;import os&#xa;import re&#xa;import signal&#xa;import shutil&#xa;import sys&#xa;import tempfile&#xa;import time&#xa;import UserDict&#xa;&#xa;# 3rd party imports&#xa;import click&#xa;import netaddr&#xa;&#xa;&#xa;def confirm_or_abort(prompt, exitcode=os.EX_TEMPFAIL, msg=None, **extra_args):&#xa;    """"""&#xa;    Prompt user for confirmation and exit on negative reply.&#xa;&#xa;    Arguments `prompt` and `extra_args` will be passed unchanged to&#xa;    `click.confirm`:func: (which is used for actual prompting).&#xa;&#xa;    :param str prompt: Prompt string to display.&#xa;    :param int exitcode: Program exit code if negative reply given.&#xa;    :param str msg: Message to display before exiting.&#xa;    """"""&#xa;    if click.confirm(prompt, **extra_args):&#xa;        return True&#xa;    else:&#xa;        # abort&#xa;        if msg:&#xa;            sys.stderr.write(msg)&#xa;            sys.stderr.write('\n')&#xa;        sys.exit(exitcode)&#xa;&#xa;&#xa;@contextmanager&#xa;def environment(**kv):&#xa;    """"""&#xa;    Context manager to run Python code with a modified UNIX process environment.&#xa;&#xa;    All key/value pairs in the keyword arguments are added (or changed, if the&#xa;    key names an existing environmental variable) in the process environment&#xa;    upon entrance into the context. Changes are undone upon exit: added&#xa;    environmental variables are removed from the environment, and those whose&#xa;    value was changed are reset to their pristine value.&#xa;    """"""&#xa;    added = []&#xa;    changed = {}&#xa;    for key, value in kv.items():&#xa;        if key not in os.environ:&#xa;            added.append(key)&#xa;        else:&#xa;            changed[key] = os.environ[key]&#xa;        os.environ[key] = value&#xa;&#xa;    yield&#xa;&#xa;    # restore pristine process environment&#xa;    for key in added:&#xa;        del os.environ[key]&#xa;    for key in changed:&#xa;        os.environ[key] = changed[key]&#xa;&#xa;&#xa;def get_num_processors():&#xa;    """"""&#xa;    Return number of online processor cores.&#xa;    """"""&#xa;    # try different strategies and use first one that succeeeds&#xa;    try:&#xa;        return os.cpu_count()  # Py3 only&#xa;    except AttributeError:&#xa;        pass&#xa;    try:&#xa;        import multiprocessing&#xa;        return multiprocessing.cpu_count()&#xa;    except ImportError:  # no multiprocessing?&#xa;        pass&#xa;    except NotImplementedError:&#xa;        # multiprocessing cannot determine CPU count&#xa;        pass&#xa;    try:&#xa;        from subprocess32 import check_output&#xa;        ncpus = check_output('nproc')&#xa;        return int(ncpus)&#xa;    except CalledProcessError:  # no `/usr/bin/nproc`&#xa;        pass&#xa;    except (ValueError, TypeError):&#xa;        # unexpected output from `nproc`&#xa;        pass&#xa;    except ImportError:  # no subprocess32?&#xa;        pass&#xa;    try:&#xa;        from subprocess import check_output&#xa;        ncpus = check_output('nproc')&#xa;        return int(ncpus)&#xa;    except CalledProcessError:  # no `/usr/bin/nproc`&#xa;        pass&#xa;    except (ValueError, TypeError):&#xa;        # unexpected output from `nproc`&#xa;        pass&#xa;    except ImportError:  # no subprocess.check_call (Py 2.6)&#xa;        pass&#xa;    raise RuntimeError(""Cannot determine number of processors"")&#xa;&#xa;&#xa;def has_nested_keys(mapping, k1, *more):&#xa;    """"""&#xa;    Return ``True`` if `mapping[k1][k2]...[kN]` is valid.&#xa;&#xa;    Example::&#xa;&#xa;      >>> D = {&#xa;      ...   'a': {&#xa;      ...     'x':0,&#xa;      ...     'y':{&#xa;      ...       'z': 1,&#xa;      ...     },&#xa;      ...   },&#xa;      ...   'b': 3&#xa;      ... }&#xa;      >>> has_nested_keys(D, 'a', 'x')&#xa;      True&#xa;      >>> has_nested_keys(D, 'a', 'y', 'z')&#xa;      True&#xa;      >>> has_nested_keys(D, 'a', 'q')&#xa;      False&#xa;&#xa;    When a single key is passed, this is just another way of writing ``k1 in&#xa;    mapping``::&#xa;&#xa;      >>> has_nested_keys(D, 'b')&#xa;      True&#xa;    """"""&#xa;    if k1 in mapping:&#xa;        if more:&#xa;            return has_nested_keys(mapping[k1], *more)&#xa;        else:&#xa;            return True&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;class memoize(object):&#xa;    """"""&#xa;    Cache a function's return value each time it is called within a TTL.&#xa;&#xa;    If called within the TTL and the same arguments, the cached value is&#xa;    returned, If called outside the TTL or a different value, a fresh value is&#xa;    returned (and cached for future occurrences).&#xa;&#xa;    .. warning::&#xa;&#xa;      Only works on functions that take *no keyword arguments*.&#xa;&#xa;    Originally taken from: http://jonebird.com/2012/02/07/python-memoize-decorator-with-ttl-argument/&#xa;    """"""&#xa;    def __init__(self, ttl):&#xa;        self.cache = {}&#xa;        self.ttl = ttl&#xa;&#xa;    def __call__(self, f):&#xa;        @functools.wraps(f)&#xa;        def wrapped_f(*args):&#xa;            now = time.time()&#xa;            try:&#xa;                value, last_update = self.cache[args]&#xa;                if self.ttl > 0 and now - last_update > self.ttl:&#xa;                    raise AttributeError&#xa;                return value&#xa;            except (KeyError, AttributeError):&#xa;                value = f(*args)&#xa;                self.cache[args] = (value, now)&#xa;                return value&#xa;            except TypeError:&#xa;                # uncachable -- for instance, passing a list as an argument.&#xa;                # Better to not cache than to blow up entirely.&#xa;                return f(*args)&#xa;        return wrapped_f&#xa;&#xa;&#xa;# this is very liberal, in that it will accept malformed address&#xa;# strings like `0:::1` or '0::1::2', but we are going to do validation&#xa;# with `netaddr.IPAddress` later on so there is little advantage in&#xa;# being strict here&#xa;_IPV6_FRAG = r'[0-9a-z:]+'&#xa;&#xa;# likewise&#xa;_IPV4_FRAG = r'[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+'&#xa;&#xa;# should match a network interface name (for which there is no&#xa;# standard, so let's just assume it's alphanumeric)&#xa;_IFACE_FRAG = r'[a-z][0-9a-z]*'&#xa;&#xa;# XXX: order is important! IPv4 must come before IPv6 otherwise the&#xa;# _IPV6_FRAG will match a *part* of an IPv4 adress....&#xa;_IP_ADDRESS_RE = [&#xa;    # IPv4 literal, optionally with port&#xa;    re.compile(&#xa;        r'(?P<ip_addr>{0})(?P<port>:\d+)?'&#xa;        .format(_IPV4_FRAG), re.I),&#xa;&#xa;    # the kind of IPv6 literals returned by Azure, e.g., `[fe80::dead:beef%eth0]:2222`&#xa;    re.compile(&#xa;        r'\[(?P<ip_addr>{0})(?P<iface>%{1})?\](?P<port>:\d+)?'&#xa;        .format(_IPV6_FRAG, _IFACE_FRAG), re.I),&#xa;&#xa;    # IPv6 literal possibly with interface spec (note this cannot provide any port)&#xa;    re.compile(&#xa;        r'(?P<ip_addr>{0})(?P<iface>%{1})?'&#xa;        .format(_IPV6_FRAG, _IFACE_FRAG), re.I),&#xa;]&#xa;&#xa;def parse_ip_address_and_port(addr, default_port=22):&#xa;    """"""&#xa;    Return a pair (IP address, port) extracted from string `addr`.&#xa;&#xa;    Different formats are accepted for the address/port string:&#xa;&#xa;    * IPv6 literals in square brackets, with or without an optional&#xa;      port specification, as used in URLs::&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef]:1234')&#xa;        (IPAddress('fe80::dead:beef'), 1234)&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef]')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;    * IPv6 literals with a ""local interface"" specification::&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef%eth0]')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef%eth0')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;    * bare IPv6 addresses::&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('2001:db8:5ca1:1f0:f816:3eff:fe05:f40f')&#xa;        (IPAddress('2001:db8:5ca1:1f0:f816:3eff:fe05:f40f'), 22)&#xa;&#xa;    * IPv4 addresses, with or without an additional port specification::&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123')&#xa;        (IPAddress('192.0.2.123'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123:999')&#xa;        (IPAddress('192.0.2.123'), 999)&#xa;&#xa;    Note that the default port can be changed by passing an additional parameter::&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123', 987)&#xa;        (IPAddress('192.0.2.123'), 987)&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef', 987)&#xa;        (IPAddress('fe80::dead:beef'), 987)&#xa;&#xa;    :raise netaddr.AddrFormatError: Upon parse failure, e.g., syntactically incorrect IP address.&#xa;    """"""&#xa;    # we assume one and only one of the regexps will match&#xa;    for regexp in _IP_ADDRESS_RE:&#xa;        match = regexp.search(addr)&#xa;        if not match:&#xa;            continue&#xa;        # can raise netaddr.AddrFormatError&#xa;        ip_addr = netaddr.IPAddress(match.group('ip_addr'))&#xa;        try:&#xa;            port = match.group('port')&#xa;        except IndexError:&#xa;            port = None&#xa;        if port is not None:&#xa;            port = int(port[1:])  # skip leading `:`&#xa;        else:&#xa;            port = default_port&#xa;        return ip_addr, port&#xa;    # parse failed&#xa;    raise netaddr.AddrFormatError(&#xa;        ""Could not extract IP address and port from `{1}`""&#xa;        .format(addr))&#xa;&#xa;&#xa;# copied over from GC3Pie's `utils.py`&#xa;def string_to_boolean(word):&#xa;    """"""&#xa;    Convert `word` to a Python boolean value and return it.&#xa;    The strings `true`, `yes`, `on`, `1` (with any&#xa;    capitalization and any amount of leading and trailing&#xa;    spaces) are recognized as meaning Python `True`::&#xa;&#xa;      >>> string_to_boolean('yes')&#xa;      True&#xa;      >>> string_to_boolean('Yes')&#xa;      True&#xa;      >>> string_to_boolean('YES')&#xa;      True&#xa;      >>> string_to_boolean(' 1 ')&#xa;      True&#xa;      >>> string_to_boolean('True')&#xa;      True&#xa;      >>> string_to_boolean('on')&#xa;      True&#xa;&#xa;    Any other word is considered as boolean `False`::&#xa;&#xa;      >>> string_to_boolean('no')&#xa;      False&#xa;      >>> string_to_boolean('No')&#xa;      False&#xa;      >>> string_to_boolean('Nay!')&#xa;      False&#xa;      >>> string_to_boolean('woo-hoo')&#xa;      False&#xa;&#xa;    This includes also the empty string and whitespace-only::&#xa;&#xa;      >>> string_to_boolean('')&#xa;      False&#xa;      >>> string_to_boolean('  ')&#xa;      False&#xa;&#xa;    """"""&#xa;    if word.strip().lower() in ['true', 'yes', 'on', '1']:&#xa;        return True&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;class Struct(object, UserDict.DictMixin):&#xa;    """"""&#xa;    A `dict`-like object, whose keys can be accessed with the usual&#xa;    '[...]' lookup syntax, or with the '.' get attribute syntax.&#xa;&#xa;    Examples::&#xa;      >>> a = Struct()&#xa;      >>> a['x'] = 1&#xa;      >>> a.x&#xa;      1&#xa;      >>> a.y = 2&#xa;      >>> a['y']&#xa;      2&#xa;&#xa;    Values can also be initially set by specifying them as keyword&#xa;    arguments to the constructor::&#xa;&#xa;      >>> a = Struct(z=3)&#xa;      >>> a['z']&#xa;      3&#xa;      >>> a.z&#xa;      3&#xa;&#xa;    Like `dict` instances, `Struct`s have a `copy` method to get a&#xa;    shallow copy of the instance:&#xa;&#xa;      >>> b = a.copy()&#xa;      >>> b.z&#xa;      3&#xa;&#xa;    .. note::&#xa;&#xa;      This class is a clone of the `gc3libs.utils.Struct` class&#xa;      from the `GC3Pie package sources <https://github.com/uzh/gc3pie>`_&#xa;    """"""&#xa;    def __init__(self, initializer=None, **extra):&#xa;        if initializer is not None:&#xa;            try:&#xa;                # initializer is `dict`-like?&#xa;                for name, value in initializer.items():&#xa;                    self[name] = value&#xa;            except AttributeError:&#xa;                # initializer is a sequence of (name,value) pairs?&#xa;                for name, value in initializer:&#xa;                    self[name] = value&#xa;        for name, value in extra.items():&#xa;            self[name] = value&#xa;&#xa;    def copy(self):&#xa;        """"""Return a (shallow) copy of this `Struct` instance.""""""&#xa;        return Struct(self)&#xa;&#xa;    # the `DictMixin` class defines all std `dict` methods, provided&#xa;    # that `__getitem__`, `__setitem__` and `keys` are defined.&#xa;    def __setitem__(self, name, val):&#xa;        self.__dict__[name] = val&#xa;&#xa;    def __getitem__(self, name):&#xa;        return self.__dict__[name]&#xa;&#xa;    def keys(self):&#xa;        return self.__dict__.keys()&#xa;&#xa;&#xa;@contextmanager&#xa;def sighandler(signum, handler):&#xa;    """"""&#xa;    Context manager to run code with UNIX signal `signum` bound to `handler`.&#xa;&#xa;    The existing handler is saved upon entering the context and restored upon&#xa;    exit.&#xa;&#xa;    The `handler` argument may be anything that can be passed to Python's&#xa;    `signal.signal <https://docs.python.org/2/library/signal.html#signal.signal>`_&#xa;    standard library call.&#xa;    """"""&#xa;    prev_handler = signal.getsignal(signum)&#xa;    signal.signal(signum, handler)&#xa;    yield&#xa;    signal.signal(signum, prev_handler)&#xa;&#xa;&#xa;@contextmanager&#xa;def temporary_dir(delete=True, dir=None,&#xa;                  prefix='elasticluster.', suffix='.d'):&#xa;    """"""&#xa;    Make a temporary directory and make it current for the code in this context.&#xa;&#xa;    Delete temporary directory upon exit from the context, unless&#xa;    ``delete=False`` is passed in the arguments.&#xa;&#xa;    Arguments *suffix*, *prefix* and *dir* are exactly as in&#xa;    :func:`tempfile.mkdtemp()` (but have different defaults).&#xa;    """"""&#xa;    cwd = os.getcwd()&#xa;    tmpdir = tempfile.mkdtemp(suffix, prefix, dir)&#xa;    os.chdir(tmpdir)&#xa;    yield&#xa;    os.chdir(cwd)&#xa;    if delete:&#xa;        shutil.rmtree(tmpdir, ignore_errors=True)&#xa;&#xa;&#xa;@contextmanager&#xa;def timeout(delay, handler=None):&#xa;    """"""&#xa;    Context manager to run code and deliver a SIGALRM signal after `delay` seconds.&#xa;&#xa;    Note that `delay` must be a whole number; otherwise it is converted to an&#xa;    integer by Python's `int()` built-in function. For floating-point numbers,&#xa;    that means rounding off to the nearest integer from below.&#xa;&#xa;    If the optional argument `handler` is supplied, it must be a callable that&#xa;    is invoked if the alarm triggers while the code is still running. If no&#xa;    `handler` is provided (default), then a `RuntimeError` with message&#xa;    ``Timeout`` is raised.&#xa;    """"""&#xa;    delay = int(delay)&#xa;    if handler is None:&#xa;        def default_handler(signum, frame):&#xa;            raise RuntimeError(""{:d} seconds timeout expired"".format(delay))&#xa;        handler = default_handler&#xa;    prev_sigalrm_handler = signal.getsignal(signal.SIGALRM)&#xa;    signal.signal(signal.SIGALRM, handler)&#xa;    signal.alarm(delay)&#xa;    yield&#xa;    signal.alarm(0)&#xa;    signal.signal(signal.SIGALRM, prev_sigalrm_handler)&#xa;&#xa;&#xa;## Warnings redirection&#xa;#&#xa;# This is a modified version of the `logging.captureWarnings()` code from&#xa;# the Python 2.7 standard library:&#xa;#&#xa;# - backport the code to Python 2.6&#xa;# - make the logger configurable&#xa;#&#xa;# The original copyright notice is reproduced below:&#xa;#&#xa;#   Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.&#xa;#&#xa;#   Permission to use, copy, modify, and distribute this software and its&#xa;#   documentation for any purpose and without fee is hereby granted,&#xa;#   provided that the above copyright notice appear in all copies and that&#xa;#   both that copyright notice and this permission notice appear in&#xa;#   supporting documentation, and that the name of Vinay Sajip&#xa;#   not be used in advertising or publicity pertaining to distribution&#xa;#   of the software without specific, written prior permission.&#xa;#   VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING&#xa;#   ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL&#xa;#   VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR&#xa;#   ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER&#xa;#   IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT&#xa;#   OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.&#xa;#&#xa;&#xa;import logging&#xa;# ensure that `logging.NullHandler` is defined on Python 2.6 as well;&#xa;# see: http://stackoverflow.com/questions/33175763/how-to-use-logging-nullhandler-in-python-2-6&#xa;try:&#xa;    logging.NullHandler&#xa;except AttributeError:&#xa;    class _NullHandler(logging.Handler):&#xa;        def emit(self, record):&#xa;            pass&#xa;    logging.NullHandler = _NullHandler&#xa;&#xa;import warnings&#xa;&#xa;_warnings_showwarning = None&#xa;&#xa;&#xa;class _WarningsLogger(object):&#xa;    """"""&#xa;    Redirect warning messages to a chosen logger.&#xa;&#xa;    This is a callable object that implements a compatible interface&#xa;    to `warnings.showwarning` (which it is supposed to replace).&#xa;    """"""&#xa;&#xa;    def __init__(self, logger_name, format_warning=warnings.formatwarning):&#xa;        self._logger = logging.getLogger(logger_name)&#xa;        if not self._logger.handlers:&#xa;            self._logger.addHandler(logging.NullHandler())&#xa;        self._format_warning = format_warning&#xa;&#xa;    def __call__(self, message, category, filename, lineno, file=None, line=None):&#xa;        """"""&#xa;        Implementation of showwarnings which redirects to logging, which will first&#xa;        check to see if the file parameter is None. If a file is specified, it will&#xa;        delegate to the original warnings implementation of showwarning. Otherwise,&#xa;        it will call warnings.formatwarning and will log the resulting string to a&#xa;        warnings logger named ""py.warnings"" with level logging.WARNING.&#xa;        """"""&#xa;        if file is not None:&#xa;            assert _warnings_showwarning is not None&#xa;            _warnings_showwarning(message, category, filename, lineno, file, line)&#xa;        else:&#xa;            self._logger.warning(&#xa;                self._format_warning(message, category, filename, lineno))&#xa;&#xa;&#xa;def format_warning_oneline(message, category, filename, lineno,&#xa;                           file=None, line=None):&#xa;    """"""&#xa;    Format a warning for logging.&#xa;&#xa;    The returned value should be a single-line string, for better&#xa;    logging style (although this is not enforced by the code).&#xa;&#xa;    This methods' arguments have the same meaning of the&#xa;    like-named arguments from `warnings.formatwarning`.&#xa;    """"""&#xa;    # `warnings.formatwarning` produces multi-line output that does&#xa;    # not look good in a log file, so let us replace it with something&#xa;    # simpler...&#xa;    return ('{category}: {message}'&#xa;            .format(message=message, category=category.__name__))&#xa;&#xa;&#xa;def redirect_warnings(capture=True, logger='py.warnings'):&#xa;    """"""&#xa;    If capture is true, redirect all warnings to the logging package.&#xa;    If capture is False, ensure that warnings are not redirected to logging&#xa;    but to their original destinations.&#xa;    """"""&#xa;    global _warnings_showwarning&#xa;    if capture:&#xa;        assert _warnings_showwarning is None&#xa;        _warnings_showwarning = warnings.showwarning&#xa;        # `warnings.showwarning` must be a function, a generic&#xa;        # callable object is not accepted ...&#xa;        warnings.showwarning = _WarningsLogger(logger, format_warning_oneline).__call__&#xa;    else:&#xa;        assert _warnings_showwarning is not None&#xa;        warnings.showwarning = _warnings_showwarning&#xa;        _warnings_showwarning = None&#xa;"
24770130|"import json&#xa;import os&#xa;&#xa;from django.conf import settings&#xa;from django.core.mail import send_mail&#xa;from django.db.models import Count&#xa;from django.http import HttpResponseRedirect, Http404&#xa;from django.http import HttpResponse&#xa;from django.shortcuts import render_to_response, get_object_or_404&#xa;from django.template import RequestContext&#xa;from django.template import loader, Context&#xa;from django.views.decorators.cache import cache_page, never_cache&#xa;&#xa;from alert.audio.models import Audio&#xa;from alert.custom_filters.decorators import check_honeypot&#xa;from alert.lib import magic&#xa;from alert.lib import search_utils&#xa;from alert.lib.bot_detector import is_bot&#xa;from alert.lib.sunburnt import sunburnt&#xa;from alert.search.models import Court, Document&#xa;from alert.search.forms import SearchForm&#xa;from alert.simple_pages.forms import ContactForm&#xa;from alert.stats import tally_stat&#xa;&#xa;&#xa;def about(request):&#xa;    """"""Loads the about page""""""&#xa;    return render_to_response(&#xa;        'simple_pages/about.html',&#xa;        {'private': False},&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def faq(request):&#xa;    """"""Loads the FAQ page""""""&#xa;    scraped_court_count = Court.objects.filter(&#xa;        in_use=True,&#xa;        has_opinion_scraper=True&#xa;    ).count()&#xa;    conn = sunburnt.SolrInterface(settings.SOLR_OPINION_URL, mode='r')&#xa;    response = conn.raw_query(&#xa;        **search_utils.build_total_count_query()).execute()&#xa;    total_opinion_count = response.result.numFound&#xa;    return contact(&#xa;        request,&#xa;        template_path='simple_pages/faq.html',&#xa;        template_data={&#xa;            'scraped_court_count': scraped_court_count,&#xa;            'total_opinion_count': total_opinion_count,&#xa;        },&#xa;        initial={'subject': 'FAQs'},&#xa;    )&#xa;&#xa;&#xa;def build_court_dicts(courts):&#xa;    """"""Takes the court objects, and manipulates them into a list of more useful&#xa;    dictionaries""""""&#xa;    court_dicts = [{'pk': 'all',&#xa;                    'short_name': u'All Courts'}]&#xa;    court_dicts.extend([{'pk': court.pk,&#xa;                         'short_name': court.full_name, }&#xa;                        #'notes': court.notes}&#xa;                        for court in courts])&#xa;    return court_dicts&#xa;&#xa;&#xa;def coverage_graph(request):&#xa;    courts = Court.objects.filter(in_use=True)&#xa;    courts_json = json.dumps(build_court_dicts(courts))&#xa;&#xa;    search_form = SearchForm(request.GET)&#xa;    precedential_statuses = [field for field in&#xa;        search_form.fields.iterkeys() if field.startswith('stat_')]&#xa;&#xa;    # Build up the sourcing stats.&#xa;    counts = Document.objects.values('source').annotate(Count('source'))&#xa;    count_pro = 0&#xa;    count_lawbox = 0&#xa;    count_scraper = 0&#xa;    for d in counts:&#xa;        if 'R' in d['source']:&#xa;            count_pro += d['source__count']&#xa;        if 'C' in d['source']:&#xa;            count_scraper += d['source__count']&#xa;        if 'L' in d['source']:&#xa;            count_lawbox += d['source__count']&#xa;&#xa;    opinion_courts = Court.objects.filter(&#xa;        in_use=True,&#xa;        has_opinion_scraper=True)&#xa;    oral_argument_courts = Court.objects.filter(&#xa;        in_use=True,&#xa;        has_oral_argument_scraper=True)&#xa;    return render_to_response(&#xa;        'simple_pages/coverage_graph.html',&#xa;        {&#xa;            'sorted_courts': courts_json,&#xa;            'precedential_statuses': precedential_statuses,&#xa;            'count_pro': count_pro,&#xa;            'count_lawbox': count_lawbox,&#xa;            'count_scraper': count_scraper,&#xa;            'courts_with_opinion_scrapers': opinion_courts,&#xa;            'courts_with_oral_argument_scrapers': oral_argument_courts,&#xa;            'private': False&#xa;        },&#xa;        RequestContext(request))&#xa;&#xa;&#xa;def feeds(request):&#xa;    opinion_courts = Court.objects.filter(&#xa;        in_use=True,&#xa;        has_opinion_scraper=True&#xa;    )&#xa;    oral_argument_courts = Court.objects.filter(&#xa;        in_use=True,&#xa;        has_oral_argument_scraper=True&#xa;    )&#xa;    return render_to_response(&#xa;        'simple_pages/feeds.html',&#xa;        {&#xa;            'opinion_courts': opinion_courts,&#xa;            'oral_argument_courts': oral_argument_courts,&#xa;            'private': False&#xa;        },&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def contribute(request):&#xa;    return render_to_response(&#xa;        'simple_pages/contribute.html',&#xa;        {'private': False},&#xa;    )&#xa;&#xa;&#xa;@check_honeypot(field_name='skip_me_if_alive')&#xa;def contact(&#xa;        request,&#xa;        template_path='simple_pages/contact_form.html',&#xa;        template_data=None,&#xa;        initial=None):&#xa;    """"""This is a fairly run-of-the-mill contact form, except that it can be&#xa;    overridden in various ways so that its logic can be called from other&#xa;    functions.&#xa;    """"""&#xa;    # For why this is necessary, see&#xa;    # https://stackoverflow.com/questions/24770130/ and the related link,&#xa;    # http://effbot.org/zone/default-values.htm. Basically, you can't have a&#xa;    # mutable data type like a dict as a default argument without its state&#xa;    # being carried around from one function call to the next.&#xa;    if template_data is None:&#xa;        template_data = {}&#xa;    if initial is None:&#xa;        initial = {}&#xa;&#xa;    if request.method == 'POST':&#xa;        form = ContactForm(request.POST)&#xa;        if form.is_valid():&#xa;            cd = form.cleaned_data&#xa;&#xa;            # pull the email addresses out of the MANAGERS tuple&#xa;            i = 0&#xa;            email_addresses = []&#xa;            while i < len(settings.MANAGERS):&#xa;                email_addresses.append(settings.MANAGERS[i][1])&#xa;                i += 1&#xa;&#xa;            # send the email to the MANAGERS&#xa;            send_mail(&#xa;                'CourtListener message from ""%s"": %s' % (cd['name'],&#xa;                                                         cd['subject']),&#xa;                cd['message'],&#xa;                cd.get('email', 'noreply@example.com'),&#xa;                email_addresses, )&#xa;            # we must redirect after success to avoid problems with people&#xa;            # using the refresh button.&#xa;            return HttpResponseRedirect('/contact/thanks/')&#xa;    else:&#xa;        # the form is loading for the first time&#xa;        try:&#xa;            initial['email'] = request.user.email&#xa;            initial['name'] = request.user.get_full_name()&#xa;            form = ContactForm(initial=initial)&#xa;        except AttributeError:&#xa;            # for anonymous users, who lack full_names, and emails&#xa;            form = ContactForm(initial=initial)&#xa;&#xa;    template_data.update(&#xa;        {'form': form,&#xa;         'private': False}&#xa;    )&#xa;    return render_to_response(&#xa;        template_path,&#xa;        template_data,&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def contact_thanks(request):&#xa;    return render_to_response(&#xa;        'simple_pages/contact_thanks.html',&#xa;        {'private': True},&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def advanced_search(request):&#xa;    return render_to_response(&#xa;        'simple_pages/advanced_search.html',&#xa;        {'private': False},&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def old_terms(request, v):&#xa;    return render_to_response(&#xa;        'simple_pages/terms/%s.html' % v,&#xa;        {'title': 'Archived Terms and Policies, v%s - CourtListener.com' % v,&#xa;         'private': True},&#xa;        RequestContext(request),&#xa;    )&#xa;&#xa;&#xa;def latest_terms(request):&#xa;    return render_to_response(&#xa;        'simple_pages/terms/latest.html',&#xa;        {'private': False},&#xa;        RequestContext(request),&#xa;    )&#xa;&#xa;&#xa;class HttpResponseTemporaryUnavailable(HttpResponse):&#xa;    status_code = 503&#xa;&#xa;&#xa;@never_cache&#xa;def show_maintenance_warning(request):&#xa;    """"""Blocks access to a URL, and instead loads a maintenance warning.&#xa;&#xa;    Uses a 503 status code, which preserves SEO. See:&#xa;    https://plus.google.com/115984868678744352358/posts/Gas8vjZ5fmB&#xa;    """"""&#xa;    t = loader.get_template('simple_pages/maintenance.html')&#xa;    return HttpResponseTemporaryUnavailable(&#xa;        t.render(Context({'private': True})))&#xa;&#xa;&#xa;@cache_page(60 * 60 * 12)  # 12 hours&#xa;def robots(request):&#xa;    """"""Generate the robots.txt file""""""&#xa;    response = HttpResponse(mimetype='text/plain')&#xa;    t = loader.get_template('simple_pages/robots.txt')&#xa;    response.write(t.render(Context({})))&#xa;    return response&#xa;&#xa;&#xa;def validate_for_bing(request):&#xa;    return HttpResponse('<?xml version=""1.0""?><users><user>8BA95D8EAA744379D80D9F70847EA156</user></users>')&#xa;&#xa;&#xa;def validate_for_google(request):&#xa;    return HttpResponse('google-site-verification: googleef3d845637ccb353.html')&#xa;&#xa;&#xa;def validate_for_google2(request):&#xa;    return HttpResponse('google-site-verification: google646349975c2495b6.html')&#xa;&#xa;&#xa;def validate_for_wot(request):&#xa;    return HttpResponse('bcb982d1e23b7091d5cf4e46826c8fc0')&#xa;&#xa;&#xa;def tools_page(request):&#xa;    return render_to_response(&#xa;        'tools.html',&#xa;        {'private': False},&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def browser_warning(request):&#xa;    return render_to_response(&#xa;        'browser_warning.html',&#xa;        {'private': True},&#xa;        RequestContext(request)&#xa;    )&#xa;&#xa;&#xa;def serve_static_file(request, file_path=''):&#xa;    """"""Sends a static file to a user.&#xa;&#xa;    This serves up the static case files such as the PDFs in a way that can be&#xa;    blocked from search engines if necessary. We do four things:&#xa;     - Look up the document  or audio file associated with the filepath&#xa;     - Check if it's blocked&#xa;     - If blocked, we set the x-robots-tag HTTP header&#xa;     - Serve up the file using Apache2's xsendfile&#xa;    """"""&#xa;    response = HttpResponse()&#xa;    file_loc = os.path.join(settings.MEDIA_ROOT, file_path.encode('utf-8'))&#xa;    if file_path.startswith('mp3'):&#xa;        item = get_object_or_404(Audio, local_path_mp3=file_path)&#xa;        mimetype = 'audio/mpeg'&#xa;    else:&#xa;        item = get_object_or_404(Document, local_path=file_path)&#xa;        try:&#xa;            mimetype = magic.from_file(file_loc, mime=True)&#xa;        except IOError:&#xa;            raise Http404&#xa;&#xa;    if item.blocked:&#xa;        response['X-Robots-Tag'] = 'noindex, noodp, noarchive, noimageindex'&#xa;&#xa;    if settings.DEVELOPMENT:&#xa;        # X-Sendfile will only confuse you in a dev env.&#xa;        response.content = open(file_loc, 'r').read()&#xa;    else:&#xa;        response['X-Sendfile'] = file_loc&#xa;    file_name = file_path.split('/')[-1]&#xa;    response['Content-Disposition'] = 'attachment; filename=""%s""' % \&#xa;                                      file_name.encode('utf-8')&#xa;    response['Content-Type'] = mimetype&#xa;    if not is_bot(request):&#xa;        tally_stat('case_page.static_file.served')&#xa;    return response&#xa;"
5371992|"from six import BytesIO, u&#xa;from datetime import date&#xa;&#xa;import mock&#xa;from unittest2 import TestCase&#xa;from six.moves.urllib.parse import urlparse, parse_qsl, unquote_plus&#xa;&#xa;from authorize.apis.transaction import PROD_URL, TEST_URL, TransactionAPI&#xa;from authorize.data import Address, CreditCard&#xa;from authorize.exceptions import AuthorizeConnectionError, \&#xa;    AuthorizeResponseError&#xa;&#xa;class MockResponse(BytesIO):&#xa;    class Headers(dict):&#xa;        def getparam(self, *args, **kwargs):&#xa;            """"""Python 2 version""""""&#xa;            return None&#xa;        def get_content_charset(self, failobj=None, *args, **kwargs):&#xa;            """"""Python 3 version""""""&#xa;            return failobj&#xa;&#xa;    def __init__(self, *args, **kwargs):&#xa;        BytesIO.__init__(self, *args, **kwargs)&#xa;        self.headers = self.Headers()&#xa;&#xa;SUCCESS = MockResponse(&#xa;    b'1;1;1;This transaction has been approved.;IKRAGJ;Y;2171062816;;;20.00;CC'&#xa;    b';auth_only;;Jeffrey;Schenck;;45 Rose Ave;Venice;CA;90291;USA;;;;;;;;;;;;'&#xa;    b';;;;;375DD9293D7605E20DF0B437EE2A7B92;P;2;;;;;;;;;;;XXXX1111;Visa;;;;;;;'&#xa;    b';;;;;;;;;;Y')&#xa;PARSED_SUCCESS = {&#xa;    'cvv_response': 'P',&#xa;    'authorization_code': 'IKRAGJ',&#xa;    'response_code': '1',&#xa;    'amount': '20.00',&#xa;    'transaction_type': 'auth_only',&#xa;    'avs_response': 'Y',&#xa;    'response_reason_code': '1',&#xa;    'response_reason_text': 'This transaction has been approved.',&#xa;    'transaction_id': '2171062816',&#xa;}&#xa;ERROR = MockResponse(&#xa;    b'2;1;2;This transaction has been declined.;000000;N;2171062816;;;20.00;CC'&#xa;    b';auth_only;;Jeffrey;Schenck;;45 Rose Ave;Venice;CA;90291;USA;;;;;;;;;;;;'&#xa;    b';;;;;375DD9293D7605E20DF0B437EE2A7B92;N;1;;;;;;;;;;;XXXX1111;Visa;;;;;;;'&#xa;    b';;;;;;;;;;Y')&#xa;PARSED_ERROR = {&#xa;    'cvv_response': 'N',&#xa;    'authorization_code': '000000',&#xa;    'response_code': '2',&#xa;    'amount': '20.00',&#xa;    'transaction_type': 'auth_only',&#xa;    'avs_response': 'N',&#xa;    'response_reason_code': '2',&#xa;    'response_reason_text': 'This transaction has been declined.',&#xa;    'transaction_id': '2171062816',&#xa;}&#xa;&#xa;class URL(object):&#xa;    """"""&#xa;    a class to enable comparing of two urls regardless of order of parameters&#xa;    see http://stackoverflow.com/questions/5371992/comparing-two-urls-in-python &#xa;    """"""&#xa;    def __init__(self, url):&#xa;        parts = urlparse(url)&#xa;        _query = frozenset(parse_qsl(parts.query))&#xa;        _path = unquote_plus(parts.path)&#xa;        parts = parts._replace(query=_query, path=_path)&#xa;        self.parts = parts&#xa;&#xa;    def __eq__(self, other):&#xa;        return self.parts == other.parts&#xa;&#xa;    def __hash__(self, other):&#xa;        return hash(self.parts)&#xa;&#xa;class TransactionAPITests(TestCase):&#xa;    def setUp(self):&#xa;        self.api = TransactionAPI('123', '456')&#xa;        self.success = lambda *args, **kwargs: SUCCESS.seek(0) or SUCCESS&#xa;        self.error = lambda *args, **kwargs: ERROR.seek(0) or ERROR&#xa;        self.year = date.today().year + 10&#xa;        self.credit_card = CreditCard('4111111111111111', self.year, 1, '911')&#xa;        self.address = Address('45 Rose Ave', 'Venice', 'CA', '90291')&#xa;&#xa;    def test_basic_api(self):&#xa;        api = TransactionAPI('123', '456')&#xa;        self.assertEqual(api.url, TEST_URL)&#xa;        api = TransactionAPI('123', '456', debug=False)&#xa;        self.assertEqual(api.url, PROD_URL)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api._make_call({'a': '1', 'b': '2'})&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('{0}?a=1&b=2'.format(TEST_URL)))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_with_unicode(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api._make_call({u('\xe3'): '1', 'b': u('\xe3')})&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('{0}?%C3%A3=1&b=%C3%A3'.format(TEST_URL)))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_connection_error(self, urlopen):&#xa;        urlopen.side_effect = IOError('Borked')&#xa;        self.assertRaises(AuthorizeConnectionError, self.api._make_call,&#xa;            {'a': '1', 'b': '2'})&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_response_error(self, urlopen):&#xa;        urlopen.side_effect = self.error&#xa;        try:&#xa;            self.api._make_call({'a': '1', 'b': '2'})&#xa;        except AuthorizeResponseError as e:&#xa;            self.assertTrue(str(e).startswith('This transaction has been declined.'))&#xa;            self.assertEqual(e.full_response, PARSED_ERROR)&#xa;&#xa;    def test_add_params(self):&#xa;        self.assertEqual(self.api._add_params({}), {})&#xa;        params = self.api._add_params({}, credit_card=self.credit_card)&#xa;        self.assertEqual(params, {&#xa;            'x_card_num': '4111111111111111',&#xa;            'x_exp_date': '01-{0}'.format(self.year),&#xa;            'x_card_code': '911',&#xa;        })&#xa;        params = self.api._add_params({}, address=self.address)&#xa;        self.assertEqual(params, {&#xa;            'x_address': '45 Rose Ave',&#xa;            'x_city': 'Venice',&#xa;            'x_state': 'CA',&#xa;            'x_zip': '90291',&#xa;            'x_country': 'US',&#xa;        })&#xa;        params = self.api._add_params({},&#xa;            credit_card=self.credit_card, address=self.address)&#xa;        self.assertEqual(params, {&#xa;            'x_card_num': '4111111111111111',&#xa;            'x_exp_date': '01-{0}'.format(self.year),&#xa;            'x_card_code': '911',&#xa;            'x_address': '45 Rose Ave',&#xa;            'x_city': 'Venice',&#xa;            'x_state': 'CA',&#xa;            'x_zip': '90291',&#xa;            'x_country': 'US',&#xa;        })&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_auth(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.auth(20, self.credit_card, self.address)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_zip=90291&x_card_num=4111111111111111&x_amount=20.00'&#xa;            '&x_tran_key=456&x_city=Venice&x_country=US&x_version=3.1'&#xa;            '&x_state=CA&x_delim_char=%3B&x_address=45+Rose+Ave'&#xa;            '&x_exp_date=01-{0}&x_test_request=FALSE&x_card_code=911'&#xa;            '&x_type=AUTH_ONLY&x_delim_data=TRUE'.format(str(self.year))))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_capture(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.capture(20, self.credit_card, self.address)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_zip=90291&x_card_num=4111111111111111&x_amount=20.00'&#xa;            '&x_tran_key=456&x_city=Venice&x_country=US&x_version=3.1'&#xa;            '&x_state=CA&x_delim_char=%3B&x_address=45+Rose+Ave'&#xa;            '&x_exp_date=01-{0}&x_test_request=FALSE&x_card_code=911'&#xa;            '&x_type=AUTH_CAPTURE&x_delim_data=TRUE'.format(str(self.year))))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_settle(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;&#xa;        # Test without specified amount&#xa;        result = self.api.settle('123456')&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B'&#xa;            '&x_type=PRIOR_AUTH_CAPTURE&x_delim_data=TRUE&x_tran_key=456'&#xa;            '&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;        # Test with specified amount&#xa;        result = self.api.settle('123456', amount=10)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B'&#xa;            '&x_type=PRIOR_AUTH_CAPTURE&x_amount=10.00&x_delim_data=TRUE'&#xa;            '&x_tran_key=456&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_credit(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;&#xa;        # Test with transaction_id, amount&#xa;        result = self.api.credit('1111', '123456', 10)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_amount=10.00&x_delim_char=%3B'&#xa;            '&x_type=CREDIT&x_card_num=1111&x_delim_data=TRUE&x_tran_key=456'&#xa;            '&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_void(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.void('123456')&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B&x_type=VOID'&#xa;            '&x_delim_data=TRUE&x_tran_key=456&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;"
653509|"from struct import pack, unpack&#xa;from struct import error as StructError&#xa;from math import log, floor, sqrt&#xa;from datetime import datetime&#xa;import os&#xa;import ntpath&#xa;import collections&#xa;import re&#xa;import copy&#xa;import sys&#xa;&#xa;from .stata_missing import (&#xa;    get_missing, MissingValue, MISSING, MISSING_VALS&#xa;)&#xa;from .stata_variable import StataVariable&#xa;&#xa;try:&#xa;    from stata import st_format&#xa;    IN_STATA = True&#xa;except ImportError:&#xa;    IN_STATA = False&#xa;&#xa;&#xa;__version__ = '0.2.0'&#xa;&#xa;__all__ = ['Dta', 'Dta115', 'Dta117', &#xa;           'display_diff', 'open_dta']&#xa;    &#xa;&#xa;VALID_NAME_RE = re.compile(r'^[_a-zA-Z][_a-zA-Z0-9]{0,31}$')&#xa;RESERVED = frozenset(('_all', '_b', 'byte', '_coef', '_cons', &#xa;            'double', 'float', 'if', 'in', 'int', 'long', '_n', '_N',&#xa;            '_pi', '_pred', '_rc', '_skip', 'using', 'with'))&#xa;# next re used with _fix_fmt, which enlarges fmts for displaying value labels&#xa;FMT_WIDTH_RE = re.compile(r'^\s*(%(-|~)?0?)([0-9]+)(\.[0-9]+)')&#xa;LARGEST_NONMISSING = 8.988465674311579e+307&#xa;SMALLEST_NONMISSING = -1.7976931348623157e+308&#xa;NUM_FMT_RE = re.compile(r'^%(-)?(0)?([0-9]+)(\.|\,)([0-9]+)(f|g|e)(c)?$')&#xa;STR_FMT_RE = re.compile(r'^%(-|~)?(0)?([0-9]+)s$')&#xa;HEX_RE = re.compile(r'^(-)?0x([0-9]+\.[0-9a-f]+)p(\+|-)([0-9]+)?$')&#xa;date_details = r'|'.join(d for d in &#xa;            ('CC', 'cc', 'YY', 'yy', 'JJJ', 'jjj', 'Month', 'Mon', 'month', &#xa;            'mon', 'NN', 'nn', 'DD', 'dd', 'DAYNAME', 'Dayname', 'Day', 'Da',&#xa;            'day', 'da', 'q', 'WW', 'ww', 'HH', 'Hh', 'hH', 'hh', 'h', 'MM', &#xa;            'mm', 'SS', 'ss', '.sss', '.ss', '.s', 'am', 'a.m.', 'AM', 'A.M.',&#xa;            '\.', ',', ':', '-', '\\\\', '_', '\+', '/', '!.'))&#xa;TIME_FMT_RE = re.compile(r'^%(-)?t(c|C|d|w|m|q|h|y|g)(' + date_details + ')*$')&#xa;TB_FMT_RE = re.compile(r'^%(-)?tb([^:]*)(:(' + date_details + ')*)?$')&#xa;MONTH_ABBREV = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', &#xa;                7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}&#xa;&#xa;                &#xa;# exception to raise when Dta file is mis-formatted&#xa;class DtaParseError(Exception):&#xa;    pass&#xa;    &#xa;    &#xa;class Dta():&#xa;    """"""A Python parent class for Stata datasets. &#xa;    Sub-classes implement methods for particular versions.&#xa;    &#xa;    """"""&#xa;    def __init__(self, *args, **kwargs):&#xa;        """"""Initialize Dta object.&#xa;        &#xa;        Dta objects can be created &#xa;        - from file, &#xa;        - from an iterable of values (as in a list of lists, with&#xa;        one sub-list per observation), &#xa;        - by subscripting an existing Dta (usually done with&#xa;        subscripting syntax like data[::2, (0,4,8)] ), or&#xa;        - by converting one sub-type of Dta to another (for example, &#xa;        converting from version 115 to 117 by converting a Dta115 &#xa;        instance to a Dta117 instance).&#xa;        &#xa;        &#xa;        New from file&#xa;        =============&#xa;        Parameters&#xa;        ----------&#xa;        address : str&#xa;            Address of dta file, including file name and "".dta"".&#xa;            &#xa;        Example&#xa;        -------&#xa;        >>> Dta115(""path/to/some_dta_v115.dta"")&#xa;        >>> Dta117(""path/to/some_dta_v117.dta"")&#xa;        # if uncertain of version, open_dta() can open 114, 115, or 117&#xa;        >>> open_dta(""recent_version.dta"")&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        If the data set has a label, the label will be printed.&#xa;        &#xa;        &#xa;        New from iterable&#xa;        =================&#xa;        Parameters&#xa;        ----------&#xa;        varvals : iterable&#xa;            Values to &#xa;        compress : bool (or coercible to bool), optional&#xa;            This sets the default type to attempt to assign to a&#xa;            data variable as byte if compress=True, or float if &#xa;            compress=False. Default type is overridden as necessary.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        single_row : bool (or coercible to bool), optional&#xa;            The code tries to be helpful with inputs. The correct way to&#xa;            specify a single row data set is with a non-string iterable&#xa;            within another iterable, as in ((0,1,2,3)) or [[0,1,2,3]].&#xa;            With single_row=True, a single row can be specified as,&#xa;            for example, just (0,1,2,3) or [0,1,2,3].&#xa;            (Don't combine ((0,1,2,3)) with single_row=True.)&#xa;            Default value is False.&#xa;            &#xa;        Example&#xa;        -------&#xa;        >>> v = [[0.0, 0.1, 0.2],[1.0, 1.1, 1.2],[2.0,2.1,2.2]]&#xa;        >>> Dta117(v)&#xa;                &#xa;        &#xa;        Subscripting a Dta instance (usually used indirectly)&#xa;        =====================================================&#xa;        Parameters&#xa;        ----------&#xa;        old_dta : Dta instance&#xa;        sel_rows : iterable of int, optional&#xa;            The rows (observations) to be selected from the `old_dta`.&#xa;            Defaults to all observations.&#xa;        sel_cols : iterable of int, optional&#xa;            The cols (data variables) to be selected from the `old_dta`.&#xa;            Defaults to all variables.&#xa;            &#xa;        Example&#xa;        -------&#xa;        # take even-numbered observations and variables 0, 4, 8&#xa;        >>> smaller_dta = some_dta[::2, (0,4,8)]&#xa;            &#xa;        &#xa;        Converting a Dta instance&#xa;        =========================&#xa;        Parameters&#xa;        ----------&#xa;        old_dta : Dta instance&#xa;        sel_rows : iterable of int, optional&#xa;            The rows (observations) to be selected from the `old_dta`.&#xa;            Defaults to all observations.&#xa;        sel_cols : iterable of int, optional&#xa;            The cols (data variables) to be selected from the `old_dta`.&#xa;            Defaults to all variables.&#xa;        &#xa;        Example&#xa;        -------&#xa;        # open a version 115 file&#xa;        >>> dta115 = open_dta(""some_dta_v115.dta"")&#xa;        # convert to version 117&#xa;        >>> dta117 = Dta117(dta115)&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Data may be changed or variables dropped if converting from a &#xa;        more permissive format to a more restrictive one.&#xa;        &#xa;        &#xa;        Returns&#xa;        =======&#xa;        All examples above return an instance of a Dta sub-class.&#xa;        &#xa;        &#xa;        Side effects&#xa;        ============&#xa;        Initializes Dta object.&#xa;        &#xa;        """"""&#xa;        nargs = len(args) + len(kwargs)&#xa;        if nargs == 0:&#xa;            raise TypeError(""one or more arguments required (0 given)"")&#xa;        &#xa;        first_arg = args[0]&#xa;        if isinstance(first_arg, str):&#xa;            if nargs > 2 or (nargs > 1 and ""quiet"" not in kwargs):&#xa;                raise TypeError(&#xa;                    ""incorrect arguments for creating Dta from file""&#xa;                )&#xa;            self._new_from_file(*args, **kwargs)&#xa;        elif isinstance(first_arg, Dta):&#xa;            if nargs > 3:&#xa;                raise TypeError(&#xa;                    ""too many arguments to create Dta from existing Dta""&#xa;                )&#xa;            self._new_from_dta(*args, **kwargs)&#xa;        elif isinstance(first_arg, collections.Iterable):&#xa;            self._new_from_iter(*args, **kwargs)&#xa;        else:&#xa;            raise TypeError(""Dta cannot be created from these arguments:"")&#xa;        &#xa;    def _new_from_dta(self, old_dta, sel_rows=None, sel_cols=None):&#xa;        """"""create data object by subscripting another data object""""""&#xa;        sel_rows = sel_rows if sel_rows is not None else range(old_dta._nobs)&#xa;        sel_cols = sel_cols if sel_cols is not None else range(old_dta._nvar)&#xa;        &#xa;        self._quiet = old_dta._quiet&#xa;        &#xa;        #header&#xa;        self._ds_format  = old_dta._ds_format&#xa;        self._byteorder  = old_dta._byteorder&#xa;        self._nvar       = len(sel_cols)&#xa;        self._nobs       = len(sel_rows)&#xa;        self._data_label = old_dta._data_label&#xa;        self._set_timestamp()&#xa;        &#xa;        #descriptors&#xa;        self._typlist = [old_dta._typlist[i] for i in sel_cols]&#xa;        self._varlist = [old_dta._varlist[i] for i in sel_cols]&#xa;        # can't copy srtlist because rows could appear out of order in sel_rows&#xa;        self._srtlist = [None for i in sel_cols]&#xa;        self._fmtlist = [old_dta._fmtlist[i] for i in sel_cols]&#xa;        self._lbllist = [old_dta._lbllist[i] for i in sel_cols]&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [old_dta._vlblist[i] for i in sel_cols]&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {k:v for k,v in old_dta._chrdict.items() &#xa;                         if k == '_dta' or k in self._varlist}&#xa;        &#xa;        # data&#xa;        self._varvals = [[old_dta._varvals[i][j] for j in sel_cols] &#xa;                         for i in sel_rows]&#xa;        &#xa;        # value labels&#xa;        self._vallabs = copy.deepcopy(old_dta._vallabs)  # copy&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # convert type if old_dta was a different version of .dta&#xa;        old_type = old_dta.__class__&#xa;        if not isinstance(self, old_type):&#xa;            self._convert_dta(old_type)&#xa;    &#xa;    def _new_from_file(self, address, quiet=False):&#xa;        """"""get data object from file""""""&#xa;        address = self._get_fullpath(address)&#xa;        &#xa;        version = self._dta_format(address)&#xa;        &#xa;        if version in (114, 115):&#xa;            self._file_to_Dta115(address)&#xa;            if not isinstance(self, Dta115):&#xa;                if not quiet:&#xa;                    msg = ""file format is {}, converting to 117""&#xa;                    print(msg.format(version))&#xa;                self._convert_dta(Dta115)&#xa;        else:&#xa;            self._file_to_Dta117(address)&#xa;            if not isinstance(self, Dta117):&#xa;                if not quiet:&#xa;                    msg = ""file format is {}, converting to 115""&#xa;                    print(msg.format(version))&#xa;                self._convert_dta(Dta117)&#xa;                &#xa;        # set self's path and filename&#xa;        self._set_path(address)&#xa;        &#xa;        # set changed to False, since dataset comes directly from file&#xa;        self._changed = False&#xa;        &#xa;        # display data label if in Stata&#xa;        if not quiet and IN_STATA and self._data_label.strip() != """":&#xa;            print(""{txt}("" + self._data_label + ""){txt}"")&#xa;            &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;            &#xa;    def __getattr__(self, name):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Raises AttributeError if name does not end with ""_"".&#xa;        Tries to find variable and return StataVariable otherwise.&#xa;        """"""&#xa;        if not name.endswith(""_""):&#xa;            msg = ""'{}' object has no attribute '{}'""&#xa;            raise AttributeError(msg.format(self.__class__.__name__, name))&#xa;            &#xa;        varname = self._find_vars((name[:-1],))[0]&#xa;        &#xa;        return StataVariable(self, varname)&#xa;        &#xa;    def __setattr__(self, name, value):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Creates or replaces variable if name ends with ""_"".&#xa;        Creates or replaces regular attribute otherwise.&#xa;        """"""&#xa;        if not name.endswith(""_""):&#xa;            self.__dict__[name] = value&#xa;        else:&#xa;            varname = name[:-1]&#xa;            if varname in self._varlist:&#xa;                self[:, self._varlist.index(varname)] = value&#xa;            else:&#xa;                self.append_var(varname, value)&#xa;        &#xa;    def __delattr__(self, name):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Raises AttributeError if name does not end with ""_"".&#xa;        Otherwise, tries to find variable and drop it.&#xa;        """"""&#xa;        if name.endswith(""_""):&#xa;            self.drop_var(name[:-1])&#xa;        else:&#xa;            if name not in self.__dict__:&#xa;                msg = ""'{}' object has no attribute '{}'""&#xa;                raise AttributeError(msg.format(self.__class__.__name__, name))&#xa;            del self.__dict__[name]&#xa;        &#xa;    def save(self, address=None, replace=False):&#xa;        """"""Save current Dta object as dta file.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        address : str&#xa;            Address of file to save to.&#xa;            Optional if Dta object was created from file&#xa;            or has been saved already, otherwise required.&#xa;        replace : bool, optional&#xa;            Default value is False.&#xa;            True is required to write over existing file.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        creates or replaces dta file&#xa;        &#xa;        """"""&#xa;        if address is None:&#xa;            if not hasattr(self, ""_fullpath""):&#xa;                raise ValueError(""address or filename needed"")&#xa;            address = self._fullpath&#xa;        elif not isinstance(address, str):&#xa;            raise TypeError(""given address or filename should be str"")&#xa;        else:&#xa;            address = self._get_fullpath(address)&#xa;            self._set_path(address)&#xa;            &#xa;        if os.path.isfile(address) and not replace:&#xa;            msg = (""file exists; use replace option to overwrite"")&#xa;            raise IOError(msg)&#xa;            &#xa;        self._dta_obj_to_file(address)&#xa;        self._changed = False&#xa;        &#xa;    def _get_fullpath(self, address):&#xa;        """"""convert address to full path of dta file, &#xa;        adding "".dta"" if necessary&#xa;        &#xa;        """"""&#xa;        address = os.path.abspath(address)&#xa;        if len(address) < 4 or address[-4:] != "".dta"":&#xa;            address = address + "".dta""&#xa;        return address&#xa;        &#xa;    def _set_path(self, address):&#xa;        """"""set self's _fullpath and _filename from address""""""&#xa;        self._fullpath = address&#xa;        # http://stackoverflow.com/questions/8384737&#xa;        split_path = ntpath.split(address)&#xa;        self._filename = split_path[1] or ntpath.basename(split_path[0])    &#xa;&#xa;    def _set_timestamp(self):&#xa;        """"""make new time stamp""""""&#xa;        d = datetime.now()&#xa;        self._time_stamp = ""{:>2} {} {} {:>2}:{:>02}"".format(&#xa;                        d.day, MONTH_ABBREV[d.month], d.year, d.hour, d.minute)&#xa;        &#xa;    def ismissing(self, item):&#xa;        """"""Determine if item qualifies as a numeric missing value.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        item : None, int, float, or MissingValue instance&#xa;        &#xa;        Returns&#xa;        -------&#xa;        bool&#xa;        &#xa;        Notes&#xa;        ------&#xa;        This function is not meant to be used with non-numeric or &#xa;        non-real values, and will raise an error when given such.&#xa;        &#xa;        """"""&#xa;        return (item is None or isinstance(item, MissingValue) &#xa;                or not (SMALLEST_NONMISSING <= item <= LARGEST_NONMISSING))&#xa;                &#xa;    def quiet(self, q=True):&#xa;        """"""Allow or disallow output messages.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        q : bool or coercible to bool&#xa;            Default value is True, meaning disallow messages.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Notes&#xa;        -----&#xa;        Methods that are primarily designed for output--like `describe`,&#xa;        `list`, and `summary`--will show output regardless. Setting this&#xa;        function affects warnings or other 'unexpected' messages.&#xa;        &#xa;        """"""&#xa;        self._quiet = bool(q)&#xa;        &#xa;    def __iter__(self):&#xa;        for row in self._varvals:&#xa;            yield row&#xa;        &#xa;    def to_list(self):&#xa;        """"""Return list of data observations.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        list&#xa;            List of observations, each of which is also a list.&#xa;        &#xa;        """"""&#xa;        return copy.deepcopy(self._varvals)&#xa;        &#xa;    def _find_vars(self, varnames, unique=False, evars=False, all_ok=False, &#xa;                    empty_ok=False, single=False):&#xa;        """"""Take tuple of string abbreviations to variable names,&#xa;        and return list of lists of all matches within varlist.&#xa;        Raises error if no match or ambiguous abbreviation.&#xa;        &#xa;        Strip out duplicates if unique==True. Allow '_dta' if evars==True.&#xa;        &#xa;        If all_ok==True, using '_all' returns entire varlist if evars==False&#xa;        or entire varlist + '_dta' if evars==True. If all_ok==True and '_all' &#xa;        is present, there should be no other varnames present.&#xa;        &#xa;        """"""&#xa;        if isinstance(varnames, str):&#xa;            varnames = (varnames,)&#xa;        elif not isinstance(varnames, collections.Iterable):&#xa;            raise TypeError(""variable names should be str or iterable of str"")&#xa;        &#xa;        # first split into list of single abbrevs per str&#xa;        split_names = []&#xa;        for name in varnames:&#xa;            if not isinstance(name, str):&#xa;                raise TypeError(""must specify variables as string(s)"")&#xa;            split_names += name.split()&#xa;        nnames = len(split_names)&#xa;        &#xa;        # check for _all, check for proper usage, and return copy of varlist&#xa;        # if evars==False or ['_dta'] + varlist if evars==True&#xa;        all_specified = False&#xa;        if '_all' in split_names:&#xa;            if not all_ok:&#xa;                raise ValueError(""\""_all\"" not allowed in this context"")&#xa;            elif not nnames == 1:&#xa;                raise ValueError(&#xa;                    ""\""_all\"" may not be combined with other names"")&#xa;            all_specified = True&#xa;            all_names = (['_dta'] if evars else []) + list(self._varlist)&#xa;            nnames = len(all_names)&#xa;            &#xa;        # check that more than 0 names specified if empty_ok==False, and&#xa;        # ignore extras (with message) if single==True&#xa;        if not empty_ok and nnames == 0:&#xa;            raise ValueError(""no variables specified"")&#xa;        if single and nnames > 1:&#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                msg = smcl + ""only one {}varname allowed; ignoring the rest""&#xa;                print(msg.format('e' if evars else ''))&#xa;            split_names = split_names[:1]&#xa;    &#xa;        # if all_specified, return aleady-constructed all_names&#xa;        if all_specified:&#xa;            return all_names&#xa;    &#xa;        # Create match list of [abbrev, match1, match2, ...].&#xa;        # The loops below identify when exact varname given, but that varname&#xa;        # happens to be abbreviation of other varnames.&#xa;        varlist = self._varlist&#xa;        matches = []&#xa;        append = matches.append&#xa;        if evars:&#xa;            for name in split_names:&#xa;                if name == ""_dta"":&#xa;                    append([name, name])&#xa;                else:&#xa;                    match = [var for var in varlist if var.startswith(name)]&#xa;                    append([name, name] if name in match else [name] + match)&#xa;        else:&#xa;            for name in split_names:&#xa;                match = [var for var in varlist if var.startswith(name)]&#xa;                append([name, name] if name in match else [name] + match)&#xa;                  &#xa;        # abbreviation was a good, unambiguous abbreviation if exactly&#xa;        # one match found, i.e. if the corresponding entry in -matches- &#xa;        # is [abbrev, match1]&#xa;        if not all(len(m) == 2 for m in matches):&#xa;            # there were unmatched or ambiguous abbreviations&#xa;            zeros = "" "".join([m[0] for m in matches if len(m) == 1])&#xa;            twos  = "" "".join([m[0] for m in matches if len(m) >= 3])&#xa;            if zeros != """" and twos != """":&#xa;                msg = ""no variables found for {}; multiple found for {}""&#xa;                raise ValueError(msg.format(zeros, twos))&#xa;            if zeros != """":&#xa;                raise ValueError(&#xa;                    ""no variables found for {}"".format(zeros, twos))&#xa;            # if getting here, twos != """" and zeros == """"&#xa;            raise ValueError(""multiple variables found for '{}'"".format(twos))&#xa;            &#xa;        if not unique:&#xa;            return [m[1] for m in matches]&#xa;        seen = set()&#xa;        # if name has not been encountered, add to list and set of encountered&#xa;        return [m[1] for m in matches &#xa;                if m[1] not in seen and not seen.add(m[1])]&#xa;    &#xa;    def describe(self, simple=False, short=False, &#xa;                 fullnames=False, numbers=False, varlist=False):&#xa;        """"""Display a description of the data set.&#xa;        &#xa;        Paramters&#xa;        ---------&#xa;        simple : bool (or coercible to bool), optional&#xa;            Specify that only a list of variable names be displayed.&#xa;            No other options may be combined with `simple`.&#xa;            Default value is False.&#xa;        short : bool (or coercible to bool), optional&#xa;            Specify that only data set information be displayed.&#xa;            No information about individual variables will be displayed.&#xa;            Default value is False.&#xa;        fullnames : bool (or coercible to bool), optional&#xa;            Specify that full variable names be used.&#xa;            Default value is False.&#xa;        numbers : bool (or coercible to bool), optional&#xa;            Specify that variables' numbers be included.&#xa;            Default value is False.&#xa;        varlist : bool (or coercible to bool), optional&#xa;            Specify that sortlist and varlist information be saved&#xa;            (accessible via the `return_list` method).&#xa;            Default value is False.&#xa;            &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays description of data set. Saves associated values,&#xa;        which can be displayed with `return_list` method.&#xa;        &#xa;        """"""&#xa;        &#xa;        varlist_opt = varlist  # ""varlist"" below will mean self._varlist&#xa;        &#xa;        squish = self._squish_name&#xa;        varlist = self._varlist&#xa;        srtlist = self._srtlist&#xa;        nvar = self._nvar&#xa;        nobs = self._nobs&#xa;        width = self.width&#xa;        txt, res = (""{txt}"", ""{res}"") if IN_STATA else ("""", """")&#xa;        &#xa;        # basic return values are the same for all descriptions&#xa;        self._return_values = {&#xa;            'changed': self._changed,&#xa;            'width': width,&#xa;            'k': nvar,&#xa;            'N': nobs,&#xa;            'key_order': ('changed', 'width', 'k', 'N')&#xa;        }&#xa;        &#xa;        if simple:&#xa;            if any((short, fullnames, numbers, varlist_opt)):&#xa;                msg = ""simple may not be combined with other options""&#xa;                raise ValueError(msg)&#xa;            &#xa;            print(txt, end="""")&#xa;            for i in range(nvar):&#xa;                print(""{:<14}"".format(varlist[i]), end="""" if (i+1)%5 else ""\n"")&#xa;            print(""\n"" if nvar%5 else """")  # to add line before next prompt&#xa;            return&#xa;        &#xa;        size = self.width * self._nobs&#xa;        sort_names = "" "".join(varlist[j] for j in srtlist if j is not None)&#xa;        &#xa;        # if varlist_opt, add srtlist and varlist info to return_values&#xa;        if varlist_opt:&#xa;            self._return_values.update({&#xa;                'sortlist': sort_names,&#xa;                'varlist': "" "".join(varlist),&#xa;                'key_order': (&#xa;                    'changed',&#xa;                    'width',&#xa;                    'k',&#xa;                    'N',&#xa;                    'sortlist',&#xa;                    'varlist'&#xa;                )&#xa;            })&#xa;        &#xa;        # short and non-short descriptions share header and footer&#xa;        print(&#xa;            ""\n{}  obs:{} {:>13}"".format(txt, res, self._nobs),&#xa;            "" "" * 25,&#xa;            end=""""&#xa;        )&#xa;        i = 34&#xa;        label = self._data_label&#xa;        lablength = len(label)&#xa;        print(label[:i])&#xa;        while i < lablength:&#xa;            i += 32&#xa;            print("" "" * 47, label[:i])&#xa;        &#xa;        print(&#xa;            ""{} vars:{} {:>13}"".format(txt, res, self._nvar),&#xa;            "" "" * 24,&#xa;            self._time_stamp&#xa;        )&#xa;        &#xa;        if '_dta' in self._chrdict and 'note0' in self._chrdict['_dta']:&#xa;            note_text = "" "" * 25 + ""(_dta has notes)""&#xa;        else:&#xa;            note_text = """"&#xa;        print(""{} size:{} {:>13}"".format(txt, res, size), note_text)&#xa;        &#xa;        # add the stuff that's not in short version&#xa;        if not short:&#xa;            typlist = self._typlist&#xa;            fmtlist = self._fmtlist&#xa;            get_type_name = self._get_type_name&#xa;            lbllist = self._lbllist&#xa;            vlblist = self._vlblist&#xa;            &#xa;            hline = ""{txt}{hline}"" if IN_STATA else ""-"" * 80&#xa;            print(hline)&#xa;            print(""              storage   display    value"")&#xa;            print(""variable name   type    format     label      variable label"")&#xa;            print(hline)&#xa;            name_size = 8 if numbers else 15&#xa;            for i in range(nvar):&#xa;                if fullnames:&#xa;                    name = varlist[i]&#xa;                    if len(name) > name_size:&#xa;                        name += ""\n"" + "" "" * 15&#xa;                else:&#xa;                    name = squish(varlist[i], name_size)&#xa;                if len(name) < name_size:&#xa;                    name += "" "" * (name_size - len(name))&#xa;                num = ""{}{:>5}. "".format(txt, i) if numbers else """"&#xa;                fmt = fmtlist[i]&#xa;                lbl = lbllist[i]&#xa;                vlb = vlblist[i]&#xa;                print(&#xa;                    num,&#xa;                    ""{}{} "".format(res, name),&#xa;                    ""{}{:<7} {:<10} {:<10} {}{}"".format(&#xa;                        txt,&#xa;                        get_type_name(typlist[i]), &#xa;                        fmt if len(fmt) <= 10 else fmt[:8] + "".."",&#xa;                        lbl if len(lbl) <= 10 else lbl[:8] + "".."",&#xa;                        res,&#xa;                        vlb if len(vlb) <= 34 else vlb[:32] + ""..""&#xa;                    ),&#xa;                    sep=''&#xa;                )&#xa;            print(hline)&#xa;        &#xa;        # footer&#xa;        print(""{}Sorted by:  {}{}"".format(txt, res, sort_names))&#xa;        if self._changed:&#xa;            print(""     Note:  dataset has changed since last saved"")&#xa;        print("""")&#xa;                &#xa;    def return_list(self):&#xa;        """"""Display any saved results for this dta object.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays contents of saved results, if any.&#xa;        &#xa;        """"""&#xa;        if (not hasattr(self, '_return_values') or not self._return_values or &#xa;                not isinstance(self._return_values, dict)):&#xa;            print("""")&#xa;            return&#xa;        rv = self._return_values&#xa;        keys = rv.keys() if 'key_order' not in rv else rv['key_order']&#xa;        tplt = ""{{txt}}{:>22} = {{res}}{}"" if IN_STATA else ""{:>22} = {}""&#xa;        &#xa;        print("""")&#xa;        for key in keys:&#xa;            value = str(rv[key])&#xa;            if len(value) > 55: value = value[:53] + ""..""&#xa;            print(tplt.format(key, value))&#xa;        if not IN_STATA: print("""")&#xa;    &#xa;    def index(self, varname):&#xa;        """"""Get index of given data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varname : str&#xa;            Single varname (abbreviation allowed if unambiguous).&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Index of variable in data set.&#xa;        &#xa;        """"""&#xa;        if not isinstance(varname, str):&#xa;            raise TypeError(""argument must be str"")&#xa;        varname = self._find_vars(varname, empty_ok=False, single=True)[0]&#xa;        return self._varlist.index(varname)&#xa;        &#xa;    def variable(self, id):&#xa;        """"""Get a list of all values of a data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        id : int or str&#xa;            Single variable index (int) or name (str).&#xa;            For str, an abbreviation is allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        list&#xa;            List of values of the specified data variable.&#xa;        &#xa;        """"""&#xa;        if isinstance(id, str):&#xa;            varname = self._find_vars(id, empty_ok=False, single=True)[0]&#xa;            col = self._varlist.index(varname)&#xa;        elif isinstance(id, int):&#xa;            if not -self._nvar <= id < self._nvar:&#xa;                raise ValueError(""data variable index out of range"")&#xa;            col = id if id >= 0 else self._nvar + id&#xa;        else:&#xa;            raise TypeError(""argument must be str name or int column index"")&#xa;        &#xa;        varvals = self._varvals&#xa;        return [row[col] for row in varvals]&#xa;        &#xa;    def _squish_name(self, name, space):&#xa;        """"""Shorten name to fit in given space.&#xa;        Characters from middle of name replaced with '~'.&#xa;        &#xa;        """"""&#xa;        if len(name) <= space:&#xa;            return name&#xa;        if space < 3:&#xa;            raise ValueError(""too much squishing!"")&#xa;        return name[:space - 2] + ""~"" + name[-1]&#xa;        &#xa;    def rename(self, oldname, newname):&#xa;        """"""Replace old variable name with new.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        oldname : str&#xa;            Single variable name (abbreviation allowed if unambiguous).&#xa;        newname : str&#xa;            New variable name.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Replace old data variable name with new.&#xa;        &#xa;        """"""&#xa;        if not isinstance(oldname, str) or not isinstance(newname, str):&#xa;            raise TypeError(""old and new variable names should be str"")&#xa;        # unabbreviate oldname&#xa;        oldname = self._find_vars(oldname, empty_ok=False)[0] &#xa;        if oldname == newname:&#xa;            return&#xa;        newname = newname.strip()&#xa;       &#xa;        if not self._is_valid_varname(newname):&#xa;            raise ValueError(newname + "" is not a valid Stata name"")&#xa;        if newname in self._varlist:&#xa;            raise ValueError(newname + "" already exists"")&#xa; &#xa;        index = self._varlist.index(oldname)&#xa;        self._varlist[index] = newname&#xa;        &#xa;        # if oldname in chrdict, change to newname&#xa;        chrdict = self._chrdict&#xa;        if oldname in chrdict:&#xa;            chrdict[newname] = chrdict[oldname]&#xa;            del chrdict[oldname]&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    def set_obs(self, num_obs):&#xa;        """"""Increase number of observations in data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        num_obs : int&#xa;            Number of observations to increase to.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Changes number of observations. Appends observations with&#xa;        MissingValue instance for numeric variables, """" for string.&#xa;        Marks data as unsorted.&#xa;        &#xa;        """"""&#xa;        curr_obs = self._nobs&#xa;        if num_obs < curr_obs:&#xa;            raise ValueError(""num_obs must be >= "" + str(curr_obs))&#xa;        if num_obs == curr_obs:&#xa;            return&#xa;        isstrvar = self._isstrvar&#xa;        empty_row = ['' if isstrvar(i) else MISSING for i in range(self._nvar)]&#xa;        self._varvals += [copy.copy(empty_row) &#xa;                          for _ in range(num_obs - curr_obs)]&#xa;        self._nobs = num_obs&#xa;        self._changed = True&#xa;        # Need to clear srtlist. If there are string variables, there &#xa;        # might now be empty strings after non-empty string. If there &#xa;        # are numerical variables with extended missing, there will now &#xa;        # be ""."" missing after extended missing. Issue pointed out at&#xa;        # http://www.stata.com/statalist/archive/2013-08/msg00576.html&#xa;        self._srtlist = [None]*self._nvar&#xa;        &#xa;    def drop_obs(self, in_ = None, if_ = None, all_obs = False):&#xa;        """"""Drop observations from the data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to drop.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to drop.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;        all_obs : bool or coercible to Boolean, optional&#xa;            Option to drop all observations. Default value is False.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the dropped observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes specified observations.&#xa;        &#xa;        """"""&#xa;        if self._nobs == 0:&#xa;            return&#xa;        if all_obs and (in_ is not None or if_ is not None):&#xa;            raise ValueError(""all_obs cannot be combined with in_ or if_"")&#xa;        if not all_obs and in_ is None and if_ is None:&#xa;            raise ValueError(""must specify one of in_, if_, or all_obs"")&#xa;        &#xa;        if all_obs:&#xa;            self._varvals = []&#xa;            self._nobs = 0&#xa;        else:&#xa;            varvals = self._varvals&#xa;            if if_ is None:&#xa;                to_drop = [i for i in in_]&#xa;            else:&#xa;                if in_ is None: in_ = range(self._nobs)&#xa;                to_drop = [i for i in in_ if if_(i)]&#xa;            to_drop.reverse()&#xa;            for i in to_drop:&#xa;                del varvals[i]&#xa;            self._nobs = len(self._varvals)&#xa;        self._changed = True&#xa;            &#xa;    def keep_obs(self, in_ = None, if_ = None):&#xa;        """"""Keep specified observations, remove all others.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to keep.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to keep.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the kept observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes all observations except those specified.&#xa;        &#xa;        """"""&#xa;        if self._nobs == 0:&#xa;            return&#xa;        if in_ is None and if_ is None:&#xa;            raise ValueError(""must specify one of in_ or if_"")&#xa;            &#xa;        if if_ is None:&#xa;            self._varvals = [self._varvals[i] for i in in_]&#xa;        else:&#xa;            if in_ is None: in_ = range(self._nobs)&#xa;            self._varvals = [self._varvals[i] for i in in_ if if_(i)]&#xa;        self._nobs = len(self._varvals)&#xa;        self._changed = True&#xa;        &#xa;    def drop_var(self, varnames):&#xa;        """"""Delete specified variable(s).&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes specified data variables.&#xa;        &#xa;        """"""&#xa;        drop_vars = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        &#xa;        find_index = self._varlist.index&#xa;        drop_indexes = set(find_index(varname) for varname in drop_vars)&#xa;        keep_indexes = sorted(set(range(self._nvar)) - drop_indexes)&#xa;        &#xa;        # temporarily shorten srtlist to relevant values&#xa;        srtlist = self._srtlist&#xa;        if None in srtlist:&#xa;            srtlist = srtlist[:srtlist.index(None)]&#xa;        &#xa;        for var in drop_vars:&#xa;            ind = find_index(var)&#xa;                      &#xa;            del self._typlist[ind]&#xa;            del self._varlist[ind]&#xa;            del self._fmtlist[ind]&#xa;            del self._lbllist[ind]&#xa;            del self._vlblist[ind]&#xa;            &#xa;            if var in self._chrdict: del self._chrdict[var]&#xa;            &#xa;            # if ind was in srtlist, &#xa;            #    1) drop entry, and drop any entries to the right&#xa;            #    2) for entries to the left, decrease by 1 if greater than ind&#xa;            if ind in srtlist:&#xa;                srt_ind = srtlist.index(ind)&#xa;                srtlist = [(s - 1 if s > ind else s) &#xa;                           for s in srtlist[:srt_ind]]&#xa;            else:&#xa;                srtlist = [(s - 1 if s > ind else s) for s in srtlist]&#xa;        &#xa;        # fill out srtlist with Nones&#xa;        self._srtlist = srtlist + [None]*(len(keep_indexes) - len(srtlist))&#xa;            &#xa;        # remove data values&#xa;        self._varvals = [[row[v] for v in keep_indexes]&#xa;                         for row in self._varvals]&#xa;            &#xa;        self._nvar = len(keep_indexes)&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    drop_vars = drop_var&#xa;        &#xa;    def keep_var(self, varnames):&#xa;        """"""Keep specified variable(s), delete all others.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes data variables other than those specified.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=False)&#xa;        vars_to_drop = set(self._varlist) - set(varnames)&#xa;        if len(vars_to_drop) > 0:&#xa;            self.drop_var(vars_to_drop)&#xa;        &#xa;    keep_vars = keep_var&#xa;        &#xa;    def _summ_stats_meanonly(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;     &#xa;        varvals = self._varvals&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                sum_v += x&#xa;                n += 1&#xa;                mean = mean + (x - mean) / n&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w += w&#xa;                mean += (x - mean) * w / sum_w&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;     &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'N': sum_w if w_type == 'f' else n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;        stats['key_order'] = ('N', 'sum_w', 'sum', 'mean', 'min', 'max')&#xa;        return stats&#xa;        &#xa;    def _summ_stats_detail(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        M2 = 0&#xa;        M3 = 0&#xa;        M4 = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;        sum_w2 = 0&#xa;        varvals = self._varvals&#xa;        values = []&#xa;        append = values.append&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                sum_v += x&#xa;                n1 = n&#xa;                n += 1&#xa;                delta = x - mean&#xa;                delta_n = delta / n&#xa;                delta_n2 = delta_n * delta_n&#xa;                term1 = delta * delta_n * n1&#xa;                mean = mean + delta_n&#xa;                M4 += (term1 * delta_n2 * (n*n - 3*n + 3) +  &#xa;                       6 * delta_n2 * M2 - 4 * delta_n * M3)&#xa;                M3 += term1 * delta_n * (n - 2) - 3 * delta_n * M2&#xa;                M2 += term1&#xa;                &#xa;                append(x)&#xa;            &#xa;            # percentiles&#xa;            values.sort()&#xa;            pospc = [(pc * n / 100, int(pc * n / 100), pc) &#xa;                     for pc in (1, 5, 10, 25, 50, 75, 90, 95, 99)]&#xa;            stats = {'p' + str(p[2]):((values[p[1]-1] + values[p[1]]) / 2 &#xa;                     if p[0] == p[1] else values[floor(p[0])]) for p in pospc}&#xa;                        &#xa;            # largest and smallest values, with .'s added if n < 4&#xa;            prt_vals = ([""{:>9g}"".format(v) for v in values[:4]] + &#xa;                        [""{:>9}"".format(""."") for i in range(4 - n)]*2 +&#xa;                        [""{:>9g}"".format(v) for v in values[-4:]])&#xa;            # this kurtosis matches Stata, but wikipedia's is this minus 3&#xa;            stats.update({&#xa;                'kurtosis': (n*M4) / (M2*M2), &#xa;                'skewness': sqrt(n) * M3 / M2**(3/2),&#xa;                'mean': mean,&#xa;                'Var': M2 / n1,&#xa;                'sd': sqrt(M2 / n1),&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': values[0],&#xa;                'max': values[-1]&#xa;            })&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w_1 = sum_w&#xa;                sum_w += w&#xa;                sum_w2 += w*w&#xa;                delta = x - mean&#xa;                delta_W = delta / sum_w&#xa;                delta_w = delta * w / sum_w&#xa;                term1 = delta * delta_w * sum_w_1&#xa;                mean += delta_w&#xa;                M4 += (sum_w_1 * delta_w * (delta_W)**3 * (sum_w_1**3 + w**3) +&#xa;                       6 * delta_w * delta_w * M2 - 4 * delta_w * M3)&#xa;                M3 += term1 * delta_W * (sum_w_1 - w) - 3 * delta_w * M2&#xa;                M2 += term1&#xa;                &#xa;                append((x, w))&#xa;                &#xa;            values.sort()&#xa;            min_val = values[0][0]&#xa;            max_val = values[-1][0]&#xa;                    &#xa;            # Assign stats that are the same for all weight types &#xa;            # (except percentiles, handled next). This kurtosis&#xa;            # matches Stata, but wikipedia's is this minus 3.&#xa;            stats = {&#xa;                'kurtosis': (sum_w * M4) / (M2 * M2),&#xa;                'skewness': sqrt(sum_w) * M3 / M2**(3/2),&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;            # get percentiles&#xa;            pcsum = 0&#xa;            pospc = [(pc * n / 100, pc) &#xa;                     for pc in (1, 5, 10, 25, 50, 75, 90, 95, 99)]&#xa;            pos, pc = pospc[0]&#xa;            for i, (x, w) in zip(range(n), values):&#xa;                pcsum += w * n / sum_w&#xa;                while pcsum >= pos:&#xa;                    if pcsum == pos:&#xa;                        # not needed for these pcs, but if code is reused&#xa;                        # elsewhere, the i+1 below should be min((n, i+1))&#xa;                        stats['p' + str(pc)] = (x + values[i+1][0]) / 2 &#xa;                    else:&#xa;                        stats['p' + str(pc)] = x&#xa;                    del pospc[0]&#xa;                    if pospc == []:&#xa;                        break&#xa;                    pos, pc = pospc[0]&#xa;                else:           # these next three lines exit the for loop&#xa;                    continue    # if -break- encountered in the while loop&#xa;                break           # i.e. if there are no more pcs to assign&#xa;                &#xa;            # in case there are any percentiles that haven't been assigned:&#xa;            for pos, pc in pospc:&#xa;                stats['p' + str(pc)] = values[-1][0]&#xa;            &#xa;            # assign stats that depend on weight type&#xa;            if w_type == 'f':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = sum_w&#xa;            else: # just aweight ; iweight not allowed&#xa;                adj = sum_w / (sum_w * sum_w - sum_w2)&#xa;                &#xa;                stats['Var'] = M2 * adj&#xa;                stats['sd']  = sqrt(M2 * adj)&#xa;                stats['N']   = n&#xa;            &#xa;            # largest and smallest values, with .'s added if n < 4&#xa;            prt_vals = ([""{:>9g}"".format(v[0]) for v in values[:4]] + &#xa;                        [""{:>9}"".format(""."") for i in range(4 - n)]*2 +&#xa;                        [""{:>9g}"".format(v[0]) for v in values[-4:]])&#xa;        &#xa;        stats['key_order'] = ('N', 'sum_w', 'mean', 'Var', 'sd', 'skewness', &#xa;                              'kurtosis', 'sum', 'min', 'max', 'p1', 'p5', &#xa;                              'p10', 'p25', 'p50', 'p75', 'p90', 'p95', 'p99')&#xa;        return stats, prt_vals&#xa;        &#xa;    def _summ_stats_default(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        M2 = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;        sum_w2 = 0&#xa;     &#xa;        varvals = self._varvals&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                n1 = n&#xa;                n += 1&#xa;                sum_v += x&#xa;                delta = x - mean&#xa;                delta_n = delta / n&#xa;                mean = mean + delta_n&#xa;                M2 += delta * delta_n * n1&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'Var': M2 / n1,&#xa;                'sd': sqrt(M2 / n1),&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w_1 = sum_w&#xa;                sum_w += w&#xa;                sum_w2 += w*w&#xa;                delta = x - mean&#xa;                delta_w = delta * w / sum_w&#xa;                mean += delta_w&#xa;                M2 += delta * delta_w * sum_w_1&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;            if w_type == 'f':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = sum_w&#xa;            elif w_type == 'i':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = n&#xa;            else:&#xa;                adj = sum_w / (sum_w * sum_w - sum_w2)&#xa;                &#xa;                stats['Var'] = M2 * adj&#xa;                stats['sd']  = sqrt(M2 * adj)&#xa;                stats['N']   = n&#xa;                &#xa;        stats['key_order'] = ('N', 'sum_w', 'mean', 'Var', &#xa;                              'sd', 'min', 'max', 'sum')&#xa;        return stats&#xa;                    &#xa;    def _pctiles_from_sorted_v2(self, values, pcs):&#xa;        """"""get percentiles from given sorted iterable of values""""""&#xa;        if not all(0 <= pc <= 100 for pc in pcs):&#xa;            raise ValueError(""pctiles must be between 0 and 100"")&#xa;        nvals = len(values)&#xa;        pctiles = []&#xa;        for pc in pcs:&#xa;            if pc == 0:&#xa;                new_pct = values[0]&#xa;            elif pc == 100:&#xa;                new_pct = values[nvals-1]&#xa;            else:&#xa;                loc = nvals * pc / 100&#xa;                loc_flr = floor(loc)&#xa;                t = loc - loc_flr&#xa;                new_pct = (1 - t) * values[loc_flr - 1] + t * values[loc_flr]&#xa;            pctiles.append(new_pct)&#xa;        return pctiles&#xa;    &#xa;    def _pctiles_from_sorted(self, values, pcs):&#xa;        """"""get percentiles from given sorted iterable of values""""""&#xa;        if not all(0 <= pc <= 100 for pc in pcs):&#xa;            raise ValueError(""pctiles must be between 0 and 100"")&#xa;        nvals = len(values)&#xa;        pctiles = []&#xa;        for pc in pcs:&#xa;            if pc == 0:&#xa;                new_pct = values[0]&#xa;            elif pc == 100:&#xa;                new_pct = values[nvals-1]&#xa;            else:&#xa;                n = pc * nvals / 100&#xa;                if n == int(n):&#xa;                    new_pct = (values[int(n)-1] + values[int(n)]) / 2&#xa;                else:&#xa;                    new_pct = values[floor(n)]&#xa;            pctiles.append(new_pct)&#xa;        return pctiles&#xa;    &#xa;    def _obs_from_in_if(self, in_=None, if_=None):&#xa;        """"""helper for any method that takes in_ and if_ observation args""""""&#xa;        &#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (isinstance(in_, str) or &#xa;                    not isinstance(in_, collections.Iterable)):&#xa;                raise TypeError(""in_ option should be int or iterable of int"")&#xa;            else:&#xa;                in_ = tuple(in_)&#xa;                if not all(isinstance(i, int) for i in in_):&#xa;                    raise TypeError(""in_ should be int or iterable of int"")&#xa;        else:&#xa;            in_ = range(self._nobs)&#xa;            &#xa;        if if_ is not None:&#xa;            if not hasattr(if_, ""__call__""):&#xa;                raise TypeError(""if_ option should be callable"")&#xa;            obs = tuple(i for i in in_ if if_(i))&#xa;        else:&#xa;            obs = tuple(i for i in in_)&#xa;        &#xa;        return obs&#xa;    &#xa;    def _summ_template(self, w_index=None, w_type=None, detail=False):&#xa;        """"""helper for summarize()""""""&#xa;        if IN_STATA:&#xa;            if detail:&#xa;                header = ""{{txt}}{}\n{{hline 61}}""&#xa;                var_tplt = """".join(&#xa;                   (""{{txt}}      Percentiles      Smallest\n"",&#xa;                    ""{{txt}} 1%    {{res}}{:>9g}      {}\n"",&#xa;                    ""{{txt}} 5%    {{res}}{:>9g}      {}\n"", &#xa;                    ""{{txt}}10%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Obs          {{res}}{:>9d}\n"",&#xa;                    ""{{txt}}25%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Sum of Wgt.  {{res}}{:>9g}\n"",&#xa;                    ""\n"",&#xa;                    ""{{txt}}50%    {{res}}{:>9g}        "",&#xa;                        ""              {{txt}}Mean         {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}                        "",&#xa;                        ""Largest       Std. Dev.    {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}75%    {{res}}{:>9g}      {}\n"",&#xa;                    ""{{txt}}90%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Variance     {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}95%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Skewness     {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}99%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Kurtosis     {{res}}{:>9g}""))&#xa;                    &#xa;                tplt = (header, var_tplt)&#xa;            elif w_index is None or w_type == 'f':&#xa;                header = """".join((""\n{txt}    Variable {c |}       "",&#xa;                    ""Obs        Mean    Std. Dev.       Min        Max""))&#xa;                sepline = ""{txt}{hline 13}{c +}{hline 56}""&#xa;                row = """".join((""{{txt}}{:>12} {{c |}} {{res}}{N:>9g} "", &#xa;                               ""{mean:>11g} {sd:>11g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{{txt}}{:>12} {{c |}} {{res}}        0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;            else:&#xa;                header = """".join((""\n{txt}    Variable {c |}     Obs      "",&#xa;                      ""Weight        Mean   Std. Dev.       Min        Max""))&#xa;                sepline = ""{txt}{hline 13}{c +}{hline 65}""&#xa;                row = """".join((""{{txt}}{:>12} {{c |}} {{res}}"",&#xa;                               ""{N:>7g} {sum_w:>11g} {mean:>11g} "", &#xa;                               ""{sd:>10g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{{txt}}{:>12} {{c |}} {{res}}      0           0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;        else:&#xa;            if detail:&#xa;                header = """".join((""{}\n"", ""-"" * 61))&#xa;                var_tplt = """".join(&#xa;                   (""      Percentiles      Smallest\n"",&#xa;                    "" 1%    {:>9g}      {}\n"",&#xa;                    "" 5%    {:>9g}      {}\n"", &#xa;                    ""10%    {:>9g}      {}       Obs          {:>9d}\n"",&#xa;                    ""25%    {:>9g}      {}       Sum of Wgt.  {:>9g}\n"",&#xa;                    ""\n"",&#xa;                    ""50%    {:>9g}"", "" "" * 22, ""Mean         {:>9g}\n"",&#xa;                    "" "" * 24, ""Largest       Std. Dev.    {:>9g}\n"",&#xa;                    ""75%    {:>9g}      {}\n"",&#xa;                    ""90%    {:>9g}      {}       Variance     {:>9g}\n"",&#xa;                    ""95%    {:>9g}      {}       Skewness     {:>9g}\n"",&#xa;                    ""99%    {:>9g}      {}       Kurtosis     {:>9g}""))&#xa;                    &#xa;                tplt = (header, var_tplt)&#xa;            elif w_index is None or w_type == 'f':&#xa;                header = """".join((""\n    Variable |       "",&#xa;                    ""Obs        Mean    Std. Dev.       Min        Max""))&#xa;                sepline = """".join((""-"" * 13, ""+"", ""-"" * 56))&#xa;                row = """".join((""{:>12} | {N:>9g} {mean:>11g} "", &#xa;                               ""{sd:>11g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{:>12} |         0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;            else:&#xa;                header = """".join((""\n    Variable |     Obs      "",&#xa;                      ""Weight        Mean   Std. Dev.       Min        Max""))&#xa;                sepline = """".join((""-"" * 13, ""+"", ""-"" * 65))&#xa;                row = """".join((""{:>12} | {N:>7g} {sum_w:>11g} {mean:>11g} "", &#xa;                               ""{sd:>10g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{:>12} |       0           0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;                            &#xa;        return tplt&#xa;        &#xa;    def _summ_meanonly(self, wt_index, wt_type, obs, varnames, indexes):&#xa;        """"""do summary if meanonly""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        index = indexes[-1]&#xa;        &#xa;        if self._isnumvar(index):&#xa;            info = self._summ_stats_meanonly(index, wt_index, wt_type, obs)&#xa;        else:&#xa;            info = zero_info&#xa;            &#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;        &#xa;    def _summ_detail(self, wt_index, wt_type, obs, varnames, indexes):&#xa;        """"""do summary if detail""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        isnumvar = self._isnumvar&#xa;        summ_stats = self._summ_stats_detail&#xa;        vlblist = self._vlblist&#xa;        &#xa;        header, var_tplt = self._summ_template(detail=True)&#xa;        print("""")&#xa;        for i, (name, index) in enumerate(zip(varnames, indexes)):&#xa;            if isnumvar(index):&#xa;                info, vals = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = zero_info&#xa;            &#xa;            label = vlblist[index]&#xa;            label = label[:60] if label != """" else name&#xa;            label = """".join(("" "" * (30 - floor(len(label)/2)), label))&#xa;            print(header.format(label))&#xa;            if info[""N""] != 0:&#xa;                print(&#xa;                    var_tplt.format(&#xa;                        info['p1'], vals[0], &#xa;                        info['p5'], vals[1], &#xa;                        info['p10'], vals[2], info['N'], &#xa;                        info['p25'], vals[3], info['sum_w'], &#xa;                        info['p50'], info['mean'], &#xa;                        info['sd'], &#xa;                        info['p75'], vals[-4], &#xa;                        info['p90'], vals[-3], info['Var'], &#xa;                        info['p95'], vals[-2], info['skewness'], &#xa;                        info['p99'], vals[-1], info['kurtosis']&#xa;                    )&#xa;                )&#xa;            else:&#xa;                print(""no observations"")&#xa;           &#xa;            print("""")&#xa;        &#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;    &#xa;    def _summ_default(self, wt_index, wt_type, obs, &#xa;                      varnames, indexes, separator):&#xa;        """"""do summary if not detail and not meanonly""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        isnumvar = self._isnumvar&#xa;        summ_stats = self._summ_stats_default&#xa;        squish_name = self._squish_name&#xa;        &#xa;        tplt = self._summ_template(wt_index, wt_type)&#xa;        header, sepline, row_tplt, zero_row = tplt&#xa;        print(header)&#xa;        for i, (name, index) in enumerate(zip(varnames, indexes)):&#xa;            if i % separator == 0: print(sepline)&#xa;            &#xa;            if isnumvar(index):&#xa;                info = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = zero_info&#xa;            &#xa;            small_name = squish_name(name, 12)&#xa;            &#xa;            if info[""N""] != 0:&#xa;                print(row_tplt.format(small_name, **info))&#xa;            else:&#xa;                print(zero_row.format(small_name))&#xa;        &#xa;        print("""")&#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;        &#xa;    def _check_summ_args(self, detail=False, meanonly=False, separator=5, &#xa;                         quietly=False, weight=None, fweight=None, &#xa;                         aweight=None, iweight=None, in_=None, if_=None):&#xa;        """"""helper for summarize()""""""&#xa;        obs = self._obs_from_in_if(in_, if_)&#xa;        &#xa;        # weight stuff&#xa;            # check that all non-None weights are string&#xa;        if any(w is not None and not isinstance(w, str) &#xa;               for w in [weight, fweight, aweight, iweight]):&#xa;            raise TypeError(""weight options must be None or string"")&#xa;            &#xa;            # count weights that are not None and not empty string&#xa;        nweights = len([w for w in [weight, fweight, aweight, iweight] &#xa;                            if w is not None and w.strip() != """"])&#xa;        &#xa;        if nweights > 1:&#xa;            raise ValueError(""weight options cannot be combined"")&#xa;        elif nweights == 1:&#xa;            wt_type, wt_name = (x for x in (('a', weight), ('f', fweight),&#xa;                                            ('a', aweight), ('i', iweight))&#xa;                                if x[1] is not None).__next__()&#xa;            wt_vars = self._find_vars(wt_name)&#xa;            if len(wt_vars) > 1:&#xa;                raise ValueError(""only one weight variable allowed"")&#xa;            wt_index = self._varlist.index(wt_name)&#xa;                &#xa;            if self._isstrvar(wt_index):&#xa;                raise TypeError(""strings cannot be used as weights"")&#xa;                    &#xa;            if wt_type == 'i' and detail:&#xa;                msg = ""iweight may not be combined with detail option""&#xa;                raise ValueError(msg)&#xa;                &#xa;            if wt_type == 'f' and not self._isintvar(wt_index):&#xa;                raise TypeError(""frequency weights must be integer"")        &#xa;                &#xa;            if wt_type == 'a' and weight is not None and not self._quiet:&#xa;                if IN_STATA: print(""{txt}(analytic weights assumed)"")&#xa;                else: print(""(analytic weights assumed)"")&#xa;        else:&#xa;            wt_type, wt_index = ('a', None)&#xa;        &#xa;        # misc.&#xa;        if detail and meanonly:&#xa;            raise ValueError(""options meanonly and detail cannot be combined"")&#xa;        if separator != 5:&#xa;            if not isinstance(separator, int):&#xa;                raise TypeError(""separator option should be an integer"")&#xa;            if separator < 0:&#xa;                separator = 5&#xa;            &#xa;        return obs, (wt_type, wt_index), detail, meanonly, quietly, separator&#xa;        &#xa;    def summarize(self, varnames="""", *args, **kwargs):&#xa;        """"""Summarize data variables.&#xa;        &#xa;        Summarize specified variable(s), or, if no variables specified,&#xa;        summarize all variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str, optional&#xa;            Default is none specified (i.e. summarize all).&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        detail : bool (or coercible to bool)&#xa;            May not be combined with `meanonly`.&#xa;        meanonly : bool (or coercible to bool)&#xa;            May not be combined with `detail`.&#xa;        separator : int&#xa;            Number of summaries to group together with dividing line.&#xa;            Has no effect&#xa;        quietly : bool (or coercible to bool)&#xa;            Create summary, but do not display. Useful if only wanting&#xa;            to save summary results, to be displayed with `return_list`.&#xa;        weight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;        aweight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;        fweight : str &#xa;            Single varname (or abbreviation), of an integer variable. &#xa;            May not be combined with other weights.&#xa;        iweight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;            May not be combined with `detail` option.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to include in summary.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to include in summary.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        Above parameters can be accessed by name.&#xa;        Otherwise, the parameters appear in the above order.&#xa;        If both `in_` and `if_` are used, the summarized observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays summary of specified variable(s). Saves the values&#xa;        from the last summary, which can be displayed with `return_list`.&#xa;        &#xa;        """"""&#xa;        (obs, (wt_type, wt_index), detail,&#xa;         meanonly, quietly, separator) = self._check_summ_args(*args, **kwargs)&#xa;         &#xa;        # get variables and their indices&#xa;        varnames = self._find_vars(varnames, empty_ok=True)&#xa;        nvarnames = len(varnames)&#xa;        if nvarnames == 0:&#xa;            varnames = self._varlist&#xa;            indexes = list(range(self._nvar))&#xa;        else:&#xa;            indexes = list(map(self._varlist.index, varnames))&#xa;        &#xa;        # do the summ&#xa;        if meanonly:&#xa;            self._summ_meanonly(wt_index, wt_type, obs, varnames, indexes)&#xa;        elif quietly:&#xa;            summ_stats = (self._summ_stats_detail &#xa;                          if detail &#xa;                          else self._summ_stats_default)&#xa;            index = indexes[-1]&#xa;            &#xa;            if self._isnumvar(index):&#xa;                info = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                        'key_order': ('N', 'sum_w', 'sum')}&#xa;            &#xa;            self._return_values = info&#xa;        elif detail:&#xa;            self._summ_detail(wt_index, wt_type, obs, varnames, indexes)&#xa;        else:&#xa;            self._summ_default(wt_index, wt_type, obs, &#xa;                               varnames, indexes, separator)&#xa;            &#xa;    summ = summarize&#xa;        &#xa;    def sort(self, varnames):&#xa;        """"""Sort data values according to given variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Sorts observations of the data set.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        var_ind_list = list(map(self._varlist.index, varnames))&#xa;        new_srtlist = var_ind_list + [None]*(self._nvar - len(varnames))&#xa;        if self._srtlist == new_srtlist:&#xa;            return&#xa;        sort_key = lambda row: [row[i] for i in var_ind_list]&#xa;        self._varvals.sort(key = sort_key)&#xa;        self._srtlist = new_srtlist&#xa;        self._changed = True&#xa;    &#xa;    def _convert_hex(self, hex_value):&#xa;        """"""convert Python's hex representation to Stata's""""""&#xa;        if not isinstance(hex_value, str):&#xa;            raise TypeError(""given hex value must be str"")&#xa;        m = HEX_RE.match(hex_value)&#xa;        if m is None:&#xa;            raise ValueError(""given string does not seem to be Python hex"")&#xa;        sign_char, base, exp_sign, exp = [m.group(i) for i in range(1,5)]&#xa;        new_sign = ""+"" if sign_char is None else sign_char&#xa;        # Line below converts exp to hex value. The ""0x"" prefix is removed &#xa;        # with [2:]. The exponent is padded with (too many) zeros (Stata &#xa;        # requires 3 digits), and reduced to last 3 digits with [-3:].&#xa;        new_exp = (""000"" + hex(int(exp))[2:])[-3:]&#xa;        return """".join((new_sign, base, 'X', exp_sign, new_exp))&#xa;        &#xa;    def _stata_hex_format(self, value):&#xa;        """"""convert numeric value to string in Stata hex format""""""&#xa;        return self._convert_hex(float(value).hex())&#xa;        &#xa;    def _stata_HL_format(self, fmt, value):&#xa;        """"""convert numeric value to string in one of Stata's H or L formats""""""&#xa;        if fmt == '%16H':&#xa;            packed_value = pack('>d', value)&#xa;        elif fmt == '%8H':&#xa;            packed_value = pack('>f', value)&#xa;        elif fmt == '%16L':&#xa;            packed_value = pack('<d', value)&#xa;        elif fmt == '%8L':&#xa;            packed_value = pack('<f', value)&#xa;        else:&#xa;            raise ValueError(""{} is not a recognized hilo format"".format(fmt))&#xa;        &#xa;        return """".join(hex(x)[2:].zfill(2) for x in packed_value)&#xa;    &#xa;    def _translate_fmts(self):&#xa;        """"""Translate Stata formats to Python. Bad formats&#xa;        are replaced by default format for given type.&#xa;        &#xa;        """"""&#xa;        fmt_info = []&#xa;        fmt_append = fmt_info.append&#xa;        &#xa;        isvalid = self._is_valid_fmt&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        default_fmts = self._default_fmts&#xa;        &#xa;        for i, fmt in enumerate(self._fmtlist):&#xa;            fmt = fmt.strip()&#xa;            &#xa;            iscalendar = (fmt[1] == 't' or fmt[1:3] == '-t')&#xa;            &#xa;            if iscalendar or not isvalid(fmt):&#xa;                if isstrvar(i):&#xa;                    wid = min(typlist[i], 10)&#xa;                    fmt_append(('s', ""{{:>{}s}}"".format(wid), wid))&#xa;                    continue&#xa;                else:&#xa;                    fmt = default_fmts[typlist[i]]&#xa;            &#xa;            last_char = fmt[-1]&#xa;            if last_char == 's': # string&#xa;                m = STR_FMT_RE.match(fmt)&#xa;                align, _, wid = m.group(1), m.group(2), m.group(3)&#xa;                new_align = (""<"" if align == ""-"" &#xa;                                 else ""^"" if align == ""~"" else "">"")&#xa;                new = """".join((""{:"", new_align, wid, ""s}""))&#xa;                fmt_append(('s', new, int(wid)))&#xa;            elif last_char == 'H' or last_char == 'L': # binary&#xa;                fmt_append((last_char, fmt, int(fmt[1:-1])))&#xa;            elif last_char == 'x': # hexadecimal&#xa;                fmt_append(('x', fmt, 21))&#xa;            elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;                m = NUM_FMT_RE.match(fmt)&#xa;                align, _, wid, delim, prec, type, com = (m.group(1), m.group(2), &#xa;                                                         m.group(3), m.group(4),&#xa;                                                         m.group(5), m.group(6),&#xa;                                                         m.group(7))&#xa;                aln = ""<"" if align == ""-"" else "">""&#xa;                sep = "","" if com is not None else """"&#xa;                if type == ""g"" and int(prec) == 0:&#xa;                    new = """".join((""{:"", aln, wid, sep, type, ""}""))&#xa;                else:&#xa;                    new = """".join((""{:"", aln, wid, sep, ""."", prec, type, ""}""))&#xa;                fmt_append((type, new, int(wid), delim, com))&#xa;                &#xa;        return fmt_info&#xa;    &#xa;    def _list_format_withstata(self, fmt, val):&#xa;        """"""helper for list()""""""&#xa;        if isinstance(val, float) or isinstance(val, int):&#xa;            return st_format(fmt, val)&#xa;        elif isinstance(val, MissingValue):&#xa;            return st_format(fmt, val.value)&#xa;        else: # str, presumably&#xa;            width = fmt[1:-1]&#xa;            return ((""{:>"" + width).replace("">-"", ""<"") + ""}"").format(val)&#xa;    &#xa;    def _list_format_nostata(self, fmt_info, val):&#xa;        """"""helper for list()""""""&#xa;        if isinstance(val, MissingValue):&#xa;            aln = fmt_info[1][2]&#xa;            wid = str(fmt_info[2])&#xa;            return """".join((""{:"", aln, wid, ""s}"")).format(str(val))&#xa;        &#xa;        fmt_type = fmt_info[0]&#xa;        decimal_comma = fmt_type in ('f', 'g', 'e') and fmt_info[3] is not None&#xa;        if fmt_type == 's':  # ie, no comma needed&#xa;            # use fmt_info[2], the intended width, to chop off, just in case&#xa;            return fmt_info[1].format(val)[:fmt_info[2]]&#xa;        if fmt_type in ('f', 'g', 'e'):&#xa;            val_str = fmt_info[1].format(val)&#xa;            if fmt_info[3] == "","":  # decimal comma&#xa;                if fmt_info[4] is None:  # no thousands separator&#xa;                    return val_str.replace(""."", "","")&#xa;                else:&#xa;                    return ""."".join(v.replace(""."", "","") &#xa;                                    for v in val_str.split("",""))&#xa;            else:&#xa;                return val_str&#xa;        elif fmt_type == 'x':&#xa;            return self._stata_hex_format(val)&#xa;        elif fmt_type in ('H', 'L'):&#xa;            return self._stata_HL_format(fmt_info[1], val)&#xa;        else:&#xa;            raise ValueError(""internal error; contact package author"")&#xa;            &#xa;    def _check_list_args(self, separator=5, in_=None, if_=None):&#xa;        """"""helper for list()""""""&#xa;        &#xa;        obs = self._obs_from_in_if(in_, if_)&#xa;        &#xa;        if separator != 5:&#xa;            if not isinstance(separator, int):&#xa;                raise TypeError(""separator option should be an integer"")&#xa;            if separator < 0:&#xa;                separator = 5&#xa;            &#xa;        return obs, separator&#xa;    &#xa;    def list(self, varnames="""", **kwargs):&#xa;        """"""Print table of data values.&#xa;        &#xa;        Print table of values for specified variable(s), or all&#xa;        variables if none specified. &#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str, optional&#xa;            Default is none specified (i.e. list all).&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to list.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to list.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the listed observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays table of values.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=True)&#xa;        if len(varnames) == 0:&#xa;            varnames = self._varlist&#xa;        ncols = len(varnames)&#xa;        varvals = self._varvals&#xa;        &#xa;        find_index = self._varlist.index&#xa;        indexes = [find_index(name) for name in varnames]&#xa;        &#xa;        if IN_STATA:&#xa;            list_format = self._list_format_withstata&#xa;            fmts = self._fmtlist&#xa;            widths = [len(list_format(fmts[i], varvals[0][i])) &#xa;                      for i in indexes]&#xa;        else:&#xa;            list_format = self._list_format_nostata&#xa;            fmts = self._translate_fmts()  # formats plus other info&#xa;            widths = [fmts[i][2] for i in indexes]&#xa;        &#xa;        obs, separator = self._check_list_args(**kwargs)&#xa;        &#xa;        &#xa;        ndigits = (1 if len(obs) == 0 or obs[-1] <= 1 &#xa;                   else floor(log(obs[-1] - 1, 10)) + 1)&#xa;        rownum_tplt = "" {{:>{}}}. "".format(ndigits)&#xa;        colnum_tplt = [""{:"" + (""<"" if self._fmtlist[i][1] == ""-"" else "">"") + &#xa;                       ""{}}}"".format(w) for i, w in zip(indexes, widths)]&#xa;        spacer = "" ""*(ndigits + 3)&#xa;        &#xa;        # table boundaries&#xa;        inner_width = 2*ncols + sum(widths)&#xa;        if IN_STATA:&#xa;            hline = ""{hline "" + str(inner_width) + ""}""&#xa;            top_line = spacer + ""{c TLC}"" + hline + ""{c TRC}""&#xa;            mid_line = spacer + ""{c LT}"" + hline + ""{c RT}""&#xa;            bot_line = spacer + ""{c BLC}"" + hline + ""{c BRC}""&#xa;            row_tplt = ""{}{{c |}} {{res}}{} {{txt}}{{c |}}""&#xa;        else:&#xa;            hline = ""-"" * inner_width&#xa;            top_line = mid_line = bot_line = spacer + ""+"" + hline + ""+""&#xa;            row_tplt = ""{}| {} |""&#xa;        &#xa;        # variable names&#xa;        if IN_STATA: print(""{txt}"")&#xa;        print(top_line)&#xa;        squish = self._squish_name&#xa;        row_info = ""  "".join(tplt.format(squish(n, w)) &#xa;            for tplt, n, w in zip(colnum_tplt, varnames, widths))&#xa;        print(row_tplt.format(spacer, row_info))&#xa;        &#xa;        # values&#xa;        for obs_count, i in enumerate(obs):&#xa;            if obs_count % separator == 0:&#xa;                print(mid_line)&#xa;            row = varvals[i]&#xa;            row_info = ""  "".join(list_format(fmts[j], row[j]) for j in indexes)&#xa;            row_info = row_tplt.format(rownum_tplt.format(i), row_info)&#xa;            try:&#xa;                print(row_info)&#xa;            except UnicodeEncodeError:&#xa;                print(row_info.encode('ascii', 'replace').decode())&#xa;        &#xa;        print(bot_line)&#xa;    &#xa;    def order(self, varnames, last=False, &#xa;              before=None, after=None, alpha=False):&#xa;        """"""Change order of varlist.&#xa;        &#xa;        Any duplicates in varnames will be ignored.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        last : bool (or coercible to bool), optional&#xa;            Signal that specified variables should come last in the&#xa;            varlist instead of first. Default is False.&#xa;            May not be combined with `before` or `after`.&#xa;        before : str, optional&#xa;            Name of variable to put the specified variables before.&#xa;            An abbreviation is allowed if unambiguous.&#xa;            By default this option is turned off.&#xa;            May not be combined with `last' or `after'&#xa;        after : str, optional&#xa;            Name of variable to put the specified variables after.&#xa;            An abbreviation is allowed if unambiguous.&#xa;            By default this option is turned off.&#xa;            May not be combined with `last' or `before'&#xa;        alpha : bool (or coercible to bool), optional&#xa;            Signal that varlist should be sorted alphabetically &#xa;            before rearranging. Default is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Reorders varlist.&#xa;        &#xa;        """"""&#xa;        # check for bad combinations of options&#xa;        if before and after:&#xa;            raise ValueError(""options before and after cannot be combined"")&#xa;        if last and (before or after):&#xa;            msg = ""options last and {} cannot be combined""&#xa;            raise ValueError(msg.format(""before"" if before else ""after""))&#xa;        &#xa;        varnames = self._find_vars(varnames, unique=True, &#xa;                                  all_ok=True, empty_ok=False)&#xa;        &#xa;        # put in alphabetic order, if requested&#xa;        if alpha:&#xa;            varnames.sort()&#xa;        &#xa;        # find sort order &#xa;        var_index = self._varlist.index&#xa;        new_order = [var_index(name) for name in varnames]&#xa;        used_vars = set(new_order)&#xa;        &#xa;        if last:&#xa;            new_order = ([i for i in range(self._nvar) if i not in used_vars] +&#xa;                         new_order)&#xa;        elif before:&#xa;            before = self._find_vars(before, single=True)[0]&#xa;            if before in varnames:&#xa;                msg = ""varname in -before- option may not be in varlist""&#xa;                raise ValueError(msg)&#xa;                &#xa;            before_index = var_index(before)&#xa;            new_order = ([i for i in range(before_index)&#xa;                            if i not in used_vars] + &#xa;                         new_order + &#xa;                         [i for i in range(before_index, self._nvar) &#xa;                            if i not in used_vars])&#xa;        elif after:&#xa;            after = self._find_vars(after, single=True)[0]&#xa;            if after in varnames:&#xa;                msg = ""varname in -after- option may not be in varlist""&#xa;                raise ValueError(msg)&#xa;                &#xa;            after_index = var_index(after)&#xa;            new_order = ([i for i in range(after_index+1) &#xa;                            if i not in used_vars] +&#xa;                         new_order + &#xa;                         [i for i in range(after_index+1, self._nvar) &#xa;                            if i not in used_vars])&#xa;        else:&#xa;            new_order += [i for i in range(self._nvar) if i not in used_vars]&#xa;          &#xa;        # if new_order same as old order, abort&#xa;        if new_order == list(range(self._nvar)):&#xa;            return&#xa;        &#xa;        # do reordering&#xa;        new_order_index = new_order.index&#xa;        &#xa;        self._typlist = [self._typlist[i] for i in new_order]&#xa;        self._varlist = [self._varlist[i] for i in new_order]&#xa;        # renumber sort entries&#xa;        self._srtlist = [new_order_index(srt) &#xa;                         if srt is not None else None for srt in self._srtlist]&#xa;        self._fmtlist = [self._fmtlist[i] for i in new_order]&#xa;        self._lbllist = [self._lbllist[i] for i in new_order]&#xa;        self._vlblist = [self._vlblist[i] for i in new_order]&#xa;        &#xa;        varvals = self._varvals&#xa;        self._varvals = [[row[i] for i in new_order] for row in varvals]&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    def clonevar(self, oldname, newname):&#xa;        """"""Create a data variable into a new variable.&#xa;        &#xa;        New data variable will have the same data values, display format,&#xa;        labels, value labels, notes, and characteristics.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        oldname : str&#xa;            Single variable name (abbreviation allowed if unambiguous).&#xa;        newname : str&#xa;            New variable name.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates new variable. Copies data values, display format, &#xa;        labels, value labels, notes, and characteristics.&#xa;        &#xa;        """"""&#xa;        if not isinstance(oldname, str) or not isinstance(newname, str):&#xa;            raise TypeError(""old and new variable names should be str"")&#xa;        # unabbreviate oldname&#xa;        oldname = self._find_vars(oldname, empty_ok=False)[0] &#xa;&#xa;        if oldname == newname:&#xa;            return&#xa;        newname = newname.strip()&#xa;&#xa;        if not self._is_valid_varname(newname):&#xa;            raise ValueError(newname + "" is not a valid Stata name"")&#xa;        if newname in self._varlist:&#xa;            raise ValueError(newname + "" already exists"")&#xa;                          &#xa;        #Make new var and index it&#xa;        self._varlist.append(newname) &#xa;                       &#xa;        #Find old and make a new var with old data                   &#xa;        index_old = self._varlist.index(oldname)&#xa;    &#xa;        for row in self._varvals:&#xa;            row.append(row[index_old])&#xa;&#xa;        #Copy Srt Lst   &#xa;        self._srtlist.append(None) &#xa;        &#xa;        #Copy Type information&#xa;        nlst = self._typlist&#xa;        num = nlst[index_old]        &#xa;        self._typlist.append(num)&#xa;       &#xa;        #Copy Display Format of New Variable from Old&#xa;        distype = self._fmtlist[index_old]&#xa;        self._fmtlist.append(distype)&#xa;&#xa;        #Copy Label List&#xa;        labellist = self._lbllist[index_old]&#xa;        self._lbllist.append(labellist)&#xa;&#xa;        #Copy variable labels&#xa;        varlab = self._vlblist[index_old]&#xa;        self._vlblist.append(varlab)&#xa;        &#xa;        #Copy characeristics&#xa;        if oldname in self._chrdict:&#xa;            chars = self._chrdict[oldname].copy()&#xa;            self._chrdict[newname] = chars&#xa;&#xa;        # increment self._nvar by 1&#xa;        self._nvar = self._nvar + 1 &#xa;    &#xa;        self._changed = True&#xa;        &#xa;    def append_obs(self, value):&#xa;        """"""Append observations to the end of the dataset.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        value : iterable&#xa;            Should be an iterable of iterables, one sub-iterable&#xa;            per observation. The observations should contain as&#xa;            many values as there are variables in the data set,&#xa;            and the values should have correct type.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates new observations in the data set, and inputs the given&#xa;        values into those observations.&#xa;        &#xa;        """"""&#xa;        &#xa;        # Create index for columns&#xa;        ncols = self._nvar&#xa;        col_nums = list(range(ncols))&#xa;              &#xa;        # Put value list of lists into the valvars form&#xa;        value = self._standardize_input(value)&#xa;       &#xa;        # Make sure value is in correct form&#xa;        if (ncols != 1 and len(value) == ncols and &#xa;                all(len(v) == 1 for v in value)):&#xa;            value = (tuple(v[0] for v in value),)&#xa;       &#xa;        # Create index for rows&#xa;        nrows = len(value)&#xa;        row_nums = list(range(self._nobs, self._nobs + nrows))&#xa;&#xa;        # Check that input is in correct shape&#xa;        if nrows == 0:&#xa;            raise ValueError(""value is empty"")&#xa;        &#xa;        if not all(len(row)==self._nvar for row in value):&#xa;            msg = ""new row length does not match number of variables""&#xa;            raise ValueError(msg)&#xa;&#xa;        # Append observation(s)&#xa;        self.set_obs(nrows + self._nobs) &#xa;        self._set_values(row_nums, col_nums, value)&#xa;       &#xa;        # self._changed set to True in set_obs&#xa;        &#xa;    def xpose(self, clear=False, varname=False):&#xa;        """"""Transpose data. &#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        clear : Boolean , required&#xa;            The purpose of this parameter is to remind the user&#xa;            that this method replaces the data in the dataset.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Note&#xa;        ----&#xa;        This method does not yet support the -promote- option in Stata.&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Can change almost everything in the data set.&#xa;        Replaces string values with missing.&#xa;        Transposes numeric values in the data, and thus changing number&#xa;        of observations, number of variables, variable names, etc.&#xa;        Removes or replaces existing variable labels, characteristics, &#xa;        display formats, sort info.&#xa;        &#xa;        """"""   &#xa;        if not clear:&#xa;            raise ValueError(""must specify clear=True to use xpose"")&#xa;        &#xa;        # Without the -promote- option, any values outside the float range&#xa;        # will be converted to MISSING.&#xa;        convert = lambda x: (&#xa;            x if (isinstance(x, MissingValue) or &#xa;                  -1.7014117331926443e+38 <= x <= 1.7014117331926443e+38) &#xa;            else MISSING&#xa;        )&#xa;        &#xa;        # If varname=True, save old varnames to be added in later&#xa;        if varname:&#xa;            old_varnames = [v for v in self._varlist]&#xa;        &#xa;        # Change string values to missing values&#xa;        nobs = range(self._nobs)&#xa;        columns = [i for i in range(self._nvar) if self._isstrvar(i)]&#xa;        varvals = self._varvals&#xa;        for i in nobs:&#xa;            for j in columns:&#xa;                varvals[i][j] = MISSING&#xa;            &#xa;        # Transpose&#xa;        self._varvals = [[convert(x) for x in row] &#xa;                         for row in zip(*self._varvals)]&#xa;        &#xa;        # Resize matrix nXm to mXn&#xa;        self._nobs, self._nvar = self._nvar, self._nobs&#xa;        new_nvar = self._nvar&#xa;    &#xa;        # Change format&#xa;        self._fmtlist = ['%9.0g'] * new_nvar&#xa;    &#xa;        # Change type&#xa;        new_type = self._default_new_type&#xa;        self._typlist = [new_type] * new_nvar&#xa;    &#xa;        # Change names of Variabls&#xa;        self._varlist = ['v' + str(i) for i in range(new_nvar)]&#xa;    &#xa;        # Change sort list to all Nones&#xa;        self._srtlist = [None] * new_nvar&#xa;    &#xa;        # Change label list to all empties&#xa;        self._lbllist = [''] * new_nvar&#xa;    &#xa;        # Change var label list to all empties&#xa;        self._vlblist = [''] * new_nvar&#xa;    &#xa;        # Empty Character Dict&#xa;        self._chrdict = {}&#xa;        &#xa;        # If varname=True , append old variable names in a new variable&#xa;        # called _varname&#xa;        if varname:&#xa;            self.append_var(""_varname"", old_varnames)&#xa;    &#xa;        # Set changed to True&#xa;        self._changed = True&#xa;        &#xa;    def replace(self, id, values, in_=None, if_=None):&#xa;        """"""Replace values in given data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        id : int or str&#xa;            Single variable index (int) or name (str).&#xa;            For str, an abbreviation is allowed if unambiguous.&#xa;        values : iterable&#xa;            Can be a flat iterable like [1, 5, 9, ...] or iterable of&#xa;            rows, like [[1], [5], [9], ...].&#xa;            Should have the same number of values as current number of&#xa;            observations or as implied by `in_` and `if_`.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations replace.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations replace.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the replaced observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Replaces values in given data variable.&#xa;        &#xa;        """"""&#xa;        # argument checking will be done with __setitem__&#xa;        &#xa;        if in_ is None:&#xa;            in_ = range(self._nobs)&#xa;        &#xa;        if if_ is None:&#xa;            rows = tuple(in_)&#xa;        else:&#xa;            rows = tuple(i for i in in_ if if_(i))&#xa;        &#xa;        # type and size checking happens in __setitem__&#xa;        self.__setitem__((rows, id), values)&#xa;        &#xa;        # __setitem__ will set self._changed = True if appropriate&#xa;&#xa;    def note_add(self, evarname, note, replace=False, in_=None):&#xa;        """"""Add given note for varname or '_dta', &#xa;        or replacing existing note if specified.&#xa;        &#xa;        Note will be truncated to 67,784 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        note : str&#xa;            Note should be ascii ('iso-8859-1'). &#xa;            Otherwise, the note will not be saved as intended.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Specify that existing note should be replaced.&#xa;            If `replace` is True, `in_` must be specified as well.&#xa;            Otherwise, `replace` will be ignored.&#xa;            Default value is False.&#xa;        in_ : int, optional&#xa;            Note number to replace (>= 1).&#xa;            Only used if `replace` is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Inserts text as new note or replacement for old note.&#xa;        &#xa;        """"""&#xa;        if not isinstance(note, str):&#xa;            raise TypeError(""note should be a string"")&#xa;        names = self._find_vars(evarname, evars=True,&#xa;                                empty_ok=False, single=True)&#xa;        evarname = names[0]&#xa;        replace = replace and (in_ is not None)&#xa;        if replace:&#xa;            if not isinstance(in_, int):&#xa;                raise TypeError(""in_ should be int or None"")&#xa;            if in_ <= 0:&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;            if (evarname not in self._chrdict &#xa;                  or 'note0' not in self._chrdict[evarname] &#xa;                  or 'note' + str(in_) not in self._chrdict[evarname]):&#xa;                raise ValueError(""note not found; could not be replaced"")&#xa;                &#xa;        if evarname not in self._chrdict:&#xa;            self._chrdict[evarname] = {}&#xa;        evar_chars = self._chrdict[evarname]&#xa;&#xa;        # In Stata, number of notes is limited to 9999. Limit here &#xa;        # is set at 10000, assuming one of the notes is 'note0'.&#xa;        nnotes = len([1 for k, v in evar_chars.items() &#xa;                      if re.match(r'^note[0-9]+$', k)])&#xa;        if nnotes > 10000 or (nnotes >= 10000 and not replace):&#xa;            raise ValueError(evarname + "" already has 10000 notes"")&#xa;            &#xa;        if 'note0' not in evar_chars:&#xa;            evar_chars['note0'] = '1'&#xa;            note_num = 1&#xa;        elif not replace:&#xa;            note_num = int(evar_chars['note0']) + 1&#xa;            evar_chars['note0'] = str(note_num)&#xa;        else:&#xa;            note_num = in_&#xa;        &#xa;        evar_chars['note' + str(note_num)] = note[:67784]&#xa;        self._changed = True&#xa;        &#xa;    notes_add = note_add&#xa;        &#xa;    def note_replace(self, evarname, note, in_):&#xa;        """"""Replace existing note for varname or '_dta'.&#xa;        &#xa;        Note will be truncated to 67,784 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        note : str&#xa;            Note should be ascii ('iso-8859-1'). &#xa;            Otherwise, the note will not be saved as intended.&#xa;        in_ : int&#xa;            Note number to replace (>= 1).&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Inserts text as replacement for old note.&#xa;        &#xa;        """"""&#xa;        self.note_add(evarname, note, replace=True, in_=in_)&#xa;        &#xa;    notes_replace = note_replace&#xa;        &#xa;    def note_renumber(self, evarname):&#xa;        """"""Remove gaps in note numbers.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Renumbers notes if necessary.&#xa;        &#xa;        """"""&#xa;        names = self._find_vars(evarname, evars=True,&#xa;                                empty_ok=False, single=True)&#xa;        evarname = names[0]&#xa;        if (evarname not in self._chrdict or &#xa;                'note0' not in self._chrdict[evarname]):&#xa;            return&#xa;        evar_chars = self._chrdict[evarname]&#xa;        last_seen_old = 0&#xa;        last_seen_new = 0&#xa;        nnotes = int(evar_chars['note0'])&#xa;        for new_num in range(1, nnotes+1):&#xa;            for old_num in range(last_seen_old+1, nnotes+1):&#xa;                old_name = 'note' + str(old_num)&#xa;                if old_name in evar_chars:&#xa;                    last_seen_old = old_num&#xa;                    if old_num == new_num: break&#xa;                    evar_chars['note' + str(new_num)] = evar_chars[old_name]&#xa;                    last_seen_new = new_num&#xa;                    del evar_chars[old_name]&#xa;                    self._changed = True&#xa;                    break&#xa;        if last_seen_new == 0: # probably shouldn't occur during normal usage&#xa;            del evar_chars['note0']&#xa;        else:&#xa;            evar_chars['note0'] = str(last_seen_new)&#xa;        &#xa;    notes_renumber = note_renumber&#xa;        &#xa;    def note_drop(self, evarnames, in_=None):&#xa;        """"""Drop notes in given numbers for given evarnames.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarnames : str or iterable of str&#xa;            Names of data variable(s) or '_dta'.&#xa;            Abbreviations of variable names are allowed if unambiguous.&#xa;        in_ : int or iterable of int, optional&#xa;            Note number(s) to drop (>= 1).&#xa;            If `in_' not specified or is None, all notes will be dropped&#xa;            for given evarnames.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes notes.&#xa;        &#xa;        """"""&#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (not isinstance(in_, collections.Iterable)&#xa;                    or not all(isinstance(n, int) for n in in_)):&#xa;                raise TypeError(""in_ should be int or iterable of int"")&#xa;            if any(n <= 0 for n in in_):&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;        else:&#xa;            in_ = ()&#xa;            &#xa;        in_intersect = set(in_).intersection&#xa;        &#xa;        evarnames = self._find_vars(evarnames, evars=True, empty_ok=False)&#xa;        chrdict = self._chrdict&#xa;        for name in evarnames:&#xa;            if name not in chrdict: continue&#xa;            chars = chrdict[name]&#xa;            &#xa;            if 'note0' not in chars: continue&#xa;            &#xa;            note_nums = {int(k[4:]) for k in chars if k.startswith(""note"")}&#xa;            drop_nums = in_intersect(note_nums) if in_ else note_nums&#xa;            &#xa;            if len(drop_nums) == 0: continue            &#xa;            &#xa;            keep_nums = note_nums - drop_nums&#xa;            &#xa;            drop_all = False&#xa;            &#xa;            if keep_nums == set() or keep_nums == {0,}:&#xa;                drop_all = True&#xa;                drop_nums.add(0)&#xa;                &#xa;            if drop_all and len(drop_nums) == len(chars):&#xa;                del chrdict[name]&#xa;            else:&#xa;                for num in drop_nums:&#xa;                    del chars['note' + str(num)]&#xa;                &#xa;                if not drop_all:&#xa;                    chars['note0'] = str(max(keep_nums))&#xa;            &#xa;            self._changed = True&#xa;        &#xa;    notes_drop = note_drop&#xa;        &#xa;    def note_list(self, evarnames="""", in_=None):&#xa;        """"""List notes in given numbers for given evarnames.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarnames : str or iterable of str&#xa;            Names of data variable(s) or '_dta'.&#xa;            Abbreviations of variable names are allowed if unambiguous.&#xa;        in_ : int or iterable of int, optional&#xa;            Note number(s) to drop (>= 1).&#xa;            If `in_' not specified or is None, all notes will be dropped&#xa;            for given evarnames.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays notes.&#xa;        &#xa;        """"""&#xa;        evarnames = self._find_vars(evarnames, evars=True, &#xa;                                    unique=True, empty_ok=True)&#xa;        if len(evarnames) == 0:&#xa;            evarnames = ['_dta'] + self._varlist&#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (not isinstance(in_, collections.Iterable)&#xa;                    or not all(isinstance(n, int) for n in in_)):&#xa;                raise TypeError(""in_ should be int or iterable of int"")&#xa;            if any(n <= 0 for n in in_):&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;        for name in evarnames:&#xa;            if name not in self._chrdict: continue&#xa;            chars = self._chrdict[name]&#xa;            if 'note0' not in chars: continue&#xa;            nnotes = int(chars['note0'])&#xa;            in_range = in_ if in_ is not None else range(1, nnotes + 1)&#xa;            note_info = []&#xa;            for note_num in in_range:&#xa;                note_name = 'note' + str(note_num)&#xa;                if note_name in chars:&#xa;                    note_info.append(note_num)&#xa;            if note_info != []:&#xa;                note_info.insert(0, name)&#xa;                self._display_notes(note_info)&#xa;        &#xa;    notes_list = note_list&#xa;        &#xa;    def _search_in_notes(self, evarname, text):&#xa;        """"""convenience function for self.note_search""""""&#xa;        matches = []&#xa;        if evarname in self._chrdict and ""note0"" in self._chrdict[evarname]:&#xa;            chars = self._chrdict[evarname]&#xa;            nnotes = int(chars['note0'])&#xa;            for note_num in range(1, nnotes + 1):&#xa;                note_name = 'note' + str(note_num)&#xa;                if note_name in chars and text in chars[note_name]:&#xa;                    matches.append(note_num)&#xa;            if matches != []:&#xa;                matches.insert(0, evarname)&#xa;        return matches&#xa;        &#xa;    def _display_notes(self, note_info):&#xa;        """"""convenience function for self.note_search""""""&#xa;        if note_info == []: return&#xa;        evarname = note_info[0]&#xa;        chars = self._chrdict[evarname]&#xa;        &#xa;        if IN_STATA:&#xa;            tplt = ""{{text}}{:>3}. {}""&#xa;            print(""\n{res}"" + evarname)&#xa;        else:&#xa;            tplt = ""{:>3}. {}""&#xa;            print(""\n"" + evarname)&#xa;        &#xa;        for num in note_info[1:]:&#xa;            print(tplt.format(num, chars['note' + str(num)]))&#xa;        &#xa;    def note_search(self, text):&#xa;        """"""Search in notes for exact matches of given text.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        text : str&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays notes matching text.&#xa;        &#xa;        """"""&#xa;        if not isinstance(text, str):&#xa;            raise TypeError(""search argument should be str"")&#xa;        search_in_notes = self._search_in_notes&#xa;        display_notes = self._display_notes&#xa;        varlist = self._varlist&#xa;        display_notes(search_in_notes('_dta', text))&#xa;        for evarname in varlist:&#xa;            display_notes(search_in_notes(evarname, text))&#xa;        &#xa;    notes_search = note_search&#xa;        &#xa;    def label_data(self, label):&#xa;        """"""Add given label to data. &#xa;        &#xa;        Label will be truncated to 80 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        label : str&#xa;            Label should be ascii ('iso-8859-1'). &#xa;            Otherwise, the label will not be saved as intended.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds label to data.&#xa;        &#xa;        """"""&#xa;        if not isinstance(label, str):&#xa;            raise TypeError(""data label should be a string"")&#xa;        if len(label) > 80:&#xa;            if not self._quiet:&#xa;                if IN_STATA:&#xa;                    print(""{err}truncating label to 80 characters"")&#xa;                else:&#xa;                    print(""truncating label to 80 characters"")&#xa;            label = label[:80]&#xa;        if self._data_label == label:&#xa;            return&#xa;        self._data_label = label&#xa;        self._changed = True&#xa;        &#xa;    def label_variable(self, varname, label):&#xa;        """"""Add given label to variable.&#xa;        &#xa;        Label will be truncated to 80 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varname : str&#xa;            Single varname (abbreviation allowed if unambiguous).&#xa;        label : str&#xa;            Label should be ascii ('iso-8859-1'). &#xa;            Otherwise, the label will not be saved as intended.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds label to variable.&#xa;        &#xa;        """"""&#xa;        if not isinstance(label, str):&#xa;            raise TypeError(""variable label should be a string"")&#xa;        names = self._find_vars(varname, empty_ok=False, single=True)&#xa;        index = self._varlist.index(names[0])&#xa;        label = label[:80]&#xa;        if self._vlblist[index] == label:&#xa;            return&#xa;        self._vlblist[index] = label&#xa;        self._changed = True&#xa;                &#xa;    def _fix_fmts(self, labname, mapping):&#xa;        """"""For use in labeling functions. This function modifies &#xa;        fmts if needed to accomodate values in labeling dict.&#xa;        &#xa;        """"""&#xa;        default_fmt_widths = self._default_fmt_widths&#xa;        &#xa;        indexes = [i for i in range(self._nvar) if self._lbllist[i] == labname]&#xa;        if indexes == []: return&#xa;        lab_size = max([len(v) for k, v in mapping.items()])&#xa;        fmtlist = self._fmtlist&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        for i in indexes:&#xa;            if isstrvar(i):&#xa;                continue # string values should not be labeled&#xa;            old_fmt = fmtlist[i]&#xa;            # check match agains numerical format&#xa;            match = NUM_FMT_RE.match(old_fmt)&#xa;            if match:&#xa;                fmt_width = int(match.group(3))&#xa;                if fmt_width < lab_size:&#xa;                    prefix = ('%' + (match.group(1) or '') + &#xa;                              (match.group(2) or ''))&#xa;                    suffix = (match.group(4) + match.group(5) + &#xa;                              match.group(6) + (match.group(7) or ''))&#xa;                    new_fmt = prefix + str(lab_size) + suffix&#xa;                    fmtlist[i] = new_fmt&#xa;                    self._changed = True&#xa;            elif TIME_FMT_RE.match(old_fmt) or TB_FMT_RE.match(old_fmt):&#xa;                continue&#xa;            else: &#xa;                # Here, some garbled format must have been entered. &#xa;                # More effort could be made to identify intended format, &#xa;                # but instead we'll just paint over it.&#xa;                fmt_width = default_fmt_widths[typlist[i]]&#xa;                fmtlist[i] = '%' + str(max((lab_size,fmt_width))) + '.0g'&#xa;                self._changed = True&#xa;        &#xa;    def label_define(self, name, mapping, &#xa;            add=False, replace=False, modify=False, fix=True):&#xa;        """"""Define a VALUE label, a mapping from numeric values to string.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of the value label, i.e. name of the mapping.&#xa;            If a mapping with this name already exists, `add`, &#xa;            `replace`, or `modify` should be set to True.&#xa;        mapping : dict&#xa;            Keys should be int or float, dict values should be str.&#xa;        add : bool (or coercible to bool), optional&#xa;            Whether mapping should be added to existing mapping.&#xa;            An error will be raised if old and new mapping share keys.&#xa;            Default value is False.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Whether mapping should replace existing mapping entirely.&#xa;            Default value is False.&#xa;        modify : bool (or coercible to bool), optional&#xa;            Whether mapping should update existing mapping, i.e.&#xa;            replace when there is an overlap in keys, add otherwise.&#xa;            Default value is False.&#xa;        fix : bool (or coercible to bool), optional&#xa;            When replacing or modifying an existing mapping, this&#xa;            determines whether display formats on any data variables &#xa;            that use this mapping should be expanded to accommodate &#xa;            the new labels, if necessary.&#xa;            Default value is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Stores value -> label map in dataset (does not apply it).&#xa;        &#xa;        """"""&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""label name should be str"")&#xa;        if not isinstance(mapping, dict):&#xa;            raise TypeError(""value, label mapping should be dict"")&#xa;        if modify: add = True&#xa;        if add and replace:&#xa;            raise ValueError(""replace option may not be combined with add"")&#xa;        if name in self._vallabs:&#xa;            if not (add or replace or modify):&#xa;                raise ValueError(""label exists; use add, replace, or modify"")&#xa;            elif add and not modify:&#xa;                # check for conflicts between new and old mapping&#xa;                old_map = self._vallabs[name]&#xa;                for k in mapping:&#xa;                    if k in old_map:&#xa;                        raise ValueError(""conflict with existing labels"")&#xa;        &#xa;        # test that keys are int and labels are str&#xa;        if not all(isinstance(k, int) and isinstance(v, str) &#xa;                   for k,v in mapping.items()):&#xa;            raise TypeError(""value, label mapping should be from int to str"")&#xa;        &#xa;        # make copy of mapping&#xa;        mapping = {k:v for k,v in mapping.items()}&#xa;            &#xa;        if not add or name not in self._vallabs:&#xa;            # also if replace=True (after passing checks above)&#xa;            self._vallabs[name] = mapping&#xa;        else:&#xa;            # update old value label map with new map&#xa;            self._vallabs[name].update(mapping)&#xa;        &#xa;        # if any variables already use this label,&#xa;        # check and possibly change fmt&#xa;        if fix:&#xa;            self._fix_fmts(name, mapping)&#xa;        &#xa;        # it would be a little complicated here to check if anything actually &#xa;        # changes (only in doubt with replace and modify), so assume changed&#xa;        self._changed = True&#xa;    &#xa;    def label_copy(self, orig_name, copy_name, replace=False):&#xa;        """"""Make a copy of mapping `orig_name` with name `copy_name`&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        orig_name : str&#xa;            Name of the existing value label mapping.&#xa;        copy_name : str&#xa;            Name to be given to the copy.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Whether the copy should replace an existing mapping.&#xa;            Required if a mapping with name `copy_name` already exists.&#xa;            Default value is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Stores or replaces a copy of a value -> label map in the data set.&#xa;        &#xa;        """"""&#xa;        if not isinstance(orig_name, str) or not isinstance(copy_name, str):&#xa;            raise TypeError(""label names should be str"")&#xa;        if orig_name not in self._vallabs:&#xa;            raise KeyError(orig_name + "" is not an existing label name"")&#xa;        if copy_name in self._vallabs and not replace:&#xa;            msg = copy_name + "" label exists; use replace option to replace""&#xa;            raise ValueError(msg)&#xa;        self._vallabs[copy_name] = self._vallabs[orig_name].copy()&#xa;        # assume something has changed (only in doubt with replace)&#xa;        self._changed = True&#xa;        &#xa;    def label_dir(self):&#xa;        """"""Display names of defined value -> label maps&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays names of existing value -> label maps.&#xa;        &#xa;        """"""&#xa;        for lblname in self._vallabs:&#xa;            print(lblname)&#xa;        &#xa;    def label_list(self, labnames=None):&#xa;        """"""Show value, label pairs for given maps,&#xa;        or for all such maps if none specified.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        labnames : str or iterable of str, optional&#xa;            One or more names of existing value -> label maps to display.&#xa;            Default is to show all defined maps.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays value -> label maps.&#xa;        &#xa;        """"""&#xa;        vallabs = self._vallabs&#xa;        if labnames is None:&#xa;            labnames = vallabs.keys()&#xa;        else:&#xa;            if isinstance(labnames, str):&#xa;                labnames = (labnames,)&#xa;            elif (not isinstance(labnames, collections.Iterable)&#xa;                    or not all(isinstance(value, str) for value in labnames)):&#xa;                raise TypeError(""labnames should be str or iterable of str"")  &#xa;            labnames = set(name for value in labnames&#xa;                                for name in value.split())&#xa;            if not labnames.issubset(vallabs.keys()):&#xa;                bad_names = "", "".join(str(lbl) for lbl in &#xa;                                     labnames.difference(vallabs.keys()))&#xa;                raise KeyError(bad_names + "" are not defined labels"")&#xa;        for name in labnames:&#xa;            print(name + "":"")&#xa;            lbldict = vallabs[name]&#xa;            for value in lbldict:&#xa;                print(""{:>12} {}"".format(value, lbldict[value]))&#xa;        &#xa;    def label_drop(self, labnames=None, drop_all=False):&#xa;        """"""Delete value -> label maps from data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        labnames : str or iterable of str, optional&#xa;            One or more names of existing value -> label maps to display.&#xa;        drop_all : bool (or coercible to bool), optional&#xa;            Whether all value -> label maps should be dropped.&#xa;            Default value is False.&#xa;            &#xa;        Note&#xa;        ----&#xa;        Nothing will be done if neither `labnames` nor `drop_all` are&#xa;        specified. If both are specified, `drop_all' will be ignored.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Removes value -> label maps from data set. Association between&#xa;        data variables and maps names are not removed.&#xa;        &#xa;        """"""&#xa;        vallabs = self._vallabs&#xa;        if labnames is None:&#xa;            if drop_all:&#xa;                # Create copy of keys. Otherwise, set of keys changes.&#xa;                labnames = set(vallabs.keys()) &#xa;            else:&#xa;                msg = ""must specify label name(s) or drop_all==True""&#xa;                raise ValueError(msg)&#xa;        else:&#xa;            if isinstance(labnames, str):&#xa;                labnames = (labnames,)&#xa;            elif (not isinstance(labnames, collections.Iterable)&#xa;                    or not all(isinstance(value, str) for value in labnames)):&#xa;                raise TypeError(""labnames should be str or iterable of str"") &#xa;            labnames = set(name for value in labnames&#xa;                                for name in value.split())&#xa;            if not labnames.issubset(vallabs.keys()):&#xa;                bad_names = "", "".join(str(lbl) for lbl in &#xa;                                     labnames.difference(vallabs.keys()))&#xa;                raise KeyError(bad_names + "" are not defined labels"")&#xa;        for name in labnames:&#xa;            del vallabs[name]&#xa;        self._changed = True&#xa;        &#xa;    def label_values(self, varnames, labname, fix=True):&#xa;        """"""Associate (possibly non-existent) value -> label map with&#xa;        given data variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str or iterable of str&#xa;            One or more names of data variables.&#xa;            Abbreviations are allowed if unambiguous.&#xa;        labname : str&#xa;            Name of value -> label mapping to use with given variables.&#xa;            `labname` does not need to be the name of an existing map.&#xa;        fix : bool (or coercible to bool), optional&#xa;            Whether the variables' display formats should be expanded &#xa;            to accommodate the labels, if necessary.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Associates (possibly non-existent) value -> label map with&#xa;        given data variables.&#xa;        &#xa;        """"""&#xa;        if labname is None: labname = """"&#xa;        if not isinstance(labname, str):&#xa;            raise TypeError(""label name should be str"")&#xa;        varnames = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        index_func = self._varlist.index&#xa;        indexes = [index_func(name) for name in varnames]&#xa;        lbllist = self._lbllist&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        for i in indexes:&#xa;            if isstrvar(i):&#xa;                raise TypeError(""may not label strings"")&#xa;            lbllist[i] = labname&#xa;        if fix and labname in self._vallabs:&#xa;            self._fix_fmts(labname, self._vallabs[labname])&#xa;        # assume there are actual changes&#xa;        self._changed = True&#xa;        &#xa;    def _label_language_list_smcl(self, nlangs, langs, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        print(""{txt}{title:Language for variable and value labels}\n"")&#xa;        &#xa;        if nlangs <= 1:&#xa;            print(""    {txt}In this dataset, value and variable labels have"", &#xa;                  ""been defined in only one language: {res}"",&#xa;                  curr_lang)&#xa;        else:&#xa;            print(""    {txt}Available languages:"")&#xa;            for lang in langs:&#xa;                print(""            {res}"" + lang)&#xa;            print(""\n    {txt}Currently set is:{col 37}{res}"",&#xa;                  ""label_language(\""{}\"")\n"".format(curr_lang),&#xa;                  ""\n    {txt}To select different language:{col 37}{res}"", &#xa;                  ""<self>.label_language(<name>)"")&#xa;        &#xa;        print(""\n    {txt}To create new language:{col 37}{res}"",&#xa;              ""<self>.label_language(<name>, new=True)"",&#xa;              ""\n    {txt}To rename current language:{col 37}{res}"",&#xa;              ""<self>.label_language(<name>, rename=True)"")&#xa;        &#xa;    def _label_language_list_nosmcl(self, nlangs, langs, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        print(""Language for variable and value labels\n"")&#xa;        &#xa;        if nlangs <= 1:&#xa;            print(""    In this dataset, value and variable labels"",&#xa;                  ""have been defined in only one language: "",&#xa;                  curr_lang)&#xa;        else:&#xa;            print(""    Available languages:"")&#xa;            for lang in langs:&#xa;                print(""            {}"".format(lang))&#xa;            print(""\n    Currently set is:              "",  &#xa;                  ""label_language(\""{}\"")\n"".format(curr_lang),&#xa;                  ""\n    To select different language:  "", &#xa;                  ""<self>.label_language(<name>)"")&#xa;        &#xa;        print(""\n    To create new language:        "",&#xa;              ""<self>.label_language(<name>, new=True)"",&#xa;              ""\n    To rename current language:    "",&#xa;              ""<self>.label_language(<name>, rename=True)"")&#xa;        &#xa;    def _label_language_delete(self, languagename, langs,&#xa;                               curr_lang, name_exists):&#xa;        """"""helper function for label_language()""""""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        &#xa;        # shorten language list&#xa;        langs = [lang for lang in langs if lang != languagename]&#xa;        &#xa;        if languagename == curr_lang:&#xa;            vlblist = self._vlblist&#xa;            lbllist = self._lbllist&#xa;            &#xa;            curr_lang = langs[0]&#xa;            if not self._quiet:&#xa;                msg = ""{}(language {} now current language)""&#xa;                print(msg.format(""{txt}"" if IN_STATA else """", curr_lang))&#xa;            &#xa;            varlab_key = ""_lang_v_"" + curr_lang&#xa;            vallab_key = ""_lang_l_"" + curr_lang&#xa;            &#xa;            # replace data label, _lang_list, and _lang_c&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_c""] = curr_lang&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            if varlab_key in dta_dict:&#xa;                self._data_label = dta_dict.pop(varlab_key)&#xa;            &#xa;            # Stata does not drop value label&#xa;            &#xa;            # Replace variable and value labels, &#xa;            # and pop these entries from chrdict.&#xa;            # If this leaves a chrdict[varname] empty, delete it.&#xa;            for varname, i in zip(varlist, range(self._nvar)):&#xa;                lbllist[i] = '' &#xa;                # Next line probably not necessary. &#xa;                # There should be a var label in chrdict, &#xa;                # even if it's empty str.&#xa;                vlblist[i] = ''&#xa;                if varname in chrdict:&#xa;                    var_dict = chrdict[varname]&#xa;                    if varlab_key in var_dict:&#xa;                        vlblist[i] = var_dict.pop(varlab_key)&#xa;                    if vallab_key in var_dict:&#xa;                        lbllist[i] = var_dict.pop(vallab_key)&#xa;                    if len(var_dict) == 0:&#xa;                        del chrdict[varname]&#xa;                        &#xa;        # if deleted language is not the current language, &#xa;        # delete entries from chrdict&#xa;        else:&#xa;            varlab_key = ""_lang_v_"" + languagename&#xa;            vallab_key = ""_lang_l_"" + languagename&#xa;            &#xa;            # delete data label (if necessary) and replace _lang_list&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            if varlab_key in dta_dict:&#xa;                del dta_dict[varlab_key]&#xa;            &#xa;            # Stata does not drop value label&#xa;            &#xa;            # Delete variable and value label entries from chrdict.&#xa;            # If this leaves the sub-dictionary empty, delete it.&#xa;            for varname, i in zip(varlist, range(self._nvar)):&#xa;                if varname in chrdict:&#xa;                    var_dict = chrdict[varname]&#xa;                    if varlab_key in var_dict:&#xa;                        del var_dict[varlab_key]&#xa;                    if vallab_key in var_dict:&#xa;                        del var_dict[vallab_key]&#xa;                    if len(var_dict) == 0:&#xa;                        del chrdict[varname]&#xa;                        &#xa;    def _label_language_swap(self, languagename, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        vlblist = self._vlblist&#xa;        lbllist = self._lbllist&#xa;            &#xa;        old_varlab_key = ""_lang_v_"" + curr_lang&#xa;        old_vallab_key = ""_lang_l_"" + curr_lang&#xa;        &#xa;        new_varlab_key = ""_lang_v_"" + languagename&#xa;        new_vallab_key = ""_lang_l_"" + languagename&#xa;        &#xa;        # Replace data label and _lang_c. No need to set _lang_list: &#xa;        # can only swap between two defined languages.&#xa;        dta_dict = chrdict[""_dta""]&#xa;        dta_dict[""_lang_c""] = languagename&#xa;        if self._data_label != '':&#xa;            dta_dict[old_varlab_key] = self._data_label&#xa;        self._data_label = (dta_dict.pop(new_varlab_key) &#xa;                            if new_varlab_key in dta_dict else '')&#xa;        &#xa;        # put current variable and value labels in chrdict &#xa;        # and replace with languagename's&#xa;        for varname, i in zip(varlist, range(self._nvar)):&#xa;            varlab = vlblist[i]&#xa;            vallab = lbllist[i]&#xa;            &#xa;            if varname not in chrdict: # then nothing to retreive&#xa;                if varlab == '' and vallab == '': # then nothing to store&#xa;                    continue&#xa;                chrdict[varname] = {}&#xa;                &#xa;            var_dict = chrdict[varname]&#xa;            &#xa;            # store current if non-empty&#xa;            if varlab != '': var_dict[old_varlab_key] = varlab&#xa;            if vallab != '': var_dict[old_vallab_key] = vallab&#xa;            &#xa;            # set languagename's labels as current&#xa;            vlblist[i] = (var_dict.pop(new_varlab_key) &#xa;                          if new_varlab_key in var_dict else '')&#xa;            lbllist[i] = (var_dict.pop(new_vallab_key) &#xa;                          if new_vallab_key in var_dict else '')&#xa;            &#xa;            # delete sub-dict from chrdict if empty&#xa;            if len(var_dict) == 0:&#xa;                del chrdict[varname]&#xa;                &#xa;    def _put_labels_in_chr(self, languagename, langs, curr_lang):&#xa;        """"""Helper function for label_language(). Should only be called &#xa;        with -new- option. The languagename is the language that will &#xa;        be current _after_ calling this function. The curr_lang is the &#xa;        language that was current _before_ calling this function.&#xa;        &#xa;        """"""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        vlblist = self._vlblist&#xa;        lbllist = self._lbllist&#xa;            &#xa;        old_varlab_key = ""_lang_v_"" + curr_lang&#xa;        old_vallab_key = ""_lang_l_"" + curr_lang&#xa;        &#xa;        # change _lang_c and _lang_list, &#xa;        # and put data_label in chrdict if non-empty&#xa;        if ""_dta"" not in chrdict:&#xa;            chrdict[""_dta""] = {}&#xa;        dta_dict = chrdict[""_dta""]&#xa;        dta_dict[""_lang_c""] = languagename&#xa;        dta_dict[""_lang_list""] = "" "".join(langs) + "" "" + languagename&#xa;        if self._data_label != '':&#xa;            dta_dict[old_varlab_key] = self._data_label&#xa;        &#xa;        # put current variable and value labels in chrdict&#xa;        for varname, i in zip(varlist, range(self._nvar)):&#xa;            varlab = vlblist[i]&#xa;            vallab = lbllist[i]&#xa;                &#xa;            if varlab == '' and vallab == '': # then nothing to store&#xa;                continue&#xa;            &#xa;            if varname not in chrdict:&#xa;                chrdict[varname] = {}&#xa;                &#xa;            var_dict = chrdict[varname]&#xa;            &#xa;            # store current if non-empty&#xa;            if varlab != '': var_dict[old_varlab_key] = varlab&#xa;            if vallab != '': var_dict[old_vallab_key] = vallab&#xa;            &#xa;    def _get_language_info(self):&#xa;        """"""helper function for label_language()""""""&#xa;        # If the following does not find _lang_list, then it assumes &#xa;        # there are no defined languages. If it finds _lang_list and &#xa;        # _lang_c, and _lang_c is listed in _lang_list then it assumes &#xa;        # everything is correct. It only does further checking if &#xa;        # _lang_list is there AND either _lang_c is missing or _lang_c &#xa;        # is not in _lang_list.&#xa;        &#xa;        chrdict = self._chrdict&#xa;    &#xa;        if ""_dta"" not in chrdict or ""_lang_list"" not in chrdict[""_dta""]:&#xa;            nlangs = 1&#xa;            curr_lang = ""default""&#xa;            langs = [curr_lang,]&#xa;        else:&#xa;            dta_dict = chrdict[""_dta""]&#xa;            langs = dta_dict[""_lang_list""].split()&#xa;            nlangs = len(langs)&#xa;            has_lang_c = (""_lang_c"" in dta_dict)&#xa;            curr_lang = dta_dict['_lang_c'] if has_lang_c else 'default'&#xa;            # Safety in case of malformed chrdict. &#xa;            # Also guards against empty lang list.&#xa;            if curr_lang not in langs or not has_lang_c:&#xa;                if IN_STATA:&#xa;                    print("""".join(&#xa;                        (""{err}"",&#xa;                        ""odd values in characteristics; "",&#xa;                        ""trying to recover"")))&#xa;                else:&#xa;                    print(""odd values in characteristics; trying to recover"")&#xa;                &#xa;                # make sure curr_lang is not one of the stored languages&#xa;                &#xa;                # get stored languages&#xa;                stored_langs = set()&#xa;                for sub_dict in chrdict.values():&#xa;                    for key in sub_dict.keys():&#xa;                        if (key.startswith('_lang_l_') or &#xa;                                key.startswith('_lang_v_')):&#xa;                            stored_langs.add(key[8:])&#xa;                &#xa;                # if curr_lang in stored_langs, change curr_lang until it isn't&#xa;                count = 1&#xa;                while curr_lang in stored_langs:&#xa;                    if curr_lang[:7] == 'default':&#xa;                        count += 1&#xa;                        curr_lang = 'default' + str(count)&#xa;                    else:&#xa;                        curr_lang = 'default'&#xa;                    &#xa;                # make new langs and nlangs&#xa;                langs = list(stored_langs.union({curr_lang,}))&#xa;                nlangs = len(langs)&#xa;                &#xa;        return curr_lang, langs, nlangs&#xa;        &#xa;    def label_language(self, languagename=None, &#xa;                      new=False, copy=False, rename=False, delete=False):&#xa;        """"""Various functionality for manipulating groups of variable&#xa;        and data labels.&#xa;        &#xa;        Users may, for example, make the same data and variable labels&#xa;        multiple times in different languages, with each language a&#xa;        separate group. This function creates, deletes, renames and &#xa;        swaps groups.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        languagename : str, optional&#xa;            Name of label language/group.&#xa;            Required if using any of the other parameters.&#xa;            If used without other parameters, the action is to&#xa;            set specified language/group as ""current"".&#xa;        new : bool (or coercible to bool), optional&#xa;            Used to create new language and set as ""current"".&#xa;            New labels are empty when `copy` is not specified.&#xa;            Default value is False.&#xa;        copy : bool (or coercible to bool), optional&#xa;            Used to copy the ""current"" group of labels.&#xa;            Cannot be used without setting `new'=True.&#xa;            Hence, also sets the copy as ""current"".&#xa;            Default value is False.&#xa;        rename : bool (or coercible to bool), optional&#xa;            Rename current group of labels, or set the name of the &#xa;            current set, if no name has been applied.&#xa;            Default value is False.&#xa;        delete : bool (or coercible to bool), optional&#xa;            Delete given label language/group. Not allowed if there&#xa;            is only one defined label language/group.&#xa;            Default value is False.&#xa;            &#xa;        Note&#xa;        ----&#xa;        If no parameters are specified, the action is to list all label&#xa;        languages/groups. Of `new`, `copy`, `rename`, and `delete`, only&#xa;        `new` and `copy` may be used together. A `languagename` is&#xa;        restricted to 24 characters and will be truncated if necessary.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates, deletes, renames, or swaps label languages/groups.&#xa;        &#xa;        """"""&#xa;        global IN_STATA&#xa;        &#xa;        curr_lang, langs, nlangs = self._get_language_info()&#xa;        &#xa;        noptions = sum((new, copy, rename, delete))&#xa;        &#xa;        # list language info&#xa;        if languagename is None:&#xa;            if noptions != 0:&#xa;                msg = ""options cannot be used without language name""&#xa;                raise ValueError(msg)&#xa;            if IN_STATA:&#xa;                self._label_language_list_smcl(nlangs, langs, curr_lang)&#xa;            else:&#xa;                self._label_language_list_nosmcl(nlangs, langs, curr_lang)&#xa;            return&#xa;        &#xa;        # more error checking&#xa;        # only options that can be combined are -new- and -copy-&#xa;        if noptions > 1 and not (noptions == 2 and new and copy):&#xa;            raise ValueError(""only options -copy- and -new- can be combined"")&#xa;        if copy and not new:&#xa;            msg = ""option -copy- may only be specified with option -new-""&#xa;            raise ValueError(msg)&#xa;        &#xa;        if not isinstance(languagename, str):&#xa;            raise TypeError(""given language name must be str"")&#xa;        if len(languagename) > 24:&#xa;            if not self._quiet:&#xa;                if IN_STATA:&#xa;                    print(""{err}shortening language name to 24 characters"")&#xa;                else:&#xa;                    print(""shortening language name to 24 characters"")&#xa;            languagename = languagename[:24]&#xa;        &#xa;        name_exists = languagename in langs&#xa;        &#xa;        # switch languages&#xa;        if noptions == 0:&#xa;            if not name_exists:&#xa;                msg = ""language {} not defined"".format(languagename)&#xa;                raise ValueError(msg)&#xa;            if languagename == curr_lang:&#xa;                if not self._quiet:&#xa;                    msg = ""{}({} already current language)""&#xa;                    print(msg.format(""{txt}"" if IN_STATA else """", curr_lang))&#xa;            else:&#xa;                self._label_language_swap(languagename, curr_lang)&#xa;            return&#xa;        &#xa;        # delete language&#xa;        if delete:&#xa;            if not name_exists:&#xa;                msg = ""language {} not defined"".format(languagename)&#xa;                raise ValueError(msg)&#xa;            if nlangs == 1:&#xa;                msg = (""language {} is the only language defined; "" + &#xa;                       ""it may not be deleted"")&#xa;                raise ValueError(msg.format(languagename))&#xa;            self._label_language_delete(languagename, langs,&#xa;                                        curr_lang, name_exists)&#xa;            return&#xa;        &#xa;        # From this point, rename == True or new == True.&#xa;        # Both require given languagename not already exist.&#xa;        if name_exists:&#xa;            raise ValueError(""language {} already exists"".format(languagename))&#xa;        &#xa;        # rename current language&#xa;        if rename:&#xa;            chrdict = self._chrdict&#xa;            # only need to change _lang_c and _lang_list&#xa;            if '_dta' not in chrdict:&#xa;                chrdict['_dta'] = {}&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_c""] = languagename&#xa;            curr_lang_index = langs.index(curr_lang)&#xa;            langs[curr_lang_index] = languagename&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            return&#xa;            &#xa;        # only option left is -new-&#xa;        # push current labels to chrdict&#xa;        if nlangs >= 100:&#xa;            raise ValueError(""100 label languages exist; limit reached"")&#xa;        self._put_labels_in_chr(languagename, langs, curr_lang)&#xa;        if copy:&#xa;            # use current labels&#xa;            if not self._quiet:&#xa;                msg = ""{}(language {} now current language)""&#xa;                print(msg.format(""{txt}"" if IN_STATA else """", languagename))&#xa;        else:&#xa;            # empty current labels&#xa;            nvar = self._nvar&#xa;            self._data_label = ''&#xa;            self._vlblist = [''] * nvar&#xa;            self._lbllist = [''] * nvar&#xa;    &#xa;    def _check_index(self, prior_len, index):&#xa;        """"""To be used with __getitem__ and __setitem__ . Checks that &#xa;        index is well-formed and puts it in consistent form.&#xa;        &#xa;        """"""&#xa;        if index is None: return range(prior_len)&#xa;        if isinstance(index, slice):&#xa;            start, stop, step = index.indices(prior_len)&#xa;            return range(start, stop, step)&#xa;        if isinstance(index, collections.Iterable):&#xa;            if not hasattr(index, ""__len__""):&#xa;                index = tuple(index)&#xa;            if not all(isinstance(i, int) for i in index):&#xa;                raise TypeError(""individual indices must be int"")&#xa;            # Integers assumed to be within proper range.&#xa;            # Later use will raise error otherwise, so no need to check here.&#xa;            return index&#xa;        # if next_index not slice and not iterable, assume it's an int&#xa;        if not isinstance(index, int):&#xa;            raise TypeError(""index must be slice, iterable (of int), or int"")&#xa;        if not -prior_len <= index < prior_len:&#xa;            raise IndexError(""index out of range"")&#xa;        return (index,)&#xa;        &#xa;    def _convert_col_index(self, index):&#xa;        """"""To be used with __getitem__ and __setitem__ . Checks that &#xa;        column index is well-formed and puts it in consistent form.&#xa;        &#xa;        """"""&#xa;        if index is None or isinstance(index, int): return index&#xa;        if isinstance(index, str):&#xa;            find_index = self._varlist.index&#xa;            return [find_index(v) for v in self._find_vars(index)]&#xa;        if isinstance(index, collections.Iterable):&#xa;            new_index = []&#xa;            append = new_index.append&#xa;            find_vars = self._find_vars&#xa;            find_index = self._varlist.index&#xa;            for i in index:&#xa;                if isinstance(i, str):&#xa;                    new_index += [find_index(i) for i in find_vars(i)]&#xa;                elif isinstance(i, int):&#xa;                    append(i)&#xa;                else:&#xa;                    msg = ""column iterable should contain only int or str""&#xa;                    raise TypeError(msg)&#xa;            if len(new_index) != len(set(new_index)):&#xa;                msg = ""columns cannot be repeated; use -clonevar- to copy""&#xa;                raise ValueError(msg)&#xa;            return new_index&#xa;        if isinstance(index, slice):&#xa;            start, stop, step = index.start, index.stop, index.step&#xa;            if not isinstance(start, int) and start is not None:&#xa;                if isinstance(start, str):&#xa;                    start = self._varlist.index(self._find_vars(start)[0])&#xa;                else:&#xa;                    raise TypeError(""column slice values must be str or int"")&#xa;            if not isinstance(stop, int) and stop is not None:&#xa;                if isinstance(stop, str):&#xa;                    stop = self._varlist.index(self._find_vars(stop)[0])&#xa;                else:&#xa;                    raise TypeError(""column slice values must be str or int"")&#xa;            return slice(start, stop, step)&#xa;        msg = ""column should be index (int), name (str), slice, or iterable""&#xa;        raise TypeError(msg)&#xa;        &#xa;    def get(self, row, col):&#xa;        """"""Get single data value.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        row : integer&#xa;            Row (observation) number of data value.&#xa;        col : integer&#xa;            Column (data variable) number of data value.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        string, numeric, missing value, or binary data value&#xa;        &#xa;        """"""&#xa;        return self._varvals[row][col]&#xa;            &#xa;    def __getitem__(self, index):&#xa;        """"""return Dta object containing obs &#xa;        and vars specified by index tuple&#xa;        &#xa;        """"""&#xa;        if not isinstance(index, tuple) or not 1 <= len(index) <= 2:&#xa;            msg = ""data subscripting must be [rows,cols] or [rows,]""&#xa;            raise ValueError(msg)&#xa;        sel_rows = self._check_index(self._nobs, index[0])&#xa;        sel_cols = (self._convert_col_index(index[1]) &#xa;                    if len(index) == 2 else None)&#xa;        sel_cols = self._check_index(self._nvar, sel_cols)&#xa;        # call instance constructor&#xa;        return self.__class__(self, sel_rows, sel_cols)&#xa;        &#xa;    def _standardize_input(self, value):&#xa;        """"""helper for functions like __setitem__ &#xa;        that put new values into the dataset&#xa;        &#xa;        """"""&#xa;        tuple_maker = lambda x: ((x,) &#xa;            if (any(isinstance(x, t) for t in (str, bytes, bytearray)) &#xa;                or not isinstance(x, collections.Iterable))&#xa;            else (x if hasattr(x, ""__len__"") else tuple(x)))&#xa;            &#xa;        if isinstance(value, Dta):&#xa;            value = value._varvals&#xa;        else: # force input into 2d structure&#xa;            if (any(isinstance(value, t) for t in (str,bytes,bytearray))&#xa;                    or not isinstance(value, collections.Iterable)):&#xa;                value = ((value,),)&#xa;            else:&#xa;                value = tuple(tuple_maker(v) for v in value)&#xa;            &#xa;        return value&#xa;        &#xa;    def __setitem__(self, index, value):&#xa;        """"""Replace values in specified obs and vars of -index- tuple.&#xa;        The shape of -value- should match the shape implied by -index-,&#xa;        and sub-values should be consistent with existing Stata types&#xa;        (e.g. non-string values cannot be added to string columns).&#xa;        &#xa;        """"""&#xa;        if not isinstance(index, tuple) or len(index) > 2:&#xa;            msg = ""data subscripting must be [rows,cols] or [rows,]""&#xa;            raise ValueError(msg)&#xa;        sel_rows = self._check_index(self._nobs, index[0])&#xa;        sel_cols = (self._convert_col_index(index[1])&#xa;                    if len(index) == 2 else None)&#xa;        sel_cols = self._check_index(self._nvar, sel_cols)&#xa;            &#xa;        nrows, ncols = len(sel_rows), len(sel_cols)&#xa;            &#xa;        value = self._standardize_input(value)&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if (nrows == 1 and ncols != 1 and &#xa;                len(value) == ncols and all(len(v) == 1 for v in value)):&#xa;            value = (tuple(v[0] for v in value),)&#xa;        else: # check that value dimensions match expected&#xa;            if not len(value) == nrows:&#xa;                raise ValueError(""length of value does not match # of rows"")&#xa;            if not all(len(v) == ncols for v in value):&#xa;                raise ValueError(""inner dimensions do not match # of columns"")&#xa;        &#xa;        # If no rows or no cols, nothing to do.&#xa;        # Could put this above the call to _standardize_input, &#xa;        # but then input of any shape allowed.&#xa;        if nrows == 0 or ncols == 0:&#xa;            return&#xa;        &#xa;        self._set_values(sel_rows, sel_cols, value)&#xa;        &#xa;        # Modify srtlist if necessary. If col_num is in srtlist, drop it&#xa;        # and any to the right. Ideally, would only make this change if &#xa;        # values were truly changed, by comparing new value with old.&#xa;        srtlist = self._srtlist&#xa;        nvar = self._nvar&#xa;        for col_num in sel_cols:&#xa;            if col_num in srtlist:&#xa;                srt_pos = srtlist.index(col_num)&#xa;                srtlist = srtlist[:srt_pos] + [None]*(nvar - srt_pos)&#xa;        self._srtlist = srtlist&#xa;        &#xa;        self._changed = True&#xa;                    &#xa;    def __len__(self):&#xa;        """"""return number of observations""""""&#xa;        return len(self._varvals)&#xa;        &#xa;    def format(self, varnames, fmts):&#xa;        """"""Set the Stata display format of given data variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        fmts : str, or iterable of str&#xa;            Can be a str containing one format (e.g. ""%9.2f""),&#xa;            a str with multiple formats (e.g. ""%9.2f %12s %9.0g""),&#xa;            or an iterable of such str&#xa;            (e.g. (""%9.2f"", ""%12s"", ""%9.0g"") or (""%9.2f"", ""%12s %9.0g"")).&#xa;        &#xa;        Notes&#xa;        -----&#xa;        If there are fewer formats than varnames, the last format will&#xa;        be repeated. Any extraneous formats will be ignored.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Sets the display format of the given data variables.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=False)&#xa;        indexes = list(map(self._varlist.index, varnames))&#xa;        &#xa;        # check that fmts are specified properly&#xa;        if isinstance(fmts, str):&#xa;            fmts = fmts.split()&#xa;        else:&#xa;            if ( not isinstance(fmts, collections.Iterable) &#xa;                    or not all(isinstance(f, str) for f in fmts) ):&#xa;                raise TypeError(""given fmts must be str or iterable of str"")&#xa;            fmts = [x for s in fmts for x in s.split()]&#xa;        if len(fmts) == 0:&#xa;            raise ValueError(""no formats specified"")&#xa;        &#xa;        # check fmts for validity&#xa;        is_valid = self._is_valid_fmt&#xa;        if not all(is_valid(fmt) for fmt in fmts):&#xa;            bad_fmts = "" "".join(fmt for fmt in fmts if not is_valid(fmt))&#xa;            raise ValueError(""invalid formats: "" + bad_fmts)&#xa;            &#xa;        # pad fmts if necessary &#xa;        nvarnames = len(varnames)&#xa;        nfmts = len(fmts)&#xa;        if nfmts < nvarnames:&#xa;            fmts = list(fmts) + [fmts[-1]]*(nvarnames - nfmts)&#xa;            &#xa;        # check that formats match Stata types&#xa;        #typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        if not all(isstrvar(i) == bool(STR_FMT_RE.match(fmt))&#xa;                for i, fmt in zip(indexes, fmts)):&#xa;            raise ValueError(""format does not match Stata variable type"")&#xa;        &#xa;        # replace fmts (extras, if any, don't get used)&#xa;        for i, fmt in zip(indexes, fmts):&#xa;            self._fmtlist[i] = fmt&#xa;        &#xa;        # assume there are changes&#xa;        self._changed = True&#xa;        &#xa;    def copy(self):&#xa;        """"""Create a copy of the current Dta instance.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        Dta instance&#xa;        &#xa;        """"""&#xa;        c = self.__class__(self) # using self's constructor on self&#xa;        c._srtlist = list(self._srtlist)  # copy, srtlist not copied in init&#xa;        return c&#xa;        &#xa;    def __eq__(self, other):&#xa;        """"""Compare two datasets for equality. &#xa;        &#xa;        Does not test data label, time stamp, or file name.&#xa;        &#xa;        Tests&#xa;        - number of data variables,&#xa;        - number of observations,&#xa;        - dta version,&#xa;        - data types,&#xa;        - data variable names,&#xa;        - 'sorted by' information,&#xa;        - display formats,&#xa;        - value -> label mappings applied to data variables,&#xa;        - data variable labels,&#xa;        - defined characteristics,&#xa;        - data values, and&#xa;        - defined value -> label mappings.&#xa;        &#xa;        If wanting information about differences between two&#xa;        Dta objects, use the function `stata_dta.display_diff`.&#xa;        &#xa;        """"""&#xa;        # check that is Dta and is same version&#xa;        if not self.__class__ == other.__class__: return False&#xa;        &#xa;        # pertinent header info&#xa;        if not self._nvar == other._nvar: return False&#xa;        if not self._nobs == other._nobs: return False&#xa;        if not self._ds_format == other._ds_format: return False&#xa;        #if not self._data_label == other._data_label: return False # keep ?&#xa;        &#xa;        #descriptors&#xa;        if not self._typlist == other._typlist: return False&#xa;        if not self._varlist == other._varlist: return False&#xa;        # Remove check on srtlist? With this here, data[:, :] != data.&#xa;        if not self._srtlist == other._srtlist: return False&#xa;        if not self._fmtlist == other._fmtlist: return False&#xa;        if not self._lbllist == other._lbllist: return False&#xa;        &#xa;        # variable labels&#xa;        if not self._vlblist == other._vlblist: return False&#xa;        &#xa;        # expansion fields&#xa;        if not self._chrdict == other._chrdict: return False&#xa;        &#xa;        # data&#xa;        if not self._varvals == other._varvals: return False&#xa;        &#xa;        # value labels&#xa;        if not self._vallabs == other._vallabs: return False&#xa;        &#xa;        return True&#xa;        &#xa;    def check(self, version=None):&#xa;        """"""Determine whether saved data set would conform to limits&#xa;        of given *Stata* version. (Not .dta version.)&#xa;        &#xa;        See -help limits- in Stata for more info.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        version : int, optional&#xa;            Specify a Stata version to check against.&#xa;            Default is to check against Stata version 13.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Display summary of limits violations, if any.&#xa;        &#xa;        """"""&#xa;        &#xa;        if version is None:&#xa;            print(""assuming Stata version 13"")&#xa;            version = 13&#xa;        if version not in (11, 12, 13):&#xa;            raise ValueError(""allowed versions are 11 through 13"")&#xa;        &#xa;        width = self.width&#xa;        nvar = self._nvar&#xa;        nobs = self._nobs&#xa;        &#xa;        chrdict = self._chrdict&#xa;        &#xa;        char_len = max([0] + [len(char) for evar, evardict in chrdict.items()&#xa;                                 for char in evardict.values()])&#xa;        num_val_encodings = max([0] + [len(mapping) &#xa;                                       for mapping in self._vallabs.values()])&#xa;        max_note_size = max([0] + [len(note) for d in chrdict.values()&#xa;                                   for name, note in d.items()&#xa;                                   if re.match(r'^note[0-9]+$', name)])&#xa;        max_num_notes = max([0] + [len([1 for name,note in d.items() &#xa;                                        if re.match(r'^note[0-9]+$', name)])&#xa;                                   for d in chrdict.values()])&#xa;                &#xa;        if '_dta' in chrdict and '_lang_list' in chrdict['_dta']:&#xa;            n_label_langs = len(chrdict['_dta'][""_lang_list""].split())&#xa;        else:&#xa;            n_label_langs = 0&#xa;                                    &#xa;        small_good = medium_good = large_good = True&#xa;        general_good = format_good = True&#xa;        &#xa;        print(""\nformat problems"")&#xa;        if self._ds_format == 117 and version <= 12:&#xa;            format_good = False&#xa;            print(""    format 117 cannot be opened by Stata version "" + &#xa;                  str(version))&#xa;        if version < 12 and any(TB_FMT_RE.match(fmt) for fmt in self._fmtlist):&#xa;            format_good = False&#xa;            print(""    Stata version "" + str(version) + &#xa;                  "" cannot understand tb format"")&#xa;        if format_good:&#xa;            print(""    none"")&#xa;        &#xa;        print(""\ngeneral size problems"")&#xa;        if len(self._data_label) > 80:&#xa;            general_good = False&#xa;            print(""    data label length > 80"")&#xa;        if any(len(name) > 32 for name in self._varlist):&#xa;            general_good = False&#xa;            print(""    variable name length > 32"")&#xa;        if any(len(v) > 80 for v in self._vlblist):&#xa;            general_good = False&#xa;            print(""    variable label length > 80"")&#xa;        if any(len(name) > 32 for name in self._vallabs.keys()):&#xa;            general_good = False&#xa;            print(""    value label name length > 32"")&#xa;        if any(len(valstr) > 32000 &#xa;               for mapping in self._vallabs.values()&#xa;               for valstr in mapping.values()):&#xa;            general_good = False&#xa;            print(""    value label string length > 32,000"")&#xa;        if max_num_notes > 10000:&#xa;            # limit here is set at 10000, assuming one of the notes is 'note0'&#xa;            general_good = False&#xa;            print(""    number of notes for single variable or _dta > 9,999"")&#xa;        if n_label_langs > 100:&#xa;            general_good = False&#xa;            print(""    number of label languages > 100"")&#xa;        if general_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata small problems"")&#xa;        if width > 800:&#xa;            small_good = False&#xa;            print(""    data set width > 800"")&#xa;        if nvar > 99:&#xa;            small_good = False&#xa;            print(""    numbar of variables > 99"")&#xa;        if nobs > 1200:&#xa;            small_good = False&#xa;            print(""    number of observations > 1,200"")&#xa;        if num_val_encodings > 1000:&#xa;            small_good = False&#xa;            print(""    number of encodings within single value label > 1,000"")&#xa;        if version == 13:&#xa;            if max_note_size > 13400:&#xa;                small_good = False&#xa;                print(""    note size > 13,400"")&#xa;            if char_len > 13400:&#xa;                small_good = False&#xa;                print(""    char length > 13,400"")&#xa;        else:&#xa;            if max_note_size > 8681:&#xa;                small_good = False&#xa;                print(""    note size > 8,681"")&#xa;            if char_len > 8681:&#xa;                small_good = False&#xa;                print(""    char length > 8,681"")&#xa;        if small_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata IC problems"")&#xa;        if width > 24564:&#xa;            medium_good = False&#xa;            print(""    data set width > 24,564"")&#xa;        if nvar > 2047:&#xa;            medium_good = False&#xa;            print(""    numbar of variables > 2,047"")&#xa;        if nobs > 2147483647:&#xa;            medium_good = False&#xa;            print(""    number of observations > 2,147,483,647"")&#xa;        if num_val_encodings > 65536:&#xa;            medium_good = False&#xa;            print(""    number of encodings within single value label > 65,536"")&#xa;        if max_note_size > 67784:&#xa;            medium_good = False&#xa;            print(""    note size > 67,784"")&#xa;        if char_len > 67784:&#xa;            medium_good = False&#xa;            print(""    char length > 67,784"")&#xa;        if medium_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata MP & SE problems"")&#xa;        if width > 393192:&#xa;            large_good = False&#xa;            print(""    data set width > 393,192"")&#xa;        if nvar > 32767:&#xa;            large_good = False&#xa;            print(""    numbar of variables > 32,767"")&#xa;        if nobs > 2147483647:&#xa;            large_good = False&#xa;            print(""    number of observations > 2,147,483,647"")&#xa;        if num_val_encodings > 65536:&#xa;            large_good = False&#xa;            print(""    number of encodings within single value label > 65,536"")&#xa;        if max_note_size > 67784:&#xa;            large_good = False&#xa;            print(""    note size > 67,784"")&#xa;        if char_len > 67784:&#xa;            large_good = False&#xa;            print(""    char length > 67,784"")&#xa;        if large_good:&#xa;            print(""    none"")&#xa;            &#xa;    def _dta_format(self, address):&#xa;        """"""find version number of any recent version dta file""""""&#xa;        with open(address, 'rb') as dta_file:&#xa;            first_bytes = dta_file.read(11)&#xa;        ds_format = first_bytes[0]&#xa;        if isinstance(ds_format, str):  # happens in Python 2.7&#xa;            ds_format = ord(ds_format)&#xa;        # If format is 117, then first_bytes[0] is ""<"", which == 60.&#xa;        if ds_format == 114 or ds_format == 115:&#xa;            return ds_format&#xa;        elif first_bytes.decode('iso-8859-1') == ""<stata_dta>"":&#xa;            return 117&#xa;        else:&#xa;            raise ValueError(""file seems to have an unsupported format"")&#xa;        &#xa;    def _get_srtlist(self, sfile):&#xa;        """"""helper function for reading dta files""""""&#xa;        srtlist = list(unpack(self._byteorder + 'H'*(self._nvar+1), &#xa;                              sfile.read(2*(self._nvar+1))))&#xa;        zero_pos = srtlist.index(0)&#xa;        srtlist = [srt - 1 for srt in srtlist]&#xa;        # srtlist contains a terminating zero int, which need not be kept&#xa;        if zero_pos == -1: return srtlist[:-1]&#xa;        return srtlist[:zero_pos] + [None]*(len(srtlist) - 1 - zero_pos)&#xa;        &#xa;    def _parse_value_label_table(self, sfile):&#xa;        """"""helper function for reading dta files""""""&#xa;        byteorder = self._byteorder&#xa;        &#xa;        nentries = unpack(byteorder + 'l', sfile.read(4))[0]&#xa;        txtlen = unpack(byteorder + 'l', sfile.read(4))[0]&#xa;        off = []&#xa;        val = []&#xa;        txt = []&#xa;        for i in range(nentries):&#xa;            off.append(unpack(byteorder+'l',sfile.read(4))[0])&#xa;        for i in range(nentries):&#xa;            val.append(unpack(byteorder+'l',sfile.read(4))[0])&#xa;        &#xa;        txt_block = unpack(str(txtlen) + ""s"", sfile.read(txtlen))&#xa;        txt = [t.decode('iso-8859-1') &#xa;               for b in txt_block for t in b.split(b'\0')]&#xa;        &#xa;        # put (off, val) pairs in same order as txt&#xa;        sorter = list(zip(off, val))&#xa;        sorter.sort()&#xa;        &#xa;        # dict of val[i]:txt[i]&#xa;        table = {sorter[i][1]: txt[i] for i in range(len(sorter))}&#xa;        &#xa;        return table&#xa;            &#xa;    def _file_to_Dta115(self, address):&#xa;        """"""populate fields of dta object with values from disk""""""&#xa;        missing_above = {251: 100, 252: 32740, 253: 2147483620, &#xa;                        254: float.fromhex('0x1.fffffep+126'), &#xa;                        255: float.fromhex('0x1.fffffffffffffp+1022')}&#xa;        # decimal numbers given in -help dta- for float and double &#xa;        # are approximations: 'f': 1.701e38, 'd': 8.988e307&#xa;        type_dict = {251: ['b',1], 252: ['h',2], 253: ['l',4], &#xa;                    254: ['f',4], 255: ['d',8]}&#xa;                    &#xa;        def get_byte_str(str_len):&#xa;            s = unpack(str(str_len) + 's', sfile.read(str_len))[0]&#xa;            return s.partition(b'\0')[0].decode('iso-8859-1')&#xa;            &#xa;        def missing_object(miss_val, st_type):&#xa;            if st_type == 251: # byte&#xa;                value = MISSING_VALS[miss_val - 101]&#xa;            elif st_type == 252: # int&#xa;                value = MISSING_VALS[miss_val - 32741]&#xa;            elif st_type == 253: # long&#xa;                value = MISSING_VALS[miss_val - 2147483621]&#xa;            elif st_type == 254: # float&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            elif st_type == 255: # double&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            return value&#xa;            &#xa;        def get_var_val(st_type):&#xa;            if st_type <= 244:&#xa;                return get_byte_str(st_type)&#xa;            else:&#xa;                fmt, nbytes = type_dict[st_type]&#xa;                val = unpack(byteorder+fmt, sfile.read(nbytes))[0]&#xa;                return (val if val <= missing_above[st_type] &#xa;                        else missing_object(val, st_type))&#xa;        &#xa;        with open(address, 'rb') as sfile:&#xa;            # header info&#xa;            self._ds_format = unpack('b', sfile.read(1))[0]&#xa;            byteorder = '>' if unpack('b', sfile.read(1))[0] == 1 else '<'&#xa;            self._byteorder = byteorder&#xa;            sfile.seek(1,1) # filetype&#xa;            sfile.seek(1,1) # padding&#xa;            self._nvar = nvar = unpack(byteorder + 'h', sfile.read(2))[0]&#xa;            self._nobs = nobs = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            self._data_label = get_byte_str(81)&#xa;            self._time_stamp = get_byte_str(18)&#xa;            &#xa;            # descriptors&#xa;            self._typlist = [ord(sfile.read(1)) for i in range(nvar)]&#xa;            self._varlist = [get_byte_str(33) for i in range(nvar)]&#xa;            self._srtlist = self._get_srtlist(sfile)&#xa;            self._fmtlist = [get_byte_str(49) for i in range(nvar)]&#xa;            self._lbllist = [get_byte_str(33) for i in range(nvar)]&#xa;            &#xa;            # variable labels&#xa;            self._vlblist = [get_byte_str(81) for i in range(nvar)]&#xa;            &#xa;            # expansion fields&#xa;            data_type = unpack(byteorder + 'b', sfile.read(1))[0]&#xa;            data_len = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            chrdict = {}&#xa;            while not (data_type == 0 and data_len == 0):&#xa;                s = unpack(str(data_len) + 's', sfile.read(data_len))[0]&#xa;                varname = s[:33].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charname = s[33:66].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charstring = s[66:].partition(b'\0')[0].decode('iso-8859-1')&#xa;                if varname not in chrdict:&#xa;                    chrdict[varname] = {}&#xa;                chrdict[varname][charname] = charstring&#xa;                data_type = unpack(byteorder + 'b', sfile.read(1))[0]&#xa;                data_len = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            self._chrdict = chrdict&#xa;            &#xa;            # data&#xa;            varvals = []&#xa;            append = varvals.append&#xa;            typlist = self._typlist&#xa;            for _ in range(nobs):&#xa;                new_row = [get_var_val(typlist[i]) for i in range(nvar)]&#xa;                append(new_row)&#xa;            self._varvals = varvals&#xa;            &#xa;            # value labels&#xa;            value_labels = {}&#xa;            parse_value_label_table = self._parse_value_label_table&#xa;            while True:&#xa;                try:&#xa;                    sfile.seek(4,1) # table length&#xa;                    labname = get_byte_str(33)&#xa;                    sfile.seek(3,1) # padding&#xa;                    vl_table = parse_value_label_table(sfile)&#xa;                    value_labels[labname] = vl_table&#xa;                except StructError:&#xa;                    break&#xa;            self._vallabs = value_labels&#xa;        &#xa;    def _file_to_Dta117(self, address):&#xa;        """"""populate fields of dta object with values from disk""""""&#xa;        missing_above = {65530: 100, 65529: 32740, 65528: 2147483620, &#xa;                        65527: float.fromhex('0x1.fffffep+126'), &#xa;                        65526: float.fromhex('0x1.fffffffffffffp+1022')}&#xa;        # decimal numbers given in -help dta- for float and double &#xa;        # are approximations: 'f': 1.701e38, 'd': 8.988e307&#xa;        type_dict = {65530: ['b',1], 65529: ['h',2], 65528: ['l',4], &#xa;                    65527: ['f',4], 65526: ['d',8]}&#xa;            &#xa;        get_str = lambda n: (&#xa;            unpack(&#xa;                str(n) + 's',&#xa;                sfile.read(n)&#xa;            )[0].decode('iso-8859-1')&#xa;        )&#xa;        &#xa;        get_term_str = lambda n: (&#xa;            unpack(&#xa;                str(n) + 's', &#xa;                sfile.read(n)&#xa;            )[0].partition(b'\0')[0].decode('iso-8859-1')&#xa;        )&#xa;&#xa;        def missing_object(miss_val, st_type):&#xa;            if st_type == 65530: # byte&#xa;                value = MISSING_VALS[miss_val - 101]&#xa;            elif st_type == 65529: # int&#xa;                value = MISSING_VALS[miss_val - 32741]&#xa;            elif st_type == 65528: # long&#xa;                value = MISSING_VALS[miss_val - 2147483621]&#xa;            elif st_type == 65527: # float&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            elif st_type == 65526: # double&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            return value&#xa;        &#xa;        with open(address, 'rb') as sfile:&#xa;            if get_str(11) != ""<stata_dta>"":&#xa;                raise DtaParseError(""expected '<stata_dta>'"")&#xa;        &#xa;            # header info&#xa;            if get_str(8) != ""<header>"":&#xa;                raise DtaParseError(""expected '<header>'"")&#xa;            &#xa;            if get_str(9) != ""<release>"": &#xa;                raise DtaParseError(""expected '<release>'"")&#xa;            self._ds_format = int(get_str(3))&#xa;            if self._ds_format != 117: &#xa;                raise DtaParseError(""expected release 117"")&#xa;            if get_str(10) != ""</release>"": &#xa;                raise DtaParseError(""expected '</release>'"")&#xa;            &#xa;            if get_str(11) != ""<byteorder>"": &#xa;                raise DtaParseError(""expected '<byteorder>'"")&#xa;            self._byteorder = byteorder = ('>' if get_str(3) == ""MSF"" else '<')&#xa;            if get_str(12) != ""</byteorder>"": &#xa;                raise DtaParseError(""expected '</byteorder>'"")&#xa;            &#xa;            if get_str(3) != ""<K>"": &#xa;                raise DtaParseError(""expected '<K>'"")&#xa;            self._nvar = nvar = unpack(byteorder + 'H', sfile.read(2))[0]&#xa;            if get_str(4) != ""</K>"": &#xa;                raise DtaParseError(""expected '</K>'"")&#xa;            &#xa;            if get_str(3) != ""<N>"": &#xa;                raise DtaParseError(""expected '<N>'"")&#xa;            self._nobs = nobs = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;            if get_str(4) != ""</N>"": &#xa;                raise DtaParseError(""expected '</N>'"")&#xa;            &#xa;            if get_str(7) != ""<label>"": &#xa;                raise DtaParseError(""expected '<label>'"")&#xa;            label_length = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            self._data_label = get_str(label_length)&#xa;            if get_str(8) != ""</label>"": &#xa;                raise DtaParseError(""expected '</label>'"")&#xa;            &#xa;            if get_str(11) != ""<timestamp>"":&#xa;                raise DtaParseError(""expected '<timestamp>'"")&#xa;            stamp_length = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            self._time_stamp = get_str(stamp_length)&#xa;            # -help dta- seems to indicate there's an optional binary zero here&#xa;            next = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            if (not (next == b'\0' and get_str(12) == ""</timestamp>"") and&#xa;                    not (next == 60 and get_str(11) == ""/timestamp>"")):&#xa;                raise DtaParseError(""'</timestamp>'"")&#xa;            # 60 is int of '<' with iso-8859-1 encoding&#xa;            &#xa;            if get_str(9) != ""</header>"": &#xa;                raise DtaParseError(""expected '</header>'"")&#xa;            &#xa;            # map&#xa;            if get_str(5) != ""<map>"": &#xa;                raise DtaParseError(""expected '<map>'"")&#xa;            locs = unpack(byteorder + 'Q'*14, sfile.read(14 * 8))&#xa;            if get_str(6) != ""</map>"": &#xa;                raise DtaParseError(""expected '</map>'"")&#xa;            &#xa;            # variable types&#xa;            if sfile.tell() != locs[2] or get_str(16) != ""<variable_types>"":&#xa;                raise DtaParseError(""expected '<variable_types>'"")&#xa;            self._typlist = [unpack(byteorder + 'H', sfile.read(2))[0] &#xa;                             for i in range(nvar)]&#xa;            if get_str(17) != ""</variable_types>"": &#xa;                raise DtaParseError(""expected '</variable_types>'"")&#xa;            &#xa;            # varnames&#xa;            if sfile.tell() != locs[3] or get_str(10) != ""<varnames>"":&#xa;                raise DtaParseError(""expected '<varnames>'"")&#xa;            self._varlist = [get_term_str(33) for i in range(nvar)]&#xa;            if get_str(11) != ""</varnames>"": &#xa;                raise DtaParseError(""expected '</varnames>'"")&#xa;            &#xa;            # sortlist&#xa;            if sfile.tell() != locs[4] or get_str(10) != ""<sortlist>"":&#xa;                raise DtaParseError(""expected '<sortlist>'"")&#xa;            self._srtlist = self._get_srtlist(sfile)&#xa;            if get_str(11) != ""</sortlist>"": &#xa;                raise DtaParseError(""expected '</sortlist>'"")&#xa;            &#xa;            # formats&#xa;            if sfile.tell() != locs[5] or get_str(9) != ""<formats>"":&#xa;                raise DtaParseError(""expected '<formats>'"")&#xa;            self._fmtlist = [get_term_str(49) for i in range(nvar)]&#xa;            if get_str(10) != ""</formats>"": &#xa;                raise DtaParseError(""expected '</formats>'"")&#xa;            &#xa;            # value label names&#xa;            if sfile.tell() != locs[6] or get_str(19) != ""<value_label_names>"":&#xa;                raise DtaParseError(""expected '<value_label_names>'"")&#xa;            self._lbllist = [get_term_str(33) for i in range(nvar)]&#xa;            if get_str(20) != ""</value_label_names>"": &#xa;                raise DtaParseError(""expected '</value_label_names>'"")&#xa;            &#xa;            # variable labels&#xa;            # Before the 02jul2013 update, Stata put a zero in location&#xa;            # map for ""<variable_labels>"". Allow zero for files created&#xa;            # before that update.&#xa;            if (not (locs[7] == 0 or sfile.tell() == locs[7]) or &#xa;                    get_str(17) != ""<variable_labels>""):&#xa;                raise DtaParseError(""expected '<variable_labels>'"")&#xa;            self._vlblist = [get_term_str(81) for i in range(nvar)]&#xa;            if get_str(18) != ""</variable_labels>"": &#xa;                raise DtaParseError(""expected '</variable_labels>'"")&#xa;            &#xa;            # characteristics&#xa;            if sfile.tell() != locs[8] or get_str(17) != ""<characteristics>"":&#xa;                raise DtaParseError(""expected '<characteristics>'"")&#xa;            chrdict = {}&#xa;            next_four = get_term_str(4)&#xa;            while next_four == ""<ch>"":&#xa;                char_len = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;                s = unpack(str(char_len) + 's', sfile.read(char_len))[0]&#xa;                varname = s[:33].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charname = s[33:66].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charstring = s[66:].partition(b'\0')[0].decode('iso-8859-1')&#xa;                if varname not in chrdict:&#xa;                    chrdict[varname] = {}&#xa;                chrdict[varname][charname] = charstring&#xa;                if get_str(5) != ""</ch>"": &#xa;                    raise DtaParseError(""expected '</ch>'"")&#xa;                next_four = get_term_str(4)&#xa;            self._chrdict = chrdict&#xa;            if next_four != ""</ch"" or get_str(14) != ""aracteristics>"":&#xa;                raise DtaParseError(""expected '</characteristics>'"")&#xa;            &#xa;            # data&#xa;            if sfile.tell() != locs[9] or get_str(6) != ""<data>"":&#xa;                raise DtaParseError(""expected '<data>'"")&#xa;            varvals = []&#xa;            data_append = varvals.append&#xa;            typlist = self._typlist&#xa;            for _ in range(nobs):&#xa;                new_row = []&#xa;                append = new_row.append&#xa;                for st_type in typlist:&#xa;                    if st_type <= 2045: # str&#xa;                        new_val = get_term_str(st_type)&#xa;                    elif st_type == 32768: # strl&#xa;                        # new_val == (v,o). Later replace (v,o) -> strl value.&#xa;                        new_val = unpack(byteorder + 'II', sfile.read(8))   &#xa;                    else:&#xa;                        fmt, nbytes = type_dict[st_type]&#xa;                        new_val = unpack(&#xa;                            byteorder + fmt, sfile.read(nbytes))[0]&#xa;                        if new_val > missing_above[st_type]:&#xa;                            new_val = missing_object(new_val, st_type)&#xa;                    append(new_val)&#xa;                data_append(new_row)&#xa;            self._varvals = varvals&#xa;            if get_str(7) != ""</data>"": &#xa;                raise DtaParseError(""expected '</data>'"")&#xa;            &#xa;            # strls&#xa;            if sfile.tell() != locs[10] or get_str(7) != ""<strls>"": &#xa;                raise DtaParseError(""expected '<strls>'"")&#xa;            strls = {(0,0): """"}&#xa;            next_three = get_str(3)&#xa;            while next_three == ""GSO"":&#xa;                vo = unpack(byteorder + ""II"", sfile.read(8))&#xa;                t = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;                str_len = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;                if t == 130:&#xa;                    new_str = (&#xa;                        unpack(str(str_len) + 's', sfile.read(str_len))[0]&#xa;                    )[:-1].decode('iso-8859-1')&#xa;                else:&#xa;                    new_str = sfile.read(str_len)&#xa;                strls.update({vo: new_str})&#xa;                next_three = get_str(3)&#xa;            if next_three != ""</s"" or get_str(5) != ""trls>"":&#xa;                raise DtaParseError(""expected '</strls>'"")&#xa;                        &#xa;            # put strls in data&#xa;            for st_type, var_num in zip(typlist, range(nvar)):&#xa;                if st_type == 32768:&#xa;                    for obs_num in range(nobs):&#xa;                        v, o = varvals[obs_num][var_num]&#xa;                        # raise error if having 'forward reference'?&#xa;                        # if not (o < j or (o == j and v < i)):&#xa;                        #    raise ...&#xa;                        varvals[obs_num][var_num] = strls[(v,o)]&#xa;            &#xa;            # value labels&#xa;            if sfile.tell() != locs[11] or get_str(14) != ""<value_labels>"":&#xa;                raise DtaParseError(""expected '<value_labels>'"")&#xa;            value_labels = {}&#xa;            parse_value_label_table = self._parse_value_label_table&#xa;            next_five = get_str(5)&#xa;            while next_five == ""<lbl>"":&#xa;                sfile.seek(4, 1) # table length&#xa;                label_name = get_term_str(33)&#xa;                sfile.seek(3, 1) # padding&#xa;                label_table = parse_value_label_table(sfile)&#xa;                value_labels[label_name] = label_table&#xa;                if get_str(6) != ""</lbl>"": &#xa;                    raise DtaParseError(""expected '</lbl>'"")&#xa;                next_five = get_str(5)&#xa;            self._vallabs = value_labels&#xa;            if next_five != ""</val"" or get_str(10) != ""ue_labels>"":&#xa;                raise DtaParseError(""expected '</value_labels>'"")&#xa;                &#xa;            &#xa;            # end tag&#xa;            if sfile.tell() != locs[12] or get_str(12) != ""</stata_dta>"":&#xa;                raise DtaParseError(""expected '</stata_dta>'"")&#xa;            &#xa;            # end of file&#xa;            if sfile.tell() != locs[13]:&#xa;                raise DtaParseError(""expected end of file"")&#xa;        &#xa;    def _get_type_name(self, st_type):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _isintvar(self, index):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        raise NotImplementedError&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        raise NotImplementedError&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        raise NotImplementedError&#xa;            &#xa;    def _dta_obj_to_file(self, address, replace=False):&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;class Dta115(Dta):&#xa;    """"""A Python class for Stata dataset version 115. It provides methods &#xa;    for creating, opening, manipulating, and saving Stata datasets.&#xa;    &#xa;    """"""  &#xa;    &#xa;    _default_fmt_widths = {251: 8, 252: 8, 253: 12, 254: 9, 255: 10}&#xa;    _default_fmts = {&#xa;        251: '%8.0g',&#xa;        252: '%8.0g', &#xa;        253: '%12.0g',&#xa;        254: '%9.0g',&#xa;        255: '%10.0g'&#xa;    }&#xa;    _default_new_type = 254&#xa;    &#xa;    _type_names = {&#xa;        251: 'byte',&#xa;        252: 'int',&#xa;        253: 'long',&#xa;        254: 'float',&#xa;        255: 'double'&#xa;    }&#xa;    &#xa;    def _get_type_name(self, st_type):&#xa;        """"""convert Stata type number to name""""""&#xa;        if st_type <= 244: return 'str' + str(st_type)&#xa;        return self._type_names[st_type]&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        """"""determine whether Stata variable is string""""""&#xa;        return self._typlist[index] <= 244&#xa;    &#xa;    def _isintvar(self, index):&#xa;        """"""determine whether Stata variable is integer""""""&#xa;        return 251 <= self._typlist[index] <= 253&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        return 251 <= self._typlist[index] <= 255&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        """"""convert other Dta version to 115""""""&#xa;        if old_type not in (Dta117,):&#xa;            msg = """".join(&#xa;                (""conversion from {} "".format(old_type.__name__),&#xa;                ""to Dta115 not supported""))&#xa;            raise TypeError(msg)&#xa;        &#xa;        self._ds_format = 115&#xa;        &#xa;        if old_type == Dta117:&#xa;            typlist = self._typlist&#xa;            fmtlist = self._fmtlist&#xa;            varlist = self._varlist&#xa;            varvals = self._varvals&#xa;            nobs = self._nobs&#xa;            nvar = self._nvar&#xa;            seen_strl = False&#xa;            seen_long_str = False&#xa;            seen_strange = False&#xa;            for j, st_type, varname in zip(range(nvar), typlist, varlist):&#xa;                if st_type == 32768:&#xa;                    if not seen_strl:&#xa;                        seen_strl = True&#xa;                        if not self._quiet:&#xa;                            msg = ""warning: strLs converted to strfs""&#xa;                            print((""{err}"" if IN_STATA else """") + msg)&#xa;                    str_len = 0&#xa;                    for i in range(nobs):&#xa;                        new_val = str(varvals[i][j])&#xa;                        val_len = len(new_val)&#xa;                        if val_len > 244:&#xa;                            if not seen_long_str:&#xa;                                seen_long_str = True&#xa;                                if not self._quiet:&#xa;                                    msg = ""warning: long strings truncated""&#xa;                                    print((""{err}"" if IN_STATA else """") + msg)&#xa;                            new_val = new_val[:244]&#xa;                            val_len = 244&#xa;                        varvals[i][j] = new_val&#xa;                        str_len = max(str_len, val_len)&#xa;                    typlist[j] = str_len&#xa;                    m = STR_FMT_RE.match(fmtlist[j])&#xa;                    if not m or m.group(0) == '%9s' or int(m.group(3)) > 244:&#xa;                        align = m.group(1) if m.group(1) else ''&#xa;                        fmtlist[j] = '%' + align + str(str_len) + 's'&#xa;                elif 0 < st_type < 245:&#xa;                    pass&#xa;                elif 245 < st_type <= 2045:&#xa;                    if not seen_long_str:&#xa;                        seen_long_str = True&#xa;                        if not self._quiet:&#xa;                            msg = ""warning: long strings truncated""&#xa;                            print((""{err}"" if IN_STATA else """") + msg)&#xa;                    str_len = 0&#xa;                    for i in range(nobs):&#xa;                        new_val = varvals[i][j]&#xa;                        val_len = len(new_val)&#xa;                        # it is possible that st_type > actual string lengths&#xa;                        if val_len > 244: &#xa;                            new_val = new_val[:244]&#xa;                            val_len = 244&#xa;                        varvals[i][j] = new_val&#xa;                        str_len = max(str_len, val_len)&#xa;                    typlist[j] = str_len&#xa;                    m = STR_FMT_RE.match(fmtlist[j])&#xa;                    if not m or m.group(0) == '%9s' or int(m.group(3)) > 244:&#xa;                        align = m.group(1) if m.group(1) else ''&#xa;                        fmtlist[j] = '%' + align + str(str_len) + 's'&#xa;                elif 65526 <= st_type <= 65530:&#xa;                    typlist[j] = 251 + (65530 - st_type)&#xa;                elif not seen_strange:&#xa;                    # just a safety; not needed in normal usage&#xa;                    seen_strange = True&#xa;                    if not self._quiet:&#xa;                        msg = ""strange Stata types encountered; ignoring""&#xa;                        print((""{err}"" if IN_STATA else """") + msg)&#xa;        &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        """"""create dataset from iterable of values""""""&#xa;        global get_missing&#xa;        &#xa;        # first, list-alize like tuplize in __setitem__&#xa;        def make_list(x):&#xa;            if isinstance(x, str) or not isinstance(x, collections.Iterable):&#xa;                return [x]&#xa;            return list(x)&#xa;        &#xa;        # force input into 2d structure&#xa;        varvals = [make_list(v) for v in varvals]&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if single_row:&#xa;            varvals = [[v[0] for v in varvals]]              &#xa;    &#xa;        # Get correct type and homogenize rows by setting same length &#xa;        # (append missing values of correct type when necessary) and &#xa;        # by coercing when necessary to make each column same type.&#xa;        ismissing = self.ismissing&#xa;        &#xa;        typlist = []&#xa;        &#xa;        str_clipped = False&#xa;        alt_missing = False&#xa;        &#xa;        curr_nvars = 0&#xa;        nrows = len(varvals)&#xa;        &#xa;        for i in range(nrows):&#xa;            row = varvals[i]&#xa;            row_len = len(row)&#xa;            if row_len < curr_nvars:&#xa;                row += ['' if st_type <= 244 else MISSING &#xa;                        for st_type in typlist[row_len:]]&#xa;            elif row_len > curr_nvars:&#xa;                # add to typelist&#xa;                diff = row_len - curr_nvars&#xa;                typlist += [251 if compress else 254]*diff &#xa;                # extend all previous rows with missing values&#xa;                padding = [MISSING]*diff&#xa;                for j in range(i):&#xa;                    varvals[j] += padding&#xa;                # update curr_nvars&#xa;                curr_nvars = row_len&#xa;            # verify type of values and convert or &#xa;            # make retroactive changes when necessary&#xa;            for k in range(curr_nvars):&#xa;                value = row[k]&#xa;                st_type = typlist[k]&#xa;                if st_type <= 244:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        typlist[k] = max(st_type, val_len)&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or &#xa;                            value > 8.988465674311579e+307):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        value = str(value)&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        typlist[k] = max(st_type, val_len)&#xa;                    row[k] = value&#xa;                else:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        row[k] = value&#xa;                        st_type = val_len&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(new_val)&#xa;                                val_len = len(new_val)&#xa;                                if val_len > 244:&#xa;                                    new_val = new_val[:244]&#xa;                                    val_len = 244&#xa;                                    str_clipped = True&#xa;                                varvals[j][k] = new_val&#xa;                                st_type = max(st_type, val_len)&#xa;                        typlist[k] = st_type&#xa;                    elif value is None:&#xa;                        row[k] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(value, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(value, float) and&#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = get_missing(value)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (value != int(value) or -2147483647 > value or&#xa;                                value > 2147483620): &#xa;                            # val is not int or is outside of bounds&#xa;                            typlist[k] = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= value <= 32740):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            typlist[k] = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= value <= 100): &#xa;                            # st_type byte, but val is outside of bounds&#xa;                            typlist[k] = 252 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 254 and &#xa;                                (-1.7014117331926443e+38 <= value or&#xa;                                 value > 1.7014117331926443e+38)):&#xa;                            # st_type float, but val is outside of bounds&#xa;                            typlist[k] = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;        &#xa;        if not quiet:&#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if str_clipped:&#xa;                msg = ""warning: some strings were shortened to 244 characters""&#xa;                print(smcl + msg)&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        # header&#xa;        self._ds_format  = 115&#xa;        self._byteorder  = "">"" if sys.byteorder == ""big"" else ""<""&#xa;        self._nvar       = curr_nvars&#xa;        self._nobs       = nrows&#xa;        self._data_label = """"&#xa;        self._set_timestamp()&#xa;           &#xa;        # descriptors&#xa;        formats = self._default_fmts&#xa;        &#xa;        self._typlist = typlist&#xa;        self._varlist = [""var"" + str(i) for i in range(curr_nvars)]&#xa;        self._srtlist = [None for i in range(curr_nvars)]&#xa;        self._fmtlist = ['%' + str(max(9,st_type)) + 's' if st_type <= 244 &#xa;                         else formats[st_type] for st_type in typlist]&#xa;        self._lbllist = [""""]*curr_nvars&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [""""]*curr_nvars&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {}&#xa;        &#xa;        # data&#xa;        self._varvals = varvals&#xa;        &#xa;        # value labels&#xa;        self._vallabs = {}&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        """"""Add new variable to data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of variable to be created.&#xa;        values : iterable&#xa;            Should be a flat iterable like [1, 5, 9, ...] or&#xa;            (1, 5, 9, ...). Not like [[1], [5], [9], ...].&#xa;        st_type : int or str, optional&#xa;            Examples: 212 or ""str212"", 254 or ""float"".&#xa;            Intended Stata type of the data variable. &#xa;            The intended type will be overridden when necessary.&#xa;            Default value depends on the given values.&#xa;        compress : bool (or coercible to bool), optional&#xa;            If st_type is None, this sets st_type to byte if &#xa;            compress=True, or float if compress=False.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds new variable to data set.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        if (isinstance(values, str) or &#xa;                not isinstance(values, collections.Iterable)):&#xa;            if self._nobs <= 1:&#xa;                values = [values]&#xa;            else:&#xa;                raise TypeError(""values to add must be in an iterable"")&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""variable name must be str"")&#xa;        &#xa;        name = name.strip()&#xa;        if name == """":&#xa;            raise ValueError(""variable name required"")&#xa;        &#xa;        if name in self._varlist:&#xa;            raise ValueError(""variable name already exists"")&#xa;        elif not self._is_valid_varname(name):&#xa;            raise ValueError(name + "" is not a valid Stata name"")&#xa;            &#xa;        type_names = (""byte"", ""int"", ""long"", ""float"", ""double"")&#xa;          &#xa;        init_st_type = st_type&#xa;        if st_type is None:&#xa;            st_type = 251 if compress else 254&#xa;        elif isinstance(st_type, str):&#xa;            if re.match(r'^str[0-9]+$', st_type):&#xa;                st_type = int(st_type[3:])&#xa;                if st_type > 244:&#xa;                    msg = ""given string type too large; shortening to 244""&#xa;                    print((""{err}"" if IN_STATA else """") + msg)&#xa;                    st_type = 244&#xa;                    init_st_type = st_type&#xa;            elif st_type in type_names:&#xa;                st_type = 251 + type_names.index(st_type)&#xa;                init_st_type = st_type&#xa;            else:&#xa;                raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        elif (st_type not in (251, 252, 253, 254, 255) &#xa;                and not (isinstance(st_type, int) and 1 <= st_type <= 244)):&#xa;            raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        &#xa;        # Given iterable could be generator. Ensure it is in static form.&#xa;        values = [v for v in values]&#xa;        nvals = len(values)&#xa;        &#xa;        varvals = self._varvals&#xa;        &#xa;        if nvals == 0:&#xa;            this_missing = '' if st_type <= 244 else MISSING&#xa;            for row in varvals:&#xa;                row.append(this_missing)&#xa;        else:&#xa;            str_clipped = False&#xa;            alt_missing = False&#xa;            &#xa;            ismissing = self.ismissing&#xa;        &#xa;            for val, i in zip(values, range(nvals)):&#xa;                if st_type <= 244:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            values[i] = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        st_type = max(st_type, val_len)&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif not (isinstance(val, int) or isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        val = str(val)&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            val = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        values[i] = val&#xa;                        st_type = max(st_type, val_len)&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            values[i] = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        st_type = val_len&#xa;                        for j in range(i):&#xa;                            valj = values[j]&#xa;                            if ismissing(valj): &#xa;                                # If encountering a missing value here,  &#xa;                                # should be instance of MissingValue. &#xa;                                # Could just check for that.&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val_j = str(values[j])&#xa;                                val_len = len(new_val_j)&#xa;                                if val_len > 244:&#xa;                                    new_val_j = new_val_j[:244]&#xa;                                    val_len = 244&#xa;                                    str_clipped = True&#xa;                                values[j] = new_val_j&#xa;                                st_type = max(st_type, val_len)&#xa;                    elif val is None:&#xa;                        values[i] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif not (isinstance(val, float) or isinstance(val, int)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (val != int(val) or &#xa;                                not (-2147483647 <= val <= 2147483620)):&#xa;                            # val is not int or is outside of bounds of long&#xa;                            st_type = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            st_type = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= val <= 100):&#xa;                            # st_type byte, but val is outside of bounds&#xa;                            st_type = 252 # int&#xa;                    else: # was float and will continue to be&#xa;                        if st_type == 254 and (-1.7014117331926443e+38 > val or&#xa;                                val > 1.7014117331926443e+38):&#xa;                            # st_type float, but val is outisde of bounds&#xa;                            st_type = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                &#xa;            if nvals < self._nobs:&#xa;                this_missing = '' if st_type <= 244 else MISSING&#xa;                values += [this_missing]*(self._nobs - nvals)&#xa;            elif nvals > self._nobs:&#xa;                self.set_obs(nvals)&#xa;            &#xa;            for row, new_val in zip(varvals, values):&#xa;                row.append(new_val)&#xa;            &#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                if init_st_type is not None and init_st_type != st_type:&#xa;                    st_type_name = self._get_type_name(st_type)&#xa;                    msg = (smcl + ""warning: some values were incompatible with "" + &#xa;                           ""specified type;\n    type changed to "" + st_type_name)&#xa;                    print(msg)&#xa;                if str_clipped:&#xa;                    print(smcl + ""warning: some strings were "" + &#xa;                          ""shortened to 244 characters"")&#xa;                if alt_missing:&#xa;                    print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        &#xa;        self._typlist.append(st_type)&#xa;        self._varlist.append(name)&#xa;        self._srtlist.append(None)&#xa;        self._fmtlist.append('%' + str(max(9,st_type)) + 's' if st_type <= 244&#xa;                             else self._default_fmts[st_type])&#xa;        self._lbllist.append('')&#xa;        self._vlblist.append('')&#xa;        &#xa;        self._nvar += 1&#xa;        self._changed = True&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        """"""Check to see if given str is a valid Stata name.&#xa;        Be sure to strip spaces before calling this function.&#xa;        &#xa;        """"""&#xa;        if name in RESERVED or re.match(r'^str[0-9]+$', name): return False&#xa;        return True if VALID_NAME_RE.match(name) else False&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        """"""check that given str fmt is a valid Stata format""""""&#xa;        # make sure there is no leading or trailing whitespace&#xa;        fmt = fmt.strip()&#xa;        &#xa;        if fmt[0] != '%':&#xa;            return False&#xa;        &#xa;        # Handle business calendars first.&#xa;        # This does not check the calendar name.&#xa;        if fmt[1:3] == ""tb"" or fmt[1:4] == ""-tb"":&#xa;            return True if TB_FMT_RE.match(fmt) else False&#xa;            &#xa;        # date formats&#xa;        if fmt[1] == 't' or fmt[1:3] == '-t':&#xa;            return True if TIME_FMT_RE.match(fmt) else False&#xa;        &#xa;        # categorize using last character&#xa;        last_char = fmt[-1]&#xa;        if last_char == 's': # string&#xa;            m = STR_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width > 244: return False&#xa;            return True&#xa;        elif last_char == 'H' or last_char == 'L': # binary&#xa;            # Valid binary formats are ^%(8|16)(H|L)$. Stata doesn't raise &#xa;            # error with -8 or -16, but the results are perhaps unexpected.&#xa;            return True if fmt[1:-1] in ('8', '16', '-8', '-16') else False&#xa;        elif last_char == 'x': # hexadecimal&#xa;            return True if fmt == '%21x' or fmt == '%-12x' else False&#xa;        elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;            m = NUM_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width <= int(m.group(5)) or width > 244: &#xa;                return False&#xa;            return True&#xa;            &#xa;        return False&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        """"""Width of an observation as saved.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Width of a single observation in bytes.&#xa;        &#xa;        """"""&#xa;        widths = { 251: 1, 252: 2, 253: 4, 254: 4, 255: 8 }&#xa;        return sum([0] + [t if t < 245 else widths[t] for t in self._typlist])&#xa;            &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        """"""Helper for functions like __setitem__ that put &#xa;        new values into the dataset. This function does the &#xa;        job of inserting the values.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        varvals = self._varvals&#xa;        typlist = self._typlist&#xa;        varlist = self._varlist&#xa;        old_typlist = [typlist[i] for i in sel_cols]&#xa;        &#xa;        str_clipped = False&#xa;        alt_missing = False&#xa;        &#xa;        for row_num, i in zip(sel_rows, range(len(sel_rows))):&#xa;            row = value[i]&#xa;            for col_num, k in zip(sel_cols, range(len(sel_cols))):&#xa;                val = row[k]&#xa;                st_type = typlist[col_num]&#xa;                if st_type <= 244:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            val = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        if val_len > st_type:&#xa;                            typlist[col_num] = val_len&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        val = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot "" + &#xa;                               ""take non-string values"")&#xa;                        raise TypeError(msg)&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot take "" + &#xa;                               ""string values; has Stata type "" + &#xa;                               self._get_type_name(st_type))&#xa;                        raise TypeError(msg)&#xa;                    elif val is None:&#xa;                        val = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif not (isinstance(val, float) or isinstance(val, int)):&#xa;                        msg = (""value in right-hand position "" + &#xa;                               ""{},{} is not of recognized type"".format(i, k))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        val = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (val != int(val) or -2147483647 > val or&#xa;                                val > 2147483620):&#xa;                            typlist[col_num] = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= val <= 32740):&#xa;                            typlist[col_num] = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= val <= 100):&#xa;                            typlist[col_num] = 252 # int&#xa;                    else: # was float and will continue to be&#xa;                        if (st_type == 254 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)):&#xa;                            typlist[col_num] = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                varvals[row_num][col_num] = val&#xa;        &#xa;        if not self._quiet:&#xa;            seen_cols = set() # same column can appear multiple times&#xa;            smcl = ""{txt}"" if IN_STATA else """"&#xa;            msg = smcl + ""Stata type for {} was {}, now {}""&#xa;            for old_type,c in zip(old_typlist, sel_cols):&#xa;                new_type = typlist[c]&#xa;                if old_type != new_type and c not in seen_cols:&#xa;                    old_name = self._get_type_name(old_type)&#xa;                    new_name = self._get_type_name(new_type)&#xa;                    print(msg.format(varlist[c], old_name, new_name))&#xa;                seen_cols.add(c)&#xa;            &#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if str_clipped:&#xa;                msg = ""warning: some strings were shortened to 244 characters""&#xa;                print(smcl + msg)&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        """"""helper function for writing dta files""""""&#xa;        n = miss_val.index&#xa;        &#xa;        if st_type == 251: # byte&#xa;            value = n + 101&#xa;        elif st_type == 252: # int&#xa;            value = n + 32741&#xa;        elif st_type == 253: # long&#xa;            value = n + 2147483621&#xa;        elif st_type == 254: # float&#xa;            value = float.fromhex('0x1.0' + hex(n)[2:].zfill(2) + 'p+127')&#xa;        elif st_type == 255: # double&#xa;            value = float.fromhex('0x1.0' + hex(n)[2:].zfill(2) + 'p+1023')&#xa;        return value&#xa;            &#xa;    def _dta_obj_to_file(self, address):&#xa;        """"""save dta object to disk""""""&#xa;        global get_missing&#xa;        &#xa;        type_dict = {&#xa;            251: ['b',1],&#xa;            252: ['h',2], &#xa;            253: ['l',4],&#xa;            254: ['f',4],&#xa;            255: ['d',8]&#xa;        }&#xa;        first_missing = {&#xa;            251: 101,&#xa;            252: 32741,&#xa;            253: 2147483620, &#xa;            254: float.fromhex('0x1.0p+127'),&#xa;            255: float.fromhex('0x1.0p+1023')&#xa;        }&#xa;        typlist = self._typlist&#xa;        nvar = self._nvar&#xa;        &#xa;        missing_save_val = self._missing_save_val&#xa;                &#xa;        def write_value_label_table(labname, table):&#xa;            # Stata limits are a bit confusing. Total length of text &#xa;            # (including null terminators) must be <= 32000? Total &#xa;            # number of vals must be <= 65536? But the limit on text &#xa;            # length forces no. of vals <= 16000 since each label must &#xa;            # occupy at least two bytes (including null terminator).&#xa;            &#xa;            labname = labname[:32]&#xa;            &#xa;            val = sorted(table.keys())&#xa;            # each value may be up to 81 chars including null&#xa;            txt = [table[v][:80] for v in val] &#xa;            &#xa;            nval = len(val)&#xa;            if nval > 65536: # max number of values allowed&#xa;                val = val[:65536]&#xa;                txt = txt[:65536]&#xa;                nval = 65536&#xa;            &#xa;            off = [0]&#xa;            for i in range(nval - 1):&#xa;                # in next line, ""+ 1"" to leave room for \0&#xa;                offset = off[i] + len(txt[i]) + 1&#xa;                if offset > 32000: # if too much text&#xa;                    off = off[:i] # cut off at before the ith one&#xa;                    val = val[:i]&#xa;                    txt = txt[:i]&#xa;                    nval = i&#xa;                    break&#xa;                off.append(offset)&#xa;            txt_len = off[-1] + len(txt[-1]) + 1&#xa;            &#xa;            table_len = 4 + 4 + 4*nval + 4*nval + txt_len&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", table_len))&#xa;            dta.write(bytearray(labname, 'iso-8859-1') +&#xa;                      b'\0'*(33-len(labname)))&#xa;            dta.write(b'\x00\x00\x00')&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", nval))&#xa;            dta.write(pack(byteorder + ""l"", txt_len))&#xa;            for o in off: dta.write(pack(byteorder + ""l"", o))&#xa;            for v in val: dta.write(pack(byteorder + ""l"", v))&#xa;            #for t in txt: write_byte_str((t,), len(t) + 1)&#xa;            for t in txt: dta.write(bytearray(t, 'iso-8859-1') + b'\0')&#xa;        &#xa;        with open(address, 'wb') as dta:&#xa;            # header&#xa;            dta.write(pack('b', 115)) # ds_format&#xa;            byteorder = self._byteorder&#xa;            dta.write(pack('b', 1 if byteorder == '>' else 2)) # byteorder&#xa;            dta.write(pack('b', 1)) # filetype&#xa;            dta.write(pack('b', 0)) # padding&#xa;            dta.write(pack(byteorder + 'h', self._nvar))&#xa;            dta.write(pack(byteorder + 'i', self._nobs))&#xa;            data_label = self._data_label[:80]&#xa;            dta.write(bytearray(data_label, 'iso-8859-1') +&#xa;                      b'\0'*(81-len(data_label)))&#xa;            self._set_timestamp() # new time_stamp&#xa;            time_stamp = self._time_stamp[:17]&#xa;            dta.write(bytearray(time_stamp, 'iso-8859-1') +&#xa;                      b'\0'*(18-len(time_stamp)))&#xa;            &#xa;            # descriptors&#xa;            dta.write(bytes(self._typlist))&#xa;            for name in self._varlist:&#xa;                name = name[:32]&#xa;                dta.write(bytearray(name, 'iso-8859-1') + b'\0'*(33-len(name)))&#xa;            # In srtlist, Nones are replaced with zeroes and &#xa;            # a terminating zero is appended (the file needs &#xa;            # nvar + 1 ints including terminating zero).&#xa;            srtlist = self._srtlist + [None]&#xa;            srtlist = [srt + 1 if srt is not None else 0 for srt in srtlist]&#xa;            dta.write(pack(byteorder + 'h'*(nvar + 1), *srtlist))&#xa;            for fmt in self._fmtlist:&#xa;                fmt = fmt[:48]&#xa;                dta.write(bytearray(fmt, 'iso-8859-1') + b'\0'*(49-len(fmt)))&#xa;            for lab in self._lbllist:&#xa;                lab = lab[:32]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(33-len(lab)))&#xa;            &#xa;            # variable labels&#xa;            for lab in self._vlblist:&#xa;                lab = lab[:80]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(81-len(lab)))&#xa;            &#xa;            # characteristics&#xa;            chrdict = self._chrdict&#xa;            for varname in chrdict:&#xa;                varname = varname[:32]&#xa;                vardict = chrdict[varname]&#xa;                for charname in vardict:&#xa;                    charname = charname[:32]&#xa;                    char = vardict[charname][:67784]  # or 8681 for Small Stata&#xa;                    data_len = 66 + len(char) + 1 # +1 for null termination&#xa;                    dta.write(b'\x01') # data_type&#xa;                    dta.write(pack(byteorder + 'i', data_len))&#xa;                    dta.write(bytearray(varname, 'iso-8859-1') + &#xa;                              b'\0'*(33 - len(varname)))&#xa;                    dta.write(bytearray(charname, 'iso-8859-1') + &#xa;                              b'\0'*(33 - len(charname)))&#xa;                    dta.write(bytearray(char, 'iso-8859-1') + b'\0')&#xa;            dta.write(b'\x00\x00\x00\x00\x00')&#xa;            &#xa;            # data&#xa;            for row in self._varvals:&#xa;                for value, st_type in zip(row, typlist):&#xa;                    if st_type <= 244:&#xa;                        dta.write(bytearray(value, 'iso-8859-1') + &#xa;                                  b'\0'*(st_type - len(value)))&#xa;                    else:&#xa;                        fmt, nbytes = type_dict[st_type]&#xa;                        # Get correct dta value if missing. As a safety, check&#xa;                        # for non-standard missing (None and large values).&#xa;                        if value is None:&#xa;                            value = first_missing[st_type]&#xa;                        elif isinstance(value, MissingValue):&#xa;                            value = missing_save_val(value, st_type)&#xa;                        elif (value > 8.988465674311579e+307 or &#xa;                                value < -1.7976931348623157e+308):&#xa;                            # is this the right way to handle this ?&#xa;                            value = missing_save_val(&#xa;                                get_missing(value), st_type) &#xa;                        dta.write(pack(byteorder + fmt, value))&#xa;                &#xa;            # value labels&#xa;            value_labels = self._vallabs&#xa;            for labname in value_labels.keys():&#xa;                write_value_label_table(labname, value_labels[labname])&#xa;&#xa;&#xa;class Dta117(Dta):&#xa;    """"""A Python class for Stata dataset version 117. It provides methods&#xa;    for creating, opening, manipulating, and saving Stata datasets.&#xa;    &#xa;    """"""&#xa;    _default_fmt_widths = {65530: 8, 65529: 8, 65528: 12, 65527: 9, 65526: 10}&#xa;    _default_fmts = {&#xa;        65530: '%8.0g',&#xa;        65529: '%8.0g', &#xa;        65528: '%12.0g',&#xa;        65527: '%9.0g',&#xa;        65526: '%10.0g'&#xa;    }&#xa;    _default_new_type = 65527&#xa;    &#xa;    _type_names = {&#xa;        65530: 'byte',&#xa;        65529: 'int',&#xa;        65528: 'long',&#xa;        65527: 'float',&#xa;        65526: 'double',&#xa;        32768: 'strL'&#xa;    }&#xa;    &#xa;    def _get_type_name(self, st_type):&#xa;        """"""convert Stata type number to name""""""&#xa;        if st_type <= 2045: return 'str' + str(st_type)&#xa;        return self._type_names[st_type]&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        """"""determine if Stata variable is string""""""&#xa;        return self._typlist[index] <= 32768&#xa;        &#xa;    def _isintvar(self, index):&#xa;        """"""determine if Stata variable is integer""""""&#xa;        return 65528 <= self._typlist[index] <= 65530&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        """"""determine if Stata variable is numeric""""""&#xa;        return 65526 <= self._typlist[index] <= 65530&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        """"""convert other Dta version to 117""""""&#xa;        if old_type not in (Dta115,):&#xa;            msg = """".join(&#xa;                (""conversion from {} "".format(old_type.__name__),&#xa;                ""to Dta117 not supported""))&#xa;            raise TypeError(msg)&#xa;        self._ds_format = 117&#xa;        self._typlist = [i if i <= 244 else 65530 + (251 - i) &#xa;                         for i in self._typlist]&#xa;        &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        """"""create dataset from iterable of values""""""&#xa;        global get_missing&#xa;        &#xa;        # first, list-alize like tuplize in __setitem__&#xa;        def make_list(x):&#xa;            if (any(isinstance(x, t) for t in (str,bytes,bytearray))&#xa;                    or not isinstance(x, collections.Iterable)):&#xa;                return [x]&#xa;            return list(x)&#xa;        &#xa;        # force input into 2d structure&#xa;        varvals = [make_list(v) for v in varvals]&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if single_row:&#xa;            varvals = [[v[0] for v in varvals]]              &#xa;    &#xa;        # Get correct type and homogenize rows by setting same length &#xa;        # (append missing values of correct type when necessary) and &#xa;        # by coercing when necessary to make each column same type.&#xa;        ismissing = self.ismissing&#xa;        &#xa;        typlist = []&#xa;        &#xa;        alt_missing = False&#xa;        &#xa;        curr_nvars = 0&#xa;        nrows = len(varvals)&#xa;        &#xa;        for i in range(nrows):&#xa;            row = varvals[i]&#xa;            row_len = len(row)&#xa;            if row_len < curr_nvars:&#xa;                row += ['' if st_type <= 32768 else MISSING &#xa;                        for st_type in typlist[row_len:]]&#xa;            elif row_len > curr_nvars:&#xa;                # add to typelist&#xa;                diff = row_len - curr_nvars&#xa;                # Default type is byte or float. Stata doesn't convert &#xa;                # from float, but we should be able to here.&#xa;                typlist += [65530 if compress else 65527]*diff&#xa;                # extend all previous rows with missing values&#xa;                padding = [MISSING]*diff&#xa;                for j in range(i):&#xa;                    varvals[j] += padding&#xa;                # update curr_nvars&#xa;                curr_nvars = row_len&#xa;            # verify type of values and convert or &#xa;            # make retroactive changes when necessary&#xa;            for k in range(curr_nvars):&#xa;                value = row[k]&#xa;                st_type = typlist[k]&#xa;                if st_type == 32768:&#xa;                    if any(isinstance(value, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        pass&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        row[k] = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(value, int) and&#xa;                            not isinstance(value, float)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        row[k] = str(value)&#xa;                elif st_type < 32768:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        typlist[k] = (32768 if val_len > 2045 &#xa;                                            else max(st_type, val_len))&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    elif (isinstance(value, bytes) or &#xa;                            isinstance(value, bytearray)):&#xa;                        typlist[k] = 32768&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        value = str(value)&#xa;                        val_len = len(value)&#xa;                        typlist[k] = (32768 if val_len > 2045 &#xa;                                            else max(st_type, val_len))&#xa;                    row[k] = value&#xa;                else:&#xa;                    if isinstance(value, str):&#xa;                        max_len = len(value)&#xa;                        row[k] = value&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered  &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(new_val)&#xa;                                max_len = max(max_len, len(new_val))&#xa;                                varvals[j][k] = new_val&#xa;                        typlist[k] = 32768 if max_len > 2045 else max_len&#xa;                    elif (isinstance(value, bytes) or &#xa;                            isinstance(value, bytearray)):&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                varvals[j][k] = str(new_val)&#xa;                        typlist[k] = 32768&#xa;                    elif value is None:&#xa;                        row[k] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(value, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = get_missing(value)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (value != int(value) or -2147483647 > value or&#xa;                                value > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            typlist[k] = 65526 # double&#xa;                        elif (st_type >= 65529 and &#xa;                                not (-32767 <= value <= 32740)):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            typlist[k] = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= value <= 100): &#xa;                            # st_type byte, but val is outside of bounds&#xa;                            typlist[k] = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > value or&#xa;                                 value > 1.7014117331926443e+38)): &#xa;                            # st_type float, but val is outside of bounds&#xa;                            typlist[k] = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;        &#xa;        if not quiet:&#xa;            if alt_missing:&#xa;                smcl = ""{err}"" if IN_STATA else """" &#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        # header&#xa;        self._ds_format  = 117&#xa;        self._byteorder  = "">"" if sys.byteorder == ""big"" else ""<""&#xa;        self._nvar       = curr_nvars&#xa;        self._nobs       = nrows&#xa;        self._data_label = """"&#xa;        self._set_timestamp()&#xa;           &#xa;        # descriptors&#xa;        formats = self._default_fmts&#xa;        &#xa;        self._typlist = typlist&#xa;        self._varlist = [""var"" + str(i) for i in range(curr_nvars)]&#xa;        self._srtlist = [None for i in range(curr_nvars)]&#xa;        self._fmtlist = [&#xa;            '%' + str(max(9,st_type) if st_type <= 2045 else 9) + 's' &#xa;            if st_type <= 32768 else formats[st_type] for st_type in typlist]&#xa;        self._lbllist = [""""]*curr_nvars&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [""""]*curr_nvars&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {}&#xa;        &#xa;        # data&#xa;        self._varvals = varvals&#xa;        &#xa;        # value labels&#xa;        self._vallabs = {}&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        """"""Add new variable to data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of variable to be created.&#xa;        values : iterable&#xa;            Should be a flat iterable like [1, 5, 9, ...] or&#xa;            (1, 5, 9, ...). Not like [[1], [5], [9], ...].&#xa;        st_type : int or str, optional&#xa;            Examples: 212 or ""str212"", 65527 or ""float"".&#xa;            Intended Stata type of the data variable. &#xa;            The intended type will be overridden when necessary.&#xa;            Default value depends on the given values.&#xa;        compress : bool (or coercible to bool), optional&#xa;            If st_type is None, this sets st_type to byte if &#xa;            compress=True, or float if compress=False.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds new variable to data set.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        if (any(isinstance(values, t) for t in (str,bytes,bytearray))&#xa;                or not isinstance(values, collections.Iterable)):&#xa;            if self._nobs <= 1:&#xa;                values = [values]&#xa;            else:&#xa;                raise TypeError(""values to add must be in an iterable"")&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""variable name must be str"")&#xa;        &#xa;        name = name.strip()&#xa;        if name == """":&#xa;            raise ValueError(""variable name required"")&#xa;        &#xa;        if name in self._varlist:&#xa;            raise ValueError(""variable name already exists"")&#xa;        elif not self._is_valid_varname(name):&#xa;            raise ValueError(name + "" is not a valid Stata name"")&#xa;            &#xa;        type_names = (""byte"", ""int"", ""long"", ""float"", ""double"")&#xa;          &#xa;        init_st_type = st_type&#xa;        if st_type is None:&#xa;            st_type = 65530 if compress else 65527&#xa;        elif isinstance(st_type, str):&#xa;            m = re.match(r'^str([0-9]+|L)$', st_type)&#xa;            if m:&#xa;                if m.group(1) == ""L"":&#xa;                    st_type = 32768&#xa;                else:&#xa;                    st_type = int(m.group(1)) &#xa;                    if st_type > 2045:&#xa;                        if not self._quiet:&#xa;                            print(""string type > 2045; appending as strL"")&#xa;                        st_type = 32768&#xa;                init_st_type = st_type&#xa;            elif st_type in type_names:&#xa;                st_type = 65530 - type_names.index(st_type)&#xa;                init_st_type = st_type&#xa;            else:&#xa;                raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        elif (st_type not in (65530, 65529, 65528, 65527, 65526, 32768) &#xa;                and not (isinstance(st_type, int) and 1 <= st_type <= 2045)):&#xa;            raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        &#xa;        # Given iterable could be generator. Ensure it is in static form.&#xa;        values = [v for v in values]&#xa;        nvals = len(values)&#xa;        &#xa;        varvals = self._varvals&#xa;        &#xa;        if nvals == 0:&#xa;            this_missing = '' if st_type <= 32768 else MISSING&#xa;            for row in varvals:&#xa;                row.append(this_missing)&#xa;        else:&#xa;            alt_missing = False&#xa;            &#xa;            ismissing = self.ismissing&#xa;        &#xa;            for val, i in zip(values, range(nvals)):&#xa;                if st_type == 32768:&#xa;                    if any(isinstance(val, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        pass&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(val, int) and &#xa;                            not isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        values[i] = str(val)&#xa;                elif st_type <= 2045:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        st_type = (32768 if val_len > 2045 &#xa;                                        else max(st_type, val_len))&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        st_type = 32768&#xa;                    elif (not isinstance(val, int) and &#xa;                            not isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        val = str(val)&#xa;                        val_len = len(val)&#xa;                        values[i] = val&#xa;                        st_type = (32768 if val_len > 2045 &#xa;                                        else max(st_type, val_len))&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        max_len = len(val)&#xa;                        for j in range(i):&#xa;                            valj = values[j]&#xa;                            if ismissing(valj): &#xa;                                # If encountering a missing value here, &#xa;                                # should be instance of MissingValue.&#xa;                                # Could just check for that.&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(valj)&#xa;                                max_len = max(max_len, len(new_val))&#xa;                                values[j] = new_val&#xa;                        st_type = 32768 if max_len > 2045 else max_len&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        for j in range(i):&#xa;                            new_val = values[j]&#xa;                            if ismissing(new_val): &#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                values[j] = str(new_val)&#xa;                        st_type = 32768&#xa;                    elif val is None:&#xa;                        values[i] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(val, float) and &#xa;                            not isinstance(val, int)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (val != int(val) or -2147483647 > val &#xa;                                or val > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            st_type = 65526 # double&#xa;                        elif st_type <= 65529 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val outside of bounds&#xa;                            st_type = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= val <= 100): &#xa;                            # st_type byte, but val outside of bounds&#xa;                            st_type = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)): &#xa;                            # st_type float, but outside of bounds&#xa;                            st_type = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                            &#xa;            if nvals < self._nobs:&#xa;                this_missing = '' if st_type <= 32768 else MISSING&#xa;                values += [this_missing]*(self._nobs - nvals)&#xa;            elif nvals > self._nobs:&#xa;                self.set_obs(nvals)&#xa;            &#xa;            for row, new_val in zip(varvals, values):&#xa;                row.append(new_val)&#xa;            &#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                if init_st_type is not None and init_st_type != st_type:&#xa;                    st_type_name = self._get_type_name(st_type)&#xa;                    msg = (""warning: some values were incompatible with "" + &#xa;                           ""specified type;\n    type changed to "" + st_type_name)&#xa;                    print(smcl + msg)&#xa;                if alt_missing:&#xa;                    print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        &#xa;        self._typlist.append(st_type)&#xa;        self._varlist.append(name)&#xa;        self._srtlist.append(None)&#xa;        self._fmtlist.append(&#xa;            '%' + str(max(9,st_type) if st_type <= 2045 else 9) + 's'&#xa;            if st_type <= 32768 else self._default_fmts[st_type])&#xa;        self._lbllist.append('')&#xa;        self._vlblist.append('')&#xa;        &#xa;        self._nvar += 1&#xa;        self._changed = True&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        """"""Check to see if given str is a valid Stata name.&#xa;        Be sure to strip spaces before calling this function.&#xa;        &#xa;        """"""&#xa;        if name in RESERVED or re.match(r'^str([0-9]+|L)$', name): return False&#xa;        return True if VALID_NAME_RE.match(name) else False&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        """"""check that given str fmt is a valid Stata format""""""&#xa;        # make sure there is no leading or trailing whitespace&#xa;        fmt = fmt.strip()&#xa;        &#xa;        if fmt[0] != '%':&#xa;            return False&#xa;        &#xa;        # Handle business calendars first.&#xa;        # This does not check the calendar name.&#xa;        if fmt[1:3] == ""tb"" or fmt[1:4] == ""-tb"":&#xa;            return True if TB_FMT_RE.match(fmt) else False&#xa;            &#xa;        # date formats&#xa;        if fmt[1] == 't' or fmt[1:3] == '-t':&#xa;            return True if TIME_FMT_RE.match(fmt) else False&#xa;        &#xa;        # categorize using last character&#xa;        last_char = fmt[-1]&#xa;        if last_char == 's': # string&#xa;            m = STR_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width > 2045: return False&#xa;            return True&#xa;        elif last_char == 'H' or last_char == 'L': # binary&#xa;            # Valid binary formats are ^%(8|16)(H|L)$. Stata doesn't raise &#xa;            # error with -8 or -16, but the results are perhaps unexpected.&#xa;            return True if fmt[1:-1] in ('8', '16', '-8', '-16') else False&#xa;        elif last_char == 'x': # hexadecimal&#xa;            return True if fmt == '%21x' or fmt == '%-12x' else False&#xa;        elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;            m = NUM_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width <= int(m.group(5)) or width > 2045: &#xa;                return False&#xa;            return True&#xa;            &#xa;        return False&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        """"""Width of an observation as saved.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Width of a single observation in bytes.&#xa;        &#xa;        """"""&#xa;        widths = {65530: 1, 65529: 2, 65528: 4, 65527: 4, 65526: 8, 32768: 8}&#xa;        return sum([0] + [t if t <= 2045 else widths[t] &#xa;                   for t in self._typlist])&#xa;    &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        """"""Replace the values in the obs and vars of the -index- tuple.&#xa;        The shape of -value- should match the shape implied by -index-, &#xa;        and sub-values should be consistent with the existing Stata &#xa;        types in the columns.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        varvals = self._varvals&#xa;        typlist = self._typlist&#xa;        varlist = self._varlist&#xa;        # copy typlist to check changes against later&#xa;        old_typlist = [typlist[i] for i in sel_cols] &#xa;        &#xa;        alt_missing = False&#xa;        &#xa;        for row_num, i in zip(sel_rows, range(len(sel_rows))):&#xa;            row = value[i]&#xa;            for col_num, k in zip(sel_cols, range(len(sel_cols))):&#xa;                val = row[k]&#xa;                st_type = typlist[col_num]&#xa;                if st_type == 32768:&#xa;                    if all(not isinstance(val, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        msg = (""values in \"""" + varlist[col_num] + &#xa;                               ""\"" must be str or bytes"")&#xa;                        raise TypeError(msg)&#xa;                elif st_type <= 2045:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        typlist[col_num] = (32768 if val_len > 2045 &#xa;                                                 else max(st_type, val_len))&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        val = ''&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        typlist[col_num] = 32768&#xa;                    else:&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot "" + &#xa;                               ""take non-string values"")&#xa;                        raise TypeError(msg)&#xa;                else:&#xa;                    if any(isinstance(val, t) &#xa;                           for t in (str, bytes, bytearray)):&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot take "" + &#xa;                               ""string or bytes values; has Stata type "" + &#xa;                               self._get_type_name(st_type))&#xa;                        raise TypeError(msg)&#xa;                    elif val is None:&#xa;                        val = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(val, float) and &#xa;                            not isinstance(val, int)):&#xa;                        msg = (""value in right-hand position "" + &#xa;                               ""{},{} is not of recognized type"".format(i, k))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        val = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (val != int(val) or -2147483647 > val or &#xa;                                val > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            typlist[col_num] = 65526 # double&#xa;                        elif st_type >= 65529 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val outside of bounds&#xa;                            typlist[col_num] = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= val <= 100): &#xa;                            # st_type byte, but val outside of bounds&#xa;                            typlist[col_num] = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)): &#xa;                            # st_type float, but outisde of bounds&#xa;                            typlist[col_num] = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                            &#xa;                varvals[row_num][col_num] = val&#xa;            &#xa;        if not self._quiet: &#xa;            # Record seen columns. &#xa;            # Use a set because same column can appear multiple times.&#xa;            seen_cols = set()&#xa;            smcl = ""{txt}"" if IN_STATA else """"&#xa;            msg = smcl + ""Stata type for {} was {}, now {}""&#xa;            for old_type,c in zip(old_typlist, sel_cols):&#xa;                new_type = typlist[c]&#xa;                if old_type != new_type and c not in seen_cols:&#xa;                    old_name = self._get_type_name(old_type)&#xa;                    new_name = self._get_type_name(new_type)&#xa;                    print(msg.format(varlist[c], old_name, new_name))&#xa;                seen_cols.add(c)&#xa;            &#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        """"""helper function for writing dta files""""""&#xa;        diff = miss_val.index&#xa;        &#xa;        if st_type == 65530: # byte&#xa;            value = diff + 101&#xa;        elif st_type == 65529: # int&#xa;            value = diff + 32741&#xa;        elif st_type == 65528: # long&#xa;            value = diff + 2147483621&#xa;        elif st_type == 65527: # float&#xa;            value = float.fromhex('0x1.0' + hex(diff)[2:].zfill(2) + 'p+127')&#xa;        elif st_type == 65526: # double&#xa;            value = float.fromhex('0x1.0' + hex(diff)[2:].zfill(2) + 'p+1023')&#xa;        return value&#xa;            &#xa;    def _dta_obj_to_file(self, address):&#xa;        """"""save dta object to disk""""""&#xa;        global get_missing&#xa;        &#xa;        type_dict = {&#xa;            65530: ['b',1],&#xa;            65529: ['h',2],&#xa;            65528: ['l',4], &#xa;            65527: ['f',4],&#xa;            65526: ['d',8]&#xa;        }&#xa;        first_missing = {&#xa;            65530: 101,&#xa;            65529: 32741,&#xa;            65528: 2147483620,&#xa;            65527: float.fromhex('0x1.0p+127'),&#xa;            65526: float.fromhex('0x1.0p+1023')&#xa;        }&#xa;        typlist = self._typlist&#xa;        byteorder = self._byteorder&#xa;        nvar = self._nvar&#xa;                &#xa;        def write_value_label_table(labname, table):&#xa;            # Stata limits are a bit confusing.&#xa;            # Total length of text (incl. null terminators) must be <= 32000 ?&#xa;            # Total number of vals must be <= 65536 ?&#xa;            # But the limit on text length forces no. of vals <= 16000 since&#xa;            # each label must occupy at least two bytes &#xa;            # (including null terminator).&#xa;            labname = labname[:32]&#xa;            &#xa;            val = sorted(table.keys())&#xa;            # each value may be up to 81 chars including null&#xa;            txt = [table[v][:80] for v in val] &#xa;            &#xa;            nval = len(val)&#xa;            if nval > 65536: # max number of values allowed&#xa;                val = val[:65536]&#xa;                txt = txt[:65536]&#xa;                nval = 65536&#xa;            &#xa;            off = [0]&#xa;            for i in range(nval - 1):&#xa;                # in next line, ""+ 1"" to leave room for \0&#xa;                offset = off[i] + len(txt[i]) + 1&#xa;                if offset > 32000: # if too much text&#xa;                    off = off[:i] # cut off at before the ith one&#xa;                    val = val[:i]&#xa;                    txt = txt[:i]&#xa;                    nval = i&#xa;                    break&#xa;                off.append(offset)&#xa;            txt_len = off[-1] + len(txt[-1]) + 1&#xa;            &#xa;            table_len = 4 + 4 + 4*nval + 4*nval + txt_len&#xa;            &#xa;            dta.write(bytearray('<lbl>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + ""l"", table_len))&#xa;            dta.write(bytearray(labname, 'iso-8859-1') + &#xa;                      b'\0'*(33-len(labname)))&#xa;            dta.write(b'\x00\x00\x00')&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", nval))&#xa;            dta.write(pack(byteorder + ""l"", txt_len))&#xa;            for o in off: dta.write(pack(byteorder + ""l"", o))&#xa;            for v in val: dta.write(pack(byteorder + ""l"", v))&#xa;            for t in txt: dta.write(bytearray(t, 'iso-8859-1') + b'\0')&#xa;            dta.write(bytearray('</lbl>', 'iso-8859-1'))&#xa;        &#xa;        with open(address, 'wb') as dta:&#xa;            dta.write(bytearray('<stata_dta>', 'iso-8859-1'))&#xa;            &#xa;            # header&#xa;            dta.write(bytearray('<header>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<release>', 'iso-8859-1'))&#xa;            dta.write(bytearray('117', 'iso-8859-1'))&#xa;            dta.write(bytearray('</release>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<byteorder>', 'iso-8859-1'))&#xa;            dta.write(&#xa;                bytearray('MSF' if byteorder == '>' else 'LSF', 'iso-8859-1'))&#xa;            dta.write(bytearray('</byteorder>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<K>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'H', self._nvar))&#xa;            dta.write(bytearray('</K>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<N>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'I', self._nobs))&#xa;            dta.write(bytearray('</N>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<label>', 'iso-8859-1'))&#xa;            label = self._data_label&#xa;            label_length = len(label)&#xa;            dta.write(pack(byteorder + 'B', label_length))&#xa;            dta.write(bytearray(label, 'iso-8859-1'))&#xa;            dta.write(bytearray('</label>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<timestamp>', 'iso-8859-1'))&#xa;            stamp = self._time_stamp&#xa;            m = re.match(&#xa;                '^([ 0-3][0-9]) ' + &#xa;                '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) ' + &#xa;                '[0-9]{4} ([ 0-2][0-9]):([0-9]{2})$', &#xa;                stamp)&#xa;            if (m and &#xa;                    1 <= int(m.group(1)) <= 31 and &#xa;                    0 <= int(m.group(3)) <= 24 and&#xa;                    0 <= int(m.group(4)) < 60):&#xa;                dta.write(pack(byteorder + 'B', 17))&#xa;                # next line includes optional binary zero&#xa;                dta.write(bytearray(stamp, 'iso-8859-1'))&#xa;            else: # there's something wrong with the time stamp, just skip it&#xa;                dta.write(pack(byteorder + 'B', 0))&#xa;            dta.write(bytearray('</timestamp>', 'iso-8859-1'))&#xa;            dta.write(bytearray('</header>', 'iso-8859-1'))&#xa;            &#xa;            # map&#xa;            offset_map = [0, dta.tell()]&#xa;            dta.write(bytearray(""<map>"", 'iso-8859-1'))&#xa;            for i in range(14):&#xa;                dta.write(pack(byteorder + 'Q', 0))&#xa;            dta.write(bytearray(""</map>"", ""iso-8859-1""))&#xa;            &#xa;            # variable types&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<variable_types>"", 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'H'*nvar, *typlist))&#xa;            dta.write(bytearray(""</variable_types>"", 'iso-8859-1'))&#xa;            &#xa;            # variable names&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<varnames>"", 'iso-8859-1'))&#xa;            for name in self._varlist:&#xa;                name = name[:32]&#xa;                dta.write(bytearray(name, 'iso-8859-1') + b'\0'*(33-len(name)))&#xa;            dta.write(bytearray(""</varnames>"", 'iso-8859-1'))&#xa;            &#xa;            # sort order&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<sortlist>"", 'iso-8859-1'))&#xa;            srtlist = self._srtlist + [None]&#xa;            srtlist = [srt + 1 if srt is not None else 0 for srt in srtlist]&#xa;            dta.write(pack(byteorder + 'H'*(nvar + 1), *srtlist))&#xa;            dta.write(bytearray(""</sortlist>"", 'iso-8859-1'))&#xa;            &#xa;            # formats&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<formats>"", 'iso-8859-1'))&#xa;            for fmt in self._fmtlist:&#xa;                fmt = fmt[:48]&#xa;                dta.write(bytearray(fmt, 'iso-8859-1') + b'\0'*(49-len(fmt)))&#xa;            dta.write(bytearray(""</formats>"", 'iso-8859-1'))&#xa;            &#xa;            # value-label names&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<value_label_names>"", 'iso-8859-1'))&#xa;            for lab in self._lbllist:&#xa;                lab = lab[:32]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(33-len(lab)))&#xa;            dta.write(bytearray(""</value_label_names>"", 'iso-8859-1'))&#xa;            &#xa;            # variable labels&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<variable_labels>"", 'iso-8859-1'))&#xa;            for lab in self._vlblist:&#xa;                lab = lab[:80]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(81-len(lab)))&#xa;            dta.write(bytearray(""</variable_labels>"", 'iso-8859-1'))&#xa;            &#xa;            # characteristics&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<characteristics>"", 'iso-8859-1'))&#xa;            chrdict = self._chrdict&#xa;            for varname in chrdict:&#xa;                varname = varname[:32]&#xa;                var_dict = chrdict[varname]&#xa;                for charname in var_dict:&#xa;                    charname = charname[:32]&#xa;                    char = var_dict[charname][:67784] # or 8681 for Small Stata&#xa;                    full_length = 66 + len(char) + 1 # +1 for null termination&#xa;                    &#xa;                    dta.write(bytearray('<ch>', 'iso-8859-1'))&#xa;                    dta.write(pack(byteorder + 'I', full_length))&#xa;                    dta.write(bytearray(varname, 'iso-8859-1') + &#xa;                              b'\0'*(33-len(varname)))&#xa;                    dta.write(bytearray(charname, 'iso-8859-1') + &#xa;                              b'\0'*(33-len(charname)))&#xa;                    dta.write(bytearray(char, 'iso-8859-1') + b'\0')&#xa;                    dta.write(bytearray('</ch>', 'iso-8859-1'))&#xa;            dta.write(bytearray(""</characteristics>"", 'iso-8859-1'))&#xa;            &#xa;            # data&#xa;            offset_map.append(dta.tell())&#xa;            strls = {}&#xa;            dta.write(bytearray(""<data>"", 'iso-8859-1'))&#xa;            varvals = self._varvals&#xa;            nvar, nobs = self._nvar, self._nobs&#xa;            missing_save_val = self._missing_save_val&#xa;            for i in range(nobs):&#xa;                row = varvals[i]&#xa;                for j in range(nvar):&#xa;                    value, st_type = row[j], typlist[j]&#xa;                    if st_type <= 2045:&#xa;                        value = value[:st_type]&#xa;                        dta.write(bytearray(value, 'iso-8859-1') + &#xa;                                  b'\0'*(st_type - len(value)))&#xa;                    elif st_type == 32768:&#xa;                        if value == """":&#xa;                            o,v = 0,0&#xa;                        elif value in strls:&#xa;                            o,v = strls[value]&#xa;                        else:&#xa;                            strls[value] = o,v = (i+1,j+1)&#xa;                        dta.write(pack(byteorder + 'II', v, o))&#xa;                    else:&#xa;                        fmt = 'bhlfd'[65530 - st_type]&#xa;                        if value is None:&#xa;                            value = first_missing[st_type]&#xa;                        elif isinstance(value, MissingValue):&#xa;                            value = missing_save_val(value, st_type)&#xa;                        elif (value > 8.988465674311579e+307 or &#xa;                                value < -1.7976931348623157e+308):&#xa;                            # is this the right way to handle this ?&#xa;                            value = missing_save_val(&#xa;                                get_missing(value), st_type)&#xa;                        dta.write(pack(byteorder + fmt, value))&#xa;            dta.write(bytearray(""</data>"", 'iso-8859-1'))&#xa;                &#xa;            # strls&#xa;            offset_map.append(dta.tell())&#xa;            strls = [(val, key) for key,val in strls.items()]&#xa;            strls.sort()&#xa;            dta.write(bytearray(""<strls>"", 'iso-8859-1'))&#xa;            for (o,v), value in strls:&#xa;                dta.write(bytearray('GSO', 'iso-8859-1'))&#xa;                dta.write(pack(byteorder + 'II', v, o))&#xa;                if isinstance(value, str):&#xa;                    try:&#xa;                        # expect error in next line if anywhere&#xa;                        value = bytes(value, 'iso-8859-1') + b'\x00'&#xa;                        dta.write(pack('B', 130))&#xa;                    except UnicodeEncodeError:&#xa;                        value = bytes(value, 'utf-8')&#xa;                        dta.write(pack('B', 129))&#xa;                elif (not isinstance(value, bytes) and &#xa;                        not isinstance(value, bytearray)):&#xa;                    msg = ""only bytes or str object allowed in Stata strl""&#xa;                    raise TypeError(msg)&#xa;                else:&#xa;                    dta.write(pack('B', 129))&#xa;                val_len = len(value)&#xa;                dta.write(pack(byteorder + 'I', val_len))&#xa;                num_vals = unpack(str(val_len) + 'b', value)&#xa;                dta.write(value)&#xa;            dta.write(bytearray(""</strls>"", 'iso-8859-1'))&#xa;            &#xa;            # value labels&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<value_labels>"", 'iso-8859-1'))&#xa;            for name, table in self._vallabs.items():&#xa;                write_value_label_table(name, table)&#xa;            dta.write(bytearray(""</value_labels>"", 'iso-8859-1'))&#xa;            &#xa;            # end file&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""</stata_dta>"", 'iso-8859-1'))&#xa;            &#xa;            offset_map.append(dta.tell())&#xa;            &#xa;            # write map&#xa;            dta.seek(offset_map[1] + 5)&#xa;            for offset in offset_map:&#xa;                dta.write(pack(byteorder + 'Q', offset))&#xa;&#xa;&#xa;def display_diff(dta1, dta2, all_data=False):&#xa;    """"""Display summary of differences between two Dta objects.&#xa;    &#xa;    Parameters&#xa;    ----------&#xa;    dta1 : Dta instance&#xa;    dta2 : Dta instance&#xa;    all_data : bool (or coercible to bool), optional&#xa;        Specify that all data values should be checked for&#xa;        equality, rather than stopping at first inequality. &#xa;        Default value is False.&#xa;    &#xa;    Returns&#xa;    -------&#xa;    None&#xa;    &#xa;    Side effects&#xa;    ------------&#xa;    Displays summary of differences.&#xa;    &#xa;    """"""&#xa;    if not isinstance(dta1, Dta) or not isinstance(dta2, Dta):&#xa;        raise TypeError(""objects to be compared must be Dta"")&#xa;    &#xa;    typlist_converters = {&#xa;        'Dta115': {&#xa;            'Dta117': lambda i: i if i <= 244 else 65530 + (251 - i)&#xa;        }&#xa;    }&#xa;    &#xa;    different = False&#xa;    &#xa;    # Python class types <-> dta version&#xa;    # ----------------------------------&#xa;    dta1_type, dta2_type = dta1.__class__.__name__, dta2.__class__.__name__&#xa;    if not dta1_type == dta2_type:&#xa;        different = True&#xa;        print(""    class types differ:"")&#xa;        print(""        {} vs {}"".format(dta1_type, dta2_type))&#xa;    &#xa;    # data set descriptors&#xa;    # --------------------&#xa;    if not dta1._ds_format == dta2._ds_format:&#xa;        different = True&#xa;        print(""    formats differ:"")&#xa;        print(""        {} vs {}"".format(dta1._ds_format, dta2._ds_format))&#xa;    &#xa;    if not dta1._data_label == dta2._data_label:&#xa;        different = True&#xa;        print(""    data labels differ:"")&#xa;        print(""        {} vs {}"".format(dta1._data_label, dta2._data_label))&#xa;    &#xa;    # time stamp&#xa;    # ----------&#xa;    stamp1 = dta1._time_stamp.split()&#xa;    stamp2 = dta2._time_stamp.split()&#xa;    stamp1[0] = int(stamp1[0]) #day&#xa;    stamp2[0] = int(stamp2[0])&#xa;    stamp1[2] = int(stamp1[2]) #year&#xa;    stamp2[2] = int(stamp2[2])&#xa;    stamp1 = stamp1[:-1] + [int(x) for x in stamp1[-1].split(':')]  # hr & min&#xa;    stamp2 = stamp2[:-1] + [int(x) for x in stamp2[-1].split(':')]&#xa;    if not stamp1 == stamp2:&#xa;        different = True&#xa;        print(""    time stamps differ:"")&#xa;        print(""        {} vs {}"".format(dta1._time_stamp, dta2._time_stamp))&#xa;    &#xa;    # number of variables and observations&#xa;    # ------------------------------------&#xa;    if not dta1._nvar == dta2._nvar:&#xa;        different = True&#xa;        print(""    # of vars differs:"")&#xa;        print(""        {} vs {}"".format(dta1._nvar, dta2._nvar))&#xa;        print(""   > comparison now limited to vars 0 .. min(nvar1, nvar2)"")&#xa;    &#xa;    if not dta1._nobs == dta2._nobs:&#xa;        different = True&#xa;        print(""    # of obs differs:"")&#xa;        print(""        {} vs {}"".format(dta1._nobs, dta2._nobs))&#xa;        print(""   > comparison now limited to obs 0 .. min(nobs1, nobs2)"")&#xa;        &#xa;    nvar = min(dta1._nvar, dta2._nvar)&#xa;    nobs = min(dta1._nobs, dta2._nobs)&#xa;    &#xa;    # descriptors&#xa;    # -----------&#xa;    &#xa;    # typlist&#xa;    # If dta versions are the same, can make direct comparison. If versions&#xa;    # are different, a direct comparison doesn't mean much if data types&#xa;    # are encoded differently, so convert one before comparing.&#xa;    if dta1_type == dta2_type:&#xa;        diff = [i for i in range(nvar) if dta1._typlist[i] != dta2._typlist[i]]&#xa;    else:&#xa;        s = sorted(((dta1_type, dta1), (dta2_type, dta2)))&#xa;        (older_type, older_dta), (newer_type, newer_dta) = s&#xa;        converter = typlist_converters[older_type][newer_type]&#xa;        diff = [i for i in range(nvar) &#xa;                if converter(older_dta._typlist[i]) != newer_dta._typlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    Stata data types differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;    &#xa;    # varlist&#xa;    diff = [i for i in range(nvar) if dta1._varlist[i] != dta2._varlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    variable names differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # srtlist&#xa;    diff = [i for i in range(nvar) if dta1._srtlist[i] != dta2._srtlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    sort lists differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;    &#xa;    # fmtlist&#xa;    diff = [i for i in range(nvar) if dta1._fmtlist[i] != dta2._fmtlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    display formats differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # lbllist&#xa;    diff = [i for i in range(nvar) if dta1._lbllist[i] != dta2._lbllist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        msg = ""    attached value labels differ in {} places"".format(len(diff))&#xa;        print(msg)&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # vlblist&#xa;    diff = [i for i in range(nvar) if dta1._vlblist[i] != dta2._vlblist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    variable labels differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;      &#xa;    # characteristics&#xa;    # ---------------&#xa;    keys1 = set(dta1._chrdict.keys())&#xa;    keys2 = set(dta2._chrdict.keys())&#xa;    diff = keys1 - keys2&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    charataristic keys in #1 but not in #2:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = keys2 - keys1&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    charataristic keys in #2 but not in #1:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = [k for k in keys1.intersection(keys2) &#xa;                if dta1._chrdict[k] != dta2._chrdict[k]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    charataristic keys with different value:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    # defined value labels&#xa;    # --------------------&#xa;    keys1 = set(dta1._vallabs.keys())&#xa;    keys2 = set(dta2._vallabs.keys())&#xa;    diff = keys1 - keys2&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    value labels defined in #1 but not in #2:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = keys2 - keys1&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    value labels defined in #2 but not in #1:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = [k for k in keys1.intersection(keys2)&#xa;                if dta1._vallabs[k] != dta2._vallabs[k]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    value labels with same name but different mapping:"")&#xa;        print(""       "", str(diff))&#xa;    &#xa;    # data values&#xa;    # -----------&#xa;    if all_data:&#xa;        diff = sum([0] + [1 for i in range(nobs) for j in range(nvar)&#xa;                    if dta1._varvals[i][j] != dta2._varvals[i][j]])&#xa;        if diff != 0:&#xa;            different = True&#xa;            print(""    data values differ in "" + str(diff) + "" places"")&#xa;    else:&#xa;        for i in range(nobs):&#xa;            for j in range(nvar):&#xa;                if dta1._varvals[i][j] != dta2._varvals[i][j]:&#xa;                    different = True&#xa;                    print("""".join(&#xa;                        (""    data values differ\n        "",&#xa;                        ""first difference in position {},{}"".format(i,j))))&#xa;                    break&#xa;            else:&#xa;                continue  # executed if the loop ended normally (no break)&#xa;            break  # executed if 'continue' was skipped (break)&#xa;            # trick from http://stackoverflow.com/questions/653509 &#xa;            # to exit from nested for loops&#xa;&#xa;    if not different:&#xa;        print(""    no difference found"")&#xa;&#xa;def open_dta(address):&#xa;    """"""Open any recent version dta file (versions 114, 115, 117) .&#xa;    &#xa;    Parameters&#xa;    ----------&#xa;    Address of file, including file name and "".dta"".&#xa;    &#xa;    Returns&#xa;    -------&#xa;    Instance of sub-class of Dta, depending on dta file.&#xa;    &#xa;    """"""&#xa;    with open(address, 'rb') as dta_file:&#xa;        first_bytes = dta_file.read(11)&#xa;    ds_format = first_bytes[0]&#xa;    if isinstance(ds_format, str):  # happens in Python 2.7&#xa;        ds_format = ord(ds_format)&#xa;    # If format is 117, then first_bytes[0] is ""<"", which gets unpacked as 60.&#xa;    if ds_format == 114 or ds_format == 115:&#xa;        return Dta115(address)&#xa;    elif first_bytes.decode('iso-8859-1') == ""<stata_dta>"":&#xa;        return Dta117(address)&#xa;    else:&#xa;        raise ValueError(""only dta formats 117, 115, and 114 are supported"")&#xa;    &#xa;    "
568271|"""""""&#xa;    ***&#xa;    Modified generic daemon class&#xa;    ***&#xa;&#xa;    Author:     http://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/&#xa;                www.boxedice.com&#xa;                www.datadoghq.com&#xa;&#xa;    License:    http://creativecommons.org/licenses/by-sa/3.0/&#xa;""""""&#xa;&#xa;# Core modules&#xa;import atexit&#xa;import errno&#xa;import logging&#xa;import os&#xa;import signal&#xa;import sys&#xa;import time&#xa;&#xa;# 3p&#xa;from psutil import pid_exists&#xa;&#xa;log = logging.getLogger(__name__)&#xa;&#xa;class AgentSupervisor(object):&#xa;    ''' A simple supervisor to keep a restart a child on expected auto-restarts&#xa;    '''&#xa;    RESTART_EXIT_STATUS = 5&#xa;&#xa;    @classmethod&#xa;    def start(cls, parent_func, child_func=None):&#xa;        ''' `parent_func` is a function that's called every time the child&#xa;            process dies.&#xa;            `child_func` is a function that should be run by the forked child&#xa;            that will auto-restart with the RESTART_EXIT_STATUS.&#xa;        '''&#xa;        exit_code = cls.RESTART_EXIT_STATUS&#xa;&#xa;        # Allow the child process to die on SIGTERM&#xa;        signal.signal(signal.SIGTERM, cls._handle_sigterm)&#xa;&#xa;        cls.need_stop = False&#xa;&#xa;        while True:&#xa;            try:&#xa;                if hasattr(cls, 'child_pid'):&#xa;                    delattr(cls, 'child_pid')&#xa;                pid = os.fork()&#xa;                if pid > 0:&#xa;                    # The parent waits on the child.&#xa;                    cls.child_pid = pid&#xa;                    while not cls.need_stop:&#xa;                        cpid, status = os.waitpid(pid, os.WNOHANG)&#xa;                        if (cpid, status) != (0, 0):&#xa;                            break&#xa;                        time.sleep(1)&#xa;                    exit_code = status >> 8&#xa;                    if parent_func is not None:&#xa;                        parent_func()&#xa;&#xa;                    if cls.need_stop:&#xa;                        break&#xa;                else:&#xa;                    # The child will call our given function&#xa;                    if child_func is not None:&#xa;                        child_func()&#xa;                    else:&#xa;                        break&#xa;            except OSError, e:&#xa;                msg = ""Agent fork failed: %d (%s)"" % (e.errno, e.strerror)&#xa;                logging.error(msg)&#xa;                sys.stderr.write(msg + ""\n"")&#xa;                sys.exit(1)&#xa;&#xa;        # Exit from the parent cleanly&#xa;        if pid > 0:&#xa;            sys.exit(0)&#xa;&#xa;    @classmethod&#xa;    def _handle_sigterm(cls, signum, frame):&#xa;        # in the parent&#xa;        if hasattr(cls, 'child_pid'):&#xa;            os.kill(cls.child_pid, signal.SIGTERM)&#xa;            cls.need_stop = True&#xa;        # in the child&#xa;        else:&#xa;            sys.exit(0)&#xa;&#xa;&#xa;class Daemon(object):&#xa;    """"""&#xa;    A generic daemon class.&#xa;&#xa;    Usage: subclass the Daemon class and override the run() method&#xa;    """"""&#xa;    def __init__(self, pidfile, stdin=os.devnull, stdout=os.devnull, stderr=os.devnull, autorestart=False):&#xa;        self.autorestart = autorestart&#xa;        self.stdin = stdin&#xa;        self.stdout = stdout&#xa;        self.stderr = stderr&#xa;        self.pidfile = pidfile&#xa;&#xa;    def daemonize(self):&#xa;        """"""&#xa;        Do the UNIX double-fork magic, see Stevens' ""Advanced&#xa;        Programming in the UNIX Environment"" for details (ISBN 0201563177)&#xa;        http://www.erlenstar.demon.co.uk/unix/faq_2.html#SEC16&#xa;        """"""&#xa;        try:&#xa;            pid = os.fork()&#xa;            if pid > 0:&#xa;                # Exit first parent&#xa;                sys.exit(0)&#xa;        except OSError, e:&#xa;            msg = ""fork #1 failed: %d (%s)"" % (e.errno, e.strerror)&#xa;            log.error(msg)&#xa;            sys.stderr.write(msg + ""\n"")&#xa;            sys.exit(1)&#xa;&#xa;        log.debug(""Fork 1 ok"")&#xa;&#xa;        # Decouple from parent environment&#xa;        os.chdir(""/"")&#xa;        os.setsid()&#xa;&#xa;        if self.autorestart:&#xa;            # Set up the supervisor callbacks and put a fork in it.&#xa;            logging.info('Running with auto-restart ON')&#xa;            AgentSupervisor.start(parent_func=None, child_func=None)&#xa;        else:&#xa;            # Do second fork&#xa;            try:&#xa;                pid = os.fork()&#xa;                if pid > 0:&#xa;                    # Exit from second parent&#xa;                    sys.exit(0)&#xa;            except OSError, e:&#xa;                msg = ""fork #2 failed: %d (%s)"" % (e.errno, e.strerror)&#xa;                logging.error(msg)&#xa;                sys.stderr.write(msg + ""\n"")&#xa;                sys.exit(1)&#xa;&#xa;        if sys.platform != 'darwin': # This block breaks on OS X&#xa;            # Redirect standard file descriptors&#xa;            sys.stdout.flush()&#xa;            sys.stderr.flush()&#xa;            si = file(self.stdin, 'r')&#xa;            so = file(self.stdout, 'a+')&#xa;            se = file(self.stderr, 'a+', 0)&#xa;            os.dup2(si.fileno(), sys.stdin.fileno())&#xa;            os.dup2(so.fileno(), sys.stdout.fileno())&#xa;            os.dup2(se.fileno(), sys.stderr.fileno())&#xa;&#xa;        log.info(""Daemon started"")&#xa;&#xa;&#xa;    def start(self, foreground=False):&#xa;        log.info(""Starting"")&#xa;        pid = self.pid()&#xa;&#xa;        if pid:&#xa;            # Check if the pid in the pidfile corresponds to a running process&#xa;            if pid_exists(pid):&#xa;                log.error(""Not starting, another instance is already running""&#xa;                          "" (using pidfile {0})"".format(self.pidfile))&#xa;                sys.exit(1)&#xa;            else:&#xa;                log.warn('pidfile contains the pid of a stopped process.'&#xa;                         ' Starting normally')&#xa;&#xa;        log.info(""Pidfile: %s"" % self.pidfile)&#xa;        if not foreground:&#xa;            self.daemonize()&#xa;        self.write_pidfile()&#xa;        self.run()&#xa;&#xa;&#xa;    def stop(self):&#xa;        log.info(""Stopping daemon"")&#xa;        pid = self.pid()&#xa;&#xa;        # Clear the pid file&#xa;        if os.path.exists(self.pidfile):&#xa;            os.remove(self.pidfile)&#xa;&#xa;        if pid > 1:&#xa;            try:&#xa;                if self.autorestart:&#xa;                    # Try killing the supervising process&#xa;                    try:&#xa;                        os.kill(os.getpgid(pid), signal.SIGTERM)&#xa;                    except OSError:&#xa;                        log.warn(""Couldn't not kill parent pid %s. Killing pid."" % os.getpgid(pid))&#xa;                        os.kill(pid, signal.SIGTERM)&#xa;                else:&#xa;                    # No supervising process present&#xa;                    os.kill(pid, signal.SIGTERM)&#xa;                log.info(""Daemon is stopped"")&#xa;            except OSError, err:&#xa;                if str(err).find(""No such process"") <= 0:&#xa;                    log.exception(""Cannot kill Agent daemon at pid %s"" % pid)&#xa;                    sys.stderr.write(str(err) + ""\n"")&#xa;        else:&#xa;            message = ""Pidfile %s does not exist. Not running?\n"" % self.pidfile&#xa;            log.info(message)&#xa;            sys.stderr.write(message)&#xa;&#xa;            # A ValueError might occur if the PID file is empty but does actually exist&#xa;            if os.path.exists(self.pidfile):&#xa;                os.remove(self.pidfile)&#xa;&#xa;            return # Not an error in a restart&#xa;&#xa;&#xa;    def restart(self):&#xa;        ""Restart the daemon""&#xa;        self.stop()&#xa;        self.start()&#xa;&#xa;&#xa;    def run(self):&#xa;        """"""&#xa;        You should override this method when you subclass Daemon. It will be called after the process has been&#xa;        daemonized by start() or restart().&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;    @classmethod&#xa;    def info(cls):&#xa;        """"""&#xa;        You should override this method when you subclass Daemon. It will be&#xa;        called to provide information about the status of the process&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;    def status(self):&#xa;        """"""&#xa;        Get the status of the daemon. Exits with 0 if running, 1 if not.&#xa;        """"""&#xa;        pid = self.pid()&#xa;&#xa;        if pid < 0:&#xa;            message = '%s is not running' % self.__class__.__name__&#xa;            exit_code = 1&#xa;        else:&#xa;            # Check for the existence of a process with the pid&#xa;            try:&#xa;                # os.kill(pid, 0) will raise an OSError exception if the process&#xa;                # does not exist, or if access to the process is denied (access denied will be an EPERM error).&#xa;                # If we get an OSError that isn't an EPERM error, the process&#xa;                # does not exist.&#xa;                # (from http://stackoverflow.com/questions/568271/check-if-pid-is-not-in-use-in-python,&#xa;                #  Giampaolo's answer)&#xa;                os.kill(pid, 0)&#xa;            except OSError, e:&#xa;                if e.errno != errno.EPERM:&#xa;                    message = '%s pidfile contains pid %s, but no running process could be found' % (self.__class__.__name__, pid)&#xa;                else:&#xa;                    message = 'You do not have sufficient permissions'&#xa;                exit_code = 1&#xa;&#xa;            else:&#xa;                message = '%s is running with pid %s' % (self.__class__.__name__, pid)&#xa;                exit_code = 0&#xa;&#xa;        log.info(message)&#xa;        sys.stdout.write(message + ""\n"")&#xa;        sys.exit(exit_code)&#xa;&#xa;&#xa;    def pid(self):&#xa;        # Get the pid from the pidfile&#xa;        try:&#xa;            pf = file(self.pidfile, 'r')&#xa;            pid = int(pf.read().strip())&#xa;            pf.close()&#xa;            return pid&#xa;        except IOError:&#xa;            return None&#xa;        except ValueError:&#xa;            return None&#xa;&#xa;&#xa;    def write_pidfile(self):&#xa;        # Write pidfile&#xa;        atexit.register(self.delpid) # Make sure pid file is removed if we quit&#xa;        pid = str(os.getpid())&#xa;        try:&#xa;            fp = open(self.pidfile, 'w+')&#xa;            fp.write(str(pid))&#xa;            fp.close()&#xa;            os.chmod(self.pidfile, 0644)&#xa;        except Exception, e:&#xa;            msg = ""Unable to write pidfile: %s"" % self.pidfile&#xa;            log.exception(msg)&#xa;            sys.stderr.write(msg + ""\n"")&#xa;            sys.exit(1)&#xa;&#xa;&#xa;    def delpid(self):&#xa;        try:&#xa;            os.remove(self.pidfile)&#xa;        except OSError:&#xa;            pass&#xa;"
267399|"# module pyparsing.py&#xa;#&#xa;# Copyright (c) 2003-2016  Paul T. McGuire&#xa;#&#xa;# Permission is hereby granted, free of charge, to any person obtaining&#xa;# a copy of this software and associated documentation files (the&#xa;# ""Software""), to deal in the Software without restriction, including&#xa;# without limitation the rights to use, copy, modify, merge, publish,&#xa;# distribute, sublicense, and/or sell copies of the Software, and to&#xa;# permit persons to whom the Software is furnished to do so, subject to&#xa;# the following conditions:&#xa;#&#xa;# The above copyright notice and this permission notice shall be&#xa;# included in all copies or substantial portions of the Software.&#xa;#&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,&#xa;# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF&#xa;# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.&#xa;# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY&#xa;# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,&#xa;# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE&#xa;# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;#&#xa;&#xa;__doc__ = \&#xa;""""""&#xa;pyparsing module - Classes and methods to define and execute parsing grammars&#xa;&#xa;The pyparsing module is an alternative approach to creating and executing simple grammars,&#xa;vs. the traditional lex/yacc approach, or the use of regular expressions.  With pyparsing, you&#xa;don't need to learn a new syntax for defining grammars or matching expressions - the parsing module&#xa;provides a library of classes that you use to construct the grammar directly in Python.&#xa;&#xa;Here is a program to parse ""Hello, World!"" (or any greeting of the form &#xa;C{""<salutation>, <addressee>!""}), built up using L{Word}, L{Literal}, and L{And} elements &#xa;(L{'+'<ParserElement.__add__>} operator gives L{And} expressions, strings are auto-converted to&#xa;L{Literal} expressions)::&#xa;&#xa;    from pyparsing import Word, alphas&#xa;&#xa;    # define grammar of a greeting&#xa;    greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;&#xa;    hello = ""Hello, World!""&#xa;    print (hello, ""->"", greet.parseString(hello))&#xa;&#xa;The program outputs the following::&#xa;&#xa;    Hello, World! -> ['Hello', ',', 'World', '!']&#xa;&#xa;The Python representation of the grammar is quite readable, owing to the self-explanatory&#xa;class names, and the use of '+', '|' and '^' operators.&#xa;&#xa;The L{ParseResults} object returned from L{ParserElement.parseString<ParserElement.parseString>} can be accessed as a nested list, a dictionary, or an&#xa;object with named attributes.&#xa;&#xa;The pyparsing module handles some of the problems that are typically vexing when writing text parsers:&#xa; - extra or missing whitespace (the above program will also handle ""Hello,World!"", ""Hello  ,  World  !"", etc.)&#xa; - quoted strings&#xa; - embedded comments&#xa;""""""&#xa;&#xa;__version__ = ""2.1.10""&#xa;__versionTime__ = ""07 Oct 2016 01:31 UTC""&#xa;__author__ = ""Paul McGuire <ptmcg@users.sourceforge.net>""&#xa;&#xa;import string&#xa;from weakref import ref as wkref&#xa;import copy&#xa;import sys&#xa;import warnings&#xa;import re&#xa;import sre_constants&#xa;import collections&#xa;import pprint&#xa;import traceback&#xa;import types&#xa;from datetime import datetime&#xa;&#xa;try:&#xa;    from _thread import RLock&#xa;except ImportError:&#xa;    from threading import RLock&#xa;&#xa;try:&#xa;    from collections import OrderedDict as _OrderedDict&#xa;except ImportError:&#xa;    try:&#xa;        from ordereddict import OrderedDict as _OrderedDict&#xa;    except ImportError:&#xa;        _OrderedDict = None&#xa;&#xa;#~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )&#xa;&#xa;__all__ = [&#xa;'And', 'CaselessKeyword', 'CaselessLiteral', 'CharsNotIn', 'Combine', 'Dict', 'Each', 'Empty',&#xa;'FollowedBy', 'Forward', 'GoToColumn', 'Group', 'Keyword', 'LineEnd', 'LineStart', 'Literal',&#xa;'MatchFirst', 'NoMatch', 'NotAny', 'OneOrMore', 'OnlyOnce', 'Optional', 'Or',&#xa;'ParseBaseException', 'ParseElementEnhance', 'ParseException', 'ParseExpression', 'ParseFatalException',&#xa;'ParseResults', 'ParseSyntaxException', 'ParserElement', 'QuotedString', 'RecursiveGrammarException',&#xa;'Regex', 'SkipTo', 'StringEnd', 'StringStart', 'Suppress', 'Token', 'TokenConverter', &#xa;'White', 'Word', 'WordEnd', 'WordStart', 'ZeroOrMore',&#xa;'alphanums', 'alphas', 'alphas8bit', 'anyCloseTag', 'anyOpenTag', 'cStyleComment', 'col',&#xa;'commaSeparatedList', 'commonHTMLEntity', 'countedArray', 'cppStyleComment', 'dblQuotedString',&#xa;'dblSlashComment', 'delimitedList', 'dictOf', 'downcaseTokens', 'empty', 'hexnums',&#xa;'htmlComment', 'javaStyleComment', 'line', 'lineEnd', 'lineStart', 'lineno',&#xa;'makeHTMLTags', 'makeXMLTags', 'matchOnlyAtCol', 'matchPreviousExpr', 'matchPreviousLiteral',&#xa;'nestedExpr', 'nullDebugAction', 'nums', 'oneOf', 'opAssoc', 'operatorPrecedence', 'printables',&#xa;'punc8bit', 'pythonStyleComment', 'quotedString', 'removeQuotes', 'replaceHTMLEntity', &#xa;'replaceWith', 'restOfLine', 'sglQuotedString', 'srange', 'stringEnd',&#xa;'stringStart', 'traceParseAction', 'unicodeString', 'upcaseTokens', 'withAttribute',&#xa;'indentedBlock', 'originalTextFor', 'ungroup', 'infixNotation','locatedExpr', 'withClass',&#xa;'CloseMatch', 'tokenMap', 'pyparsing_common',&#xa;]&#xa;&#xa;system_version = tuple(sys.version_info)[:3]&#xa;PY_3 = system_version[0] == 3&#xa;if PY_3:&#xa;    _MAX_INT = sys.maxsize&#xa;    basestring = str&#xa;    unichr = chr&#xa;    _ustr = str&#xa;&#xa;    # build list of single arg builtins, that can be used as parse actions&#xa;    singleArgBuiltins = [sum, len, sorted, reversed, list, tuple, set, any, all, min, max]&#xa;&#xa;else:&#xa;    _MAX_INT = sys.maxint&#xa;    range = xrange&#xa;&#xa;    def _ustr(obj):&#xa;        """"""Drop-in replacement for str(obj) that tries to be Unicode friendly. It first tries&#xa;           str(obj). If that fails with a UnicodeEncodeError, then it tries unicode(obj). It&#xa;           then < returns the unicode object | encodes it with the default encoding | ... >.&#xa;        """"""&#xa;        if isinstance(obj,unicode):&#xa;            return obj&#xa;&#xa;        try:&#xa;            # If this works, then _ustr(obj) has the same behaviour as str(obj), so&#xa;            # it won't break any existing code.&#xa;            return str(obj)&#xa;&#xa;        except UnicodeEncodeError:&#xa;            # Else encode it&#xa;            ret = unicode(obj).encode(sys.getdefaultencoding(), 'xmlcharrefreplace')&#xa;            xmlcharref = Regex('&#\d+;')&#xa;            xmlcharref.setParseAction(lambda t: '\\u' + hex(int(t[0][2:-1]))[2:])&#xa;            return xmlcharref.transformString(ret)&#xa;&#xa;    # build list of single arg builtins, tolerant of Python version, that can be used as parse actions&#xa;    singleArgBuiltins = []&#xa;    import __builtin__&#xa;    for fname in ""sum len sorted reversed list tuple set any all min max"".split():&#xa;        try:&#xa;            singleArgBuiltins.append(getattr(__builtin__,fname))&#xa;        except AttributeError:&#xa;            continue&#xa;            &#xa;_generatorType = type((y for y in range(1)))&#xa; &#xa;def _xml_escape(data):&#xa;    """"""Escape &, <, >, "", ', etc. in a string of data.""""""&#xa;&#xa;    # ampersand must be replaced first&#xa;    from_symbols = '&><""\''&#xa;    to_symbols = ('&'+s+';' for s in ""amp gt lt quot apos"".split())&#xa;    for from_,to_ in zip(from_symbols, to_symbols):&#xa;        data = data.replace(from_, to_)&#xa;    return data&#xa;&#xa;class _Constants(object):&#xa;    pass&#xa;&#xa;alphas     = string.ascii_uppercase + string.ascii_lowercase&#xa;nums       = ""0123456789""&#xa;hexnums    = nums + ""ABCDEFabcdef""&#xa;alphanums  = alphas + nums&#xa;_bslash    = chr(92)&#xa;printables = """".join(c for c in string.printable if c not in string.whitespace)&#xa;&#xa;class ParseBaseException(Exception):&#xa;    """"""base exception class for all parsing runtime exceptions""""""&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, pstr, loc=0, msg=None, elem=None ):&#xa;        self.loc = loc&#xa;        if msg is None:&#xa;            self.msg = pstr&#xa;            self.pstr = """"&#xa;        else:&#xa;            self.msg = msg&#xa;            self.pstr = pstr&#xa;        self.parserElement = elem&#xa;        self.args = (pstr, loc, msg)&#xa;&#xa;    @classmethod&#xa;    def _from_exception(cls, pe):&#xa;        """"""&#xa;        internal factory method to simplify creating one type of ParseException &#xa;        from another - avoids having __init__ signature conflicts among subclasses&#xa;        """"""&#xa;        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)&#xa;&#xa;    def __getattr__( self, aname ):&#xa;        """"""supported attributes by name are:&#xa;            - lineno - returns the line number of the exception text&#xa;            - col - returns the column number of the exception text&#xa;            - line - returns the line containing the exception text&#xa;        """"""&#xa;        if( aname == ""lineno"" ):&#xa;            return lineno( self.loc, self.pstr )&#xa;        elif( aname in (""col"", ""column"") ):&#xa;            return col( self.loc, self.pstr )&#xa;        elif( aname == ""line"" ):&#xa;            return line( self.loc, self.pstr )&#xa;        else:&#xa;            raise AttributeError(aname)&#xa;&#xa;    def __str__( self ):&#xa;        return ""%s (at char %d), (line:%d, col:%d)"" % \&#xa;                ( self.msg, self.loc, self.lineno, self.column )&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;    def markInputline( self, markerString = "">!<"" ):&#xa;        """"""Extracts the exception line from the input string, and marks&#xa;           the location of the exception with a special symbol.&#xa;        """"""&#xa;        line_str = self.line&#xa;        line_column = self.column - 1&#xa;        if markerString:&#xa;            line_str = """".join((line_str[:line_column],&#xa;                                markerString, line_str[line_column:]))&#xa;        return line_str.strip()&#xa;    def __dir__(self):&#xa;        return ""lineno col line"".split() + dir(type(self))&#xa;&#xa;class ParseException(ParseBaseException):&#xa;    """"""&#xa;    Exception thrown when parse expressions don't match class;&#xa;    supported attributes by name are:&#xa;     - lineno - returns the line number of the exception text&#xa;     - col - returns the column number of the exception text&#xa;     - line - returns the line containing the exception text&#xa;        &#xa;    Example::&#xa;        try:&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")&#xa;        except ParseException as pe:&#xa;            print(pe)&#xa;            print(""column: {}"".format(pe.col))&#xa;            &#xa;    prints::&#xa;       Expected integer (at char 0), (line:1, col:1)&#xa;        column: 1&#xa;    """"""&#xa;    pass&#xa;&#xa;class ParseFatalException(ParseBaseException):&#xa;    """"""user-throwable exception thrown when inconsistent parse content&#xa;       is found; stops all parsing immediately""""""&#xa;    pass&#xa;&#xa;class ParseSyntaxException(ParseFatalException):&#xa;    """"""just like L{ParseFatalException}, but thrown internally when an&#xa;       L{ErrorStop<And._ErrorStop>} ('-' operator) indicates that parsing is to stop &#xa;       immediately because an unbacktrackable syntax error has been found""""""&#xa;    pass&#xa;&#xa;#~ class ReparseException(ParseBaseException):&#xa;    #~ """"""Experimental class - parse actions can raise this exception to cause&#xa;       #~ pyparsing to reparse the input string:&#xa;        #~ - with a modified input string, and/or&#xa;        #~ - with a modified start location&#xa;       #~ Set the values of the ReparseException in the constructor, and raise the&#xa;       #~ exception in a parse action to cause pyparsing to use the new string/location.&#xa;       #~ Setting the values as None causes no change to be made.&#xa;       #~ """"""&#xa;    #~ def __init_( self, newstring, restartLoc ):&#xa;        #~ self.newParseText = newstring&#xa;        #~ self.reparseLoc = restartLoc&#xa;&#xa;class RecursiveGrammarException(Exception):&#xa;    """"""exception thrown by L{ParserElement.validate} if the grammar could be improperly recursive""""""&#xa;    def __init__( self, parseElementList ):&#xa;        self.parseElementTrace = parseElementList&#xa;&#xa;    def __str__( self ):&#xa;        return ""RecursiveGrammarException: %s"" % self.parseElementTrace&#xa;&#xa;class _ParseResultsWithOffset(object):&#xa;    def __init__(self,p1,p2):&#xa;        self.tup = (p1,p2)&#xa;    def __getitem__(self,i):&#xa;        return self.tup[i]&#xa;    def __repr__(self):&#xa;        return repr(self.tup[0])&#xa;    def setOffset(self,i):&#xa;        self.tup = (self.tup[0],i)&#xa;&#xa;class ParseResults(object):&#xa;    """"""&#xa;    Structured parse results, to provide multiple means of access to the parsed data:&#xa;       - as a list (C{len(results)})&#xa;       - by list index (C{results[0], results[1]}, etc.)&#xa;       - by attribute (C{results.<resultsName>} - see L{ParserElement.setResultsName})&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;        # equivalent form:&#xa;        # date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;&#xa;        # parseString returns a ParseResults object&#xa;        result = date_str.parseString(""1999/12/31"")&#xa;&#xa;        def test(s, fn=repr):&#xa;            print(""%s -> %s"" % (s, fn(eval(s))))&#xa;        test(""list(result)"")&#xa;        test(""result[0]"")&#xa;        test(""result['month']"")&#xa;        test(""result.day"")&#xa;        test(""'month' in result"")&#xa;        test(""'minutes' in result"")&#xa;        test(""result.dump()"", str)&#xa;    prints::&#xa;        list(result) -> ['1999', '/', '12', '/', '31']&#xa;        result[0] -> '1999'&#xa;        result['month'] -> '12'&#xa;        result.day -> '31'&#xa;        'month' in result -> True&#xa;        'minutes' in result -> False&#xa;        result.dump() -> ['1999', '/', '12', '/', '31']&#xa;        - day: 31&#xa;        - month: 12&#xa;        - year: 1999&#xa;    """"""&#xa;    def __new__(cls, toklist=None, name=None, asList=True, modal=True ):&#xa;        if isinstance(toklist, cls):&#xa;            return toklist&#xa;        retobj = object.__new__(cls)&#xa;        retobj.__doinit = True&#xa;        return retobj&#xa;&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance ):&#xa;        if self.__doinit:&#xa;            self.__doinit = False&#xa;            self.__name = None&#xa;            self.__parent = None&#xa;            self.__accumNames = {}&#xa;            self.__asList = asList&#xa;            self.__modal = modal&#xa;            if toklist is None:&#xa;                toklist = []&#xa;            if isinstance(toklist, list):&#xa;                self.__toklist = toklist[:]&#xa;            elif isinstance(toklist, _generatorType):&#xa;                self.__toklist = list(toklist)&#xa;            else:&#xa;                self.__toklist = [toklist]&#xa;            self.__tokdict = dict()&#xa;&#xa;        if name is not None and name:&#xa;            if not modal:&#xa;                self.__accumNames[name] = 0&#xa;            if isinstance(name,int):&#xa;                name = _ustr(name) # will always return a str, but use _ustr for consistency&#xa;            self.__name = name&#xa;            if not (isinstance(toklist, (type(None), basestring, list)) and toklist in (None,'',[])):&#xa;                if isinstance(toklist,basestring):&#xa;                    toklist = [ toklist ]&#xa;                if asList:&#xa;                    if isinstance(toklist,ParseResults):&#xa;                        self[name] = _ParseResultsWithOffset(toklist.copy(),0)&#xa;                    else:&#xa;                        self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]),0)&#xa;                    self[name].__name = name&#xa;                else:&#xa;                    try:&#xa;                        self[name] = toklist[0]&#xa;                    except (KeyError,TypeError,IndexError):&#xa;                        self[name] = toklist&#xa;&#xa;    def __getitem__( self, i ):&#xa;        if isinstance( i, (int,slice) ):&#xa;            return self.__toklist[i]&#xa;        else:&#xa;            if i not in self.__accumNames:&#xa;                return self.__tokdict[i][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[i] ])&#xa;&#xa;    def __setitem__( self, k, v, isinstance=isinstance ):&#xa;        if isinstance(v,_ParseResultsWithOffset):&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]&#xa;            sub = v[0]&#xa;        elif isinstance(k,(int,slice)):&#xa;            self.__toklist[k] = v&#xa;            sub = v&#xa;        else:&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [_ParseResultsWithOffset(v,0)]&#xa;            sub = v&#xa;        if isinstance(sub,ParseResults):&#xa;            sub.__parent = wkref(self)&#xa;&#xa;    def __delitem__( self, i ):&#xa;        if isinstance(i,(int,slice)):&#xa;            mylen = len( self.__toklist )&#xa;            del self.__toklist[i]&#xa;&#xa;            # convert int to slice&#xa;            if isinstance(i, int):&#xa;                if i < 0:&#xa;                    i += mylen&#xa;                i = slice(i, i+1)&#xa;            # get removed indices&#xa;            removed = list(range(*i.indices(mylen)))&#xa;            removed.reverse()&#xa;            # fixup indices in token dictionary&#xa;            for name,occurrences in self.__tokdict.items():&#xa;                for j in removed:&#xa;                    for k, (value, position) in enumerate(occurrences):&#xa;                        occurrences[k] = _ParseResultsWithOffset(value, position - (position > j))&#xa;        else:&#xa;            del self.__tokdict[i]&#xa;&#xa;    def __contains__( self, k ):&#xa;        return k in self.__tokdict&#xa;&#xa;    def __len__( self ): return len( self.__toklist )&#xa;    def __bool__(self): return ( not not self.__toklist )&#xa;    __nonzero__ = __bool__&#xa;    def __iter__( self ): return iter( self.__toklist )&#xa;    def __reversed__( self ): return iter( self.__toklist[::-1] )&#xa;    def _iterkeys( self ):&#xa;        if hasattr(self.__tokdict, ""iterkeys""):&#xa;            return self.__tokdict.iterkeys()&#xa;        else:&#xa;            return iter(self.__tokdict)&#xa;&#xa;    def _itervalues( self ):&#xa;        return (self[k] for k in self._iterkeys())&#xa;            &#xa;    def _iteritems( self ):&#xa;        return ((k, self[k]) for k in self._iterkeys())&#xa;&#xa;    if PY_3:&#xa;        keys = _iterkeys       &#xa;        """"""Returns an iterator of all named result keys (Python 3.x only).""""""&#xa;&#xa;        values = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 3.x only).""""""&#xa;&#xa;        items = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 3.x only).""""""&#xa;&#xa;    else:&#xa;        iterkeys = _iterkeys&#xa;        """"""Returns an iterator of all named result keys (Python 2.x only).""""""&#xa;&#xa;        itervalues = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 2.x only).""""""&#xa;&#xa;        iteritems = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 2.x only).""""""&#xa;&#xa;        def keys( self ):&#xa;            """"""Returns all named result keys (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iterkeys())&#xa;&#xa;        def values( self ):&#xa;            """"""Returns all named result values (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.itervalues())&#xa;                &#xa;        def items( self ):&#xa;            """"""Returns all named result key-values (as a list of tuples in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iteritems())&#xa;&#xa;    def haskeys( self ):&#xa;        """"""Since keys() returns an iterator, this method is helpful in bypassing&#xa;           code that looks for the existence of any defined results names.""""""&#xa;        return bool(self.__tokdict)&#xa;        &#xa;    def pop( self, *args, **kwargs):&#xa;        """"""&#xa;        Removes and returns item at specified index (default=C{last}).&#xa;        Supports both C{list} and C{dict} semantics for C{pop()}. If passed no&#xa;        argument or an integer argument, it will use C{list} semantics&#xa;        and pop tokens from the list of parsed tokens. If passed a &#xa;        non-integer argument (most likely a string), it will use C{dict}&#xa;        semantics and pop the corresponding value from any defined &#xa;        results names. A second default return value argument is &#xa;        supported, just as in C{dict.pop()}.&#xa;&#xa;        Example::&#xa;            def remove_first(tokens):&#xa;                tokens.pop(0)&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(""0 123 321"")) # -> ['123', '321']&#xa;&#xa;            label = Word(alphas)&#xa;            patt = label(""LABEL"") + OneOrMore(Word(nums))&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;&#xa;            # Use pop() in a parse action to remove named result (note that corresponding value is not&#xa;            # removed from list form of results)&#xa;            def remove_LABEL(tokens):&#xa;                tokens.pop(""LABEL"")&#xa;                return tokens&#xa;            patt.addParseAction(remove_LABEL)&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;        prints::&#xa;            ['AAB', '123', '321']&#xa;            - LABEL: AAB&#xa;&#xa;            ['AAB', '123', '321']&#xa;        """"""&#xa;        if not args:&#xa;            args = [-1]&#xa;        for k,v in kwargs.items():&#xa;            if k == 'default':&#xa;                args = (args[0], v)&#xa;            else:&#xa;                raise TypeError(""pop() got an unexpected keyword argument '%s'"" % k)&#xa;        if (isinstance(args[0], int) or &#xa;                        len(args) == 1 or &#xa;                        args[0] in self):&#xa;            index = args[0]&#xa;            ret = self[index]&#xa;            del self[index]&#xa;            return ret&#xa;        else:&#xa;            defaultvalue = args[1]&#xa;            return defaultvalue&#xa;&#xa;    def get(self, key, defaultValue=None):&#xa;        """"""&#xa;        Returns named result matching the given key, or if there is no&#xa;        such name, then returns the given C{defaultValue} or C{None} if no&#xa;        C{defaultValue} is specified.&#xa;&#xa;        Similar to C{dict.get()}.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            result = date_str.parseString(""1999/12/31"")&#xa;            print(result.get(""year"")) # -> '1999'&#xa;            print(result.get(""hour"", ""not specified"")) # -> 'not specified'&#xa;            print(result.get(""hour"")) # -> None&#xa;        """"""&#xa;        if key in self:&#xa;            return self[key]&#xa;        else:&#xa;            return defaultValue&#xa;&#xa;    def insert( self, index, insStr ):&#xa;        """"""&#xa;        Inserts new element at location index in the list of parsed tokens.&#xa;        &#xa;        Similar to C{list.insert()}.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;&#xa;            # use a parse action to insert the parse location in the front of the parsed results&#xa;            def insert_locn(locn, tokens):&#xa;                tokens.insert(0, locn)&#xa;            print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString(""0 123 321"")) # -> [0, '0', '123', '321']&#xa;        """"""&#xa;        self.__toklist.insert(index, insStr)&#xa;        # fixup indices in token dictionary&#xa;        for name,occurrences in self.__tokdict.items():&#xa;            for k, (value, position) in enumerate(occurrences):&#xa;                occurrences[k] = _ParseResultsWithOffset(value, position + (position > index))&#xa;&#xa;    def append( self, item ):&#xa;        """"""&#xa;        Add single element to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            &#xa;            # use a parse action to compute the sum of the parsed integers, and add it to the end&#xa;            def append_sum(tokens):&#xa;                tokens.append(sum(map(int, tokens)))&#xa;            print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString(""0 123 321"")) # -> ['0', '123', '321', 444]&#xa;        """"""&#xa;        self.__toklist.append(item)&#xa;&#xa;    def extend( self, itemseq ):&#xa;        """"""&#xa;        Add sequence of elements to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            &#xa;            # use a parse action to append the reverse of the matched strings, to make a palindrome&#xa;            def make_palindrome(tokens):&#xa;                tokens.extend(reversed([t[::-1] for t in tokens]))&#xa;                return ''.join(tokens)&#xa;            print(patt.addParseAction(make_palindrome).parseString(""lskdj sdlkjf lksd"")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'&#xa;        """"""&#xa;        if isinstance(itemseq, ParseResults):&#xa;            self += itemseq&#xa;        else:&#xa;            self.__toklist.extend(itemseq)&#xa;&#xa;    def clear( self ):&#xa;        """"""&#xa;        Clear all elements and results names.&#xa;        """"""&#xa;        del self.__toklist[:]&#xa;        self.__tokdict.clear()&#xa;&#xa;    def __getattr__( self, name ):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            return """"&#xa;            &#xa;        if name in self.__tokdict:&#xa;            if name not in self.__accumNames:&#xa;                return self.__tokdict[name][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[name] ])&#xa;        else:&#xa;            return """"&#xa;&#xa;    def __add__( self, other ):&#xa;        ret = self.copy()&#xa;        ret += other&#xa;        return ret&#xa;&#xa;    def __iadd__( self, other ):&#xa;        if other.__tokdict:&#xa;            offset = len(self.__toklist)&#xa;            addoffset = lambda a: offset if a<0 else a+offset&#xa;            otheritems = other.__tokdict.items()&#xa;            otherdictitems = [(k, _ParseResultsWithOffset(v[0],addoffset(v[1])) )&#xa;                                for (k,vlist) in otheritems for v in vlist]&#xa;            for k,v in otherdictitems:&#xa;                self[k] = v&#xa;                if isinstance(v[0],ParseResults):&#xa;                    v[0].__parent = wkref(self)&#xa;            &#xa;        self.__toklist += other.__toklist&#xa;        self.__accumNames.update( other.__accumNames )&#xa;        return self&#xa;&#xa;    def __radd__(self, other):&#xa;        if isinstance(other,int) and other == 0:&#xa;            # useful for merging many ParseResults using sum() builtin&#xa;            return self.copy()&#xa;        else:&#xa;            # this may raise a TypeError - so be it&#xa;            return other + self&#xa;        &#xa;    def __repr__( self ):&#xa;        return ""(%s, %s)"" % ( repr( self.__toklist ), repr( self.__tokdict ) )&#xa;&#xa;    def __str__( self ):&#xa;        return '[' + ', '.join(_ustr(i) if isinstance(i, ParseResults) else repr(i) for i in self.__toklist) + ']'&#xa;&#xa;    def _asStringList( self, sep='' ):&#xa;        out = []&#xa;        for item in self.__toklist:&#xa;            if out and sep:&#xa;                out.append(sep)&#xa;            if isinstance( item, ParseResults ):&#xa;                out += item._asStringList()&#xa;            else:&#xa;                out.append( _ustr(item) )&#xa;        return out&#xa;&#xa;    def asList( self ):&#xa;        """"""&#xa;        Returns the parse results as a nested list of matching tokens, all converted to strings.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            result = patt.parseString(""sldkj lsdkj sldkj"")&#xa;            # even though the result prints in string-like form, it is actually a pyparsing ParseResults&#xa;            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']&#xa;            &#xa;            # Use asList() to create an actual list&#xa;            result_list = result.asList()&#xa;            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']&#xa;        """"""&#xa;        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]&#xa;&#xa;    def asDict( self ):&#xa;        """"""&#xa;        Returns the named parse results as a nested dictionary.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})&#xa;            &#xa;            result_dict = result.asDict()&#xa;            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}&#xa;&#xa;            # even though a ParseResults supports dict-like access, sometime you just need to have a dict&#xa;            import json&#xa;            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable&#xa;            print(json.dumps(result.asDict())) # -> {""month"": ""31"", ""day"": ""1999"", ""year"": ""12""}&#xa;        """"""&#xa;        if PY_3:&#xa;            item_fn = self.items&#xa;        else:&#xa;            item_fn = self.iteritems&#xa;            &#xa;        def toItem(obj):&#xa;            if isinstance(obj, ParseResults):&#xa;                if obj.haskeys():&#xa;                    return obj.asDict()&#xa;                else:&#xa;                    return [toItem(v) for v in obj]&#xa;            else:&#xa;                return obj&#xa;                &#xa;        return dict((k,toItem(v)) for k,v in item_fn())&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Returns a new copy of a C{ParseResults} object.&#xa;        """"""&#xa;        ret = ParseResults( self.__toklist )&#xa;        ret.__tokdict = self.__tokdict.copy()&#xa;        ret.__parent = self.__parent&#xa;        ret.__accumNames.update( self.__accumNames )&#xa;        ret.__name = self.__name&#xa;        return ret&#xa;&#xa;    def asXML( self, doctag=None, namedItemsOnly=False, indent="""", formatted=True ):&#xa;        """"""&#xa;        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.&#xa;        """"""&#xa;        nl = ""\n""&#xa;        out = []&#xa;        namedItems = dict((v[1],k) for (k,vlist) in self.__tokdict.items()&#xa;                                                            for v in vlist)&#xa;        nextLevelIndent = indent + ""  ""&#xa;&#xa;        # collapse out indents if formatting is not desired&#xa;        if not formatted:&#xa;            indent = """"&#xa;            nextLevelIndent = """"&#xa;            nl = """"&#xa;&#xa;        selfTag = None&#xa;        if doctag is not None:&#xa;            selfTag = doctag&#xa;        else:&#xa;            if self.__name:&#xa;                selfTag = self.__name&#xa;&#xa;        if not selfTag:&#xa;            if namedItemsOnly:&#xa;                return """"&#xa;            else:&#xa;                selfTag = ""ITEM""&#xa;&#xa;        out += [ nl, indent, ""<"", selfTag, "">"" ]&#xa;&#xa;        for i,res in enumerate(self.__toklist):&#xa;            if isinstance(res,ParseResults):&#xa;                if i in namedItems:&#xa;                    out += [ res.asXML(namedItems[i],&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;                else:&#xa;                    out += [ res.asXML(None,&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;            else:&#xa;                # individual token, see if there is a name for it&#xa;                resTag = None&#xa;                if i in namedItems:&#xa;                    resTag = namedItems[i]&#xa;                if not resTag:&#xa;                    if namedItemsOnly:&#xa;                        continue&#xa;                    else:&#xa;                        resTag = ""ITEM""&#xa;                xmlBodyText = _xml_escape(_ustr(res))&#xa;                out += [ nl, nextLevelIndent, ""<"", resTag, "">"",&#xa;                                                xmlBodyText,&#xa;                                                ""</"", resTag, "">"" ]&#xa;&#xa;        out += [ nl, indent, ""</"", selfTag, "">"" ]&#xa;        return """".join(out)&#xa;&#xa;    def __lookup(self,sub):&#xa;        for k,vlist in self.__tokdict.items():&#xa;            for v,loc in vlist:&#xa;                if sub is v:&#xa;                    return k&#xa;        return None&#xa;&#xa;    def getName(self):&#xa;        """"""&#xa;        Returns the results name for this token expression. Useful when several &#xa;        different expressions might match at a particular location.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            ssn_expr = Regex(r""\d\d\d-\d\d-\d\d\d\d"")&#xa;            house_number_expr = Suppress('#') + Word(nums, alphanums)&#xa;            user_data = (Group(house_number_expr)(""house_number"") &#xa;                        | Group(ssn_expr)(""ssn"")&#xa;                        | Group(integer)(""age""))&#xa;            user_info = OneOrMore(user_data)&#xa;            &#xa;            result = user_info.parseString(""22 111-22-3333 #221B"")&#xa;            for item in result:&#xa;                print(item.getName(), ':', item[0])&#xa;        prints::&#xa;            age : 22&#xa;            ssn : 111-22-3333&#xa;            house_number : 221B&#xa;        """"""&#xa;        if self.__name:&#xa;            return self.__name&#xa;        elif self.__parent:&#xa;            par = self.__parent()&#xa;            if par:&#xa;                return par.__lookup(self)&#xa;            else:&#xa;                return None&#xa;        elif (len(self) == 1 and&#xa;               len(self.__tokdict) == 1 and&#xa;               next(iter(self.__tokdict.values()))[0][1] in (0,-1)):&#xa;            return next(iter(self.__tokdict.keys()))&#xa;        else:&#xa;            return None&#xa;&#xa;    def dump(self, indent='', depth=0, full=True):&#xa;        """"""&#xa;        Diagnostic method for listing out the contents of a C{ParseResults}.&#xa;        Accepts an optional C{indent} argument so that this string can be embedded&#xa;        in a nested display of other data.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(result.dump())&#xa;        prints::&#xa;            ['12', '/', '31', '/', '1999']&#xa;            - day: 1999&#xa;            - month: 31&#xa;            - year: 12&#xa;        """"""&#xa;        out = []&#xa;        NL = '\n'&#xa;        out.append( indent+_ustr(self.asList()) )&#xa;        if full:&#xa;            if self.haskeys():&#xa;                items = sorted((str(k), v) for k,v in self.items())&#xa;                for k,v in items:&#xa;                    if out:&#xa;                        out.append(NL)&#xa;                    out.append( ""%s%s- %s: "" % (indent,('  '*depth), k) )&#xa;                    if isinstance(v,ParseResults):&#xa;                        if v:&#xa;                            out.append( v.dump(indent,depth+1) )&#xa;                        else:&#xa;                            out.append(_ustr(v))&#xa;                    else:&#xa;                        out.append(repr(v))&#xa;            elif any(isinstance(vv,ParseResults) for vv in self):&#xa;                v = self&#xa;                for i,vv in enumerate(v):&#xa;                    if isinstance(vv,ParseResults):&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),vv.dump(indent,depth+1) ))&#xa;                    else:&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),_ustr(vv)))&#xa;            &#xa;        return """".join(out)&#xa;&#xa;    def pprint(self, *args, **kwargs):&#xa;        """"""&#xa;        Pretty-printer for parsed results as a list, using the C{pprint} module.&#xa;        Accepts additional positional or keyword args as defined for the &#xa;        C{pprint.pprint} method. (U{http://docs.python.org/3/library/pprint.html#pprint.pprint})&#xa;&#xa;        Example::&#xa;            ident = Word(alphas, alphanums)&#xa;            num = Word(nums)&#xa;            func = Forward()&#xa;            term = ident | num | Group('(' + func + ')')&#xa;            func <<= ident + Group(Optional(delimitedList(term)))&#xa;            result = func.parseString(""fna a,b,(fnb c,d,200),100"")&#xa;            result.pprint(width=40)&#xa;        prints::&#xa;            ['fna',&#xa;             ['a',&#xa;              'b',&#xa;              ['(', 'fnb', ['c', 'd', '200'], ')'],&#xa;              '100']]&#xa;        """"""&#xa;        pprint.pprint(self.asList(), *args, **kwargs)&#xa;&#xa;    # add support for pickle protocol&#xa;    def __getstate__(self):&#xa;        return ( self.__toklist,&#xa;                 ( self.__tokdict.copy(),&#xa;                   self.__parent is not None and self.__parent() or None,&#xa;                   self.__accumNames,&#xa;                   self.__name ) )&#xa;&#xa;    def __setstate__(self,state):&#xa;        self.__toklist = state[0]&#xa;        (self.__tokdict,&#xa;         par,&#xa;         inAccumNames,&#xa;         self.__name) = state[1]&#xa;        self.__accumNames = {}&#xa;        self.__accumNames.update(inAccumNames)&#xa;        if par is not None:&#xa;            self.__parent = wkref(par)&#xa;        else:&#xa;            self.__parent = None&#xa;&#xa;    def __getnewargs__(self):&#xa;        return self.__toklist, self.__name, self.__asList, self.__modal&#xa;&#xa;    def __dir__(self):&#xa;        return (dir(type(self)) + list(self.keys()))&#xa;&#xa;collections.MutableMapping.register(ParseResults)&#xa;&#xa;def col (loc,strg):&#xa;    """"""Returns current column within a string, counting newlines as line separators.&#xa;   The first column is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    s = strg&#xa;    return 1 if 0<loc<len(s) and s[loc-1] == '\n' else loc - s.rfind(""\n"", 0, loc)&#xa;&#xa;def lineno(loc,strg):&#xa;    """"""Returns current line number within a string, counting newlines as line separators.&#xa;   The first line is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    return strg.count(""\n"",0,loc) + 1&#xa;&#xa;def line( loc, strg ):&#xa;    """"""Returns the line of text containing loc within a string, counting newlines as line separators.&#xa;       """"""&#xa;    lastCR = strg.rfind(""\n"", 0, loc)&#xa;    nextCR = strg.find(""\n"", loc)&#xa;    if nextCR >= 0:&#xa;        return strg[lastCR+1:nextCR]&#xa;    else:&#xa;        return strg[lastCR+1:]&#xa;&#xa;def _defaultStartDebugAction( instring, loc, expr ):&#xa;    print ((""Match "" + _ustr(expr) + "" at loc "" + _ustr(loc) + ""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) )))&#xa;&#xa;def _defaultSuccessDebugAction( instring, startloc, endloc, expr, toks ):&#xa;    print (""Matched "" + _ustr(expr) + "" -> "" + str(toks.asList()))&#xa;&#xa;def _defaultExceptionDebugAction( instring, loc, expr, exc ):&#xa;    print (""Exception raised:"" + _ustr(exc))&#xa;&#xa;def nullDebugAction(*args):&#xa;    """"""'Do-nothing' debug action, to suppress debugging output during parsing.""""""&#xa;    pass&#xa;&#xa;# Only works on Python 3.x - nonlocal is toxic to Python 2 installs&#xa;#~ 'decorator to trim function calls to match the arity of the target'&#xa;#~ def _trim_arity(func, maxargs=3):&#xa;    #~ if func in singleArgBuiltins:&#xa;        #~ return lambda s,l,t: func(t)&#xa;    #~ limit = 0&#xa;    #~ foundArity = False&#xa;    #~ def wrapper(*args):&#xa;        #~ nonlocal limit,foundArity&#xa;        #~ while 1:&#xa;            #~ try:&#xa;                #~ ret = func(*args[limit:])&#xa;                #~ foundArity = True&#xa;                #~ return ret&#xa;            #~ except TypeError:&#xa;                #~ if limit == maxargs or foundArity:&#xa;                    #~ raise&#xa;                #~ limit += 1&#xa;                #~ continue&#xa;    #~ return wrapper&#xa;&#xa;# this version is Python 2.x-3.x cross-compatible&#xa;'decorator to trim function calls to match the arity of the target'&#xa;def _trim_arity(func, maxargs=2):&#xa;    if func in singleArgBuiltins:&#xa;        return lambda s,l,t: func(t)&#xa;    limit = [0]&#xa;    foundArity = [False]&#xa;    &#xa;    # traceback return data structure changed in Py3.5 - normalize back to plain tuples&#xa;    if system_version[:2] >= (3,5):&#xa;        def extract_stack(limit=0):&#xa;            # special handling for Python 3.5.0 - extra deep call stack by 1&#xa;            offset = -3 if system_version == (3,5,0) else -2&#xa;            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;        def extract_tb(tb, limit=0):&#xa;            frames = traceback.extract_tb(tb, limit=limit)&#xa;            frame_summary = frames[-1]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;    else:&#xa;        extract_stack = traceback.extract_stack&#xa;        extract_tb = traceback.extract_tb&#xa;    &#xa;    # synthesize what would be returned by traceback.extract_stack at the call to &#xa;    # user's parse action 'func', so that we don't incur call penalty at parse time&#xa;    &#xa;    LINE_DIFF = 6&#xa;    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND &#xa;    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!&#xa;    this_line = extract_stack(limit=2)[-1]&#xa;    pa_call_line_synth = (this_line[0], this_line[1]+LINE_DIFF)&#xa;&#xa;    def wrapper(*args):&#xa;        while 1:&#xa;            try:&#xa;                ret = func(*args[limit[0]:])&#xa;                foundArity[0] = True&#xa;                return ret&#xa;            except TypeError:&#xa;                # re-raise TypeErrors if they did not come from our arity testing&#xa;                if foundArity[0]:&#xa;                    raise&#xa;                else:&#xa;                    try:&#xa;                        tb = sys.exc_info()[-1]&#xa;                        if not extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth:&#xa;                            raise&#xa;                    finally:&#xa;                        del tb&#xa;&#xa;                if limit[0] <= maxargs:&#xa;                    limit[0] += 1&#xa;                    continue&#xa;                raise&#xa;&#xa;    # copy func name to wrapper for sensible debug output&#xa;    func_name = ""<parse action>""&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    wrapper.__name__ = func_name&#xa;&#xa;    return wrapper&#xa;&#xa;class ParserElement(object):&#xa;    """"""Abstract base level parser element class.""""""&#xa;    DEFAULT_WHITE_CHARS = "" \n\t\r""&#xa;    verbose_stacktrace = False&#xa;&#xa;    @staticmethod&#xa;    def setDefaultWhitespaceChars( chars ):&#xa;        r""""""&#xa;        Overrides the default whitespace chars&#xa;&#xa;        Example::&#xa;            # default whitespace chars are space, <TAB> and newline&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def', 'ghi', 'jkl']&#xa;            &#xa;            # change to just treat newline as significant&#xa;            ParserElement.setDefaultWhitespaceChars("" \t"")&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def']&#xa;        """"""&#xa;        ParserElement.DEFAULT_WHITE_CHARS = chars&#xa;&#xa;    @staticmethod&#xa;    def inlineLiteralsUsing(cls):&#xa;        """"""&#xa;        Set class to be used for inclusion of string literals into a parser.&#xa;        &#xa;        Example::&#xa;            # default literal class used is Literal&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;&#xa;            # change to Suppress&#xa;            ParserElement.inlineLiteralsUsing(Suppress)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '12', '31']&#xa;        """"""&#xa;        ParserElement._literalStringClass = cls&#xa;&#xa;    def __init__( self, savelist=False ):&#xa;        self.parseAction = list()&#xa;        self.failAction = None&#xa;        #~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall&#xa;        self.strRepr = None&#xa;        self.resultsName = None&#xa;        self.saveAsList = savelist&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        self.copyDefaultWhiteChars = True&#xa;        self.mayReturnEmpty = False # used when checking for left-recursion&#xa;        self.keepTabs = False&#xa;        self.ignoreExprs = list()&#xa;        self.debug = False&#xa;        self.streamlined = False&#xa;        self.mayIndexError = True # used to optimize exception handling for subclasses that don't advance parse index&#xa;        self.errmsg = """"&#xa;        self.modalResults = True # used to mark results names as modal (report only last) or cumulative (list all)&#xa;        self.debugActions = ( None, None, None ) #custom debug actions&#xa;        self.re = None&#xa;        self.callPreparse = True # used to avoid redundant calls to preParse&#xa;        self.callDuringTry = False&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Make a copy of this C{ParserElement}.  Useful for defining different parse actions&#xa;        for the same parsing pattern, using copies of the original parse element.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress(""K"")&#xa;            integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;            &#xa;            print(OneOrMore(integerK | integerM | integer).parseString(""5K 100 640K 256M""))&#xa;        prints::&#xa;            [5120, 100, 655360, 268435456]&#xa;        Equivalent form of C{expr.copy()} is just C{expr()}::&#xa;            integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;        """"""&#xa;        cpy = copy.copy( self )&#xa;        cpy.parseAction = self.parseAction[:]&#xa;        cpy.ignoreExprs = self.ignoreExprs[:]&#xa;        if self.copyDefaultWhiteChars:&#xa;            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        return cpy&#xa;&#xa;    def setName( self, name ):&#xa;        """"""&#xa;        Define name for this expression, makes debugging and exception messages clearer.&#xa;        &#xa;        Example::&#xa;            Word(nums).parseString(""ABC"")  # -> Exception: Expected W:(0123...) (at char 0), (line:1, col:1)&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")  # -> Exception: Expected integer (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        self.name = name&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        if hasattr(self,""exception""):&#xa;            self.exception.msg = self.errmsg&#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        """"""&#xa;        Define name for referencing matching tokens as a nested attribute&#xa;        of the returned parse results.&#xa;        NOTE: this returns a *copy* of the original C{ParserElement} object;&#xa;        this is so that the client can define a basic element, such as an&#xa;        integer, and reference it in multiple places with different names.&#xa;&#xa;        You can also set results names using the abbreviated syntax,&#xa;        C{expr(""name"")} in place of C{expr.setResultsName(""name"")} - &#xa;        see L{I{__call__}<__call__>}.&#xa;&#xa;        Example::&#xa;            date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;&#xa;            # equivalent form:&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;        """"""&#xa;        newself = self.copy()&#xa;        if name.endswith(""*""):&#xa;            name = name[:-1]&#xa;            listAllMatches=True&#xa;        newself.resultsName = name&#xa;        newself.modalResults = not listAllMatches&#xa;        return newself&#xa;&#xa;    def setBreak(self,breakFlag = True):&#xa;        """"""Method to invoke the Python pdb debugger when this element is&#xa;           about to be parsed. Set C{breakFlag} to True to enable, False to&#xa;           disable.&#xa;        """"""&#xa;        if breakFlag:&#xa;            _parseMethod = self._parse&#xa;            def breaker(instring, loc, doActions=True, callPreParse=True):&#xa;                import pdb&#xa;                pdb.set_trace()&#xa;                return _parseMethod( instring, loc, doActions, callPreParse )&#xa;            breaker._originalParseMethod = _parseMethod&#xa;            self._parse = breaker&#xa;        else:&#xa;            if hasattr(self._parse,""_originalParseMethod""):&#xa;                self._parse = self._parse._originalParseMethod&#xa;        return self&#xa;&#xa;    def setParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Define action to perform when successfully matching parse element definition.&#xa;        Parse action fn is a callable method with 0-3 arguments, called as C{fn(s,loc,toks)},&#xa;        C{fn(loc,toks)}, C{fn(toks)}, or just C{fn()}, where:&#xa;         - s   = the original string being parsed (see note below)&#xa;         - loc = the location of the matching substring&#xa;         - toks = a list of the matched tokens, packaged as a C{L{ParseResults}} object&#xa;        If the functions in fns modify the tokens, they can return them as the return&#xa;        value from fn, and the modified list of tokens will replace the original.&#xa;        Otherwise, fn does not need to return any value.&#xa;&#xa;        Optional keyword arguments:&#xa;         - callDuringTry = (default=C{False}) indicate if parse action should be run during lookaheads and alternate testing&#xa;&#xa;        Note: the default parsing behavior is to expand tabs in the input string&#xa;        before starting the parsing process.  See L{I{parseString}<parseString>} for more information&#xa;        on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;        consistent view of the parsed string, the parse location, and line and column&#xa;        positions within the parsed string.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;            # use parse action to convert to ints at parse time&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            # note that integer fields are now ints, not strings&#xa;            date_str.parseString(""1999/12/31"")  # -> [1999, '/', 12, '/', 31]&#xa;        """"""&#xa;        self.parseAction = list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Add parse action to expression's list of parse actions. See L{I{setParseAction}<setParseAction>}.&#xa;        &#xa;        See examples in L{I{copy}<copy>}.&#xa;        """"""&#xa;        self.parseAction += list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addCondition(self, *fns, **kwargs):&#xa;        """"""Add a boolean predicate function to expression's list of parse actions. See &#xa;        L{I{setParseAction}<setParseAction>} for function call signatures. Unlike C{setParseAction}, &#xa;        functions passed to C{addCondition} need to return boolean success/fail of the condition.&#xa;&#xa;        Optional keyword arguments:&#xa;         - message = define a custom message to be used in the raised exception&#xa;         - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException&#xa;         &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            year_int = integer.copy()&#xa;            year_int.addCondition(lambda toks: toks[0] >= 2000, message=""Only support years 2000 and later"")&#xa;            date_str = year_int + '/' + integer + '/' + integer&#xa;&#xa;            result = date_str.parseString(""1999/12/31"")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        msg = kwargs.get(""message"", ""failed user-defined condition"")&#xa;        exc_type = ParseFatalException if kwargs.get(""fatal"", False) else ParseException&#xa;        for fn in fns:&#xa;            def pa(s,l,t):&#xa;                if not bool(_trim_arity(fn)(s,l,t)):&#xa;                    raise exc_type(s,l,msg)&#xa;            self.parseAction.append(pa)&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def setFailAction( self, fn ):&#xa;        """"""Define action to perform if parsing fails at this expression.&#xa;           Fail acton fn is a callable function that takes the arguments&#xa;           C{fn(s,loc,expr,err)} where:&#xa;            - s = string being parsed&#xa;            - loc = location where expression match was attempted and failed&#xa;            - expr = the parse expression that failed&#xa;            - err = the exception thrown&#xa;           The function returns no value.  It may throw C{L{ParseFatalException}}&#xa;           if it is desired to stop parsing immediately.""""""&#xa;        self.failAction = fn&#xa;        return self&#xa;&#xa;    def _skipIgnorables( self, instring, loc ):&#xa;        exprsFound = True&#xa;        while exprsFound:&#xa;            exprsFound = False&#xa;            for e in self.ignoreExprs:&#xa;                try:&#xa;                    while 1:&#xa;                        loc,dummy = e._parse( instring, loc )&#xa;                        exprsFound = True&#xa;                except ParseException:&#xa;                    pass&#xa;        return loc&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if self.ignoreExprs:&#xa;            loc = self._skipIgnorables( instring, loc )&#xa;&#xa;        if self.skipWhitespace:&#xa;            wt = self.whiteChars&#xa;            instrlen = len(instring)&#xa;            while loc < instrlen and instring[loc] in wt:&#xa;                loc += 1&#xa;&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        return loc, []&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return tokenlist&#xa;&#xa;    #~ @profile&#xa;    def _parseNoCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        debugging = ( self.debug ) #and doActions )&#xa;&#xa;        if debugging or self.failAction:&#xa;            #~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))&#xa;            if (self.debugActions[0] ):&#xa;                self.debugActions[0]( instring, loc, self )&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            try:&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            except ParseBaseException as err:&#xa;                #~ print (""Exception raised:"", err)&#xa;                if self.debugActions[2]:&#xa;                    self.debugActions[2]( instring, tokensStart, self, err )&#xa;                if self.failAction:&#xa;                    self.failAction( instring, tokensStart, self, err )&#xa;                raise&#xa;        else:&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            if self.mayIndexError or loc >= len(instring):&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            else:&#xa;                loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;&#xa;        tokens = self.postParse( instring, loc, tokens )&#xa;&#xa;        retTokens = ParseResults( tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults )&#xa;        if self.parseAction and (doActions or self.callDuringTry):&#xa;            if debugging:&#xa;                try:&#xa;                    for fn in self.parseAction:&#xa;                        tokens = fn( instring, tokensStart, retTokens )&#xa;                        if tokens is not None:&#xa;                            retTokens = ParseResults( tokens,&#xa;                                                      self.resultsName,&#xa;                                                      asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                      modal=self.modalResults )&#xa;                except ParseBaseException as err:&#xa;                    #~ print ""Exception raised in user parse action:"", err&#xa;                    if (self.debugActions[2] ):&#xa;                        self.debugActions[2]( instring, tokensStart, self, err )&#xa;                    raise&#xa;            else:&#xa;                for fn in self.parseAction:&#xa;                    tokens = fn( instring, tokensStart, retTokens )&#xa;                    if tokens is not None:&#xa;                        retTokens = ParseResults( tokens,&#xa;                                                  self.resultsName,&#xa;                                                  asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                  modal=self.modalResults )&#xa;&#xa;        if debugging:&#xa;            #~ print (""Matched"",self,""->"",retTokens.asList())&#xa;            if (self.debugActions[1] ):&#xa;                self.debugActions[1]( instring, tokensStart, loc, self, retTokens )&#xa;&#xa;        return loc, retTokens&#xa;&#xa;    def tryParse( self, instring, loc ):&#xa;        try:&#xa;            return self._parse( instring, loc, doActions=False )[0]&#xa;        except ParseFatalException:&#xa;            raise ParseException( instring, loc, self.errmsg, self)&#xa;    &#xa;    def canParseNext(self, instring, loc):&#xa;        try:&#xa;            self.tryParse(instring, loc)&#xa;        except (ParseException, IndexError):&#xa;            return False&#xa;        else:&#xa;            return True&#xa;&#xa;    class _UnboundedCache(object):&#xa;        def __init__(self):&#xa;            cache = {}&#xa;            self.not_in_cache = not_in_cache = object()&#xa;&#xa;            def get(self, key):&#xa;                return cache.get(key, not_in_cache)&#xa;&#xa;            def set(self, key, value):&#xa;                cache[key] = value&#xa;&#xa;            def clear(self):&#xa;                cache.clear()&#xa;&#xa;            self.get = types.MethodType(get, self)&#xa;            self.set = types.MethodType(set, self)&#xa;            self.clear = types.MethodType(clear, self)&#xa;&#xa;    if _OrderedDict is not None:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = _OrderedDict()&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.popitem(False)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    else:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = {}&#xa;                key_fifo = collections.deque([], size)&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.pop(key_fifo.popleft(), None)&#xa;                    key_fifo.append(key)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;                    key_fifo.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    # argument cache for optimizing repeated calls when backtracking through recursive expressions&#xa;    packrat_cache = {} # this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail&#xa;    packrat_cache_lock = RLock()&#xa;    packrat_cache_stats = [0, 0]&#xa;&#xa;    # this method gets repeatedly called during backtracking with the same arguments -&#xa;    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression&#xa;    def _parseCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        HIT, MISS = 0, 1&#xa;        lookup = (self, instring, loc, callPreParse, doActions)&#xa;        with ParserElement.packrat_cache_lock:&#xa;            cache = ParserElement.packrat_cache&#xa;            value = cache.get(lookup)&#xa;            if value is cache.not_in_cache:&#xa;                ParserElement.packrat_cache_stats[MISS] += 1&#xa;                try:&#xa;                    value = self._parseNoCache(instring, loc, doActions, callPreParse)&#xa;                except ParseBaseException as pe:&#xa;                    # cache a copy of the exception, without the traceback&#xa;                    cache.set(lookup, pe.__class__(*pe.args))&#xa;                    raise&#xa;                else:&#xa;                    cache.set(lookup, (value[0], value[1].copy()))&#xa;                    return value&#xa;            else:&#xa;                ParserElement.packrat_cache_stats[HIT] += 1&#xa;                if isinstance(value, Exception):&#xa;                    raise value&#xa;                return (value[0], value[1].copy())&#xa;&#xa;    _parse = _parseNoCache&#xa;&#xa;    @staticmethod&#xa;    def resetCache():&#xa;        ParserElement.packrat_cache.clear()&#xa;        ParserElement.packrat_cache_stats[:] = [0] * len(ParserElement.packrat_cache_stats)&#xa;&#xa;    _packratEnabled = False&#xa;    @staticmethod&#xa;    def enablePackrat(cache_size_limit=128):&#xa;        """"""Enables ""packrat"" parsing, which adds memoizing to the parsing logic.&#xa;           Repeated parse attempts at the same string location (which happens&#xa;           often in many complex grammars) can immediately return a cached value,&#xa;           instead of re-executing parsing/validating code.  Memoizing is done of&#xa;           both valid results and parsing exceptions.&#xa;           &#xa;           Parameters:&#xa;            - cache_size_limit - (default=C{128}) - if an integer value is provided&#xa;              will limit the size of the packrat cache; if None is passed, then&#xa;              the cache size will be unbounded; if 0 is passed, the cache will&#xa;              be effectively disabled.&#xa;            &#xa;           This speedup may break existing programs that use parse actions that&#xa;           have side-effects.  For this reason, packrat parsing is disabled when&#xa;           you first import pyparsing.  To activate the packrat feature, your&#xa;           program must call the class method C{ParserElement.enablePackrat()}.  If&#xa;           your program uses C{psyco} to ""compile as you go"", you must call&#xa;           C{enablePackrat} before calling C{psyco.full()}.  If you do not do this,&#xa;           Python will crash.  For best results, call C{enablePackrat()} immediately&#xa;           after importing pyparsing.&#xa;           &#xa;           Example::&#xa;               import pyparsing&#xa;               pyparsing.ParserElement.enablePackrat()&#xa;        """"""&#xa;        if not ParserElement._packratEnabled:&#xa;            ParserElement._packratEnabled = True&#xa;            if cache_size_limit is None:&#xa;                ParserElement.packrat_cache = ParserElement._UnboundedCache()&#xa;            else:&#xa;                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)&#xa;            ParserElement._parse = ParserElement._parseCache&#xa;&#xa;    def parseString( self, instring, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression with the given string.&#xa;        This is the main interface to the client code, once the complete&#xa;        expression has been built.&#xa;&#xa;        If you want the grammar to require that the entire input string be&#xa;        successfully parsed, then set C{parseAll} to True (equivalent to ending&#xa;        the grammar with C{L{StringEnd()}}).&#xa;&#xa;        Note: C{parseString} implicitly calls C{expandtabs()} on the input string,&#xa;        in order to report proper column numbers in parse actions.&#xa;        If the input string contains tabs and&#xa;        the grammar uses parse actions that use the C{loc} argument to index into the&#xa;        string being parsed, you can ensure you have a consistent view of the input&#xa;        string by:&#xa;         - calling C{parseWithTabs} on your grammar before calling C{parseString}&#xa;           (see L{I{parseWithTabs}<parseWithTabs>})&#xa;         - define your parse action using the full C{(s,loc,toks)} signature, and&#xa;           reference the input string using the parse action's C{s} argument&#xa;         - explictly expand the tabs in your input string before calling&#xa;           C{parseString}&#xa;        &#xa;        Example::&#xa;            Word('a').parseString('aaaaabaaa')  # -> ['aaaaa']&#xa;            Word('a').parseString('aaaaabaaa', parseAll=True)  # -> Exception: Expected end of text&#xa;        """"""&#xa;        ParserElement.resetCache()&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;            #~ self.saveAsList = True&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;        if not self.keepTabs:&#xa;            instring = instring.expandtabs()&#xa;        try:&#xa;            loc, tokens = self._parse( instring, 0 )&#xa;            if parseAll:&#xa;                loc = self.preParse( instring, loc )&#xa;                se = Empty() + StringEnd()&#xa;                se._parse( instring, loc )&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;        else:&#xa;            return tokens&#xa;&#xa;    def scanString( self, instring, maxMatches=_MAX_INT, overlap=False ):&#xa;        """"""&#xa;        Scan the input string for expression matches.  Each match will return the&#xa;        matching tokens, start location, and end location.  May be called with optional&#xa;        C{maxMatches} argument, to clip scanning after 'n' matches are found.  If&#xa;        C{overlap} is specified, then overlapping matches will be reported.&#xa;&#xa;        Note that the start and end locations are reported relative to the string&#xa;        being parsed.  See L{I{parseString}<parseString>} for more information on parsing&#xa;        strings with embedded tabs.&#xa;&#xa;        Example::&#xa;            source = ""sldjf123lsdjjkf345sldkjf879lkjsfd987""&#xa;            print(source)&#xa;            for tokens,start,end in Word(alphas).scanString(source):&#xa;                print(' '*start + '^'*(end-start))&#xa;                print(' '*start + tokens[0])&#xa;        &#xa;        prints::&#xa;        &#xa;            sldjf123lsdjjkf345sldkjf879lkjsfd987&#xa;            ^^^^^&#xa;            sldjf&#xa;                    ^^^^^^^&#xa;                    lsdjjkf&#xa;                              ^^^^^^&#xa;                              sldkjf&#xa;                                       ^^^^^^&#xa;                                       lkjsfd&#xa;        """"""&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;&#xa;        if not self.keepTabs:&#xa;            instring = _ustr(instring).expandtabs()&#xa;        instrlen = len(instring)&#xa;        loc = 0&#xa;        preparseFn = self.preParse&#xa;        parseFn = self._parse&#xa;        ParserElement.resetCache()&#xa;        matches = 0&#xa;        try:&#xa;            while loc <= instrlen and matches < maxMatches:&#xa;                try:&#xa;                    preloc = preparseFn( instring, loc )&#xa;                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )&#xa;                except ParseException:&#xa;                    loc = preloc+1&#xa;                else:&#xa;                    if nextLoc > loc:&#xa;                        matches += 1&#xa;                        yield tokens, preloc, nextLoc&#xa;                        if overlap:&#xa;                            nextloc = preparseFn( instring, loc )&#xa;                            if nextloc > loc:&#xa;                                loc = nextLoc&#xa;                            else:&#xa;                                loc += 1&#xa;                        else:&#xa;                            loc = nextLoc&#xa;                    else:&#xa;                        loc = preloc+1&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def transformString( self, instring ):&#xa;        """"""&#xa;        Extension to C{L{scanString}}, to modify matching text with modified tokens that may&#xa;        be returned from a parse action.  To use C{transformString}, define a grammar and&#xa;        attach a parse action to it that modifies the returned token list.&#xa;        Invoking C{transformString()} on a target string will then scan for matches,&#xa;        and replace the matched text patterns according to the logic in the parse&#xa;        action.  C{transformString()} returns the resulting transformed string.&#xa;        &#xa;        Example::&#xa;            wd = Word(alphas)&#xa;            wd.setParseAction(lambda toks: toks[0].title())&#xa;            &#xa;            print(wd.transformString(""now is the winter of our discontent made glorious summer by this sun of york.""))&#xa;        Prints::&#xa;            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.&#xa;        """"""&#xa;        out = []&#xa;        lastE = 0&#xa;        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to&#xa;        # keep string locs straight between transformString and scanString&#xa;        self.keepTabs = True&#xa;        try:&#xa;            for t,s,e in self.scanString( instring ):&#xa;                out.append( instring[lastE:s] )&#xa;                if t:&#xa;                    if isinstance(t,ParseResults):&#xa;                        out += t.asList()&#xa;                    elif isinstance(t,list):&#xa;                        out += t&#xa;                    else:&#xa;                        out.append(t)&#xa;                lastE = e&#xa;            out.append(instring[lastE:])&#xa;            out = [o for o in out if o]&#xa;            return """".join(map(_ustr,_flatten(out)))&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def searchString( self, instring, maxMatches=_MAX_INT ):&#xa;        """"""&#xa;        Another extension to C{L{scanString}}, simplifying the access to the tokens found&#xa;        to match the given parse expression.  May be called with optional&#xa;        C{maxMatches} argument, to clip searching after 'n' matches are found.&#xa;        &#xa;        Example::&#xa;            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters&#xa;            cap_word = Word(alphas.upper(), alphas.lower())&#xa;            &#xa;            print(cap_word.searchString(""More than Iron, more than Lead, more than Gold I need Electricity""))&#xa;        prints::&#xa;            ['More', 'Iron', 'Lead', 'Gold', 'I']&#xa;        """"""&#xa;        try:&#xa;            return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):&#xa;        """"""&#xa;        Generator method to split a string using the given expression as a separator.&#xa;        May be called with optional C{maxsplit} argument, to limit the number of splits;&#xa;        and the optional C{includeSeparators} argument (default=C{False}), if the separating&#xa;        matching text should be included in the split results.&#xa;        &#xa;        Example::        &#xa;            punc = oneOf(list("".,;:/-!?""))&#xa;            print(list(punc.split(""This, this?, this sentence, is badly punctuated!"")))&#xa;        prints::&#xa;            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']&#xa;        """"""&#xa;        splits = 0&#xa;        last = 0&#xa;        for t,s,e in self.scanString(instring, maxMatches=maxsplit):&#xa;            yield instring[last:s]&#xa;            if includeSeparators:&#xa;                yield t[0]&#xa;            last = e&#xa;        yield instring[last:]&#xa;&#xa;    def __add__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator - returns C{L{And}}. Adding strings to a ParserElement&#xa;        converts them to L{Literal}s by default.&#xa;        &#xa;        Example::&#xa;            greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;            hello = ""Hello, World!""&#xa;            print (hello, ""->"", greet.parseString(hello))&#xa;        Prints::&#xa;            Hello, World! -> ['Hello', ',', 'World', '!']&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, other ] )&#xa;&#xa;    def __radd__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other + self&#xa;&#xa;    def __sub__(self, other):&#xa;        """"""&#xa;        Implementation of - operator, returns C{L{And}} with error stop&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, And._ErrorStop(), other ] )&#xa;&#xa;    def __rsub__(self, other ):&#xa;        """"""&#xa;        Implementation of - operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other - self&#xa;&#xa;    def __mul__(self,other):&#xa;        """"""&#xa;        Implementation of * operator, allows use of C{expr * 3} in place of&#xa;        C{expr + expr + expr}.  Expressions may also me multiplied by a 2-integer&#xa;        tuple, similar to C{{min,max}} multipliers in regular expressions.  Tuples&#xa;        may also include C{None} as in:&#xa;         - C{expr*(n,None)} or C{expr*(n,)} is equivalent&#xa;              to C{expr*n + L{ZeroOrMore}(expr)}&#xa;              (read as ""at least n instances of C{expr}"")&#xa;         - C{expr*(None,n)} is equivalent to C{expr*(0,n)}&#xa;              (read as ""0 to n instances of C{expr}"")&#xa;         - C{expr*(None,None)} is equivalent to C{L{ZeroOrMore}(expr)}&#xa;         - C{expr*(1,None)} is equivalent to C{L{OneOrMore}(expr)}&#xa;&#xa;        Note that C{expr*(None,n)} does not raise an exception if&#xa;        more than n exprs exist in the input stream; that is,&#xa;        C{expr*(None,n)} does not enforce a maximum number of expr&#xa;        occurrences.  If this behavior is desired, then write&#xa;        C{expr*(None,n) + ~expr}&#xa;        """"""&#xa;        if isinstance(other,int):&#xa;            minElements, optElements = other,0&#xa;        elif isinstance(other,tuple):&#xa;            other = (other + (None, None))[:2]&#xa;            if other[0] is None:&#xa;                other = (0, other[1])&#xa;            if isinstance(other[0],int) and other[1] is None:&#xa;                if other[0] == 0:&#xa;                    return ZeroOrMore(self)&#xa;                if other[0] == 1:&#xa;                    return OneOrMore(self)&#xa;                else:&#xa;                    return self*other[0] + ZeroOrMore(self)&#xa;            elif isinstance(other[0],int) and isinstance(other[1],int):&#xa;                minElements, optElements = other&#xa;                optElements -= minElements&#xa;            else:&#xa;                raise TypeError(""cannot multiply 'ParserElement' and ('%s','%s') objects"", type(other[0]),type(other[1]))&#xa;        else:&#xa;            raise TypeError(""cannot multiply 'ParserElement' and '%s' objects"", type(other))&#xa;&#xa;        if minElements < 0:&#xa;            raise ValueError(""cannot multiply ParserElement by negative value"")&#xa;        if optElements < 0:&#xa;            raise ValueError(""second tuple value must be greater or equal to first tuple value"")&#xa;        if minElements == optElements == 0:&#xa;            raise ValueError(""cannot multiply ParserElement by 0 or (0,0)"")&#xa;&#xa;        if (optElements):&#xa;            def makeOptionalList(n):&#xa;                if n>1:&#xa;                    return Optional(self + makeOptionalList(n-1))&#xa;                else:&#xa;                    return Optional(self)&#xa;            if minElements:&#xa;                if minElements == 1:&#xa;                    ret = self + makeOptionalList(optElements)&#xa;                else:&#xa;                    ret = And([self]*minElements) + makeOptionalList(optElements)&#xa;            else:&#xa;                ret = makeOptionalList(optElements)&#xa;        else:&#xa;            if minElements == 1:&#xa;                ret = self&#xa;            else:&#xa;                ret = And([self]*minElements)&#xa;        return ret&#xa;&#xa;    def __rmul__(self, other):&#xa;        return self.__mul__(other)&#xa;&#xa;    def __or__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator - returns C{L{MatchFirst}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return MatchFirst( [ self, other ] )&#xa;&#xa;    def __ror__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other | self&#xa;&#xa;    def __xor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator - returns C{L{Or}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Or( [ self, other ] )&#xa;&#xa;    def __rxor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other ^ self&#xa;&#xa;    def __and__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator - returns C{L{Each}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Each( [ self, other ] )&#xa;&#xa;    def __rand__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other & self&#xa;&#xa;    def __invert__( self ):&#xa;        """"""&#xa;        Implementation of ~ operator - returns C{L{NotAny}}&#xa;        """"""&#xa;        return NotAny( self )&#xa;&#xa;    def __call__(self, name=None):&#xa;        """"""&#xa;        Shortcut for C{L{setResultsName}}, with C{listAllMatches=False}.&#xa;        &#xa;        If C{name} is given with a trailing C{'*'} character, then C{listAllMatches} will be&#xa;        passed as C{True}.&#xa;           &#xa;        If C{name} is omitted, same as calling C{L{copy}}.&#xa;&#xa;        Example::&#xa;            # these are equivalent&#xa;            userdata = Word(alphas).setResultsName(""name"") + Word(nums+""-"").setResultsName(""socsecno"")&#xa;            userdata = Word(alphas)(""name"") + Word(nums+""-"")(""socsecno"")             &#xa;        """"""&#xa;        if name is not None:&#xa;            return self.setResultsName(name)&#xa;        else:&#xa;            return self.copy()&#xa;&#xa;    def suppress( self ):&#xa;        """"""&#xa;        Suppresses the output of this C{ParserElement}; useful to keep punctuation from&#xa;        cluttering up returned output.&#xa;        """"""&#xa;        return Suppress( self )&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""&#xa;        Disables the skipping of whitespace before matching the characters in the&#xa;        C{ParserElement}'s defined pattern.  This is normally only used internally by&#xa;        the pyparsing module, but may be needed in some whitespace-sensitive grammars.&#xa;        """"""&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def setWhitespaceChars( self, chars ):&#xa;        """"""&#xa;        Overrides the default whitespace chars&#xa;        """"""&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = chars&#xa;        self.copyDefaultWhiteChars = False&#xa;        return self&#xa;&#xa;    def parseWithTabs( self ):&#xa;        """"""&#xa;        Overrides default behavior to expand C{<TAB>}s to spaces before parsing the input string.&#xa;        Must be called before C{parseString} when the input grammar contains elements that&#xa;        match C{<TAB>} characters.&#xa;        """"""&#xa;        self.keepTabs = True&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        """"""&#xa;        Define expression to be ignored (e.g., comments) while doing pattern&#xa;        matching; may be called repeatedly, to define multiple comment or other&#xa;        ignorable patterns.&#xa;        &#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']&#xa;            &#xa;            patt.ignore(cStyleComment)&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']&#xa;        """"""&#xa;        if isinstance(other, basestring):&#xa;            other = Suppress(other)&#xa;&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                self.ignoreExprs.append(other)&#xa;        else:&#xa;            self.ignoreExprs.append( Suppress( other.copy() ) )&#xa;        return self&#xa;&#xa;    def setDebugActions( self, startAction, successAction, exceptionAction ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        """"""&#xa;        self.debugActions = (startAction or _defaultStartDebugAction,&#xa;                             successAction or _defaultSuccessDebugAction,&#xa;                             exceptionAction or _defaultExceptionDebugAction)&#xa;        self.debug = True&#xa;        return self&#xa;&#xa;    def setDebug( self, flag=True ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        Set C{flag} to True to enable, False to disable.&#xa;&#xa;        Example::&#xa;            wd = Word(alphas).setName(""alphaword"")&#xa;            integer = Word(nums).setName(""numword"")&#xa;            term = wd | integer&#xa;            &#xa;            # turn on debugging for wd&#xa;            wd.setDebug()&#xa;&#xa;            OneOrMore(term).parseString(""abc 123 xyz 890"")&#xa;        &#xa;        prints::&#xa;            Match alphaword at loc 0(1,1)&#xa;            Matched alphaword -> ['abc']&#xa;            Match alphaword at loc 3(1,4)&#xa;            Exception raised:Expected alphaword (at char 4), (line:1, col:5)&#xa;            Match alphaword at loc 7(1,8)&#xa;            Matched alphaword -> ['xyz']&#xa;            Match alphaword at loc 11(1,12)&#xa;            Exception raised:Expected alphaword (at char 12), (line:1, col:13)&#xa;            Match alphaword at loc 15(1,16)&#xa;            Exception raised:Expected alphaword (at char 15), (line:1, col:16)&#xa;&#xa;        The output shown is that produced by the default debug actions - custom debug actions can be&#xa;        specified using L{setDebugActions}. Prior to attempting&#xa;        to match the C{wd} expression, the debugging message C{""Match <exprname> at loc <n>(<line>,<col>)""}&#xa;        is shown. Then if the parse succeeds, a C{""Matched""} message is shown, or an C{""Exception raised""}&#xa;        message is shown. Also note the use of L{setName} to assign a human-readable name to the expression,&#xa;        which makes debugging and exception messages easier to understand - for instance, the default&#xa;        name created for the C{Word} expression without calling C{setName} is C{""W:(ABCD...)""}.&#xa;        """"""&#xa;        if flag:&#xa;            self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )&#xa;        else:&#xa;            self.debug = False&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        return self.name&#xa;&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;&#xa;    def streamline( self ):&#xa;        self.streamlined = True&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        pass&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        """"""&#xa;        Check defined expressions for valid structure, check for infinite recursive definitions.&#xa;        """"""&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def parseFile( self, file_or_filename, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression on the given file or filename.&#xa;        If a filename is specified (instead of a file object),&#xa;        the entire file is opened, read, and closed before parsing.&#xa;        """"""&#xa;        try:&#xa;            file_contents = file_or_filename.read()&#xa;        except AttributeError:&#xa;            with open(file_or_filename, ""r"") as f:&#xa;                file_contents = f.read()&#xa;        try:&#xa;            return self.parseString(file_contents, parseAll)&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def __eq__(self,other):&#xa;        if isinstance(other, ParserElement):&#xa;            return self is other or vars(self) == vars(other)&#xa;        elif isinstance(other, basestring):&#xa;            return self.matches(other)&#xa;        else:&#xa;            return super(ParserElement,self)==other&#xa;&#xa;    def __ne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def __hash__(self):&#xa;        return hash(id(self))&#xa;&#xa;    def __req__(self,other):&#xa;        return self == other&#xa;&#xa;    def __rne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def matches(self, testString, parseAll=True):&#xa;        """"""&#xa;        Method for quick testing of a parser against a test string. Good for simple &#xa;        inline microtests of sub expressions while building up larger parser.&#xa;           &#xa;        Parameters:&#xa;         - testString - to test against this expression for a match&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests&#xa;            &#xa;        Example::&#xa;            expr = Word(nums)&#xa;            assert expr.matches(""100"")&#xa;        """"""&#xa;        try:&#xa;            self.parseString(_ustr(testString), parseAll=parseAll)&#xa;            return True&#xa;        except ParseBaseException:&#xa;            return False&#xa;                &#xa;    def runTests(self, tests, parseAll=True, comment='#', fullDump=True, printResults=True, failureTests=False):&#xa;        """"""&#xa;        Execute the parse expression on a series of test strings, showing each&#xa;        test, the parsed results or where the parse failed. Quick and easy way to&#xa;        run a parse expression against a list of sample strings.&#xa;           &#xa;        Parameters:&#xa;         - tests - a list of separate test strings, or a multiline string of test strings&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests           &#xa;         - comment - (default=C{'#'}) - expression for indicating embedded comments in the test &#xa;              string; pass None to disable comment filtering&#xa;         - fullDump - (default=C{True}) - dump results as list followed by results names in nested outline;&#xa;              if False, only dump nested list&#xa;         - printResults - (default=C{True}) prints test output to stdout&#xa;         - failureTests - (default=C{False}) indicates if these tests are expected to fail parsing&#xa;&#xa;        Returns: a (success, results) tuple, where success indicates that all tests succeeded&#xa;        (or failed if C{failureTests} is True), and the results contain a list of lines of each &#xa;        test's output&#xa;        &#xa;        Example::&#xa;            number_expr = pyparsing_common.number.copy()&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # unsigned integer&#xa;                100&#xa;                # negative integer&#xa;                -100&#xa;                # float with scientific notation&#xa;                6.02e23&#xa;                # integer with scientific notation&#xa;                1e-12&#xa;                ''')&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # stray character&#xa;                100Z&#xa;                # missing leading digit before '.'&#xa;                -.100&#xa;                # too many '.'&#xa;                3.14.159&#xa;                ''', failureTests=True)&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;        prints::&#xa;            # unsigned integer&#xa;            100&#xa;            [100]&#xa;&#xa;            # negative integer&#xa;            -100&#xa;            [-100]&#xa;&#xa;            # float with scientific notation&#xa;            6.02e23&#xa;            [6.02e+23]&#xa;&#xa;            # integer with scientific notation&#xa;            1e-12&#xa;            [1e-12]&#xa;&#xa;            Success&#xa;            &#xa;            # stray character&#xa;            100Z&#xa;               ^&#xa;            FAIL: Expected end of text (at char 3), (line:1, col:4)&#xa;&#xa;            # missing leading digit before '.'&#xa;            -.100&#xa;            ^&#xa;            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)&#xa;&#xa;            # too many '.'&#xa;            3.14.159&#xa;                ^&#xa;            FAIL: Expected end of text (at char 4), (line:1, col:5)&#xa;&#xa;            Success&#xa;&#xa;        Each test string must be on a single line. If you want to test a string that spans multiple&#xa;        lines, create a test like this::&#xa;&#xa;            expr.runTest(r""this is a test\\n of strings that spans \\n 3 lines"")&#xa;        &#xa;        (Note that this is a raw string literal, you must include the leading 'r'.)&#xa;        """"""&#xa;        if isinstance(tests, basestring):&#xa;            tests = list(map(str.strip, tests.rstrip().splitlines()))&#xa;        if isinstance(comment, basestring):&#xa;            comment = Literal(comment)&#xa;        allResults = []&#xa;        comments = []&#xa;        success = True&#xa;        for t in tests:&#xa;            if comment is not None and comment.matches(t, False) or comments and not t:&#xa;                comments.append(t)&#xa;                continue&#xa;            if not t:&#xa;                continue&#xa;            out = ['\n'.join(comments), t]&#xa;            comments = []&#xa;            try:&#xa;                t = t.replace(r'\n','\n')&#xa;                result = self.parseString(t, parseAll=parseAll)&#xa;                out.append(result.dump(full=fullDump))&#xa;                success = success and not failureTests&#xa;            except ParseBaseException as pe:&#xa;                fatal = ""(FATAL)"" if isinstance(pe, ParseFatalException) else """"&#xa;                if '\n' in t:&#xa;                    out.append(line(pe.loc, t))&#xa;                    out.append(' '*(col(pe.loc,t)-1) + '^' + fatal)&#xa;                else:&#xa;                    out.append(' '*pe.loc + '^' + fatal)&#xa;                out.append(""FAIL: "" + str(pe))&#xa;                success = success and failureTests&#xa;                result = pe&#xa;            except Exception as exc:&#xa;                out.append(""FAIL-EXCEPTION: "" + str(exc))&#xa;                success = success and failureTests&#xa;                result = exc&#xa;&#xa;            if printResults:&#xa;                if fullDump:&#xa;                    out.append('')&#xa;                print('\n'.join(out))&#xa;&#xa;            allResults.append((t, result))&#xa;        &#xa;        return success, allResults&#xa;&#xa;        &#xa;class Token(ParserElement):&#xa;    """"""&#xa;    Abstract C{ParserElement} subclass, for defining atomic matching patterns.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Token,self).__init__( savelist=False )&#xa;&#xa;&#xa;class Empty(Token):&#xa;    """"""&#xa;    An empty token, will always match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Empty,self).__init__()&#xa;        self.name = ""Empty""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;&#xa;class NoMatch(Token):&#xa;    """"""&#xa;    A token that will never match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(NoMatch,self).__init__()&#xa;        self.name = ""NoMatch""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.errmsg = ""Unmatchable token""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Literal(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string.&#xa;    &#xa;    Example::&#xa;        Literal('blah').parseString('blah')  # -> ['blah']&#xa;        Literal('blah').parseString('blahfooblah')  # -> ['blah']&#xa;        Literal('blah').parseString('bla')  # -> Exception: Expected ""blah""&#xa;    &#xa;    For case-insensitive matching, use L{CaselessLiteral}.&#xa;    &#xa;    For keyword matching (force word break before and after the matched string),&#xa;    use L{Keyword} or L{CaselessKeyword}.&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(Literal,self).__init__()&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Literal; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;            self.__class__ = Empty&#xa;        self.name = '""%s""' % _ustr(self.match)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;&#xa;    # Performance tuning: this routine gets called a *lot*&#xa;    # if this is a single character match string  and the first character matches,&#xa;    # short-circuit as quickly as possible, and avoid calling startswith&#xa;    #~ @profile&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if (instring[loc] == self.firstMatchChar and&#xa;            (self.matchLen==1 or instring.startswith(self.match,loc)) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;_L = Literal&#xa;ParserElement._literalStringClass = Literal&#xa;&#xa;class Keyword(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string as a keyword, that is, it must be&#xa;    immediately followed by a non-keyword character.  Compare with C{L{Literal}}:&#xa;     - C{Literal(""if"")} will match the leading C{'if'} in C{'ifAndOnlyIf'}.&#xa;     - C{Keyword(""if"")} will not; it will only match the leading C{'if'} in C{'if x=1'}, or C{'if(y==2)'}&#xa;    Accepts two optional constructor arguments in addition to the keyword string:&#xa;     - C{identChars} is a string of characters that would be valid identifier characters,&#xa;          defaulting to all alphanumerics + ""_"" and ""$""&#xa;     - C{caseless} allows case-insensitive matching, default is C{False}.&#xa;       &#xa;    Example::&#xa;        Keyword(""start"").parseString(""start"")  # -> ['start']&#xa;        Keyword(""start"").parseString(""starting"")  # -> Exception&#xa;&#xa;    For case-insensitive matching, use L{CaselessKeyword}.&#xa;    """"""&#xa;    DEFAULT_KEYWORD_CHARS = alphanums+""_$""&#xa;&#xa;    def __init__( self, matchString, identChars=None, caseless=False ):&#xa;        super(Keyword,self).__init__()&#xa;        if identChars is None:&#xa;            identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Keyword; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;        self.name = '""%s""' % self.match&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;        self.caseless = caseless&#xa;        if caseless:&#xa;            self.caselessmatch = matchString.upper()&#xa;            identChars = identChars.upper()&#xa;        self.identChars = set(identChars)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.caseless:&#xa;            if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;                 (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) and&#xa;                 (loc == 0 or instring[loc-1].upper() not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        else:&#xa;            if (instring[loc] == self.firstMatchChar and&#xa;                (self.matchLen==1 or instring.startswith(self.match,loc)) and&#xa;                (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen] not in self.identChars) and&#xa;                (loc == 0 or instring[loc-1] not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;    def copy(self):&#xa;        c = super(Keyword,self).copy()&#xa;        c.identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        return c&#xa;&#xa;    @staticmethod&#xa;    def setDefaultKeywordChars( chars ):&#xa;        """"""Overrides the default Keyword chars&#xa;        """"""&#xa;        Keyword.DEFAULT_KEYWORD_CHARS = chars&#xa;&#xa;class CaselessLiteral(Literal):&#xa;    """"""&#xa;    Token to match a specified string, ignoring case of letters.&#xa;    Note: the matched results will always be in the case of the given&#xa;    match string, NOT the case of the input text.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessLiteral(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessKeyword}.)&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(CaselessLiteral,self).__init__( matchString.upper() )&#xa;        # Preserve the defining literal.&#xa;        self.returnString = matchString&#xa;        self.name = ""'%s'"" % self.returnString&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[ loc:loc+self.matchLen ].upper() == self.match:&#xa;            return loc+self.matchLen, self.returnString&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CaselessKeyword(Keyword):&#xa;    """"""&#xa;    Caseless version of L{Keyword}.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessKeyword(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessLiteral}.)&#xa;    """"""&#xa;    def __init__( self, matchString, identChars=None ):&#xa;        super(CaselessKeyword,self).__init__( matchString, identChars, caseless=True )&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;             (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CloseMatch(Token):&#xa;    """"""&#xa;    A variation on L{Literal} which matches ""close"" matches, that is, &#xa;    strings with at most 'n' mismatching characters. C{CloseMatch} takes parameters:&#xa;     - C{match_string} - string to be matched&#xa;     - C{maxMismatches} - (C{default=1}) maximum number of mismatches allowed to count as a match&#xa;    &#xa;    The results from a successful parse will contain the matched text from the input string and the following named results:&#xa;     - C{mismatches} - a list of the positions within the match_string where mismatches were found&#xa;     - C{original} - the original match_string used to compare against the input string&#xa;    &#xa;    If C{mismatches} is an empty list, then the match was an exact match.&#xa;    &#xa;    Example::&#xa;        patt = CloseMatch(""ATCATCGAATGGA"")&#xa;        patt.parseString(""ATCATCGAAXGGA"") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)&#xa;&#xa;        # exact match&#xa;        patt.parseString(""ATCATCGAATGGA"") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})&#xa;&#xa;        # close match allowing up to 2 mismatches&#xa;        patt = CloseMatch(""ATCATCGAATGGA"", maxMismatches=2)&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})&#xa;    """"""&#xa;    def __init__(self, match_string, maxMismatches=1):&#xa;        super(CloseMatch,self).__init__()&#xa;        self.name = match_string&#xa;        self.match_string = match_string&#xa;        self.maxMismatches = maxMismatches&#xa;        self.errmsg = ""Expected %r (with up to %d mismatches)"" % (self.match_string, self.maxMismatches)&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        start = loc&#xa;        instrlen = len(instring)&#xa;        maxloc = start + len(self.match_string)&#xa;&#xa;        if maxloc <= instrlen:&#xa;            match_string = self.match_string&#xa;            match_stringloc = 0&#xa;            mismatches = []&#xa;            maxMismatches = self.maxMismatches&#xa;&#xa;            for match_stringloc,s_m in enumerate(zip(instring[loc:maxloc], self.match_string)):&#xa;                src,mat = s_m&#xa;                if src != mat:&#xa;                    mismatches.append(match_stringloc)&#xa;                    if len(mismatches) > maxMismatches:&#xa;                        break&#xa;            else:&#xa;                loc = match_stringloc + 1&#xa;                results = ParseResults([instring[start:loc]])&#xa;                results['original'] = self.match_string&#xa;                results['mismatches'] = mismatches&#xa;                return loc, results&#xa;&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Word(Token):&#xa;    """"""&#xa;    Token for matching words composed of allowed character sets.&#xa;    Defined with string containing all allowed initial characters,&#xa;    an optional string containing allowed body characters (if omitted,&#xa;    defaults to the initial character set), and an optional minimum,&#xa;    maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction. An optional&#xa;    C{excludeChars} parameter can list characters that might be found in &#xa;    the input C{bodyChars} string; useful to define a word of all printables&#xa;    except for one or two characters, for instance.&#xa;    &#xa;    L{srange} is useful for defining custom character set strings for defining &#xa;    C{Word} expressions, using range notation from regular expression character sets.&#xa;    &#xa;    A common mistake is to use C{Word} to match a specific literal string, as in &#xa;    C{Word(""Address"")}. Remember that C{Word} uses the string argument to define&#xa;    I{sets} of matchable characters. This expression would match ""Add"", ""AAA"",&#xa;    ""dAred"", or any other word made up of the characters 'A', 'd', 'r', 'e', and 's'.&#xa;    To match an exact literal string, use L{Literal} or L{Keyword}.&#xa;&#xa;    pyparsing includes helper strings for building Words:&#xa;     - L{alphas}&#xa;     - L{nums}&#xa;     - L{alphanums}&#xa;     - L{hexnums}&#xa;     - L{alphas8bit} (alphabetic characters in ASCII range 128-255 - accented, tilded, umlauted, etc.)&#xa;     - L{punc8bit} (non-alphabetic characters in ASCII range 128-255 - currency, symbols, superscripts, diacriticals, etc.)&#xa;     - L{printables} (any non-whitespace character)&#xa;&#xa;    Example::&#xa;        # a word composed of digits&#xa;        integer = Word(nums) # equivalent to Word(""0123456789"") or Word(srange(""0-9""))&#xa;        &#xa;        # a word with a leading capital, and zero or more lowercase&#xa;        capital_word = Word(alphas.upper(), alphas.lower())&#xa;&#xa;        # hostnames are alphanumeric, with leading alpha, and '-'&#xa;        hostname = Word(alphas, alphanums+'-')&#xa;        &#xa;        # roman numeral (not a strict parser, accepts invalid mix of characters)&#xa;        roman = Word(""IVXLCDM"")&#xa;        &#xa;        # any string of non-whitespace characters, except for ','&#xa;        csv_value = Word(printables, excludeChars="","")&#xa;    """"""&#xa;    def __init__( self, initChars, bodyChars=None, min=1, max=0, exact=0, asKeyword=False, excludeChars=None ):&#xa;        super(Word,self).__init__()&#xa;        if excludeChars:&#xa;            initChars = ''.join(c for c in initChars if c not in excludeChars)&#xa;            if bodyChars:&#xa;                bodyChars = ''.join(c for c in bodyChars if c not in excludeChars)&#xa;        self.initCharsOrig = initChars&#xa;        self.initChars = set(initChars)&#xa;        if bodyChars :&#xa;            self.bodyCharsOrig = bodyChars&#xa;            self.bodyChars = set(bodyChars)&#xa;        else:&#xa;            self.bodyCharsOrig = initChars&#xa;            self.bodyChars = set(initChars)&#xa;&#xa;        self.maxSpecified = max > 0&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(Word()) if zero-length word is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.asKeyword = asKeyword&#xa;&#xa;        if ' ' not in self.initCharsOrig+self.bodyCharsOrig and (min==1 and max==0 and exact==0):&#xa;            if self.bodyCharsOrig == self.initCharsOrig:&#xa;                self.reString = ""[%s]+"" % _escapeRegexRangeChars(self.initCharsOrig)&#xa;            elif len(self.initCharsOrig) == 1:&#xa;                self.reString = ""%s[%s]*"" % \&#xa;                                      (re.escape(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            else:&#xa;                self.reString = ""[%s][%s]*"" % \&#xa;                                      (_escapeRegexRangeChars(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            if self.asKeyword:&#xa;                self.reString = r""\b""+self.reString+r""\b""&#xa;            try:&#xa;                self.re = re.compile( self.reString )&#xa;            except Exception:&#xa;                self.re = None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.re:&#xa;            result = self.re.match(instring,loc)&#xa;            if not result:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;            loc = result.end()&#xa;            return loc, result.group()&#xa;&#xa;        if not(instring[ loc ] in self.initChars):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        instrlen = len(instring)&#xa;        bodychars = self.bodyChars&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, instrlen )&#xa;        while loc < maxloc and instring[loc] in bodychars:&#xa;            loc += 1&#xa;&#xa;        throwException = False&#xa;        if loc - start < self.minLen:&#xa;            throwException = True&#xa;        if self.maxSpecified and loc < instrlen and instring[loc] in bodychars:&#xa;            throwException = True&#xa;        if self.asKeyword:&#xa;            if (start>0 and instring[start-1] in bodychars) or (loc<instrlen and instring[loc] in bodychars):&#xa;                throwException = True&#xa;&#xa;        if throwException:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Word,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;&#xa;        if self.strRepr is None:&#xa;&#xa;            def charsAsStr(s):&#xa;                if len(s)>4:&#xa;                    return s[:4]+""...""&#xa;                else:&#xa;                    return s&#xa;&#xa;            if ( self.initCharsOrig != self.bodyCharsOrig ):&#xa;                self.strRepr = ""W:(%s,%s)"" % ( charsAsStr(self.initCharsOrig), charsAsStr(self.bodyCharsOrig) )&#xa;            else:&#xa;                self.strRepr = ""W:(%s)"" % charsAsStr(self.initCharsOrig)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Regex(Token):&#xa;    """"""&#xa;    Token for matching strings that match a given regular expression.&#xa;    Defined with string specifying the regular expression in a form recognized by the inbuilt Python re module.&#xa;    If the given regex contains named groups (defined using C{(?P<name>...)}), these will be preserved as &#xa;    named parse results.&#xa;&#xa;    Example::&#xa;        realnum = Regex(r""[+-]?\d+\.\d*"")&#xa;        date = Regex(r'(?P<year>\d{4})-(?P<month>\d\d?)-(?P<day>\d\d?)')&#xa;        # ref: http://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression&#xa;        roman = Regex(r""M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})"")&#xa;    """"""&#xa;    compiledREtype = type(re.compile(""[A-Z]""))&#xa;    def __init__( self, pattern, flags=0):&#xa;        """"""The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.""""""&#xa;        super(Regex,self).__init__()&#xa;&#xa;        if isinstance(pattern, basestring):&#xa;            if not pattern:&#xa;                warnings.warn(""null string passed to Regex; use Empty() instead"",&#xa;                        SyntaxWarning, stacklevel=2)&#xa;&#xa;            self.pattern = pattern&#xa;            self.flags = flags&#xa;&#xa;            try:&#xa;                self.re = re.compile(self.pattern, self.flags)&#xa;                self.reString = self.pattern&#xa;            except sre_constants.error:&#xa;                warnings.warn(""invalid pattern (%s) passed to Regex"" % pattern,&#xa;                    SyntaxWarning, stacklevel=2)&#xa;                raise&#xa;&#xa;        elif isinstance(pattern, Regex.compiledREtype):&#xa;            self.re = pattern&#xa;            self.pattern = \&#xa;            self.reString = str(pattern)&#xa;            self.flags = flags&#xa;            &#xa;        else:&#xa;            raise ValueError(""Regex may only be constructed with a string or a compiled RE object"")&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = self.re.match(instring,loc)&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        d = result.groupdict()&#xa;        ret = ParseResults(result.group())&#xa;        if d:&#xa;            for k in d:&#xa;                ret[k] = d[k]&#xa;        return loc,ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Regex,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""Re:(%s)"" % repr(self.pattern)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class QuotedString(Token):&#xa;    r""""""&#xa;    Token for matching strings that are delimited by quoting characters.&#xa;    &#xa;    Defined with the following parameters:&#xa;        - quoteChar - string of one or more characters defining the quote delimiting string&#xa;        - escChar - character to escape quotes, typically backslash (default=C{None})&#xa;        - escQuote - special quote sequence to escape an embedded quote string (such as SQL's """" to escape an embedded "") (default=C{None})&#xa;        - multiline - boolean indicating whether quotes can span multiple lines (default=C{False})&#xa;        - unquoteResults - boolean indicating whether the matched text should be unquoted (default=C{True})&#xa;        - endQuoteChar - string of one or more characters defining the end of the quote delimited string (default=C{None} => same as quoteChar)&#xa;        - convertWhitespaceEscapes - convert escaped whitespace (C{'\t'}, C{'\n'}, etc.) to actual whitespace (default=C{True})&#xa;&#xa;    Example::&#xa;        qs = QuotedString('""')&#xa;        print(qs.searchString('lsjdf ""This is the quote"" sldjf'))&#xa;        complex_qs = QuotedString('{{', endQuoteChar='}}')&#xa;        print(complex_qs.searchString('lsjdf {{This is the ""quote""}} sldjf'))&#xa;        sql_qs = QuotedString('""', escQuote='""""')&#xa;        print(sql_qs.searchString('lsjdf ""This is the quote with """"embedded"""" quotes"" sldjf'))&#xa;    prints::&#xa;        [['This is the quote']]&#xa;        [['This is the ""quote""']]&#xa;        [['This is the quote with ""embedded"" quotes']]&#xa;    """"""&#xa;    def __init__( self, quoteChar, escChar=None, escQuote=None, multiline=False, unquoteResults=True, endQuoteChar=None, convertWhitespaceEscapes=True):&#xa;        super(QuotedString,self).__init__()&#xa;&#xa;        # remove white space from quote chars - wont work anyway&#xa;        quoteChar = quoteChar.strip()&#xa;        if not quoteChar:&#xa;            warnings.warn(""quoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;            raise SyntaxError()&#xa;&#xa;        if endQuoteChar is None:&#xa;            endQuoteChar = quoteChar&#xa;        else:&#xa;            endQuoteChar = endQuoteChar.strip()&#xa;            if not endQuoteChar:&#xa;                warnings.warn(""endQuoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;                raise SyntaxError()&#xa;&#xa;        self.quoteChar = quoteChar&#xa;        self.quoteCharLen = len(quoteChar)&#xa;        self.firstQuoteChar = quoteChar[0]&#xa;        self.endQuoteChar = endQuoteChar&#xa;        self.endQuoteCharLen = len(endQuoteChar)&#xa;        self.escChar = escChar&#xa;        self.escQuote = escQuote&#xa;        self.unquoteResults = unquoteResults&#xa;        self.convertWhitespaceEscapes = convertWhitespaceEscapes&#xa;&#xa;        if multiline:&#xa;            self.flags = re.MULTILINE | re.DOTALL&#xa;            self.pattern = r'%s(?:[^%s%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        else:&#xa;            self.flags = 0&#xa;            self.pattern = r'%s(?:[^%s\n\r%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        if len(self.endQuoteChar) > 1:&#xa;            self.pattern += (&#xa;                '|(?:' + ')|(?:'.join(""%s[^%s]"" % (re.escape(self.endQuoteChar[:i]),&#xa;                                               _escapeRegexRangeChars(self.endQuoteChar[i]))&#xa;                                    for i in range(len(self.endQuoteChar)-1,0,-1)) + ')'&#xa;                )&#xa;        if escQuote:&#xa;            self.pattern += (r'|(?:%s)' % re.escape(escQuote))&#xa;        if escChar:&#xa;            self.pattern += (r'|(?:%s.)' % re.escape(escChar))&#xa;            self.escCharReplacePattern = re.escape(self.escChar)+""(.)""&#xa;        self.pattern += (r')*%s' % re.escape(self.endQuoteChar))&#xa;&#xa;        try:&#xa;            self.re = re.compile(self.pattern, self.flags)&#xa;            self.reString = self.pattern&#xa;        except sre_constants.error:&#xa;            warnings.warn(""invalid pattern (%s) passed to Regex"" % self.pattern,&#xa;                SyntaxWarning, stacklevel=2)&#xa;            raise&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = instring[loc] == self.firstQuoteChar and self.re.match(instring,loc) or None&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        ret = result.group()&#xa;&#xa;        if self.unquoteResults:&#xa;&#xa;            # strip off quotes&#xa;            ret = ret[self.quoteCharLen:-self.endQuoteCharLen]&#xa;&#xa;            if isinstance(ret,basestring):&#xa;                # replace escaped whitespace&#xa;                if '\\' in ret and self.convertWhitespaceEscapes:&#xa;                    ws_map = {&#xa;                        r'\t' : '\t',&#xa;                        r'\n' : '\n',&#xa;                        r'\f' : '\f',&#xa;                        r'\r' : '\r',&#xa;                    }&#xa;                    for wslit,wschar in ws_map.items():&#xa;                        ret = ret.replace(wslit, wschar)&#xa;&#xa;                # replace escaped characters&#xa;                if self.escChar:&#xa;                    ret = re.sub(self.escCharReplacePattern,""\g<1>"",ret)&#xa;&#xa;                # replace escaped quotes&#xa;                if self.escQuote:&#xa;                    ret = ret.replace(self.escQuote, self.endQuoteChar)&#xa;&#xa;        return loc, ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(QuotedString,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""quoted string, starting with %s ending with %s"" % (self.quoteChar, self.endQuoteChar)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class CharsNotIn(Token):&#xa;    """"""&#xa;    Token for matching words composed of characters I{not} in a given set (will&#xa;    include whitespace in matched characters if not listed in the provided exclusion set - see example).&#xa;    Defined with string containing all disallowed characters, and an optional&#xa;    minimum, maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction.&#xa;&#xa;    Example::&#xa;        # define a comma-separated-value as anything that is not a ','&#xa;        csv_value = CharsNotIn(',')&#xa;        print(delimitedList(csv_value).parseString(""dkls,lsdkjf,s12 34,@!#,213""))&#xa;    prints::&#xa;        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']&#xa;    """"""&#xa;    def __init__( self, notChars, min=1, max=0, exact=0 ):&#xa;        super(CharsNotIn,self).__init__()&#xa;        self.skipWhitespace = False&#xa;        self.notChars = notChars&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(CharsNotIn()) if zero-length char group is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = ( self.minLen == 0 )&#xa;        self.mayIndexError = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[loc] in self.notChars:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        notchars = self.notChars&#xa;        maxlen = min( start+self.maxLen, len(instring) )&#xa;        while loc < maxlen and \&#xa;              (instring[loc] not in notchars):&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(CharsNotIn, self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            if len(self.notChars) > 4:&#xa;                self.strRepr = ""!W:(%s...)"" % self.notChars[:4]&#xa;            else:&#xa;                self.strRepr = ""!W:(%s)"" % self.notChars&#xa;&#xa;        return self.strRepr&#xa;&#xa;class White(Token):&#xa;    """"""&#xa;    Special matching class for matching whitespace.  Normally, whitespace is ignored&#xa;    by pyparsing grammars.  This class is included when some whitespace structures&#xa;    are significant.  Define with a string containing the whitespace characters to be&#xa;    matched; default is C{"" \\t\\r\\n""}.  Also takes optional C{min}, C{max}, and C{exact} arguments,&#xa;    as defined for the C{L{Word}} class.&#xa;    """"""&#xa;    whiteStrs = {&#xa;        "" "" : ""<SPC>"",&#xa;        ""\t"": ""<TAB>"",&#xa;        ""\n"": ""<LF>"",&#xa;        ""\r"": ""<CR>"",&#xa;        ""\f"": ""<FF>"",&#xa;        }&#xa;    def __init__(self, ws="" \t\r\n"", min=1, max=0, exact=0):&#xa;        super(White,self).__init__()&#xa;        self.matchWhite = ws&#xa;        self.setWhitespaceChars( """".join(c for c in self.whiteChars if c not in self.matchWhite) )&#xa;        #~ self.leaveWhitespace()&#xa;        self.name = ("""".join(White.whiteStrs[c] for c in self.matchWhite))&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if not(instring[ loc ] in self.matchWhite):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        start = loc&#xa;        loc += 1&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, len(instring) )&#xa;        while loc < maxloc and instring[loc] in self.matchWhite:&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;&#xa;class _PositionToken(Token):&#xa;    def __init__( self ):&#xa;        super(_PositionToken,self).__init__()&#xa;        self.name=self.__class__.__name__&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;class GoToColumn(_PositionToken):&#xa;    """"""&#xa;    Token to advance to a specific column of input text; useful for tabular report scraping.&#xa;    """"""&#xa;    def __init__( self, colno ):&#xa;        super(GoToColumn,self).__init__()&#xa;        self.col = colno&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if col(loc,instring) != self.col:&#xa;            instrlen = len(instring)&#xa;            if self.ignoreExprs:&#xa;                loc = self._skipIgnorables( instring, loc )&#xa;            while loc < instrlen and instring[loc].isspace() and col( loc, instring ) != self.col :&#xa;                loc += 1&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        thiscol = col( loc, instring )&#xa;        if thiscol > self.col:&#xa;            raise ParseException( instring, loc, ""Text not in expected column"", self )&#xa;        newloc = loc + self.col - thiscol&#xa;        ret = instring[ loc: newloc ]&#xa;        return newloc, ret&#xa;&#xa;&#xa;class LineStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of a line within the parse string&#xa;    &#xa;    Example::&#xa;    &#xa;        test = '''\&#xa;        AAA this line&#xa;        AAA and this line&#xa;          AAA but not this one&#xa;        B AAA and definitely not this one&#xa;        '''&#xa;&#xa;        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):&#xa;            print(t)&#xa;    &#xa;    Prints::&#xa;        ['AAA', ' this line']&#xa;        ['AAA', ' and this line']    &#xa;&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineStart,self).__init__()&#xa;        self.errmsg = ""Expected start of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if col(loc, instring) == 1:&#xa;            return loc, []&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class LineEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of a line within the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineEnd,self).__init__()&#xa;        self.setWhitespaceChars( ParserElement.DEFAULT_WHITE_CHARS.replace(""\n"","""") )&#xa;        self.errmsg = ""Expected end of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc<len(instring):&#xa;            if instring[loc] == ""\n"":&#xa;                return loc+1, ""\n""&#xa;            else:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class StringStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringStart,self).__init__()&#xa;        self.errmsg = ""Expected start of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            # see if entire string up to here is just whitespace and ignoreables&#xa;            if loc != self.preParse( instring, 0 ):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class StringEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringEnd,self).__init__()&#xa;        self.errmsg = ""Expected end of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc < len(instring):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        elif loc > len(instring):&#xa;            return loc, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class WordStart(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the beginning of a Word, and&#xa;    is not preceded by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordStart(alphanums)}. C{WordStart} will also match at the beginning of&#xa;    the string being parsed, or at the beginning of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordStart,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.errmsg = ""Not at the start of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            if (instring[loc-1] in self.wordChars or&#xa;                instring[loc] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class WordEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the end of a Word, and&#xa;    is not followed by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordEnd(alphanums)}. C{WordEnd} will also match at the end of&#xa;    the string being parsed, or at the end of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordEnd,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.skipWhitespace = False&#xa;        self.errmsg = ""Not at the end of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        instrlen = len(instring)&#xa;        if instrlen>0 and loc<instrlen:&#xa;            if (instring[loc] in self.wordChars or&#xa;                instring[loc-1] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;&#xa;class ParseExpression(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of ParserElement, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(ParseExpression,self).__init__(savelist)&#xa;        if isinstance( exprs, _generatorType ):&#xa;            exprs = list(exprs)&#xa;&#xa;        if isinstance( exprs, basestring ):&#xa;            self.exprs = [ ParserElement._literalStringClass( exprs ) ]&#xa;        elif isinstance( exprs, collections.Iterable ):&#xa;            exprs = list(exprs)&#xa;            # if sequence of strings provided, wrap with Literal&#xa;            if all(isinstance(expr, basestring) for expr in exprs):&#xa;                exprs = map(ParserElement._literalStringClass, exprs)&#xa;            self.exprs = list(exprs)&#xa;        else:&#xa;            try:&#xa;                self.exprs = list( exprs )&#xa;            except TypeError:&#xa;                self.exprs = [ exprs ]&#xa;        self.callPreparse = False&#xa;&#xa;    def __getitem__( self, i ):&#xa;        return self.exprs[i]&#xa;&#xa;    def append( self, other ):&#xa;        self.exprs.append( other )&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""Extends C{leaveWhitespace} defined in base class, and also invokes C{leaveWhitespace} on&#xa;           all contained expressions.""""""&#xa;        self.skipWhitespace = False&#xa;        self.exprs = [ e.copy() for e in self.exprs ]&#xa;        for e in self.exprs:&#xa;            e.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseExpression, self).ignore( other )&#xa;                for e in self.exprs:&#xa;                    e.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseExpression, self).ignore( other )&#xa;            for e in self.exprs:&#xa;                e.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseExpression,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.exprs) )&#xa;        return self.strRepr&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseExpression,self).streamline()&#xa;&#xa;        for e in self.exprs:&#xa;            e.streamline()&#xa;&#xa;        # collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )&#xa;        # but only if there are no parse actions or resultsNames on the nested And's&#xa;        # (likewise for Or's and MatchFirst's)&#xa;        if ( len(self.exprs) == 2 ):&#xa;            other = self.exprs[0]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = other.exprs[:] + [ self.exprs[1] ]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;            other = self.exprs[-1]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = self.exprs[:-1] + other.exprs[:]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;        self.errmsg = ""Expected "" + _ustr(self)&#xa;        &#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        ret = super(ParseExpression,self).setResultsName(name,listAllMatches)&#xa;        return ret&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        for e in self.exprs:&#xa;            e.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;        &#xa;    def copy(self):&#xa;        ret = super(ParseExpression,self).copy()&#xa;        ret.exprs = [e.copy() for e in self.exprs]&#xa;        return ret&#xa;&#xa;class And(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found in the given order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'+'} operator.&#xa;    May also be constructed using the C{'-'} operator, which will suppress backtracking.&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        name_expr = OneOrMore(Word(alphas))&#xa;&#xa;        expr = And([integer(""id""),name_expr(""name""),integer(""age"")])&#xa;        # more easily written as:&#xa;        expr = integer(""id"") + name_expr(""name"") + integer(""age"")&#xa;    """"""&#xa;&#xa;    class _ErrorStop(Empty):&#xa;        def __init__(self, *args, **kwargs):&#xa;            super(And._ErrorStop,self).__init__(*args, **kwargs)&#xa;            self.name = '-'&#xa;            self.leaveWhitespace()&#xa;&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(And,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.setWhitespaceChars( self.exprs[0].whiteChars )&#xa;        self.skipWhitespace = self.exprs[0].skipWhitespace&#xa;        self.callPreparse = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        # pass False as last arg to _parse for first element, since we already&#xa;        # pre-parsed the string as part of our And pre-parsing&#xa;        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )&#xa;        errorStop = False&#xa;        for e in self.exprs[1:]:&#xa;            if isinstance(e, And._ErrorStop):&#xa;                errorStop = True&#xa;                continue&#xa;            if errorStop:&#xa;                try:&#xa;                    loc, exprtokens = e._parse( instring, loc, doActions )&#xa;                except ParseSyntaxException:&#xa;                    raise&#xa;                except ParseBaseException as pe:&#xa;                    pe.__traceback__ = None&#xa;                    raise ParseSyntaxException._from_exception(pe)&#xa;                except IndexError:&#xa;                    raise ParseSyntaxException(instring, len(instring), self.errmsg, self)&#xa;            else:&#xa;                loc, exprtokens = e._parse( instring, loc, doActions )&#xa;            if exprtokens or exprtokens.haskeys():&#xa;                resultlist += exprtokens&#xa;        return loc, resultlist&#xa;&#xa;    def __iadd__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #And( [ self, other ] )&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;            if not e.mayReturnEmpty:&#xa;                break&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Or(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the expression that matches the longest string will be used.&#xa;    May be constructed using the C{'^'} operator.&#xa;&#xa;    Example::&#xa;        # construct Or using '^' operator&#xa;        &#xa;        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789""))&#xa;    prints::&#xa;        [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(Or,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        matches = []&#xa;        for e in self.exprs:&#xa;            try:&#xa;                loc2 = e.tryParse( instring, loc )&#xa;            except ParseException as err:&#xa;                err.__traceback__ = None&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;            else:&#xa;                # save match among all matches, to retry longest to shortest&#xa;                matches.append((loc2, e))&#xa;&#xa;        if matches:&#xa;            matches.sort(key=lambda x: -x[0])&#xa;            for _,e in matches:&#xa;                try:&#xa;                    return e._parse( instring, loc, doActions )&#xa;                except ParseException as err:&#xa;                    err.__traceback__ = None&#xa;                    if err.loc > maxExcLoc:&#xa;                        maxException = err&#xa;                        maxExcLoc = err.loc&#xa;&#xa;        if maxException is not None:&#xa;            maxException.msg = self.errmsg&#xa;            raise maxException&#xa;        else:&#xa;            raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;&#xa;    def __ixor__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #Or( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" ^ "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class MatchFirst(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the first one listed is the one that will match.&#xa;    May be constructed using the C{'|'} operator.&#xa;&#xa;    Example::&#xa;        # construct MatchFirst using '|' operator&#xa;        &#xa;        # watch the order of expressions to match&#xa;        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789"")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]&#xa;&#xa;        # put more selective expression first&#xa;        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)&#xa;        print(number.searchString(""123 3.1416 789"")) #  Better -> [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(MatchFirst,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        for e in self.exprs:&#xa;            try:&#xa;                ret = e._parse( instring, loc, doActions )&#xa;                return ret&#xa;            except ParseException as err:&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;&#xa;        # only got here if no expression matched, raise exception for match that made it the furthest&#xa;        else:&#xa;            if maxException is not None:&#xa;                maxException.msg = self.errmsg&#xa;                raise maxException&#xa;            else:&#xa;                raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;    def __ior__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #MatchFirst( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" | "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class Each(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found, but in any order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'&'} operator.&#xa;&#xa;    Example::&#xa;        color = oneOf(""RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN"")&#xa;        shape_type = oneOf(""SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON"")&#xa;        integer = Word(nums)&#xa;        shape_attr = ""shape:"" + shape_type(""shape"")&#xa;        posn_attr = ""posn:"" + Group(integer(""x"") + ',' + integer(""y""))(""posn"")&#xa;        color_attr = ""color:"" + color(""color"")&#xa;        size_attr = ""size:"" + integer(""size"")&#xa;&#xa;        # use Each (using operator '&') to accept attributes in any order &#xa;        # (shape and posn are required, color and size are optional)&#xa;        shape_spec = shape_attr & posn_attr & Optional(color_attr) & Optional(size_attr)&#xa;&#xa;        shape_spec.runTests('''&#xa;            shape: SQUARE color: BLACK posn: 100, 120&#xa;            shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;            color:GREEN size:20 shape:TRIANGLE posn:20,40&#xa;            '''&#xa;            )&#xa;    prints::&#xa;        shape: SQUARE color: BLACK posn: 100, 120&#xa;        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]&#xa;        - color: BLACK&#xa;        - posn: ['100', ',', '120']&#xa;          - x: 100&#xa;          - y: 120&#xa;        - shape: SQUARE&#xa;&#xa;&#xa;        shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]&#xa;        - color: BLUE&#xa;        - posn: ['50', ',', '80']&#xa;          - x: 50&#xa;          - y: 80&#xa;        - shape: CIRCLE&#xa;        - size: 50&#xa;&#xa;&#xa;        color: GREEN size: 20 shape: TRIANGLE posn: 20,40&#xa;        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]&#xa;        - color: GREEN&#xa;        - posn: ['20', ',', '40']&#xa;          - x: 20&#xa;          - y: 40&#xa;        - shape: TRIANGLE&#xa;        - size: 20&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(Each,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.skipWhitespace = True&#xa;        self.initExprGroups = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.initExprGroups:&#xa;            self.opt1map = dict((id(e.expr),e) for e in self.exprs if isinstance(e,Optional))&#xa;            opt1 = [ e.expr for e in self.exprs if isinstance(e,Optional) ]&#xa;            opt2 = [ e for e in self.exprs if e.mayReturnEmpty and not isinstance(e,Optional)]&#xa;            self.optionals = opt1 + opt2&#xa;            self.multioptionals = [ e.expr for e in self.exprs if isinstance(e,ZeroOrMore) ]&#xa;            self.multirequired = [ e.expr for e in self.exprs if isinstance(e,OneOrMore) ]&#xa;            self.required = [ e for e in self.exprs if not isinstance(e,(Optional,ZeroOrMore,OneOrMore)) ]&#xa;            self.required += self.multirequired&#xa;            self.initExprGroups = False&#xa;        tmpLoc = loc&#xa;        tmpReqd = self.required[:]&#xa;        tmpOpt  = self.optionals[:]&#xa;        matchOrder = []&#xa;&#xa;        keepMatching = True&#xa;        while keepMatching:&#xa;            tmpExprs = tmpReqd + tmpOpt + self.multioptionals + self.multirequired&#xa;            failed = []&#xa;            for e in tmpExprs:&#xa;                try:&#xa;                    tmpLoc = e.tryParse( instring, tmpLoc )&#xa;                except ParseException:&#xa;                    failed.append(e)&#xa;                else:&#xa;                    matchOrder.append(self.opt1map.get(id(e),e))&#xa;                    if e in tmpReqd:&#xa;                        tmpReqd.remove(e)&#xa;                    elif e in tmpOpt:&#xa;                        tmpOpt.remove(e)&#xa;            if len(failed) == len(tmpExprs):&#xa;                keepMatching = False&#xa;&#xa;        if tmpReqd:&#xa;            missing = "", "".join(_ustr(e) for e in tmpReqd)&#xa;            raise ParseException(instring,loc,""Missing one or more required elements (%s)"" % missing )&#xa;&#xa;        # add any unmatched Optionals, in case they have default values defined&#xa;        matchOrder += [e for e in self.exprs if isinstance(e,Optional) and e.expr in tmpOpt]&#xa;&#xa;        resultlist = []&#xa;        for e in matchOrder:&#xa;            loc,results = e._parse(instring,loc,doActions)&#xa;            resultlist.append(results)&#xa;&#xa;        finalResults = sum(resultlist, ParseResults([]))&#xa;        return loc, finalResults&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" & "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class ParseElementEnhance(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of C{ParserElement}, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(ParseElementEnhance,self).__init__(savelist)&#xa;        if isinstance( expr, basestring ):&#xa;            if issubclass(ParserElement._literalStringClass, Token):&#xa;                expr = ParserElement._literalStringClass(expr)&#xa;            else:&#xa;                expr = ParserElement._literalStringClass(Literal(expr))&#xa;        self.expr = expr&#xa;        self.strRepr = None&#xa;        if expr is not None:&#xa;            self.mayIndexError = expr.mayIndexError&#xa;            self.mayReturnEmpty = expr.mayReturnEmpty&#xa;            self.setWhitespaceChars( expr.whiteChars )&#xa;            self.skipWhitespace = expr.skipWhitespace&#xa;            self.saveAsList = expr.saveAsList&#xa;            self.callPreparse = expr.callPreparse&#xa;            self.ignoreExprs.extend(expr.ignoreExprs)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr is not None:&#xa;            return self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        else:&#xa;            raise ParseException("""",loc,self.errmsg,self)&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        self.expr = self.expr.copy()&#xa;        if self.expr is not None:&#xa;            self.expr.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseElementEnhance, self).ignore( other )&#xa;                if self.expr is not None:&#xa;                    self.expr.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseElementEnhance, self).ignore( other )&#xa;            if self.expr is not None:&#xa;                self.expr.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseElementEnhance,self).streamline()&#xa;        if self.expr is not None:&#xa;            self.expr.streamline()&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        if self in parseElementList:&#xa;            raise RecursiveGrammarException( parseElementList+[self] )&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        if self.expr is not None:&#xa;            self.expr.checkRecursion( subRecCheckList )&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        if self.expr is not None:&#xa;            self.expr.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseElementEnhance,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None and self.expr is not None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.expr) )&#xa;        return self.strRepr&#xa;&#xa;&#xa;class FollowedBy(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead matching of the given parse expression.  C{FollowedBy}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression matches at the current&#xa;    position.  C{FollowedBy} always returns a null token list.&#xa;&#xa;    Example::&#xa;        # use FollowedBy to match a label only if it is followed by a ':'&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        OneOrMore(attr_expr).parseString(""shape: SQUARE color: BLACK posn: upper left"").pprint()&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(FollowedBy,self).__init__(expr)&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self.expr.tryParse( instring, loc )&#xa;        return loc, []&#xa;&#xa;&#xa;class NotAny(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead to disallow matching with the given parse expression.  C{NotAny}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression does I{not} match at the current&#xa;    position.  Also, C{NotAny} does I{not} skip over leading whitespace. C{NotAny}&#xa;    always returns a null token list.  May be constructed using the '~' operator.&#xa;&#xa;    Example::&#xa;        &#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(NotAny,self).__init__(expr)&#xa;        #~ self.leaveWhitespace()&#xa;        self.skipWhitespace = False  # do NOT use self.leaveWhitespace(), don't want to propagate to exprs&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Found unwanted token, ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr.canParseNext(instring, loc):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""~{"" + _ustr(self.expr) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _MultipleMatch(ParseElementEnhance):&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(_MultipleMatch, self).__init__(expr)&#xa;        self.saveAsList = True&#xa;        ender = stopOn&#xa;        if isinstance(ender, basestring):&#xa;            ender = ParserElement._literalStringClass(ender)&#xa;        self.not_ender = ~ender if ender is not None else None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self_expr_parse = self.expr._parse&#xa;        self_skip_ignorables = self._skipIgnorables&#xa;        check_ender = self.not_ender is not None&#xa;        if check_ender:&#xa;            try_not_ender = self.not_ender.tryParse&#xa;        &#xa;        # must be at least one (but first see if we are the stopOn sentinel;&#xa;        # if so, fail)&#xa;        if check_ender:&#xa;            try_not_ender(instring, loc)&#xa;        loc, tokens = self_expr_parse( instring, loc, doActions, callPreParse=False )&#xa;        try:&#xa;            hasIgnoreExprs = (not not self.ignoreExprs)&#xa;            while 1:&#xa;                if check_ender:&#xa;                    try_not_ender(instring, loc)&#xa;                if hasIgnoreExprs:&#xa;                    preloc = self_skip_ignorables( instring, loc )&#xa;                else:&#xa;                    preloc = loc&#xa;                loc, tmptokens = self_expr_parse( instring, preloc, doActions )&#xa;                if tmptokens or tmptokens.haskeys():&#xa;                    tokens += tmptokens&#xa;        except (ParseException,IndexError):&#xa;            pass&#xa;&#xa;        return loc, tokens&#xa;        &#xa;class OneOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Repetition of one or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match one or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: BLACK""&#xa;        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]&#xa;&#xa;        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]&#xa;        &#xa;        # could also be written as&#xa;        (attr_expr * (1,)).parseString(text).pprint()&#xa;    """"""&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + _ustr(self.expr) + ""}...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class ZeroOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Optional repetition of zero or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example: similar to L{OneOrMore}&#xa;    """"""&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(ZeroOrMore,self).__init__(expr, stopOn=stopOn)&#xa;        self.mayReturnEmpty = True&#xa;        &#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)&#xa;        except (ParseException,IndexError):&#xa;            return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _NullToken(object):&#xa;    def __bool__(self):&#xa;        return False&#xa;    __nonzero__ = __bool__&#xa;    def __str__(self):&#xa;        return """"&#xa;&#xa;_optionalNotMatched = _NullToken()&#xa;class Optional(ParseElementEnhance):&#xa;    """"""&#xa;    Optional matching of the given expression.&#xa;&#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - default (optional) - value to be returned if the optional expression is not found.&#xa;&#xa;    Example::&#xa;        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier&#xa;        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))&#xa;        zip.runTests('''&#xa;            # traditional ZIP code&#xa;            12345&#xa;            &#xa;            # ZIP+4 form&#xa;            12101-0001&#xa;            &#xa;            # invalid ZIP&#xa;            98765-&#xa;            ''')&#xa;    prints::&#xa;        # traditional ZIP code&#xa;        12345&#xa;        ['12345']&#xa;&#xa;        # ZIP+4 form&#xa;        12101-0001&#xa;        ['12101-0001']&#xa;&#xa;        # invalid ZIP&#xa;        98765-&#xa;             ^&#xa;        FAIL: Expected end of text (at char 5), (line:1, col:6)&#xa;    """"""&#xa;    def __init__( self, expr, default=_optionalNotMatched ):&#xa;        super(Optional,self).__init__( expr, savelist=False )&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.defaultValue = default&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        except (ParseException,IndexError):&#xa;            if self.defaultValue is not _optionalNotMatched:&#xa;                if self.expr.resultsName:&#xa;                    tokens = ParseResults([ self.defaultValue ])&#xa;                    tokens[self.expr.resultsName] = self.defaultValue&#xa;                else:&#xa;                    tokens = [ self.defaultValue ]&#xa;            else:&#xa;                tokens = []&#xa;        return loc, tokens&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class SkipTo(ParseElementEnhance):&#xa;    """"""&#xa;    Token for skipping over all undefined text until the matched expression is found.&#xa;&#xa;    Parameters:&#xa;     - expr - target expression marking the end of the data to be skipped&#xa;     - include - (default=C{False}) if True, the target expression is also parsed &#xa;          (the skipped text and target expression are returned as a 2-element list).&#xa;     - ignore - (default=C{None}) used to define grammars (typically quoted strings and &#xa;          comments) that might contain false matches to the target expression&#xa;     - failOn - (default=C{None}) define expressions that are not allowed to be &#xa;          included in the skipped test; if found before the target expression is found, &#xa;          the SkipTo is not a match&#xa;&#xa;    Example::&#xa;        report = '''&#xa;            Outstanding Issues Report - 1 Jan 2000&#xa;&#xa;               # | Severity | Description                               |  Days Open&#xa;            -----+----------+-------------------------------------------+-----------&#xa;             101 | Critical | Intermittent system crash                 |          6&#xa;              94 | Cosmetic | Spelling error on Login ('log|n')         |         14&#xa;              79 | Minor    | System slow when running too many reports |         47&#xa;            '''&#xa;        integer = Word(nums)&#xa;        SEP = Suppress('|')&#xa;        # use SkipTo to simply match everything up until the next SEP&#xa;        # - ignore quoted strings, so that a '|' character inside a quoted string does not match&#xa;        # - parse action will call token.strip() for each matched token, i.e., the description body&#xa;        string_data = SkipTo(SEP, ignore=quotedString)&#xa;        string_data.setParseAction(tokenMap(str.strip))&#xa;        ticket_expr = (integer(""issue_num"") + SEP &#xa;                      + string_data(""sev"") + SEP &#xa;                      + string_data(""desc"") + SEP &#xa;                      + integer(""days_open""))&#xa;        &#xa;        for tkt in ticket_expr.searchString(report):&#xa;            print tkt.dump()&#xa;    prints::&#xa;        ['101', 'Critical', 'Intermittent system crash', '6']&#xa;        - days_open: 6&#xa;        - desc: Intermittent system crash&#xa;        - issue_num: 101&#xa;        - sev: Critical&#xa;        ['94', 'Cosmetic', ""Spelling error on Login ('log|n')"", '14']&#xa;        - days_open: 14&#xa;        - desc: Spelling error on Login ('log|n')&#xa;        - issue_num: 94&#xa;        - sev: Cosmetic&#xa;        ['79', 'Minor', 'System slow when running too many reports', '47']&#xa;        - days_open: 47&#xa;        - desc: System slow when running too many reports&#xa;        - issue_num: 79&#xa;        - sev: Minor&#xa;    """"""&#xa;    def __init__( self, other, include=False, ignore=None, failOn=None ):&#xa;        super( SkipTo, self ).__init__( other )&#xa;        self.ignoreExpr = ignore&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.includeMatch = include&#xa;        self.asList = False&#xa;        if isinstance(failOn, basestring):&#xa;            self.failOn = ParserElement._literalStringClass(failOn)&#xa;        else:&#xa;            self.failOn = failOn&#xa;        self.errmsg = ""No match found for ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        startloc = loc&#xa;        instrlen = len(instring)&#xa;        expr = self.expr&#xa;        expr_parse = self.expr._parse&#xa;        self_failOn_canParseNext = self.failOn.canParseNext if self.failOn is not None else None&#xa;        self_ignoreExpr_tryParse = self.ignoreExpr.tryParse if self.ignoreExpr is not None else None&#xa;        &#xa;        tmploc = loc&#xa;        while tmploc <= instrlen:&#xa;            if self_failOn_canParseNext is not None:&#xa;                # break if failOn expression matches&#xa;                if self_failOn_canParseNext(instring, tmploc):&#xa;                    break&#xa;                    &#xa;            if self_ignoreExpr_tryParse is not None:&#xa;                # advance past ignore expressions&#xa;                while 1:&#xa;                    try:&#xa;                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)&#xa;                    except ParseBaseException:&#xa;                        break&#xa;            &#xa;            try:&#xa;                expr_parse(instring, tmploc, doActions=False, callPreParse=False)&#xa;            except (ParseException, IndexError):&#xa;                # no match, advance loc in string&#xa;                tmploc += 1&#xa;            else:&#xa;                # matched skipto expr, done&#xa;                break&#xa;&#xa;        else:&#xa;            # ran off the end of the input string without matching skipto expr, fail&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        # build up return values&#xa;        loc = tmploc&#xa;        skiptext = instring[startloc:loc]&#xa;        skipresult = ParseResults(skiptext)&#xa;        &#xa;        if self.includeMatch:&#xa;            loc, mat = expr_parse(instring,loc,doActions,callPreParse=False)&#xa;            skipresult += mat&#xa;&#xa;        return loc, skipresult&#xa;&#xa;class Forward(ParseElementEnhance):&#xa;    """"""&#xa;    Forward declaration of an expression to be defined later -&#xa;    used for recursive grammars, such as algebraic infix notation.&#xa;    When the expression is known, it is assigned to the C{Forward} variable using the '<<' operator.&#xa;&#xa;    Note: take care when assigning to C{Forward} not to overlook precedence of operators.&#xa;    Specifically, '|' has a lower precedence than '<<', so that::&#xa;        fwdExpr << a | b | c&#xa;    will actually be evaluated as::&#xa;        (fwdExpr << a) | b | c&#xa;    thereby leaving b and c out as parseable alternatives.  It is recommended that you&#xa;    explicitly group the values inserted into the C{Forward}::&#xa;        fwdExpr << (a | b | c)&#xa;    Converting to use the '<<=' operator instead will avoid this problem.&#xa;&#xa;    See L{ParseResults.pprint} for an example of a recursive parser created using&#xa;    C{Forward}.&#xa;    """"""&#xa;    def __init__( self, other=None ):&#xa;        super(Forward,self).__init__( other, savelist=False )&#xa;&#xa;    def __lshift__( self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass(other)&#xa;        self.expr = other&#xa;        self.strRepr = None&#xa;        self.mayIndexError = self.expr.mayIndexError&#xa;        self.mayReturnEmpty = self.expr.mayReturnEmpty&#xa;        self.setWhitespaceChars( self.expr.whiteChars )&#xa;        self.skipWhitespace = self.expr.skipWhitespace&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.ignoreExprs.extend(self.expr.ignoreExprs)&#xa;        return self&#xa;        &#xa;    def __ilshift__(self, other):&#xa;        return self << other&#xa;    &#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        if not self.streamlined:&#xa;            self.streamlined = True&#xa;            if self.expr is not None:&#xa;                self.expr.streamline()&#xa;        return self&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        if self not in validateTrace:&#xa;            tmp = validateTrace[:]+[self]&#xa;            if self.expr is not None:&#xa;                self.expr.validate(tmp)&#xa;        self.checkRecursion([])&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;        return self.__class__.__name__ + "": ...""&#xa;&#xa;        # stubbed out for now - creates awful memory and perf issues&#xa;        self._revertClass = self.__class__&#xa;        self.__class__ = _ForwardNoRecurse&#xa;        try:&#xa;            if self.expr is not None:&#xa;                retString = _ustr(self.expr)&#xa;            else:&#xa;                retString = ""None""&#xa;        finally:&#xa;            self.__class__ = self._revertClass&#xa;        return self.__class__.__name__ + "": "" + retString&#xa;&#xa;    def copy(self):&#xa;        if self.expr is not None:&#xa;            return super(Forward,self).copy()&#xa;        else:&#xa;            ret = Forward()&#xa;            ret <<= self&#xa;            return ret&#xa;&#xa;class _ForwardNoRecurse(Forward):&#xa;    def __str__( self ):&#xa;        return ""...""&#xa;&#xa;class TokenConverter(ParseElementEnhance):&#xa;    """"""&#xa;    Abstract subclass of C{ParseExpression}, for converting parsed results.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(TokenConverter,self).__init__( expr )#, savelist )&#xa;        self.saveAsList = False&#xa;&#xa;class Combine(TokenConverter):&#xa;    """"""&#xa;    Converter to concatenate all matching tokens to a single string.&#xa;    By default, the matching patterns must also be contiguous in the input string;&#xa;    this can be disabled by specifying C{'adjacent=False'} in the constructor.&#xa;&#xa;    Example::&#xa;        real = Word(nums) + '.' + Word(nums)&#xa;        print(real.parseString('3.1416')) # -> ['3', '.', '1416']&#xa;        # will also erroneously match the following&#xa;        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']&#xa;&#xa;        real = Combine(Word(nums) + '.' + Word(nums))&#xa;        print(real.parseString('3.1416')) # -> ['3.1416']&#xa;        # no match when there are internal spaces&#xa;        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)&#xa;    """"""&#xa;    def __init__( self, expr, joinString="""", adjacent=True ):&#xa;        super(Combine,self).__init__( expr )&#xa;        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself&#xa;        if adjacent:&#xa;            self.leaveWhitespace()&#xa;        self.adjacent = adjacent&#xa;        self.skipWhitespace = True&#xa;        self.joinString = joinString&#xa;        self.callPreparse = True&#xa;&#xa;    def ignore( self, other ):&#xa;        if self.adjacent:&#xa;            ParserElement.ignore(self, other)&#xa;        else:&#xa;            super( Combine, self).ignore( other )&#xa;        return self&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        retToks = tokenlist.copy()&#xa;        del retToks[:]&#xa;        retToks += ParseResults([ """".join(tokenlist._asStringList(self.joinString)) ], modal=self.modalResults)&#xa;&#xa;        if self.resultsName and retToks.haskeys():&#xa;            return [ retToks ]&#xa;        else:&#xa;            return retToks&#xa;&#xa;class Group(TokenConverter):&#xa;    """"""&#xa;    Converter to return the matched tokens as a list - useful for returning tokens of C{L{ZeroOrMore}} and C{L{OneOrMore}} expressions.&#xa;&#xa;    Example::&#xa;        ident = Word(alphas)&#xa;        num = Word(nums)&#xa;        term = ident | num&#xa;        func = ident + Optional(delimitedList(term))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', 'a', 'b', '100']&#xa;&#xa;        func = ident + Group(Optional(delimitedList(term)))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', ['a', 'b', '100']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Group,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return [ tokenlist ]&#xa;&#xa;class Dict(TokenConverter):&#xa;    """"""&#xa;    Converter to return a repetitive expression as a list, but also as a dictionary.&#xa;    Each element can also be referenced using the first token in the expression as its key.&#xa;    Useful for tabular report scraping when the first column can be used as a item key.&#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        # print attributes as plain groups&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names&#xa;        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)&#xa;        print(result.dump())&#xa;        &#xa;        # access named fields as dict entries, or output as dict&#xa;        print(result['shape'])        &#xa;        print(result.asDict())&#xa;    prints::&#xa;        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']&#xa;&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}&#xa;    See more examples at L{ParseResults} of accessing fields by results name.&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Dict,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        for i,tok in enumerate(tokenlist):&#xa;            if len(tok) == 0:&#xa;                continue&#xa;            ikey = tok[0]&#xa;            if isinstance(ikey,int):&#xa;                ikey = _ustr(tok[0]).strip()&#xa;            if len(tok)==1:&#xa;                tokenlist[ikey] = _ParseResultsWithOffset("""",i)&#xa;            elif len(tok)==2 and not isinstance(tok[1],ParseResults):&#xa;                tokenlist[ikey] = _ParseResultsWithOffset(tok[1],i)&#xa;            else:&#xa;                dictvalue = tok.copy() #ParseResults(i)&#xa;                del dictvalue[0]&#xa;                if len(dictvalue)!= 1 or (isinstance(dictvalue,ParseResults) and dictvalue.haskeys()):&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue,i)&#xa;                else:&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0],i)&#xa;&#xa;        if self.resultsName:&#xa;            return [ tokenlist ]&#xa;        else:&#xa;            return tokenlist&#xa;&#xa;&#xa;class Suppress(TokenConverter):&#xa;    """"""&#xa;    Converter for ignoring the results of a parsed expression.&#xa;&#xa;    Example::&#xa;        source = ""a, b, c,d""&#xa;        wd = Word(alphas)&#xa;        wd_list1 = wd + ZeroOrMore(',' + wd)&#xa;        print(wd_list1.parseString(source))&#xa;&#xa;        # often, delimiters that are useful during parsing are just in the&#xa;        # way afterward - use Suppress to keep them out of the parsed output&#xa;        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)&#xa;        print(wd_list2.parseString(source))&#xa;    prints::&#xa;        ['a', ',', 'b', ',', 'c', ',', 'd']&#xa;        ['a', 'b', 'c', 'd']&#xa;    (See also L{delimitedList}.)&#xa;    """"""&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return []&#xa;&#xa;    def suppress( self ):&#xa;        return self&#xa;&#xa;&#xa;class OnlyOnce(object):&#xa;    """"""&#xa;    Wrapper for parse actions, to ensure they are only called once.&#xa;    """"""&#xa;    def __init__(self, methodCall):&#xa;        self.callable = _trim_arity(methodCall)&#xa;        self.called = False&#xa;    def __call__(self,s,l,t):&#xa;        if not self.called:&#xa;            results = self.callable(s,l,t)&#xa;            self.called = True&#xa;            return results&#xa;        raise ParseException(s,l,"""")&#xa;    def reset(self):&#xa;        self.called = False&#xa;&#xa;def traceParseAction(f):&#xa;    """"""&#xa;    Decorator for debugging parse actions. &#xa;    &#xa;    When the parse action is called, this decorator will print C{"">> entering I{method-name}(line:I{current_source_line}, I{parse_location}, I{matched_tokens})"".}&#xa;    When the parse action completes, the decorator will print C{""<<""} followed by the returned value, or any exception that the parse action raised.&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;&#xa;        @traceParseAction&#xa;        def remove_duplicate_chars(tokens):&#xa;            return ''.join(sorted(set(''.join(tokens)))&#xa;&#xa;        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)&#xa;        print(wds.parseString(""slkdjs sld sldd sdlf sdljf""))&#xa;    prints::&#xa;        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))&#xa;        <<leaving remove_duplicate_chars (ret: 'dfjkls')&#xa;        ['dfjkls']&#xa;    """"""&#xa;    f = _trim_arity(f)&#xa;    def z(*paArgs):&#xa;        thisFunc = f.__name__&#xa;        s,l,t = paArgs[-3:]&#xa;        if len(paArgs)>3:&#xa;            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc&#xa;        sys.stderr.write( "">>entering %s(line: '%s', %d, %r)\n"" % (thisFunc,line(l,s),l,t) )&#xa;        try:&#xa;            ret = f(*paArgs)&#xa;        except Exception as exc:&#xa;            sys.stderr.write( ""<<leaving %s (exception: %s)\n"" % (thisFunc,exc) )&#xa;            raise&#xa;        sys.stderr.write( ""<<leaving %s (ret: %r)\n"" % (thisFunc,ret) )&#xa;        return ret&#xa;    try:&#xa;        z.__name__ = f.__name__&#xa;    except AttributeError:&#xa;        pass&#xa;    return z&#xa;&#xa;#&#xa;# global helpers&#xa;#&#xa;def delimitedList( expr, delim="","", combine=False ):&#xa;    """"""&#xa;    Helper to define a delimited list of expressions - the delimiter defaults to ','.&#xa;    By default, the list elements and delimiters can have intervening whitespace, and&#xa;    comments, but this can be overridden by passing C{combine=True} in the constructor.&#xa;    If C{combine} is set to C{True}, the matching tokens are returned as a single token&#xa;    string, with the delimiters included; otherwise, the matching tokens are returned&#xa;    as a list of tokens, with the delimiters suppressed.&#xa;&#xa;    Example::&#xa;        delimitedList(Word(alphas)).parseString(""aa,bb,cc"") # -> ['aa', 'bb', 'cc']&#xa;        delimitedList(Word(hexnums), delim=':', combine=True).parseString(""AA:BB:CC:DD:EE"") # -> ['AA:BB:CC:DD:EE']&#xa;    """"""&#xa;    dlName = _ustr(expr)+"" [""+_ustr(delim)+"" ""+_ustr(expr)+""]...""&#xa;    if combine:&#xa;        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)&#xa;    else:&#xa;        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)&#xa;&#xa;def countedArray( expr, intExpr=None ):&#xa;    """"""&#xa;    Helper to define a counted list of expressions.&#xa;    This helper defines a pattern of the form::&#xa;        integer expr expr expr...&#xa;    where the leading integer tells how many expr expressions follow.&#xa;    The matched tokens returns the array of expr tokens as a list - the leading count token is suppressed.&#xa;    &#xa;    If C{intExpr} is specified, it should be a pyparsing expression that produces an integer value.&#xa;&#xa;    Example::&#xa;        countedArray(Word(alphas)).parseString('2 ab cd ef')  # -> ['ab', 'cd']&#xa;&#xa;        # in this parser, the leading integer value is given in binary,&#xa;        # '10' indicating that 2 values are in the array&#xa;        binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))&#xa;        countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')  # -> ['ab', 'cd']&#xa;    """"""&#xa;    arrayExpr = Forward()&#xa;    def countFieldParseAction(s,l,t):&#xa;        n = t[0]&#xa;        arrayExpr << (n and Group(And([expr]*n)) or Group(empty))&#xa;        return []&#xa;    if intExpr is None:&#xa;        intExpr = Word(nums).setParseAction(lambda t:int(t[0]))&#xa;    else:&#xa;        intExpr = intExpr.copy()&#xa;    intExpr.setName(""arrayLen"")&#xa;    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)&#xa;    return ( intExpr + arrayExpr ).setName('(len) ' + _ustr(expr) + '...')&#xa;&#xa;def _flatten(L):&#xa;    ret = []&#xa;    for i in L:&#xa;        if isinstance(i,list):&#xa;            ret.extend(_flatten(i))&#xa;        else:&#xa;            ret.append(i)&#xa;    return ret&#xa;&#xa;def matchPreviousLiteral(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousLiteral(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches a&#xa;    previous literal, will also match the leading C{""1:1""} in C{""1:10""}.&#xa;    If this is not desired, use C{matchPreviousExpr}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        if t:&#xa;            if len(t) == 1:&#xa;                rep << t[0]&#xa;            else:&#xa;                # flatten t tokens&#xa;                tflat = _flatten(t.asList())&#xa;                rep << And(Literal(tt) for tt in tflat)&#xa;        else:&#xa;            rep << Empty()&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def matchPreviousExpr(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousExpr(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches by&#xa;    expressions, will I{not} match the leading C{""1:1""} in C{""1:10""};&#xa;    the expressions are evaluated first, and then compared, so&#xa;    C{""1""} is compared with C{""10""}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    e2 = expr.copy()&#xa;    rep <<= e2&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        matchTokens = _flatten(t.asList())&#xa;        def mustMatchTheseTokens(s,l,t):&#xa;            theseTokens = _flatten(t.asList())&#xa;            if  theseTokens != matchTokens:&#xa;                raise ParseException("""",0,"""")&#xa;        rep.setParseAction( mustMatchTheseTokens, callDuringTry=True )&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def _escapeRegexRangeChars(s):&#xa;    #~  escape these chars: ^-]&#xa;    for c in r""\^-]"":&#xa;        s = s.replace(c,_bslash+c)&#xa;    s = s.replace(""\n"",r""\n"")&#xa;    s = s.replace(""\t"",r""\t"")&#xa;    return _ustr(s)&#xa;&#xa;def oneOf( strs, caseless=False, useRegex=True ):&#xa;    """"""&#xa;    Helper to quickly define a set of alternative Literals, and makes sure to do&#xa;    longest-first testing when there is a conflict, regardless of the input order,&#xa;    but returns a C{L{MatchFirst}} for best performance.&#xa;&#xa;    Parameters:&#xa;     - strs - a string of space-delimited literals, or a collection of string literals&#xa;     - caseless - (default=C{False}) - treat all literals as caseless&#xa;     - useRegex - (default=C{True}) - as an optimization, will generate a Regex&#xa;          object; otherwise, will generate a C{MatchFirst} object (if C{caseless=True}, or&#xa;          if creating a C{Regex} raises an exception)&#xa;&#xa;    Example::&#xa;        comp_oper = oneOf(""< = > <= >= !="")&#xa;        var = Word(alphas)&#xa;        number = Word(nums)&#xa;        term = var | number&#xa;        comparison_expr = term + comp_oper + term&#xa;        print(comparison_expr.searchString(""B = 12  AA=23 B<=AA AA>12""))&#xa;    prints::&#xa;        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]&#xa;    """"""&#xa;    if caseless:&#xa;        isequal = ( lambda a,b: a.upper() == b.upper() )&#xa;        masks = ( lambda a,b: b.upper().startswith(a.upper()) )&#xa;        parseElementClass = CaselessLiteral&#xa;    else:&#xa;        isequal = ( lambda a,b: a == b )&#xa;        masks = ( lambda a,b: b.startswith(a) )&#xa;        parseElementClass = Literal&#xa;&#xa;    symbols = []&#xa;    if isinstance(strs,basestring):&#xa;        symbols = strs.split()&#xa;    elif isinstance(strs, collections.Iterable):&#xa;        symbols = list(strs)&#xa;    else:&#xa;        warnings.warn(""Invalid argument to oneOf, expected string or iterable"",&#xa;                SyntaxWarning, stacklevel=2)&#xa;    if not symbols:&#xa;        return NoMatch()&#xa;&#xa;    i = 0&#xa;    while i < len(symbols)-1:&#xa;        cur = symbols[i]&#xa;        for j,other in enumerate(symbols[i+1:]):&#xa;            if ( isequal(other, cur) ):&#xa;                del symbols[i+j+1]&#xa;                break&#xa;            elif ( masks(cur, other) ):&#xa;                del symbols[i+j+1]&#xa;                symbols.insert(i,other)&#xa;                cur = other&#xa;                break&#xa;        else:&#xa;            i += 1&#xa;&#xa;    if not caseless and useRegex:&#xa;        #~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))&#xa;        try:&#xa;            if len(symbols)==len("""".join(symbols)):&#xa;                return Regex( ""[%s]"" % """".join(_escapeRegexRangeChars(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;            else:&#xa;                return Regex( ""|"".join(re.escape(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;        except Exception:&#xa;            warnings.warn(""Exception creating Regex for oneOf, building MatchFirst"",&#xa;                    SyntaxWarning, stacklevel=2)&#xa;&#xa;&#xa;    # last resort, just use MatchFirst&#xa;    return MatchFirst(parseElementClass(sym) for sym in symbols).setName(' | '.join(symbols))&#xa;&#xa;def dictOf( key, value ):&#xa;    """"""&#xa;    Helper to easily and clearly define a dictionary by specifying the respective patterns&#xa;    for the key and value.  Takes care of defining the C{L{Dict}}, C{L{ZeroOrMore}}, and C{L{Group}} tokens&#xa;    in the proper order.  The key pattern can include delimiting markers or punctuation,&#xa;    as long as they are suppressed, thereby leaving the significant key text.  The value&#xa;    pattern can include named results, so that the C{Dict} results can include named token&#xa;    fields.&#xa;&#xa;    Example::&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        attr_label = label&#xa;        attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)&#xa;&#xa;        # similar to Dict, but simpler call format&#xa;        result = dictOf(attr_label, attr_value).parseString(text)&#xa;        print(result.dump())&#xa;        print(result['shape'])&#xa;        print(result.shape)  # object attribute access works too&#xa;        print(result.asDict())&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        SQUARE&#xa;        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}&#xa;    """"""&#xa;    return Dict( ZeroOrMore( Group ( key + value ) ) )&#xa;&#xa;def originalTextFor(expr, asString=True):&#xa;    """"""&#xa;    Helper to return the original, untokenized text for a given expression.  Useful to&#xa;    restore the parsed fields of an HTML start tag into the raw tag text itself, or to&#xa;    revert separate tokens with intervening whitespace back to the original matching&#xa;    input text. By default, returns astring containing the original parsed text.  &#xa;       &#xa;    If the optional C{asString} argument is passed as C{False}, then the return value is a &#xa;    C{L{ParseResults}} containing any results names that were originally matched, and a &#xa;    single token containing the original matched text from the input string.  So if &#xa;    the expression passed to C{L{originalTextFor}} contains expressions with defined&#xa;    results names, you must set C{asString} to C{False} if you want to preserve those&#xa;    results name values.&#xa;&#xa;    Example::&#xa;        src = ""this is test <b> bold <i>text</i> </b> normal text ""&#xa;        for tag in (""b"",""i""):&#xa;            opener,closer = makeHTMLTags(tag)&#xa;            patt = originalTextFor(opener + SkipTo(closer) + closer)&#xa;            print(patt.searchString(src)[0])&#xa;    prints::&#xa;        ['<b> bold <i>text</i> </b>']&#xa;        ['<i>text</i>']&#xa;    """"""&#xa;    locMarker = Empty().setParseAction(lambda s,loc,t: loc)&#xa;    endlocMarker = locMarker.copy()&#xa;    endlocMarker.callPreparse = False&#xa;    matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")&#xa;    if asString:&#xa;        extractText = lambda s,l,t: s[t._original_start:t._original_end]&#xa;    else:&#xa;        def extractText(s,l,t):&#xa;            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]&#xa;    matchExpr.setParseAction(extractText)&#xa;    matchExpr.ignoreExprs = expr.ignoreExprs&#xa;    return matchExpr&#xa;&#xa;def ungroup(expr): &#xa;    """"""&#xa;    Helper to undo pyparsing's default grouping of And expressions, even&#xa;    if all but one are non-empty.&#xa;    """"""&#xa;    return TokenConverter(expr).setParseAction(lambda t:t[0])&#xa;&#xa;def locatedExpr(expr):&#xa;    """"""&#xa;    Helper to decorate a returned token with its starting and ending locations in the input string.&#xa;    This helper adds the following results names:&#xa;     - locn_start = location where matched expression begins&#xa;     - locn_end = location where matched expression ends&#xa;     - value = the actual parsed results&#xa;&#xa;    Be careful if the input text contains C{<TAB>} characters, you may want to call&#xa;    C{L{ParserElement.parseWithTabs}}&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;        for match in locatedExpr(wd).searchString(""ljsdf123lksdjjf123lkkjj1222""):&#xa;            print(match)&#xa;    prints::&#xa;        [[0, 'ljsdf', 5]]&#xa;        [[8, 'lksdjjf', 15]]&#xa;        [[18, 'lkkjj', 23]]&#xa;    """"""&#xa;    locator = Empty().setParseAction(lambda s,l,t: l)&#xa;    return Group(locator(""locn_start"") + expr(""value"") + locator.copy().leaveWhitespace()(""locn_end""))&#xa;&#xa;&#xa;# convenience constants for positional expressions&#xa;empty       = Empty().setName(""empty"")&#xa;lineStart   = LineStart().setName(""lineStart"")&#xa;lineEnd     = LineEnd().setName(""lineEnd"")&#xa;stringStart = StringStart().setName(""stringStart"")&#xa;stringEnd   = StringEnd().setName(""stringEnd"")&#xa;&#xa;_escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])&#xa;_escapedHexChar = Regex(r""\\0?[xX][0-9a-fA-F]+"").setParseAction(lambda s,l,t:unichr(int(t[0].lstrip(r'\0x'),16)))&#xa;_escapedOctChar = Regex(r""\\0[0-7]+"").setParseAction(lambda s,l,t:unichr(int(t[0][1:],8)))&#xa;_singleChar = _escapedPunc | _escapedHexChar | _escapedOctChar | Word(printables, excludeChars=r'\]', exact=1) | Regex(r""\w"", re.UNICODE)&#xa;_charRange = Group(_singleChar + Suppress(""-"") + _singleChar)&#xa;_reBracketExpr = Literal(""["") + Optional(""^"").setResultsName(""negate"") + Group( OneOrMore( _charRange | _singleChar ) ).setResultsName(""body"") + ""]""&#xa;&#xa;def srange(s):&#xa;    r""""""&#xa;    Helper to easily define string ranges for use in Word construction.  Borrows&#xa;    syntax from regexp '[]' string range definitions::&#xa;        srange(""[0-9]"")   -> ""0123456789""&#xa;        srange(""[a-z]"")   -> ""abcdefghijklmnopqrstuvwxyz""&#xa;        srange(""[a-z$_]"") -> ""abcdefghijklmnopqrstuvwxyz$_""&#xa;    The input string must be enclosed in []'s, and the returned string is the expanded&#xa;    character set joined into a single string.&#xa;    The values enclosed in the []'s may be:&#xa;     - a single character&#xa;     - an escaped character with a leading backslash (such as C{\-} or C{\]})&#xa;     - an escaped hex character with a leading C{'\x'} (C{\x21}, which is a C{'!'} character) &#xa;         (C{\0x##} is also supported for backwards compatibility) &#xa;     - an escaped octal character with a leading C{'\0'} (C{\041}, which is a C{'!'} character)&#xa;     - a range of any of the above, separated by a dash (C{'a-z'}, etc.)&#xa;     - any combination of the above (C{'aeiouy'}, C{'a-zA-Z0-9_$'}, etc.)&#xa;    """"""&#xa;    _expanded = lambda p: p if not isinstance(p,ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]),ord(p[1])+1))&#xa;    try:&#xa;        return """".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)&#xa;    except Exception:&#xa;        return """"&#xa;&#xa;def matchOnlyAtCol(n):&#xa;    """"""&#xa;    Helper method for defining parse actions that require matching at a specific&#xa;    column in the input text.&#xa;    """"""&#xa;    def verifyCol(strg,locn,toks):&#xa;        if col(locn,strg) != n:&#xa;            raise ParseException(strg,locn,""matched token not at column %d"" % n)&#xa;    return verifyCol&#xa;&#xa;def replaceWith(replStr):&#xa;    """"""&#xa;    Helper method for common parse actions that simply return a literal value.  Especially&#xa;    useful when used with C{L{transformString<ParserElement.transformString>}()}.&#xa;&#xa;    Example::&#xa;        num = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;        na = oneOf(""N/A NA"").setParseAction(replaceWith(math.nan))&#xa;        term = na | num&#xa;        &#xa;        OneOrMore(term).parseString(""324 234 N/A 234"") # -> [324, 234, nan, 234]&#xa;    """"""&#xa;    return lambda s,l,t: [replStr]&#xa;&#xa;def removeQuotes(s,l,t):&#xa;    """"""&#xa;    Helper parse action for removing quotation marks from parsed quoted strings.&#xa;&#xa;    Example::&#xa;        # by default, quotation marks are included in parsed results&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""'Now is the Winter of our Discontent'""]&#xa;&#xa;        # use removeQuotes to strip quotation marks from parsed results&#xa;        quotedString.setParseAction(removeQuotes)&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""Now is the Winter of our Discontent""]&#xa;    """"""&#xa;    return t[0][1:-1]&#xa;&#xa;def tokenMap(func, *args):&#xa;    """"""&#xa;    Helper to define a parse action by mapping a function to all elements of a ParseResults list.If any additional &#xa;    args are passed, they are forwarded to the given function as additional arguments after&#xa;    the token, as in C{hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))}, which will convert the&#xa;    parsed data to an integer using base 16.&#xa;&#xa;    Example (compare the last to example in L{ParserElement.transformString}::&#xa;        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))&#xa;        hex_ints.runTests('''&#xa;            00 11 22 aa FF 0a 0d 1a&#xa;            ''')&#xa;        &#xa;        upperword = Word(alphas).setParseAction(tokenMap(str.upper))&#xa;        OneOrMore(upperword).runTests('''&#xa;            my kingdom for a horse&#xa;            ''')&#xa;&#xa;        wd = Word(alphas).setParseAction(tokenMap(str.title))&#xa;        OneOrMore(wd).setParseAction(' '.join).runTests('''&#xa;            now is the winter of our discontent made glorious summer by this sun of york&#xa;            ''')&#xa;    prints::&#xa;        00 11 22 aa FF 0a 0d 1a&#xa;        [0, 17, 34, 170, 255, 10, 13, 26]&#xa;&#xa;        my kingdom for a horse&#xa;        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']&#xa;&#xa;        now is the winter of our discontent made glorious summer by this sun of york&#xa;        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']&#xa;    """"""&#xa;    def pa(s,l,t):&#xa;        return [func(tokn, *args) for tokn in t]&#xa;&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    pa.__name__ = func_name&#xa;&#xa;    return pa&#xa;&#xa;upcaseTokens = tokenMap(lambda t: _ustr(t).upper())&#xa;""""""(Deprecated) Helper parse action to convert tokens to upper case. Deprecated in favor of L{pyparsing_common.upcaseTokens}""""""&#xa;&#xa;downcaseTokens = tokenMap(lambda t: _ustr(t).lower())&#xa;""""""(Deprecated) Helper parse action to convert tokens to lower case. Deprecated in favor of L{pyparsing_common.downcaseTokens}""""""&#xa;    &#xa;def _makeTags(tagStr, xml):&#xa;    """"""Internal helper to construct opening and closing tag expressions, given a tag name""""""&#xa;    if isinstance(tagStr,basestring):&#xa;        resname = tagStr&#xa;        tagStr = Keyword(tagStr, caseless=not xml)&#xa;    else:&#xa;        resname = tagStr.name&#xa;&#xa;    tagAttrName = Word(alphas,alphanums+""_-:"")&#xa;    if (xml):&#xa;        tagAttrValue = dblQuotedString.copy().setParseAction( removeQuotes )&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName + Suppress(""="") + tagAttrValue ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    else:&#xa;        printablesLessRAbrack = """".join(c for c in printables if c not in "">"")&#xa;        tagAttrValue = quotedString.copy().setParseAction( removeQuotes ) | Word(printablesLessRAbrack)&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \&#xa;                Optional( Suppress(""="") + tagAttrValue ) ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    closeTag = Combine(_L(""</"") + tagStr + "">"")&#xa;&#xa;    openTag = openTag.setResultsName(""start""+"""".join(resname.replace("":"","" "").title().split())).setName(""<%s>"" % resname)&#xa;    closeTag = closeTag.setResultsName(""end""+"""".join(resname.replace("":"","" "").title().split())).setName(""</%s>"" % resname)&#xa;    openTag.tag = resname&#xa;    closeTag.tag = resname&#xa;    return openTag, closeTag&#xa;&#xa;def makeHTMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for HTML, given a tag name. Matches&#xa;    tags in either upper or lower case, attributes with namespaces and with quoted or unquoted values.&#xa;&#xa;    Example::&#xa;        text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;        # makeHTMLTags returns pyparsing expressions for the opening and closing tags as a 2-tuple&#xa;        a,a_end = makeHTMLTags(""A"")&#xa;        link_expr = a + SkipTo(a_end)(""link_text"") + a_end&#xa;        &#xa;        for link in link_expr.searchString(text):&#xa;            # attributes in the <A> tag (like ""href"" shown here) are also accessible as named results&#xa;            print(link.link_text, '->', link.href)&#xa;    prints::&#xa;        pyparsing -> http://pyparsing.wikispaces.com&#xa;    """"""&#xa;    return _makeTags( tagStr, False )&#xa;&#xa;def makeXMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for XML, given a tag name. Matches&#xa;    tags only in the given upper/lower case.&#xa;&#xa;    Example: similar to L{makeHTMLTags}&#xa;    """"""&#xa;    return _makeTags( tagStr, True )&#xa;&#xa;def withAttribute(*args,**attrDict):&#xa;    """"""&#xa;    Helper to create a validating parse action to be used with start tags created&#xa;    with C{L{makeXMLTags}} or C{L{makeHTMLTags}}. Use C{withAttribute} to qualify a starting tag&#xa;    with a required attribute value, to avoid false matches on common tags such as&#xa;    C{<TD>} or C{<DIV>}.&#xa;&#xa;    Call C{withAttribute} with a series of attribute names and values. Specify the list&#xa;    of filter attributes names and values as:&#xa;     - keyword arguments, as in C{(align=""right"")}, or&#xa;     - as an explicit dict with C{**} operator, when an attribute name is also a Python&#xa;          reserved word, as in C{**{""class"":""Customer"", ""align"":""right""}}&#xa;     - a list of name-value tuples, as in ( (""ns1:class"", ""Customer""), (""ns2:align"",""right"") )&#xa;    For attribute names with a namespace prefix, you must use the second form.  Attribute&#xa;    names are matched insensitive to upper/lower case.&#xa;       &#xa;    If just testing for C{class} (with or without a namespace), use C{L{withClass}}.&#xa;&#xa;    To verify that the attribute exists, but without specifying a value, pass&#xa;    C{withAttribute.ANY_VALUE} as the value.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div type=""grid"">1 4 0 1 0</div>&#xa;            <div type=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this has no type</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;&#xa;        # only match div tag having a type attribute with value ""grid""&#xa;        div_grid = div().setParseAction(withAttribute(type=""grid""))&#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        # construct a match with any div tag having a type attribute, regardless of the value&#xa;        div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    if args:&#xa;        attrs = args[:]&#xa;    else:&#xa;        attrs = attrDict.items()&#xa;    attrs = [(k,v) for k,v in attrs]&#xa;    def pa(s,l,tokens):&#xa;        for attrName,attrValue in attrs:&#xa;            if attrName not in tokens:&#xa;                raise ParseException(s,l,""no matching attribute "" + attrName)&#xa;            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:&#xa;                raise ParseException(s,l,""attribute '%s' has value '%s', must be '%s'"" %&#xa;                                            (attrName, tokens[attrName], attrValue))&#xa;    return pa&#xa;withAttribute.ANY_VALUE = object()&#xa;&#xa;def withClass(classname, namespace=''):&#xa;    """"""&#xa;    Simplified version of C{L{withAttribute}} when matching on a div class - made&#xa;    difficult because C{class} is a reserved word in Python.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div class=""grid"">1 4 0 1 0</div>&#xa;            <div class=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this &lt;div&gt; has no class</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;        div_grid = div().setParseAction(withClass(""grid""))&#xa;        &#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    classattr = ""%s:class"" % namespace if namespace else ""class""&#xa;    return withAttribute(**{classattr : classname})        &#xa;&#xa;opAssoc = _Constants()&#xa;opAssoc.LEFT = object()&#xa;opAssoc.RIGHT = object()&#xa;&#xa;def infixNotation( baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')') ):&#xa;    """"""&#xa;    Helper method for constructing grammars of expressions made up of&#xa;    operators working in a precedence hierarchy.  Operators may be unary or&#xa;    binary, left- or right-associative.  Parse actions can also be attached&#xa;    to operator expressions. The generated parser will also recognize the use &#xa;    of parentheses to override operator precedences (see example below).&#xa;    &#xa;    Note: if you define a deep operator list, you may see performance issues&#xa;    when using infixNotation. See L{ParserElement.enablePackrat} for a&#xa;    mechanism to potentially improve your parser performance.&#xa;&#xa;    Parameters:&#xa;     - baseExpr - expression representing the most basic element for the nested&#xa;     - opList - list of tuples, one for each operator precedence level in the&#xa;      expression grammar; each tuple is of the form&#xa;      (opExpr, numTerms, rightLeftAssoc, parseAction), where:&#xa;       - opExpr is the pyparsing expression for the operator;&#xa;          may also be a string, which will be converted to a Literal;&#xa;          if numTerms is 3, opExpr is a tuple of two expressions, for the&#xa;          two operators separating the 3 terms&#xa;       - numTerms is the number of terms for this operator (must&#xa;          be 1, 2, or 3)&#xa;       - rightLeftAssoc is the indicator whether the operator is&#xa;          right or left associative, using the pyparsing-defined&#xa;          constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}.&#xa;       - parseAction is the parse action to be associated with&#xa;          expressions matching this operator expression (the&#xa;          parse action tuple member may be omitted)&#xa;     - lpar - expression for matching left-parentheses (default=C{Suppress('(')})&#xa;     - rpar - expression for matching right-parentheses (default=C{Suppress(')')})&#xa;&#xa;    Example::&#xa;        # simple example of four-function arithmetic with ints and variable names&#xa;        integer = pyparsing_common.signed_integer&#xa;        varname = pyparsing_common.identifier &#xa;        &#xa;        arith_expr = infixNotation(integer | varname,&#xa;            [&#xa;            ('-', 1, opAssoc.RIGHT),&#xa;            (oneOf('* /'), 2, opAssoc.LEFT),&#xa;            (oneOf('+ -'), 2, opAssoc.LEFT),&#xa;            ])&#xa;        &#xa;        arith_expr.runTests('''&#xa;            5+3*6&#xa;            (5+3)*6&#xa;            -2--11&#xa;            ''', fullDump=False)&#xa;    prints::&#xa;        5+3*6&#xa;        [[5, '+', [3, '*', 6]]]&#xa;&#xa;        (5+3)*6&#xa;        [[[5, '+', 3], '*', 6]]&#xa;&#xa;        -2--11&#xa;        [[['-', 2], '-', ['-', 11]]]&#xa;    """"""&#xa;    ret = Forward()&#xa;    lastExpr = baseExpr | ( lpar + ret + rpar )&#xa;    for i,operDef in enumerate(opList):&#xa;        opExpr,arity,rightLeftAssoc,pa = (operDef + (None,))[:4]&#xa;        termName = ""%s term"" % opExpr if arity < 3 else ""%s%s term"" % opExpr&#xa;        if arity == 3:&#xa;            if opExpr is None or len(opExpr) != 2:&#xa;                raise ValueError(""if numterms=3, opExpr must be a tuple or list of two expressions"")&#xa;            opExpr1, opExpr2 = opExpr&#xa;        thisExpr = Forward().setName(termName)&#xa;        if rightLeftAssoc == opAssoc.LEFT:&#xa;            if arity == 1:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr) + Group( lastExpr + OneOrMore( opExpr ) )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + lastExpr) + Group( lastExpr + OneOrMore( opExpr + lastExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr+lastExpr) + Group( lastExpr + OneOrMore(lastExpr) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr) + \&#xa;                            Group( lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        elif rightLeftAssoc == opAssoc.RIGHT:&#xa;            if arity == 1:&#xa;                # try to avoid LR with this extra test&#xa;                if not isinstance(opExpr, Optional):&#xa;                    opExpr = Optional(opExpr)&#xa;                matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + thisExpr) + Group( lastExpr + OneOrMore( opExpr + thisExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr + thisExpr) + Group( lastExpr + OneOrMore( thisExpr ) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr) + \&#xa;                            Group( lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        else:&#xa;            raise ValueError(""operator must indicate right or left associativity"")&#xa;        if pa:&#xa;            matchExpr.setParseAction( pa )&#xa;        thisExpr <<= ( matchExpr.setName(termName) | lastExpr )&#xa;        lastExpr = thisExpr&#xa;    ret <<= lastExpr&#xa;    return ret&#xa;&#xa;operatorPrecedence = infixNotation&#xa;""""""(Deprecated) Former name of C{L{infixNotation}}, will be dropped in a future release.""""""&#xa;&#xa;dblQuotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""').setName(""string enclosed in double quotes"")&#xa;sglQuotedString = Combine(Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""string enclosed in single quotes"")&#xa;quotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""'|&#xa;                       Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""quotedString using single or double quotes"")&#xa;unicodeString = Combine(_L('u') + quotedString.copy()).setName(""unicode string literal"")&#xa;&#xa;def nestedExpr(opener=""("", closer="")"", content=None, ignoreExpr=quotedString.copy()):&#xa;    """"""&#xa;    Helper method for defining nested lists enclosed in opening and closing&#xa;    delimiters (""("" and "")"" are the default).&#xa;&#xa;    Parameters:&#xa;     - opener - opening character for a nested list (default=C{""(""}); can also be a pyparsing expression&#xa;     - closer - closing character for a nested list (default=C{"")""}); can also be a pyparsing expression&#xa;     - content - expression for items within the nested lists (default=C{None})&#xa;     - ignoreExpr - expression for ignoring opening and closing delimiters (default=C{quotedString})&#xa;&#xa;    If an expression is not provided for the content argument, the nested&#xa;    expression will capture all whitespace-delimited content between delimiters&#xa;    as a list of separate values.&#xa;&#xa;    Use the C{ignoreExpr} argument to define expressions that may contain&#xa;    opening or closing characters that should not be treated as opening&#xa;    or closing characters for nesting, such as quotedString or a comment&#xa;    expression.  Specify multiple expressions using an C{L{Or}} or C{L{MatchFirst}}.&#xa;    The default is L{quotedString}, but if no expressions are to be ignored,&#xa;    then pass C{None} for this argument.&#xa;&#xa;    Example::&#xa;        data_type = oneOf(""void int short long char float double"")&#xa;        decl_data_type = Combine(data_type + Optional(Word('*')))&#xa;        ident = Word(alphas+'_', alphanums+'_')&#xa;        number = pyparsing_common.number&#xa;        arg = Group(decl_data_type + ident)&#xa;        LPAR,RPAR = map(Suppress, ""()"")&#xa;&#xa;        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))&#xa;&#xa;        c_function = (decl_data_type(""type"") &#xa;                      + ident(""name"")&#xa;                      + LPAR + Optional(delimitedList(arg), [])(""args"") + RPAR &#xa;                      + code_body(""body""))&#xa;        c_function.ignore(cStyleComment)&#xa;        &#xa;        source_code = '''&#xa;            int is_odd(int x) { &#xa;                return (x%2); &#xa;            }&#xa;                &#xa;            int dec_to_hex(char hchar) { &#xa;                if (hchar >= '0' && hchar <= '9') { &#xa;                    return (ord(hchar)-ord('0')); &#xa;                } else { &#xa;                    return (10+ord(hchar)-ord('A'));&#xa;                } &#xa;            }&#xa;        '''&#xa;        for func in c_function.searchString(source_code):&#xa;            print(""%(name)s (%(type)s) args: %(args)s"" % func)&#xa;&#xa;    prints::&#xa;        is_odd (int) args: [['int', 'x']]&#xa;        dec_to_hex (int) args: [['char', 'hchar']]&#xa;    """"""&#xa;    if opener == closer:&#xa;        raise ValueError(""opening and closing strings cannot be the same"")&#xa;    if content is None:&#xa;        if isinstance(opener,basestring) and isinstance(closer,basestring):&#xa;            if len(opener) == 1 and len(closer)==1:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr +&#xa;                                    CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (empty.copy()+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;            else:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr + &#xa;                                    ~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (Combine(OneOrMore(~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;        else:&#xa;            raise ValueError(""opening and closing arguments must be strings if no content expression is given"")&#xa;    ret = Forward()&#xa;    if ignoreExpr is not None:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )&#xa;    else:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )&#xa;    ret.setName('nested %s%s expression' % (opener,closer))&#xa;    return ret&#xa;&#xa;def indentedBlock(blockStatementExpr, indentStack, indent=True):&#xa;    """"""&#xa;    Helper method for defining space-delimited indentation blocks, such as&#xa;    those used to define block statements in Python source code.&#xa;&#xa;    Parameters:&#xa;     - blockStatementExpr - expression defining syntax of statement that&#xa;            is repeated within the indented block&#xa;     - indentStack - list created by caller to manage indentation stack&#xa;            (multiple statementWithIndentedBlock expressions within a single grammar&#xa;            should share a common indentStack)&#xa;     - indent - boolean indicating whether block must be indented beyond the&#xa;            the current level; set to False for block of left-most statements&#xa;            (default=C{True})&#xa;&#xa;    A valid block must contain at least one C{blockStatement}.&#xa;&#xa;    Example::&#xa;        data = '''&#xa;        def A(z):&#xa;          A1&#xa;          B = 100&#xa;          G = A2&#xa;          A2&#xa;          A3&#xa;        B&#xa;        def BB(a,b,c):&#xa;          BB1&#xa;          def BBA():&#xa;            bba1&#xa;            bba2&#xa;            bba3&#xa;        C&#xa;        D&#xa;        def spam(x,y):&#xa;             def eggs(z):&#xa;                 pass&#xa;        '''&#xa;&#xa;&#xa;        indentStack = [1]&#xa;        stmt = Forward()&#xa;&#xa;        identifier = Word(alphas, alphanums)&#xa;        funcDecl = (""def"" + identifier + Group( ""("" + Optional( delimitedList(identifier) ) + "")"" ) + "":"")&#xa;        func_body = indentedBlock(stmt, indentStack)&#xa;        funcDef = Group( funcDecl + func_body )&#xa;&#xa;        rvalue = Forward()&#xa;        funcCall = Group(identifier + ""("" + Optional(delimitedList(rvalue)) + "")"")&#xa;        rvalue << (funcCall | identifier | Word(nums))&#xa;        assignment = Group(identifier + ""="" + rvalue)&#xa;        stmt << ( funcDef | assignment | identifier )&#xa;&#xa;        module_body = OneOrMore(stmt)&#xa;&#xa;        parseTree = module_body.parseString(data)&#xa;        parseTree.pprint()&#xa;    prints::&#xa;        [['def',&#xa;          'A',&#xa;          ['(', 'z', ')'],&#xa;          ':',&#xa;          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],&#xa;         'B',&#xa;         ['def',&#xa;          'BB',&#xa;          ['(', 'a', 'b', 'c', ')'],&#xa;          ':',&#xa;          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],&#xa;         'C',&#xa;         'D',&#xa;         ['def',&#xa;          'spam',&#xa;          ['(', 'x', 'y', ')'],&#xa;          ':',&#xa;          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]] &#xa;    """"""&#xa;    def checkPeerIndent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if curCol != indentStack[-1]:&#xa;            if curCol > indentStack[-1]:&#xa;                raise ParseFatalException(s,l,""illegal nesting"")&#xa;            raise ParseException(s,l,""not a peer entry"")&#xa;&#xa;    def checkSubIndent(s,l,t):&#xa;        curCol = col(l,s)&#xa;        if curCol > indentStack[-1]:&#xa;            indentStack.append( curCol )&#xa;        else:&#xa;            raise ParseException(s,l,""not a subentry"")&#xa;&#xa;    def checkUnindent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if not(indentStack and curCol < indentStack[-1] and curCol <= indentStack[-2]):&#xa;            raise ParseException(s,l,""not an unindent"")&#xa;        indentStack.pop()&#xa;&#xa;    NL = OneOrMore(LineEnd().setWhitespaceChars(""\t "").suppress())&#xa;    INDENT = (Empty() + Empty().setParseAction(checkSubIndent)).setName('INDENT')&#xa;    PEER   = Empty().setParseAction(checkPeerIndent).setName('')&#xa;    UNDENT = Empty().setParseAction(checkUnindent).setName('UNINDENT')&#xa;    if indent:&#xa;        smExpr = Group( Optional(NL) +&#xa;            #~ FollowedBy(blockStatementExpr) +&#xa;            INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT)&#xa;    else:&#xa;        smExpr = Group( Optional(NL) +&#xa;            (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) )&#xa;    blockStatementExpr.ignore(_bslash + LineEnd())&#xa;    return smExpr.setName('indented block')&#xa;&#xa;alphas8bit = srange(r""[\0xc0-\0xd6\0xd8-\0xf6\0xf8-\0xff]"")&#xa;punc8bit = srange(r""[\0xa1-\0xbf\0xd7\0xf7]"")&#xa;&#xa;anyOpenTag,anyCloseTag = makeHTMLTags(Word(alphas,alphanums+""_:"").setName('any tag'))&#xa;_htmlEntityMap = dict(zip(""gt lt amp nbsp quot apos"".split(),'><& ""\''))&#xa;commonHTMLEntity = Regex('&(?P<entity>' + '|'.join(_htmlEntityMap.keys()) +"");"").setName(""common HTML entity"")&#xa;def replaceHTMLEntity(t):&#xa;    """"""Helper parser action to replace common HTML entities with their special characters""""""&#xa;    return _htmlEntityMap.get(t.entity)&#xa;&#xa;# it's easy to get these comment structures wrong - they're very common, so may as well make them available&#xa;cStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/').setName(""C style comment"")&#xa;""Comment of the form C{/* ... */}""&#xa;&#xa;htmlComment = Regex(r""<!--[\s\S]*?-->"").setName(""HTML comment"")&#xa;""Comment of the form C{<!-- ... -->}""&#xa;&#xa;restOfLine = Regex(r"".*"").leaveWhitespace().setName(""rest of line"")&#xa;dblSlashComment = Regex(r""//(?:\\\n|[^\n])*"").setName(""// comment"")&#xa;""Comment of the form C{// ... (to end of line)}""&#xa;&#xa;cppStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/'| dblSlashComment).setName(""C++ style comment"")&#xa;""Comment of either form C{L{cStyleComment}} or C{L{dblSlashComment}}""&#xa;&#xa;javaStyleComment = cppStyleComment&#xa;""Same as C{L{cppStyleComment}}""&#xa;&#xa;pythonStyleComment = Regex(r""#.*"").setName(""Python style comment"")&#xa;""Comment of the form C{# ... (to end of line)}""&#xa;&#xa;_commasepitem = Combine(OneOrMore(Word(printables, excludeChars=',') +&#xa;                                  Optional( Word("" \t"") +&#xa;                                            ~Literal("","") + ~LineEnd() ) ) ).streamline().setName(""commaItem"")&#xa;commaSeparatedList = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""commaSeparatedList"")&#xa;""""""(Deprecated) Predefined expression of 1 or more printable words or quoted strings, separated by commas.&#xa;   This expression is deprecated in favor of L{pyparsing_common.comma_separated_list}.""""""&#xa;&#xa;# some other useful expressions - using lower-case class name since we are really using this as a namespace&#xa;class pyparsing_common:&#xa;    """"""&#xa;    Here are some common low-level expressions that may be useful in jump-starting parser development:&#xa;     - numeric forms (L{integers<integer>}, L{reals<real>}, L{scientific notation<sci_real>})&#xa;     - common L{programming identifiers<identifier>}&#xa;     - network addresses (L{MAC<mac_address>}, L{IPv4<ipv4_address>}, L{IPv6<ipv6_address>})&#xa;     - ISO8601 L{dates<iso8601_date>} and L{datetime<iso8601_datetime>}&#xa;     - L{UUID<uuid>}&#xa;     - L{comma-separated list<comma_separated_list>}&#xa;    Parse actions:&#xa;     - C{L{convertToInteger}}&#xa;     - C{L{convertToFloat}}&#xa;     - C{L{convertToDate}}&#xa;     - C{L{convertToDatetime}}&#xa;     - C{L{stripHTMLTags}}&#xa;     - C{L{upcaseTokens}}&#xa;     - C{L{downcaseTokens}}&#xa;&#xa;    Example::&#xa;        pyparsing_common.number.runTests('''&#xa;            # any int or real number, returned as the appropriate type&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.fnumber.runTests('''&#xa;            # any int or real number, returned as float&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.hex_integer.runTests('''&#xa;            # hex numbers&#xa;            100&#xa;            FF&#xa;            ''')&#xa;&#xa;        pyparsing_common.fraction.runTests('''&#xa;            # fractions&#xa;            1/2&#xa;            -3/4&#xa;            ''')&#xa;&#xa;        pyparsing_common.mixed_integer.runTests('''&#xa;            # mixed fractions&#xa;            1&#xa;            1/2&#xa;            -3/4&#xa;            1-3/4&#xa;            ''')&#xa;&#xa;        import uuid&#xa;        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;        pyparsing_common.uuid.runTests('''&#xa;            # uuid&#xa;            12345678-1234-5678-1234-567812345678&#xa;            ''')&#xa;    prints::&#xa;        # any int or real number, returned as the appropriate type&#xa;        100&#xa;        [100]&#xa;&#xa;        -100&#xa;        [-100]&#xa;&#xa;        +100&#xa;        [100]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # any int or real number, returned as float&#xa;        100&#xa;        [100.0]&#xa;&#xa;        -100&#xa;        [-100.0]&#xa;&#xa;        +100&#xa;        [100.0]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # hex numbers&#xa;        100&#xa;        [256]&#xa;&#xa;        FF&#xa;        [255]&#xa;&#xa;        # fractions&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        # mixed fractions&#xa;        1&#xa;        [1]&#xa;&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        1-3/4&#xa;        [1.75]&#xa;&#xa;        # uuid&#xa;        12345678-1234-5678-1234-567812345678&#xa;        [UUID('12345678-1234-5678-1234-567812345678')]&#xa;    """"""&#xa;&#xa;    convertToInteger = tokenMap(int)&#xa;    """"""&#xa;    Parse action for converting parsed integers to Python int&#xa;    """"""&#xa;&#xa;    convertToFloat = tokenMap(float)&#xa;    """"""&#xa;    Parse action for converting parsed numbers to Python float&#xa;    """"""&#xa;&#xa;    integer = Word(nums).setName(""integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an unsigned integer, returns an int""""""&#xa;&#xa;    hex_integer = Word(hexnums).setName(""hex integer"").setParseAction(tokenMap(int,16))&#xa;    """"""expression that parses a hexadecimal integer, returns an int""""""&#xa;&#xa;    signed_integer = Regex(r'[+-]?\d+').setName(""signed integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an integer with optional leading sign, returns an int""""""&#xa;&#xa;    fraction = (signed_integer().setParseAction(convertToFloat) + '/' + signed_integer().setParseAction(convertToFloat)).setName(""fraction"")&#xa;    """"""fractional expression of an integer divided by an integer, returns a float""""""&#xa;    fraction.addParseAction(lambda t: t[0]/t[-1])&#xa;&#xa;    mixed_integer = (fraction | signed_integer + Optional(Optional('-').suppress() + fraction)).setName(""fraction or mixed integer-fraction"")&#xa;    """"""mixed integer of the form 'integer - fraction', with optional leading integer, returns float""""""&#xa;    mixed_integer.addParseAction(sum)&#xa;&#xa;    real = Regex(r'[+-]?\d+\.\d*').setName(""real number"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number and returns a float""""""&#xa;&#xa;    sci_real = Regex(r'[+-]?\d+([eE][+-]?\d+|\.\d*([eE][+-]?\d+)?)').setName(""real number with scientific notation"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number with optional scientific notation and returns a float""""""&#xa;&#xa;    # streamlining this expression makes the docs nicer-looking&#xa;    number = (sci_real | real | signed_integer).streamline()&#xa;    """"""any numeric expression, returns the corresponding Python type""""""&#xa;&#xa;    fnumber = Regex(r'[+-]?\d+\.?\d*([eE][+-]?\d+)?').setName(""fnumber"").setParseAction(convertToFloat)&#xa;    """"""any int or real number, returned as float""""""&#xa;    &#xa;    identifier = Word(alphas+'_', alphanums+'_').setName(""identifier"")&#xa;    """"""typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')""""""&#xa;    &#xa;    ipv4_address = Regex(r'(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}').setName(""IPv4 address"")&#xa;    ""IPv4 address (C{0.0.0.0 - 255.255.255.255})""&#xa;&#xa;    _ipv6_part = Regex(r'[0-9a-fA-F]{1,4}').setName(""hex_integer"")&#xa;    _full_ipv6_address = (_ipv6_part + (':' + _ipv6_part)*7).setName(""full IPv6 address"")&#xa;    _short_ipv6_address = (Optional(_ipv6_part + (':' + _ipv6_part)*(0,6)) + ""::"" + Optional(_ipv6_part + (':' + _ipv6_part)*(0,6))).setName(""short IPv6 address"")&#xa;    _short_ipv6_address.addCondition(lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8)&#xa;    _mixed_ipv6_address = (""::ffff:"" + ipv4_address).setName(""mixed IPv6 address"")&#xa;    ipv6_address = Combine((_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).setName(""IPv6 address"")).setName(""IPv6 address"")&#xa;    ""IPv6 address (long, short, or mixed form)""&#xa;    &#xa;    mac_address = Regex(r'[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\1[0-9a-fA-F]{2}){4}').setName(""MAC address"")&#xa;    ""MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)""&#xa;&#xa;    @staticmethod&#xa;    def convertToDate(fmt=""%Y-%m-%d""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed date string to Python datetime.date&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%d""})&#xa;&#xa;        Example::&#xa;            date_expr = pyparsing_common.iso8601_date.copy()&#xa;            date_expr.setParseAction(pyparsing_common.convertToDate())&#xa;            print(date_expr.parseString(""1999-12-31""))&#xa;        prints::&#xa;            [datetime.date(1999, 12, 31)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt).date()&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    @staticmethod&#xa;    def convertToDatetime(fmt=""%Y-%m-%dT%H:%M:%S.%f""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed datetime string to Python datetime.datetime&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%dT%H:%M:%S.%f""})&#xa;&#xa;        Example::&#xa;            dt_expr = pyparsing_common.iso8601_datetime.copy()&#xa;            dt_expr.setParseAction(pyparsing_common.convertToDatetime())&#xa;            print(dt_expr.parseString(""1999-12-31T23:59:59.999""))&#xa;        prints::&#xa;            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt)&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    iso8601_date = Regex(r'(?P<year>\d{4})(?:-(?P<month>\d\d)(?:-(?P<day>\d\d))?)?').setName(""ISO8601 date"")&#xa;    ""ISO8601 date (C{yyyy-mm-dd})""&#xa;&#xa;    iso8601_datetime = Regex(r'(?P<year>\d{4})-(?P<month>\d\d)-(?P<day>\d\d)[T ](?P<hour>\d\d):(?P<minute>\d\d)(:(?P<second>\d\d(\.\d*)?)?)?(?P<tz>Z|[+-]\d\d:?\d\d)?').setName(""ISO8601 datetime"")&#xa;    ""ISO8601 datetime (C{yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)}) - trailing seconds, milliseconds, and timezone optional; accepts separating C{'T'} or C{' '}""&#xa;&#xa;    uuid = Regex(r'[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}').setName(""UUID"")&#xa;    ""UUID (C{xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx})""&#xa;&#xa;    _html_stripper = anyOpenTag.suppress() | anyCloseTag.suppress()&#xa;    @staticmethod&#xa;    def stripHTMLTags(s, l, tokens):&#xa;        """"""&#xa;        Parse action to remove HTML tags from web page HTML source&#xa;&#xa;        Example::&#xa;            # strip HTML links from normal text &#xa;            text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;            td,td_end = makeHTMLTags(""TD"")&#xa;            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)(""body"") + td_end&#xa;            &#xa;            print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'&#xa;        """"""&#xa;        return pyparsing_common._html_stripper.transformString(tokens[0])&#xa;&#xa;    _commasepitem = Combine(OneOrMore(~Literal("","") + ~LineEnd() + Word(printables, excludeChars=',') &#xa;                                        + Optional( White("" \t"") ) ) ).streamline().setName(""commaItem"")&#xa;    comma_separated_list = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""comma separated list"")&#xa;    """"""Predefined expression of 1 or more printable words or quoted strings, separated by commas.""""""&#xa;&#xa;    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))&#xa;    """"""Parse action to convert tokens to upper case.""""""&#xa;&#xa;    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))&#xa;    """"""Parse action to convert tokens to lower case.""""""&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    selectToken    = CaselessLiteral(""select"")&#xa;    fromToken      = CaselessLiteral(""from"")&#xa;&#xa;    ident          = Word(alphas, alphanums + ""_$"")&#xa;&#xa;    columnName     = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    columnNameList = Group(delimitedList(columnName)).setName(""columns"")&#xa;    columnSpec     = ('*' | columnNameList)&#xa;&#xa;    tableName      = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    tableNameList  = Group(delimitedList(tableName)).setName(""tables"")&#xa;    &#xa;    simpleSQL      = selectToken(""command"") + columnSpec(""columns"") + fromToken + tableNameList(""tables"")&#xa;&#xa;    # demo runTests method, including embedded comments in test string&#xa;    simpleSQL.runTests(""""""&#xa;        # '*' as column list and dotted table name&#xa;        select * from SYS.XYZZY&#xa;&#xa;        # caseless match on ""SELECT"", and casts back to ""select""&#xa;        SELECT * from XYZZY, ABC&#xa;&#xa;        # list of column names, and mixed case SELECT keyword&#xa;        Select AA,BB,CC from Sys.dual&#xa;&#xa;        # multiple tables&#xa;        Select A, B, C from Sys.dual, Table2&#xa;&#xa;        # invalid SELECT keyword - should fail&#xa;        Xelect A, B, C from Sys.dual&#xa;&#xa;        # incomplete command - should fail&#xa;        Select&#xa;&#xa;        # invalid column name - should fail&#xa;        Select ^^^ frox Sys.dual&#xa;&#xa;        """""")&#xa;&#xa;    pyparsing_common.number.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    # any int or real number, returned as float&#xa;    pyparsing_common.fnumber.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    pyparsing_common.hex_integer.runTests(""""""&#xa;        100&#xa;        FF&#xa;        """""")&#xa;&#xa;    import uuid&#xa;    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;    pyparsing_common.uuid.runTests(""""""&#xa;        12345678-1234-5678-1234-567812345678&#xa;        """""")&#xa;"
6811241|"""""""Tools for spectral analysis.&#xa;""""""&#xa;&#xa;from __future__ import division, print_function, absolute_import&#xa;&#xa;import numpy as np&#xa;from scipy import fftpack&#xa;from . import signaltools&#xa;from .windows import get_window&#xa;from ._spectral import lombscargle&#xa;import warnings&#xa;&#xa;from scipy._lib.six import string_types&#xa;&#xa;__all__ = ['periodogram', 'welch', 'lombscargle', 'csd', 'coherence',&#xa;           'spectrogram']&#xa;&#xa;&#xa;def periodogram(x, fs=1.0, window=None, nfft=None, detrend='constant',&#xa;                return_onesided=True, scaling='density', axis=-1):&#xa;    """"""&#xa;    Estimate power spectral density using a periodogram.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Time series of measurement values&#xa;    fs : float, optional&#xa;        Sampling frequency of the `x` time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is an array it will be used&#xa;        directly as the window. Defaults to None; equivalent to 'boxcar'.&#xa;    nfft : int, optional&#xa;        Length of the FFT used. If None the length of `x` will be used.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend `x` prior to computing the spectrum. If&#xa;        `detrend` is a string, it is passed as the ``type`` argument to&#xa;        `detrend`.  If it is a function, it should return a detrended array.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    return_onesided : bool, optional&#xa;        If True, return a one-sided spectrum for real data. If False return&#xa;        a two-sided spectrum. Note that for complex data, a two-sided&#xa;        spectrum is always returned.&#xa;    scaling : { 'density', 'spectrum' }, optional&#xa;        Selects between computing the power spectral density ('density')&#xa;        where `Pxx` has units of V**2/Hz and computing the power spectrum&#xa;        ('spectrum') where `Pxx` has units of V**2, if `x` is measured in V&#xa;        and fs is measured in Hz.  Defaults to 'density'&#xa;    axis : int, optional&#xa;        Axis along which the periodogram is computed; the default is over&#xa;        the last axis (i.e. ``axis=-1``).&#xa;&#xa;    Returns&#xa;    -------&#xa;    f : ndarray&#xa;        Array of sample frequencies.&#xa;    Pxx : ndarray&#xa;        Power spectral density or power spectrum of `x`.&#xa;&#xa;    Notes&#xa;    -----&#xa;    .. versionadded:: 0.12.0&#xa;&#xa;    See Also&#xa;    --------&#xa;    welch: Estimate power spectral density using Welch's method&#xa;    lombscargle: Lomb-Scargle periodogram for unevenly sampled data&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import signal&#xa;    >>> import matplotlib.pyplot as plt&#xa;    >>> np.random.seed(1234)&#xa;&#xa;    Generate a test signal, a 2 Vrms sine wave at 1234 Hz, corrupted by&#xa;    0.001 V**2/Hz of white noise sampled at 10 kHz.&#xa;&#xa;    >>> fs = 10e3&#xa;    >>> N = 1e5&#xa;    >>> amp = 2*np.sqrt(2)&#xa;    >>> freq = 1234.0&#xa;    >>> noise_power = 0.001 * fs / 2&#xa;    >>> time = np.arange(N) / fs&#xa;    >>> x = amp*np.sin(2*np.pi*freq*time)&#xa;    >>> x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)&#xa;&#xa;    Compute and plot the power spectral density.&#xa;&#xa;    >>> f, Pxx_den = signal.periodogram(x, fs)&#xa;    >>> plt.semilogy(f, Pxx_den)&#xa;    >>> plt.ylim([1e-7, 1e2])&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('PSD [V**2/Hz]')&#xa;    >>> plt.show()&#xa;&#xa;    If we average the last half of the spectral density, to exclude the&#xa;    peak, we can recover the noise power on the signal.&#xa;&#xa;    >>> np.mean(Pxx_den[256:])&#xa;    0.0018156616014838548&#xa;&#xa;    Now compute and plot the power spectrum.&#xa;&#xa;    >>> f, Pxx_spec = signal.periodogram(x, fs, 'flattop', scaling='spectrum')&#xa;    >>> plt.figure()&#xa;    >>> plt.semilogy(f, np.sqrt(Pxx_spec))&#xa;    >>> plt.ylim([1e-4, 1e1])&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('Linear spectrum [V RMS]')&#xa;    >>> plt.show()&#xa;&#xa;    The peak height in the power spectrum is an estimate of the RMS amplitude.&#xa;&#xa;    >>> np.sqrt(Pxx_spec.max())&#xa;    2.0077340678640727&#xa;&#xa;    """"""&#xa;    x = np.asarray(x)&#xa;&#xa;    if x.size == 0:&#xa;        return np.empty(x.shape), np.empty(x.shape)&#xa;&#xa;    if window is None:&#xa;        window = 'boxcar'&#xa;&#xa;    if nfft is None:&#xa;        nperseg = x.shape[axis]&#xa;    elif nfft == x.shape[axis]:&#xa;        nperseg = nfft&#xa;    elif nfft > x.shape[axis]:&#xa;        nperseg = x.shape[axis]&#xa;    elif nfft < x.shape[axis]:&#xa;        s = [np.s_[:]]*len(x.shape)&#xa;        s[axis] = np.s_[:nfft]&#xa;        x = x[s]&#xa;        nperseg = nfft&#xa;        nfft = None&#xa;&#xa;    return welch(x, fs, window, nperseg, 0, nfft, detrend, return_onesided,&#xa;                 scaling, axis)&#xa;&#xa;&#xa;def welch(x, fs=1.0, window='hanning', nperseg=256, noverlap=None, nfft=None,&#xa;          detrend='constant', return_onesided=True, scaling='density', axis=-1):&#xa;    """"""&#xa;    Estimate power spectral density using Welch's method.&#xa;&#xa;    Welch's method [1]_ computes an estimate of the power spectral density&#xa;    by dividing the data into overlapping segments, computing a modified&#xa;    periodogram for each segment and averaging the periodograms.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Time series of measurement values&#xa;    fs : float, optional&#xa;        Sampling frequency of the `x` time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is array_like it will be used&#xa;        directly as the window and its length will be used for nperseg.&#xa;        Defaults to 'hanning'.&#xa;    nperseg : int, optional&#xa;        Length of each segment.  Defaults to 256.&#xa;    noverlap : int, optional&#xa;        Number of points to overlap between segments. If None,&#xa;        ``noverlap = nperseg // 2``.  Defaults to None.&#xa;    nfft : int, optional&#xa;        Length of the FFT used, if a zero padded FFT is desired.  If None,&#xa;        the FFT length is `nperseg`. Defaults to None.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend each segment. If `detrend` is a string,&#xa;        it is passed as the ``type`` argument to `detrend`.  If it is a&#xa;        function, it takes a segment and returns a detrended segment.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    return_onesided : bool, optional&#xa;        If True, return a one-sided spectrum for real data. If False return&#xa;        a two-sided spectrum. Note that for complex data, a two-sided&#xa;        spectrum is always returned.&#xa;    scaling : { 'density', 'spectrum' }, optional&#xa;        Selects between computing the power spectral density ('density')&#xa;        where `Pxx` has units of V**2/Hz and computing the power spectrum&#xa;        ('spectrum') where `Pxx` has units of V**2, if `x` is measured in V&#xa;        and fs is measured in Hz.  Defaults to 'density'&#xa;    axis : int, optional&#xa;        Axis along which the periodogram is computed; the default is over&#xa;        the last axis (i.e. ``axis=-1``).&#xa;&#xa;    Returns&#xa;    -------&#xa;    f : ndarray&#xa;        Array of sample frequencies.&#xa;    Pxx : ndarray&#xa;        Power spectral density or power spectrum of x.&#xa;&#xa;    See Also&#xa;    --------&#xa;    periodogram: Simple, optionally modified periodogram&#xa;    lombscargle: Lomb-Scargle periodogram for unevenly sampled data&#xa;&#xa;    Notes&#xa;    -----&#xa;    An appropriate amount of overlap will depend on the choice of window&#xa;    and on your requirements.  For the default 'hanning' window an&#xa;    overlap of 50% is a reasonable trade off between accurately estimating&#xa;    the signal power, while not over counting any of the data.  Narrower&#xa;    windows may require a larger overlap.&#xa;&#xa;    If `noverlap` is 0, this method is equivalent to Bartlett's method [2]_.&#xa;&#xa;    .. versionadded:: 0.12.0&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] P. Welch, ""The use of the fast Fourier transform for the&#xa;           estimation of power spectra: A method based on time averaging&#xa;           over short, modified periodograms"", IEEE Trans. Audio&#xa;           Electroacoust. vol. 15, pp. 70-73, 1967.&#xa;    .. [2] M.S. Bartlett, ""Periodogram Analysis and Continuous Spectra"",&#xa;           Biometrika, vol. 37, pp. 1-16, 1950.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import signal&#xa;    >>> import matplotlib.pyplot as plt&#xa;    >>> np.random.seed(1234)&#xa;&#xa;    Generate a test signal, a 2 Vrms sine wave at 1234 Hz, corrupted by&#xa;    0.001 V**2/Hz of white noise sampled at 10 kHz.&#xa;&#xa;    >>> fs = 10e3&#xa;    >>> N = 1e5&#xa;    >>> amp = 2*np.sqrt(2)&#xa;    >>> freq = 1234.0&#xa;    >>> noise_power = 0.001 * fs / 2&#xa;    >>> time = np.arange(N) / fs&#xa;    >>> x = amp*np.sin(2*np.pi*freq*time)&#xa;    >>> x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)&#xa;&#xa;    Compute and plot the power spectral density.&#xa;&#xa;    >>> f, Pxx_den = signal.welch(x, fs, nperseg=1024)&#xa;    >>> plt.semilogy(f, Pxx_den)&#xa;    >>> plt.ylim([0.5e-3, 1])&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('PSD [V**2/Hz]')&#xa;    >>> plt.show()&#xa;&#xa;    If we average the last half of the spectral density, to exclude the&#xa;    peak, we can recover the noise power on the signal.&#xa;&#xa;    >>> np.mean(Pxx_den[256:])&#xa;    0.0009924865443739191&#xa;&#xa;    Now compute and plot the power spectrum.&#xa;&#xa;    >>> f, Pxx_spec = signal.welch(x, fs, 'flattop', 1024, scaling='spectrum')&#xa;    >>> plt.figure()&#xa;    >>> plt.semilogy(f, np.sqrt(Pxx_spec))&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('Linear spectrum [V RMS]')&#xa;    >>> plt.show()&#xa;&#xa;    The peak height in the power spectrum is an estimate of the RMS amplitude.&#xa;&#xa;    >>> np.sqrt(Pxx_spec.max())&#xa;    2.0077340678640727&#xa;&#xa;    """"""&#xa;&#xa;    freqs, Pxx = csd(x, x, fs, window, nperseg, noverlap, nfft, detrend,&#xa;                     return_onesided, scaling, axis)&#xa;&#xa;    return freqs, Pxx.real&#xa;&#xa;&#xa;def csd(x, y, fs=1.0, window='hanning', nperseg=256, noverlap=None, nfft=None,&#xa;        detrend='constant', return_onesided=True, scaling='density', axis=-1):&#xa;    """"""&#xa;    Estimate the cross power spectral density, Pxy, using Welch's method.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Time series of measurement values&#xa;    y : array_like&#xa;        Time series of measurement values&#xa;    fs : float, optional&#xa;        Sampling frequency of the `x` and `y` time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is array_like it will be used&#xa;        directly as the window and its length will be used for nperseg.&#xa;        Defaults to 'hanning'.&#xa;    nperseg : int, optional&#xa;        Length of each segment.  Defaults to 256.&#xa;    noverlap: int, optional&#xa;        Number of points to overlap between segments. If None,&#xa;        ``noverlap = nperseg // 2``.  Defaults to None.&#xa;    nfft : int, optional&#xa;        Length of the FFT used, if a zero padded FFT is desired.  If None,&#xa;        the FFT length is `nperseg`. Defaults to None.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend each segment. If `detrend` is a string,&#xa;        it is passed as the ``type`` argument to `detrend`.  If it is a&#xa;        function, it takes a segment and returns a detrended segment.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    return_onesided : bool, optional&#xa;        If True, return a one-sided spectrum for real data. If False return&#xa;        a two-sided spectrum. Note that for complex data, a two-sided&#xa;        spectrum is always returned.&#xa;    scaling : { 'density', 'spectrum' }, optional&#xa;        Selects between computing the cross spectral density ('density')&#xa;        where `Pxy` has units of V**2/Hz and computing the cross spectrum&#xa;        ('spectrum') where `Pxy` has units of V**2, if `x` and `y` are&#xa;        measured in V and fs is measured in Hz.  Defaults to 'density'&#xa;    axis : int, optional&#xa;        Axis along which the CSD is computed for both inputs; the default is&#xa;        over the last axis (i.e. ``axis=-1``).&#xa;&#xa;    Returns&#xa;    -------&#xa;    f : ndarray&#xa;        Array of sample frequencies.&#xa;    Pxy : ndarray&#xa;        Cross spectral density or cross power spectrum of x,y.&#xa;&#xa;    See Also&#xa;    --------&#xa;    periodogram: Simple, optionally modified periodogram&#xa;    lombscargle: Lomb-Scargle periodogram for unevenly sampled data&#xa;    welch: Power spectral density by Welch's method. [Equivalent to csd(x,x)]&#xa;    coherence: Magnitude squared coherence by Welch's method.&#xa;&#xa;    Notes&#xa;    --------&#xa;    By convention, Pxy is computed with the conjugate FFT of X multiplied by&#xa;    the FFT of Y.&#xa;&#xa;    If the input series differ in length, the shorter series will be&#xa;    zero-padded to match.&#xa;&#xa;    An appropriate amount of overlap will depend on the choice of window&#xa;    and on your requirements.  For the default 'hanning' window an&#xa;    overlap of 50\% is a reasonable trade off between accurately estimating&#xa;    the signal power, while not over counting any of the data.  Narrower&#xa;    windows may require a larger overlap.&#xa;&#xa;    .. versionadded:: 0.16.0&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] P. Welch, ""The use of the fast Fourier transform for the&#xa;           estimation of power spectra: A method based on time averaging&#xa;           over short, modified periodograms"", IEEE Trans. Audio&#xa;           Electroacoust. vol. 15, pp. 70-73, 1967.&#xa;    .. [2] Rabiner, Lawrence R., and B. Gold. ""Theory and Application of&#xa;           Digital Signal Processing"" Prentice-Hall, pp. 414-419, 1975&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import signal&#xa;    >>> import matplotlib.pyplot as plt&#xa;&#xa;    Generate two test signals with some common features.&#xa;&#xa;    >>> fs = 10e3&#xa;    >>> N = 1e5&#xa;    >>> amp = 20&#xa;    >>> freq = 1234.0&#xa;    >>> noise_power = 0.001 * fs / 2&#xa;    >>> time = np.arange(N) / fs&#xa;    >>> b, a = signal.butter(2, 0.25, 'low')&#xa;    >>> x = np.random.normal(scale=np.sqrt(noise_power), size=time.shape)&#xa;    >>> y = signal.lfilter(b, a, x)&#xa;    >>> x += amp*np.sin(2*np.pi*freq*time)&#xa;    >>> y += np.random.normal(scale=0.1*np.sqrt(noise_power), size=time.shape)&#xa;&#xa;    Compute and plot the magnitude of the cross spectral density.&#xa;&#xa;    >>> f, Pxy = signal.csd(x, y, fs, nperseg=1024)&#xa;    >>> plt.semilogy(f, np.abs(Pxy))&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('CSD [V**2/Hz]')&#xa;    >>> plt.show()&#xa;    """"""&#xa;&#xa;    freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap, nfft,&#xa;                                     detrend, return_onesided, scaling, axis,&#xa;                                     mode='psd')&#xa;&#xa;    # Average over windows.&#xa;    if len(Pxy.shape) >= 2 and Pxy.size > 0:&#xa;        if Pxy.shape[-1] > 1:&#xa;            Pxy = Pxy.mean(axis=-1)&#xa;        else:&#xa;            Pxy = np.reshape(Pxy, Pxy.shape[:-1])&#xa;&#xa;    return freqs, Pxy&#xa;&#xa;&#xa;def spectrogram(x, fs=1.0, window=('tukey',.25), nperseg=256, noverlap=None,&#xa;                nfft=None, detrend='constant', return_onesided=True,&#xa;                scaling='density', axis=-1):&#xa;    """"""&#xa;    Compute a spectrogram with consecutive Fourier transforms.&#xa;&#xa;    Spectrograms can be used as a way of visualizing the change of a&#xa;    nonstationary signal's frequency content over time.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Time series of measurement values&#xa;    fs : float, optional&#xa;        Sampling frequency of the `x` time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is array_like it will be used&#xa;        directly as the window and its length will be used for nperseg.&#xa;        Defaults to a Tukey window with shape parameter of 0.25.&#xa;    nperseg : int, optional&#xa;        Length of each segment.  Defaults to 256.&#xa;    noverlap : int, optional&#xa;        Number of points to overlap between segments. If None,&#xa;        ``noverlap = nperseg // 8``.  Defaults to None.&#xa;    nfft : int, optional&#xa;        Length of the FFT used, if a zero padded FFT is desired.  If None,&#xa;        the FFT length is `nperseg`. Defaults to None.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend each segment. If `detrend` is a string,&#xa;        it is passed as the ``type`` argument to `detrend`.  If it is a&#xa;        function, it takes a segment and returns a detrended segment.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    return_onesided : bool, optional&#xa;        If True, return a one-sided spectrum for real data. If False return&#xa;        a two-sided spectrum. Note that for complex data, a two-sided&#xa;        spectrum is always returned.&#xa;    scaling : { 'density', 'spectrum' }, optional&#xa;        Selects between computing the power spectral density ('density')&#xa;        where `Pxx` has units of V**2/Hz and computing the power spectrum&#xa;        ('spectrum') where `Pxx` has units of V**2, if `x` is measured in V&#xa;        and fs is measured in Hz.  Defaults to 'density'&#xa;    axis : int, optional&#xa;        Axis along which the spectrogram is computed; the default is over&#xa;        the last axis (i.e. ``axis=-1``).&#xa;&#xa;    Returns&#xa;    -------&#xa;    f : ndarray&#xa;        Array of sample frequencies.&#xa;    t : ndarray&#xa;        Array of segment times.&#xa;    Sxx : ndarray&#xa;        Spectrogram of x. By default, the last axis of Sxx corresponds to the&#xa;        segment times.&#xa;&#xa;    See Also&#xa;    --------&#xa;    periodogram: Simple, optionally modified periodogram&#xa;    lombscargle: Lomb-Scargle periodogram for unevenly sampled data&#xa;    welch: Power spectral density by Welch's method.&#xa;    csd: Cross spectral density by Welch's method.&#xa;&#xa;    Notes&#xa;    -----&#xa;    An appropriate amount of overlap will depend on the choice of window&#xa;    and on your requirements. In contrast to welch's method, where the entire&#xa;    data stream is averaged over, one may wish to use a smaller overlap (or&#xa;    perhaps none at all) when computing a spectrogram, to maintain some&#xa;    statistical independence between individual segments.&#xa;&#xa;    .. versionadded:: 0.16.0&#xa;&#xa;    References&#xa;    ----------&#xa;    ...[1] Oppenheim, Alan V., Ronald W. Schafer, John R. Buck ""Discrete-Time&#xa;           Signal Processing"", Prentice Hall, 1999.&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import signal&#xa;    >>> import matplotlib.pyplot as plt&#xa;&#xa;    Generate a test signal, a 2 Vrms sine wave whose frequency linearly changes&#xa;    with time from 1kHz to 2kHz, corrupted by 0.001 V**2/Hz of white noise&#xa;    sampled at 10 kHz.&#xa;&#xa;    >>> fs = 10e3&#xa;    >>> N = 1e5&#xa;    >>> amp = 2 * np.sqrt(2)&#xa;    >>> noise_power = 0.001 * fs / 2&#xa;    >>> time = np.arange(N) / fs&#xa;    >>> freq = np.linspace(1e3, 2e3, N)&#xa;    >>> x = amp * np.sin(2*np.pi*freq*time)&#xa;    >>> x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)&#xa;&#xa;    Compute and plot the spectrogram.&#xa;&#xa;    >>> f, t, Sxx = signal.spectrogram(x, fs)&#xa;    >>> plt.pcolormesh(t, f, Sxx)&#xa;    >>> plt.ylabel('Frequency [Hz]')&#xa;    >>> plt.xlabel('Time [sec]')&#xa;    >>> plt.show()&#xa;    """"""&#xa;    # Less overlap than welch, so samples are more statisically independent&#xa;    if noverlap is None:&#xa;        noverlap = nperseg // 8&#xa;&#xa;    freqs, time, Pxy = _spectral_helper(x, x, fs, window, nperseg, noverlap,&#xa;                                        nfft, detrend, return_onesided, scaling,&#xa;                                        axis, mode='psd')&#xa;&#xa;    return freqs, time, Pxy&#xa;&#xa;&#xa;def coherence(x, y, fs=1.0, window='hanning', nperseg=256, noverlap=None,&#xa;              nfft=None, detrend='constant', axis=-1):&#xa;    """"""&#xa;    Estimate the magnitude squared coherence estimate, Cxy, of discrete-time&#xa;    signals X and Y using Welch's method.&#xa;&#xa;    Cxy = abs(Pxy)**2/(Pxx*Pyy), where Pxx and Pyy are power spectral density&#xa;    estimates of X and Y, and Pxy is the cross spectral density estimate of X&#xa;    and Y.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    x : array_like&#xa;        Time series of measurement values&#xa;    y : array_like&#xa;        Time series of measurement values&#xa;    fs : float, optional&#xa;        Sampling frequency of the `x` and `y` time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is array_like it will be used&#xa;        directly as the window and its length will be used for nperseg.&#xa;        Defaults to 'hanning'.&#xa;    nperseg : int, optional&#xa;        Length of each segment.  Defaults to 256.&#xa;    noverlap: int, optional&#xa;        Number of points to overlap between segments. If None,&#xa;        ``noverlap = nperseg // 2``.  Defaults to None.&#xa;    nfft : int, optional&#xa;        Length of the FFT used, if a zero padded FFT is desired.  If None,&#xa;        the FFT length is `nperseg`. Defaults to None.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend each segment. If `detrend` is a string,&#xa;        it is passed as the ``type`` argument to `detrend`.  If it is a&#xa;        function, it takes a segment and returns a detrended segment.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    axis : int, optional&#xa;        Axis along which the coherence is computed for both inputs; the default is&#xa;        over the last axis (i.e. ``axis=-1``).&#xa;&#xa;    Returns&#xa;    -------&#xa;    f : ndarray&#xa;        Array of sample frequencies.&#xa;    Cxy : ndarray&#xa;        Magnitude squared coherence of x and y.&#xa;&#xa;    See Also&#xa;    --------&#xa;    periodogram: Simple, optionally modified periodogram&#xa;    lombscargle: Lomb-Scargle periodogram for unevenly sampled data&#xa;    welch: Power spectral density by Welch's method.&#xa;    csd: Cross spectral density by Welch's method.&#xa;&#xa;    Notes&#xa;    --------&#xa;    An appropriate amount of overlap will depend on the choice of window&#xa;    and on your requirements.  For the default 'hanning' window an&#xa;    overlap of 50\% is a reasonable trade off between accurately estimating&#xa;    the signal power, while not over counting any of the data.  Narrower&#xa;    windows may require a larger overlap.&#xa;&#xa;    .. versionadded:: 0.16.0&#xa;&#xa;    References&#xa;    ----------&#xa;    .. [1] P. Welch, ""The use of the fast Fourier transform for the&#xa;           estimation of power spectra: A method based on time averaging&#xa;           over short, modified periodograms"", IEEE Trans. Audio&#xa;           Electroacoust. vol. 15, pp. 70-73, 1967.&#xa;    .. [2] Stoica, Petre, and Randolph Moses, ""Spectral Analysis of Signals""&#xa;           Prentice Hall, 2005&#xa;&#xa;    Examples&#xa;    --------&#xa;    >>> from scipy import signal&#xa;    >>> import matplotlib.pyplot as plt&#xa;&#xa;    Generate two test signals with some common features.&#xa;&#xa;    >>> fs = 10e3&#xa;    >>> N = 1e5&#xa;    >>> amp = 20&#xa;    >>> freq = 1234.0&#xa;    >>> noise_power = 0.001 * fs / 2&#xa;    >>> time = np.arange(N) / fs&#xa;    >>> b, a = signal.butter(2, 0.25, 'low')&#xa;    >>> x = np.random.normal(scale=np.sqrt(noise_power), size=time.shape)&#xa;    >>> y = signal.lfilter(b, a, x)&#xa;    >>> x += amp*np.sin(2*np.pi*freq*time)&#xa;    >>> y += np.random.normal(scale=0.1*np.sqrt(noise_power), size=time.shape)&#xa;&#xa;    Compute and plot the coherence.&#xa;&#xa;    >>> f, Cxy = signal.coherence(x, y, fs, nperseg=1024)&#xa;    >>> plt.semilogy(f, Cxy)&#xa;    >>> plt.xlabel('frequency [Hz]')&#xa;    >>> plt.ylabel('Coherence')&#xa;    >>> plt.show()&#xa;    """"""&#xa;&#xa;    freqs, Pxx = welch(x, fs, window, nperseg, noverlap, nfft, detrend,&#xa;                       axis=axis)&#xa;    _, Pyy = welch(y, fs, window, nperseg, noverlap, nfft, detrend, axis=axis)&#xa;    _, Pxy = csd(x, y, fs, window, nperseg, noverlap, nfft, detrend, axis=axis)&#xa;&#xa;    Cxy = np.abs(Pxy)**2 / Pxx / Pyy&#xa;&#xa;    return freqs, Cxy&#xa;&#xa;&#xa;def _spectral_helper(x, y, fs=1.0, window='hanning', nperseg=256,&#xa;                    noverlap=None, nfft=None, detrend='constant',&#xa;                    return_onesided=True, scaling='spectrum', axis=-1,&#xa;                    mode='psd'):&#xa;    '''&#xa;    Calculate various forms of windowed FFTs for PSD, CSD, etc.&#xa;&#xa;    This is a helper function that implements the commonality between the&#xa;    psd, csd, and spectrogram functions. It is not designed to be called&#xa;    externally. The windows are not averaged over; the result from each window&#xa;    is returned.&#xa;&#xa;    Parameters&#xa;    ---------&#xa;    x : array_like&#xa;        Array or sequence containing the data to be analyzed.&#xa;    y : array_like&#xa;        Array or sequence containing the data to be analyzed. If this is&#xa;        the same object in memoery as x (i.e. _spectral_helper(x, x, ...)),&#xa;        the extra computations are spared.&#xa;    fs : float, optional&#xa;        Sampling frequency of the time series. Defaults to 1.0.&#xa;    window : str or tuple or array_like, optional&#xa;        Desired window to use. See `get_window` for a list of windows and&#xa;        required parameters. If `window` is array_like it will be used&#xa;        directly as the window and its length will be used for nperseg.&#xa;        Defaults to 'hanning'.&#xa;    nperseg : int, optional&#xa;        Length of each segment.  Defaults to 256.&#xa;    noverlap : int, optional&#xa;        Number of points to overlap between segments. If None,&#xa;        ``noverlap = nperseg // 2``.  Defaults to None.&#xa;    nfft : int, optional&#xa;        Length of the FFT used, if a zero padded FFT is desired.  If None,&#xa;        the FFT length is `nperseg`. Defaults to None.&#xa;    detrend : str or function or False, optional&#xa;        Specifies how to detrend each segment. If `detrend` is a string,&#xa;        it is passed as the ``type`` argument to `detrend`.  If it is a&#xa;        function, it takes a segment and returns a detrended segment.&#xa;        If `detrend` is False, no detrending is done.  Defaults to 'constant'.&#xa;    return_onesided : bool, optional&#xa;        If True, return a one-sided spectrum for real data. If False return&#xa;        a two-sided spectrum. Note that for complex data, a two-sided&#xa;        spectrum is always returned.&#xa;    scaling : { 'density', 'spectrum' }, optional&#xa;        Selects between computing the cross spectral density ('density')&#xa;        where `Pxy` has units of V**2/Hz and computing the cross spectrum&#xa;        ('spectrum') where `Pxy` has units of V**2, if `x` and `y` are&#xa;        measured in V and fs is measured in Hz.  Defaults to 'density'&#xa;    axis : int, optional&#xa;        Axis along which the periodogram is computed; the default is over&#xa;        the last axis (i.e. ``axis=-1``).&#xa;    mode : str, optional&#xa;        Defines what kind of return values are expected. Options are ['psd',&#xa;        'complex', 'magnitude', 'angle', 'phase'].&#xa;&#xa;    Returns&#xa;    -------&#xa;    freqs : ndarray&#xa;        Array of sample frequencies.&#xa;    result : ndarray&#xa;        Array of output data, contents dependant on *mode* kwarg.&#xa;    t : ndarray&#xa;        Array of times corresponding to each data segment&#xa;&#xa;    References&#xa;    ----------&#xa;    stackoverflow: Rolling window for 1D arrays in Numpy?&#xa;    <http://stackoverflow.com/a/6811241>&#xa;    stackoverflow: Using strides for an efficient moving average filter&#xa;    <http://stackoverflow.com/a/4947453>&#xa;&#xa;    Notes&#xa;    -----&#xa;    Adapted from matplotlib.mlab&#xa;&#xa;    .. versionadded:: 0.16.0&#xa;    '''&#xa;    if mode not in ['psd', 'complex', 'magnitude', 'angle', 'phase']:&#xa;        raise ValueError(""Unknown value for mode %s, must be one of: ""&#xa;                         ""'default', 'psd', 'complex', ""&#xa;                         ""'magnitude', 'angle', 'phase'"" % mode)&#xa;&#xa;    # If x and y are the same object we can save ourselves some computation.&#xa;    same_data = y is x&#xa;&#xa;    if not same_data and mode != 'psd':&#xa;        raise ValueError(""x and y must be equal if mode is not 'psd'"")&#xa;&#xa;    axis = int(axis)&#xa;&#xa;    # Ensure we have np.arrays, get outdtype&#xa;    x = np.asarray(x)&#xa;    if not same_data:&#xa;        y = np.asarray(y)&#xa;        outdtype = np.result_type(x,y,np.complex64)&#xa;    else:&#xa;        outdtype = np.result_type(x,np.complex64)&#xa;&#xa;    if not same_data:&#xa;        # Check if we can broadcast the outer axes together&#xa;        xouter = list(x.shape)&#xa;        youter = list(y.shape)&#xa;        xouter.pop(axis)&#xa;        youter.pop(axis)&#xa;        try:&#xa;            outershape = np.broadcast(np.empty(xouter), np.empty(youter)).shape&#xa;        except ValueError:&#xa;            raise ValueError('x and y cannot be broadcast together.')&#xa;&#xa;    if same_data:&#xa;        if x.size == 0:&#xa;            return np.empty(x.shape), np.empty(x.shape), np.empty(x.shape)&#xa;    else:&#xa;        if x.size == 0 or y.size == 0:&#xa;            outshape = outershape + (min([x.shape[axis], y.shape[axis]]),)&#xa;            emptyout = np.rollaxis(np.empty(outshape), -1, axis)&#xa;            return emptyout, emptyout, emptyout&#xa;&#xa;    if x.ndim > 1:&#xa;        if axis != -1:&#xa;            x = np.rollaxis(x, axis, len(x.shape))&#xa;            if not same_data and y.ndim > 1:&#xa;                y = np.rollaxis(y, axis, len(y.shape))&#xa;&#xa;    # Check if x and y are the same length, zero-pad if neccesary&#xa;    if not same_data:&#xa;        if x.shape[-1] != y.shape[-1]:&#xa;            if x.shape[-1] < y.shape[-1]:&#xa;                pad_shape = list(x.shape)&#xa;                pad_shape[-1] = y.shape[-1] - x.shape[-1]&#xa;                x = np.concatenate((x, np.zeros(pad_shape)), -1)&#xa;            else:&#xa;                pad_shape = list(y.shape)&#xa;                pad_shape[-1] = x.shape[-1] - y.shape[-1]&#xa;                y = np.concatenate((y, np.zeros(pad_shape)), -1)&#xa;&#xa;    # X and Y are same length now, can test nperseg with either&#xa;    if x.shape[-1] < nperseg:&#xa;        warnings.warn('nperseg = {0:d}, is greater than input length = {1:d}, '&#xa;                      'using nperseg = {1:d}'.format(nperseg, x.shape[-1]))&#xa;        nperseg = x.shape[-1]&#xa;&#xa;    nperseg = int(nperseg)&#xa;    if nperseg < 1:&#xa;        raise ValueError('nperseg must be a positive integer')&#xa;&#xa;    if nfft is None:&#xa;        nfft = nperseg&#xa;    elif nfft < nperseg:&#xa;        raise ValueError('nfft must be greater than or equal to nperseg.')&#xa;    else:&#xa;        nfft = int(nfft)&#xa;&#xa;    if noverlap is None:&#xa;        noverlap = nperseg//2&#xa;    elif noverlap >= nperseg:&#xa;        raise ValueError('noverlap must be less than nperseg.')&#xa;    else:&#xa;        noverlap = int(noverlap)&#xa;&#xa;    # Handle detrending and window functions&#xa;    if not detrend:&#xa;        def detrend_func(d):&#xa;            return d&#xa;    elif not hasattr(detrend, '__call__'):&#xa;        def detrend_func(d):&#xa;            return signaltools.detrend(d, type=detrend, axis=-1)&#xa;    elif axis != -1:&#xa;        # Wrap this function so that it receives a shape that it could&#xa;        # reasonably expect to receive.&#xa;        def detrend_func(d):&#xa;            d = np.rollaxis(d, -1, axis)&#xa;            d = detrend(d)&#xa;            return np.rollaxis(d, axis, len(d.shape))&#xa;    else:&#xa;        detrend_func = detrend&#xa;&#xa;    if isinstance(window, string_types) or type(window) is tuple:&#xa;        win = get_window(window, nperseg)&#xa;    else:&#xa;        win = np.asarray(window)&#xa;        if len(win.shape) != 1:&#xa;            raise ValueError('window must be 1-D')&#xa;        if win.shape[0] != nperseg:&#xa;            raise ValueError('window must have length of nperseg')&#xa;&#xa;    if np.result_type(win,np.complex64) != outdtype:&#xa;        win = win.astype(outdtype)&#xa;&#xa;    if mode == 'psd':&#xa;        if scaling == 'density':&#xa;            scale = 1.0 / (fs * (win*win).sum())&#xa;        elif scaling == 'spectrum':&#xa;            scale = 1.0 / win.sum()**2&#xa;        else:&#xa;            raise ValueError('Unknown scaling: %r' % scaling)&#xa;    else:&#xa;        scale = 1&#xa;&#xa;    if return_onesided is True:&#xa;        if np.iscomplexobj(x):&#xa;            sides = 'twosided'&#xa;        else:&#xa;            sides = 'onesided'&#xa;            if not same_data:&#xa;                if np.iscomplexobj(y):&#xa;                    sides = 'twosided'&#xa;    else:&#xa;        sides = 'twosided'&#xa;&#xa;    if sides == 'twosided':&#xa;        num_freqs = nfft&#xa;    elif sides == 'onesided':&#xa;        if nfft % 2:&#xa;            num_freqs = (nfft + 1)//2&#xa;        else:&#xa;            num_freqs = nfft//2 + 1&#xa;&#xa;    # Perform the windowed FFTs&#xa;    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft)&#xa;    result = result[..., :num_freqs]&#xa;    freqs = fftpack.fftfreq(nfft, 1/fs)[:num_freqs]&#xa;&#xa;    if not same_data:&#xa;        # All the same operations on the y data&#xa;        result_y = _fft_helper(y, win, detrend_func, nperseg, noverlap, nfft)&#xa;        result_y = result_y[..., :num_freqs]&#xa;        result = np.conjugate(result) * result_y&#xa;    elif mode == 'psd':&#xa;        result = np.conjugate(result) * result&#xa;    elif mode == 'magnitude':&#xa;        result = np.absolute(result)&#xa;    elif mode == 'angle' or mode == 'phase':&#xa;        result = np.angle(result)&#xa;    elif mode == 'complex':&#xa;        pass&#xa;&#xa;    result *= scale&#xa;    if sides == 'onesided':&#xa;        if nfft % 2:&#xa;            result[...,1:] *= 2&#xa;        else:&#xa;            # Last point is unpaired Nyquist freq point, don't double&#xa;            result[...,1:-1] *= 2&#xa;&#xa;    t = np.arange(nperseg/2, x.shape[-1] - nperseg/2 + 1, nperseg - noverlap)/float(fs)&#xa;&#xa;    if sides != 'twosided' and not nfft % 2:&#xa;        # get the last value correctly, it is negative otherwise&#xa;        freqs[-1] *= -1&#xa;&#xa;    # we unwrap the phase here to handle the onesided vs. twosided case&#xa;    if mode == 'phase':&#xa;        result = np.unwrap(result, axis=-1)&#xa;&#xa;    result = result.astype(outdtype)&#xa;&#xa;    # All imaginary parts are zero anyways&#xa;    if same_data and mode != 'complex':&#xa;        result = result.real&#xa;&#xa;    # Output is going to have new last axis for window index&#xa;    if axis != -1:&#xa;        # Specify as positive axis index&#xa;        if axis < 0:&#xa;            axis = len(result.shape)-1-axis&#xa;&#xa;        # Roll frequency axis back to axis where the data came from&#xa;        result = np.rollaxis(result, -1, axis)&#xa;    else:&#xa;        # Make sure window/time index is last axis&#xa;        result = np.rollaxis(result, -1, -2)&#xa;&#xa;    return freqs, t, result&#xa;&#xa;&#xa;def _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft):&#xa;    '''&#xa;    Calculate windowed FFT, for internal use by scipy.signal._spectral_helper&#xa;&#xa;    This is a helper function that does the main FFT calculation for&#xa;    _spectral helper. All input valdiation is performed there, and the data&#xa;    axis is assumed to be the last axis of x. It is not designed to be called&#xa;    externally. The windows are not averaged over; the result from each window&#xa;    is returned.&#xa;&#xa;    Returns&#xa;    -------&#xa;    result : ndarray&#xa;        Array of FFT data&#xa;&#xa;    References&#xa;    ----------&#xa;    stackoverflow: Repeat NumPy array without replicating data?&#xa;    <http://stackoverflow.com/a/5568169>&#xa;&#xa;    Notes&#xa;    -----&#xa;    Adapted from matplotlib.mlab&#xa;&#xa;    .. versionadded:: 0.16.0&#xa;    '''&#xa;    # Created strided array of data segments&#xa;    if nperseg == 1 and noverlap == 0:&#xa;        result = x[..., np.newaxis]&#xa;    else:&#xa;        step = nperseg - noverlap&#xa;        shape = x.shape[:-1]+((x.shape[-1]-noverlap)//step, nperseg)&#xa;        strides = x.strides[:-1]+(step*x.strides[-1], x.strides[-1])&#xa;        result = np.lib.stride_tricks.as_strided(x, shape=shape,&#xa;                                                 strides=strides)&#xa;&#xa;    # Detrend each data segment individually&#xa;    result = detrend_func(result)&#xa;&#xa;    # Apply window by multiplication&#xa;    result = win * result&#xa;&#xa;    # Perform the fft. Acts on last axis by default. Zero-pads automatically&#xa;    result = fftpack.fft(result, n=nfft)&#xa;&#xa;    return result&#xa;"
15546273|"#!/usr/bin/python&#xa;&#xa;import json&#xa;import os&#xa;import re&#xa;import subprocess as subp # for shell commands&#xa;import math&#xa;from operator import itemgetter # for sorting lists by dict value&#xa;from lxml import etree as et&#xa;&#xa;try:&#xa;    # Python 3&#xa;    import html.parser as HTMLParser&#xa;except:&#xa;    # Python 2&#xa;    import HTMLParser&#xa;&#xa;from pkg_resources import get_distribution&#xa;&#xa;import pcbmode.config as config&#xa;&#xa;# pcbmode modules&#xa;from .point import Point&#xa;from . import messages as msg&#xa;import hashlib&#xa;&#xa;&#xa;&#xa;&#xa;def dictToStyleText(style_dict):&#xa;    """"""&#xa;    Convert a dictionary into an SVG/CSS style attribute&#xa;    """"""&#xa;&#xa;    style = ''&#xa;    for key in style_dict:&#xa;        style += ""%s:%s;"" % (key, style_dict[key])&#xa;&#xa;    return style&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def openBoardSVG():&#xa;    """"""&#xa;    Opens the built PCBmodE board SVG.&#xa;    Returns an ElementTree object&#xa;    """"""&#xa;&#xa;    filename = os.path.join(config.cfg['base-dir'],&#xa;                            config.cfg['locations']['build'],&#xa;                            config.cfg['name'] + '.svg')&#xa;    try:&#xa;        data = et.ElementTree(file=filename) &#xa;    except IOError as e:&#xa;        msg.error(""Cannot open %s; has the board been made using the '-m' option yet?"" % filename)&#xa;&#xa;    return data&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parseDimension(string):&#xa;    """"""&#xa;    Parses a dimention recieved from the source files, separating the units,&#xa;    if specified, from the value&#xa;    """"""&#xa;    if string != None:&#xa;        result = re.match('(-?\d*\.?\d+)\s?(\w+)?', string)&#xa;        value = float(result.group(1))&#xa;        unit = result.group(2)&#xa;    else:&#xa;        value = None&#xa;        unit = None&#xa;    return value, unit&#xa;&#xa;&#xa;&#xa;&#xa;def to_Point(coord=[0, 0]):&#xa;    """"""&#xa;    Takes a coordinate in the form of [x,y] and&#xa;    returns a Point type&#xa;    """"""&#xa;    return Point(coord[0], coord[1])&#xa;&#xa;&#xa;&#xa;&#xa;def toPoint(coord=[0, 0]):&#xa;    """"""&#xa;    Takes a coordinate in the form of [x,y] and&#xa;    returns a Point type&#xa;    """"""&#xa;    if coord == None:&#xa;        return None&#xa;    else:&#xa;        return Point(coord[0], coord[1])&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def get_git_revision():&#xa;&#xa;    return get_distribution('pcbmode').version&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def makePngs():&#xa;    """"""&#xa;    Creates a PNG of the board using Inkscape&#xa;    """"""&#xa;&#xa;    # Directory for storing the Gerbers within the build path&#xa;    images_path = os.path.join(config.cfg['base-dir'], &#xa;                               config.cfg['locations']['build'], &#xa;                               'images')&#xa;    # Create it if it doesn't exist&#xa;    create_dir(images_path)&#xa;&#xa;    # create individual PNG files for layers&#xa;    png_dpi = 600&#xa;    msg.subInfo(""Generating PNGs for each layer of the board"")&#xa;&#xa;    command = ['inkscape', &#xa;               '--without-gui', &#xa;               '--file=%s' % os.path.join(config.cfg['base-dir'], &#xa;                                          config.cfg['locations']['build'], &#xa;                                          config.cfg['name'] + '.svg'), &#xa;               '--export-png=%s' % os.path.join(images_path, config.cfg['name'] + '_rev_' + &#xa;                                                config.brd['config']['rev'] +&#xa;                                                '.png'),&#xa;               '--export-dpi=%s' % str(png_dpi),&#xa;               '--export-area-drawing',&#xa;               '--export-background=#FFFFFF']&#xa;    &#xa;    try:&#xa;        subp.call(command)&#xa;    except OSError as e:&#xa;        msg.error(""Cannot find, or run, Inkscape in commandline mode"")&#xa;&#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;# get_json_data_from_file&#xa;def dictFromJsonFile(filename, error=True):&#xa;    """"""&#xa;    Open a json file and returns its content as a dict&#xa;    """"""&#xa;&#xa;    def checking_for_unique_keys(pairs):&#xa;        """"""&#xa;        Check if there are duplicate keys defined; this is useful&#xa;        for any hand-edited file&#xa;  &#xa;        This SO answer was useful here:&#xa;          http://stackoverflow.com/questions/16172011/json-in-python-receive-check-duplicate-key-error&#xa;        """"""&#xa;        result = dict()&#xa;        for key,value in pairs:&#xa;            if key in result:&#xa;                msg.error(""duplicate key ('%s') specified in %s"" % (key, filename), KeyError)&#xa;            result[key] = value&#xa;        return result&#xa;&#xa;    try:&#xa;        with open(filename, 'r') as f:&#xa;            json_data = json.load(f, object_pairs_hook=checking_for_unique_keys)&#xa;    except (IOError, OSError):&#xa;        if error == True:&#xa;            msg.error(""Couldn't open JSON file: %s"" % filename, IOError)&#xa;        else:&#xa;            msg.info(""Couldn't open JSON file: %s"" % filename, IOError)&#xa;&#xa;    return json_data&#xa;&#xa;&#xa;&#xa;&#xa;def getLayerList():&#xa;    """"""&#xa;    """"""&#xa;    layer_list = []&#xa;    for record in config.stk['stackup']:&#xa;        if record['type'] == 'signal-layer-surface' or record['type'] == 'signal-layer-internal':&#xa;            layer_list.append(record)&#xa;&#xa;    layer_names = []&#xa;    for record in layer_list:&#xa;        layer_names.append(record['name'])&#xa;&#xa;    return layer_list, layer_names&#xa;&#xa;&#xa;&#xa;def getSurfaceLayers():&#xa;    """"""&#xa;    Returns a list of surface layer names&#xa;    Only here until this function is purged from the&#xa;    codebase&#xa;    """"""    &#xa;    return config.stk['surface-layer-names']&#xa;&#xa;&#xa;&#xa;&#xa;def getInternalLayers():&#xa;    """"""&#xa;    Returns a list of internal layer names&#xa;    Only here until this function is purged from the&#xa;    codebase&#xa;    """"""    &#xa;    return config.stk['internal-layer-names']&#xa;&#xa;&#xa;&#xa;&#xa;def getExtendedLayerList(layers):&#xa;    """"""&#xa;    For the list of layers we may get a list of all&#xa;    internal layers ('internal-1', 'internal-2, etc.) or&#xa;    simply 'internal', meaning that that shape is meant&#xa;    to go into all internal layers, which is the most&#xa;    common case. The following 'expands' the layer list&#xa;    """"""&#xa;    if 'internal' in layers:&#xa;        layers.remove('internal')&#xa;        layers.extend(config.stk['internal-layer-names']) &#xa;    return layers&#xa;&#xa;&#xa;&#xa;&#xa;def getExtendedSheetList(layer, sheet):&#xa;    """"""&#xa;    We may want multiple sheets of the same type, such as two&#xa;    soldermask layers on the same physical layer. This function&#xa;    expands the list if such layers are defined in the stackup&#xa;    """"""&#xa;    &#xa;    for layer_dict in config.stk['layers-dict']:&#xa;        if layer_dict['name'] == layer:&#xa;            break&#xa;    stack_sheets = layer_dict['stack']&#xa;&#xa;    sheet_names = []&#xa;    for stack_sheet in stack_sheets:&#xa;        sheet_names.append(stack_sheet['name'])&#xa;&#xa;    new_list = []&#xa;    for sheet_name in sheet_names:&#xa;        if sheet_name.startswith(sheet):&#xa;            new_list.append(sheet_name)&#xa;&#xa;    return new_list&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def create_dir(path):&#xa;    """"""&#xa;    Checks if a directory exists, and creates one if not&#xa;    """"""&#xa;    &#xa;    try:&#xa;        # try to create directory first; this prevents TOCTTOU-type race condition&#xa;        os.makedirs(path)&#xa;    except OSError:&#xa;        # if the dir exists, pass&#xa;        if os.path.isdir(path):&#xa;            pass&#xa;        else:&#xa;            print(""ERROR: couldn't create build path %s"" % path)&#xa;            raise&#xa;&#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def add_dict_values(d1, d2):&#xa;    """"""&#xa;    Add the values of two dicts&#xa;    Helpful code here:&#xa;      http://stackoverflow.com/questions/1031199/adding-dictionaries-in-python&#xa;    """"""&#xa;&#xa;    return dict((n, d1.get(n, 0)+d2.get(n, 0)) for n in set(d1)|set(d2) )&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def process_meander_type(type_string, meander_type):&#xa;    """"""&#xa;    Extract meander path type parameters and return them as a dict&#xa;    """"""&#xa;&#xa;    if (meander_type == 'meander-round'):&#xa;        look_for = ['radius', 'theta', 'bus-width', 'pitch']&#xa;    elif (meander_type == 'meander-sawtooth'):&#xa;        look_for = ['base-length', 'amplitude', 'bus-width', 'pitch']&#xa;    else:&#xa;        print(""ERROR: unrecognised meander type"")&#xa;        reaise&#xa;&#xa;    meander = {}&#xa;&#xa;    regex = '\s*%s\s*:\s*(?P<v>[^;]*)'&#xa;&#xa;    for param in look_for:&#xa;        tmp = re.search(regex % param, type_string)&#xa;        if tmp is not None:&#xa;            meander[param] = float(tmp.group('v'))&#xa;&#xa;    # add optional fields as 'None'&#xa;    for param in look_for:    &#xa;        if meander.get(param) is None:&#xa;            meander[param] = None&#xa;&#xa;    return meander&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def checkForPoursInLayer(layer):&#xa;    """"""&#xa;    Returns True or False if there are pours in the specified layer&#xa;    """"""&#xa;&#xa;    # In case there are no 'shapes' defined&#xa;    try:&#xa;        pours = config.brd['shapes'].get('pours')&#xa;    except:&#xa;        pours = {}&#xa;&#xa;    if pours is not None:&#xa;        for pour_dict in pours:&#xa;            layers = getExtendedLayerList(pour_dict.get('layers'))&#xa;            if layer in layers:&#xa;                return True&#xa; &#xa;    #return False&#xa;    return True&#xa;&#xa;&#xa;&#xa;&#xa;def interpret_svg_matrix(matrix_data):&#xa;    """"""&#xa;    Takes an array for six SVG parameters and returns angle, scale &#xa;    and placement coordinate&#xa;&#xa;    This SO answer was helpful here:&#xa;      http://stackoverflow.com/questions/15546273/svg-matrix-to-rotation-degrees&#xa;    """"""&#xa;&#xa;    # apply float() to all elements, just in case&#xa;    matrix_data = [ float(x) for x in matrix_data ]&#xa;&#xa;    coord = Point(matrix_data[4], -matrix_data[5])&#xa;    if matrix_data[0] == 0:&#xa;        angle = math.degrees(0)&#xa;    else:&#xa;        angle = math.atan(matrix_data[2] / matrix_data[0])&#xa;    &#xa;    scale = Point(math.fabs(matrix_data[0] / math.cos(angle)), &#xa;                  math.fabs(matrix_data[3] / math.cos(angle)))&#xa;&#xa;    # convert angle to degrees&#xa;    angle = math.degrees(angle)&#xa;&#xa;    # Inkscape rotates anti-clockwise, PCBmodE ""thinks"" clockwise. The following &#xa;    # adjusts these two views, although at some point we'd&#xa;    # need to have the same view, or make it configurable&#xa;    angle = -angle&#xa;&#xa;    return coord, angle, scale&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parse_refdef(refdef):&#xa;    """"""&#xa;    Parses a reference designator and returns the refdef categoty,&#xa;    number, and extra characters&#xa;    """"""&#xa;&#xa;    regex = r'^(?P<t>[a-zA-z\D]+?)(?P<n>\d+)(?P<e>[\-\s].*)?'&#xa;    parse = re.match(regex, refdef)&#xa;&#xa;    # TODO: there has to be a more elegant way for doing this!&#xa;    if parse == None:&#xa;        return None, None, None&#xa;    else:&#xa;        t = parse.group('t')&#xa;        n = int(parse.group('n'))&#xa;        e = parse.group('e')&#xa;        return t, n, e&#xa;    &#xa;&#xa;&#xa;&#xa;&#xa;&#xa;#def renumber_refdefs(cfg, order):&#xa;def renumberRefdefs(order):&#xa;    """"""&#xa;    Renumber the refdefs in the specified order&#xa;    """"""&#xa;&#xa;    components = config.brd['components']&#xa;    comp_dict = {}&#xa;    new_dict = {}&#xa;&#xa;    for refdef in components:&#xa;&#xa;        rd_type, rd_number, rd_extra = parse_refdef(refdef)&#xa;        location = to_Point(components[refdef].get('location') or [0, 0])&#xa;        tmp = {}&#xa;        tmp['record'] = components[refdef]&#xa;        tmp['type'] = rd_type&#xa;        tmp['number'] = rd_number&#xa;        tmp['extra'] = rd_extra&#xa;        tmp['coord-x'] = location.x&#xa;        tmp['coord-y'] = location.y&#xa;&#xa;        if comp_dict.get(rd_type) == None:&#xa;            comp_dict[rd_type] = []&#xa;        comp_dict[rd_type].append(tmp)&#xa;&#xa;        # Sort list according to 'order'&#xa;        for comp_type in comp_dict:&#xa;           if order == 'left-to-right':&#xa;               reverse = False&#xa;               itemget_param = 'coord_x'&#xa;           elif order == 'right-to-left':&#xa;               reverse = True&#xa;               itemget_param = 'coord_x'&#xa;           elif order == 'top-to-bottom':&#xa;               reverse = True&#xa;               itemget_param = 'coord-y'&#xa;           elif order == 'bottom-to-top':&#xa;               reverse = False&#xa;               itemget_param = 'coord-y'&#xa;           else:&#xa;               msg.error('Unrecognised renumbering order %s' % (order)) &#xa; &#xa;           sorted_list = sorted(comp_dict[comp_type], &#xa;                                key=itemgetter(itemget_param), &#xa;                                reverse=reverse)&#xa;&#xa;&#xa;           for i, record in enumerate(sorted_list):&#xa;               new_refdef = ""%s%s"" % (record['type'], i+1)&#xa;               if record['extra'] is not None:&#xa;                   new_refdef += ""%s"" % (record['extra'])&#xa;               new_dict[new_refdef] = record['record']&#xa; &#xa;    config.brd['components'] = new_dict&#xa;&#xa;    # Save board config to file (everything is saved, not only the&#xa;    # component data)&#xa;    filename = os.path.join(config.cfg['locations']['boards'], &#xa;                            config.cfg['name'], &#xa;                            config.cfg['name'] + '.json')&#xa;    try:&#xa;        with open(filename, 'wb') as f:&#xa;            f.write(json.dumps(config.brd, sort_keys=True, indent=2))&#xa;    except:&#xa;        msg.error(""Cannot save file %s"" % filename)&#xa; &#xa; &#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def getTextParams(font_size, letter_spacing, line_height):&#xa;    try:&#xa;        letter_spacing, letter_spacing_unit = parseDimension(letter_spacing)&#xa;    except:&#xa;        msg.error(""There's a problem with parsing the 'letter-spacing' property with value '%s'. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '-2 mm' should work."" % letter_spacing)&#xa;&#xa;    if letter_spacing_unit == None:&#xa;        letter_spacing_unit = 'mm'&#xa;&#xa;    try:&#xa;        line_height, line_height_unit = parseDimension(line_height)&#xa;    except:&#xa;        msg.error(""There's a problem parsing the 'line-height' property with value '%s'. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '-2 mm' should work."" % line_height)&#xa;&#xa;    if line_height_unit == None:&#xa;        line_height_unit = 'mm'&#xa;    &#xa;    try:&#xa;        font_size, font_size_unit = parseDimension(font_size)&#xa;    except:&#xa;        throw(""There's a problem parsing the 'font-size'. It's most likely missing. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '2 mm' should work. Of course, it needs to be a positive figure."")&#xa;&#xa;    if font_size_unit == None:&#xa;        font_size_unit = 'mm'&#xa;&#xa;    return float(font_size), float(letter_spacing), float(line_height)&#xa;&#xa;&#xa;&#xa;&#xa;def textToPath(font_data, text, letter_spacing, line_height, scale_factor):&#xa;    from .svgpath import SvgPath&#xa;    """"""&#xa;    Convert a text string (unicode and newlines allowed) to a path.&#xa;    The 'scale_factor' is needed in order to scale rp 'letter_spacing' and 'line_height'&#xa;    to the original scale of the font.&#xa;    """"""&#xa;&#xa;    # This the horizontal advance that applied to all glyphs unless there's a specification for&#xa;    # for the glyph itself&#xa;    font_horiz_adv_x = float(font_data.find(""//n:font"", namespaces={'n': config.cfg['namespace']['svg']}).get('horiz-adv-x'))&#xa;    &#xa;    # This is the number if 'units' per 'em'. The default, in the absence of a definition is 1000&#xa;    # according to the SVG spec&#xa;    units_per_em = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('units-per-em')) or 1000&#xa;&#xa;    glyph_ascent = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('ascent'))&#xa;    glyph_decent = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('descent'))&#xa; &#xa;    text_width = 0&#xa;    text_path = ''&#xa;&#xa;    # split text into charcters and find unicade chars&#xa;    try:&#xa;        text = re.findall(r'(\&#x[0-9abcdef]*;|.|\n)', text)&#xa;    except:&#xa;        throw(""There's a problem parsing the text '%s'. Unicode and \\n newline should be fine, by the way."" % text)&#xa; &#xa;&#xa;    # instantiate HTML parser&#xa;    htmlpar = HTMLParser.HTMLParser()&#xa;    gerber_lp = ''&#xa;    text_height = 0&#xa;&#xa;    if line_height == None:&#xa;        line_height = units_per_em&#xa;&#xa;    for i, symbol in enumerate(text[:]):&#xa;&#xa;        symbol = htmlpar.unescape(symbol)&#xa;        # get the glyph definition from the file&#xa;        if symbol == '\n':&#xa;            text_width = 0&#xa;            text_height += units_per_em + (line_height/scale_factor-units_per_em)&#xa;        else:&#xa;            glyph = font_data.find(u'//n:glyph[@unicode=""%s""]' % symbol, namespaces={'n': config.cfg['namespace']['svg']})&#xa;            if glyph == None:&#xa;                utils.throw(""Damn, there's no glyph definition for '%s' in the '%s' font :("" % (symbol, font))&#xa;            else:&#xa;                # Unless the glyph has its own width, use the global font width&#xa;                glyph_width = float(glyph.get('horiz-adv-x') or font_horiz_adv_x)&#xa;                if symbol != ' ':&#xa;                    glyph_path = SvgPath(glyph.get('d'))&#xa;                    first_point = glyph_path.getFirstPoint()&#xa;                    offset_x = float(first_point[0])&#xa;                    offset_y = float(first_point[1])&#xa;                    path = glyph_path.getRelative()&#xa;                    path = re.sub('^(m\s?[-\d\.]+\s?,\s?[-\d\.]+)', 'M %s,%s' % (str(text_width+offset_x), str(offset_y-text_height)), path)&#xa;                    gerber_lp += (glyph.get('gerber-lp') or &#xa;                                  glyph.get('gerber_lp') or &#xa;                                  ""%s"" % 'd'*glyph_path.getNumberOfSegments())&#xa;                    text_path += ""%s "" % (path)&#xa;&#xa;            text_width += glyph_width+letter_spacing/scale_factor&#xa;&#xa;&#xa;    # Mirror text &#xa;    text_path = SvgPath(text_path)&#xa;    text_path.transform()&#xa;    text_path = text_path.getTransformedMirrored()&#xa;&#xa;    return text_path, gerber_lp&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def digest(string):&#xa;    digits = config.cfg['digest-digits']&#xa;    return hashlib.md5(string.encode()).hexdigest()[:digits-1]&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def getStyleAttrib(style, attrib):&#xa;    """"""&#xa;    """"""&#xa;    regex = r"".*?%s:\s?(?P<s>[^;]*)(?:;|$)""&#xa;    match = re.match(regex % attrib, style)&#xa;    if match == None:&#xa;        return None&#xa;    else:&#xa;        return match.group('s')&#xa;&#xa;&#xa;&#xa;&#xa;def niceFloat(f):&#xa;    if f.is_integer():&#xa;        return int(f)&#xa;    else:&#xa;        return round(f, 6)&#xa;&#xa;&#xa;&#xa;&#xa;def parseTransform(transform):&#xa;    """"""&#xa;    Returns a Point() for the input transform&#xa;    """"""&#xa;    data = {}&#xa;    if transform == None:&#xa;        data['type'] = 'translate'&#xa;        data['location'] = Point()&#xa;    elif 'translate' in transform.lower():&#xa;        regex = r"".*?translate\s?\(\s?(?P<x>[+-]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?)\s?[\s,]\s?(?P<y>[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?)\s?\).*""&#xa;#       regex = r"".*?translate\s?\(\s?(?P<x>-?[0-9]*\.?[0-9]+)\s?[\s,]\s?(?P<y>-?[0-9]*\.?[0-9]+\s?)\s?\).*""&#xa;        coord = re.match(regex, transform)&#xa;        data['type'] = 'translate'&#xa;        x = coord.group('x')&#xa;        y = coord.group('y')&#xa;        data['location'] = Point(x,y)&#xa;    elif 'matrix' in transform.lower():&#xa;        data['type'] = 'matrix'&#xa;        data['location'], data['rotate'], data['scale'] = parseSvgMatrix(transform)&#xa;    else:&#xa;        msg.error(""Found a path transform that cannot be handled, %s. SVG stansforms should be in the form of 'translate(num,num)' or 'matrix(num,num,num,num,num,num)"" % transform)&#xa;&#xa;    return data &#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parseSvgMatrix(matrix):&#xa;    """"""&#xa;    Takes an array for six SVG parameters and returns angle, scale &#xa;    and placement coordinate&#xa;&#xa;    This SO answer was helpful here:&#xa;      http://stackoverflow.com/questions/15546273/svg-matrix-to-rotation-degrees&#xa;    """"""&#xa;    regex = r"".*?matrix\((?P<m>.*?)\).*""&#xa;    matrix = re.match(regex, matrix)&#xa;    matrix = matrix.group('m')&#xa;    matrix = matrix.split(',')&#xa;&#xa;    # Apply float() to all elements&#xa;    matrix = [ float(x) for x in matrix ]&#xa;&#xa;    coord = Point(matrix[4], matrix[5])&#xa;    if matrix[0] == 0:&#xa;        angle = math.degrees(0)&#xa;    else:&#xa;        angle = math.atan(matrix[2] / matrix[0])&#xa;    &#xa;    #scale = Point(math.fabs(matrix[0] / math.cos(angle)), &#xa;    #              math.fabs(matrix[3] / math.cos(angle)))&#xa;    scale_x = math.sqrt(matrix[0]*matrix[0] + matrix[1]*matrix[1]),&#xa;    scale_y = math.sqrt(matrix[2]*matrix[2] + matrix[3]*matrix[3]),    &#xa;&#xa;    scale = max(scale_x, scale_y)[0]&#xa;&#xa;    # convert angle to degrees&#xa;    angle = math.degrees(angle)&#xa;&#xa;    # Inkscape rotates anti-clockwise, PCBmodE ""thinks"" clockwise. The following &#xa;    # adjusts these two views, although at some point we'd&#xa;    # need to have the same view, or make it configurable&#xa;    angle = -angle&#xa;&#xa;    return coord, angle, scale&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;"
3209233|"#!/usr/bin/env python&#xa;""""""&#xa;# Inbound Proxy Module developed by Bharadwaj Machiraju (blog.tunnelshade.in) as a part of Google Summer of Code 2013.&#xa;""""""&#xa;&#xa;import os&#xa;import re&#xa;import ssl&#xa;import socket&#xa;import datetime&#xa;from multiprocessing import Value&#xa;&#xa;import pycurl&#xa;&#xa;import tornado.httpserver&#xa;import tornado.ioloop&#xa;import tornado.iostream&#xa;import tornado.web&#xa;import tornado.httpclient&#xa;import tornado.curl_httpclient&#xa;import tornado.escape&#xa;import tornado.httputil&#xa;import tornado.options&#xa;import tornado.template&#xa;import tornado.websocket&#xa;import tornado.gen&#xa;&#xa;from socket_wrapper import wrap_socket&#xa;from cache_handler import CacheHandler&#xa;from framework.dependency_management.dependency_resolver import BaseComponent, ComponentNotFoundException&#xa;from framework.utils import FileOperations&#xa;from framework.lib.owtf_process import OWTFProcess&#xa;&#xa;&#xa;def prepare_curl_callback(curl):&#xa;    curl.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_SOCKS5)&#xa;&#xa;&#xa;class ProxyHandler(tornado.web.RequestHandler):&#xa;&#xa;    """"""This RequestHandler processes all the requests that the application received.""""""&#xa;&#xa;    SUPPORTED_METHODS = ['GET', 'POST', 'CONNECT', 'HEAD', 'PUT', 'DELETE', 'OPTIONS', 'TRACE']&#xa;    server = None&#xa;    restricted_request_headers = None&#xa;    restricted_response_headers = None&#xa;&#xa;    def __new__(cls, application, request, **kwargs):&#xa;        # http://stackoverflow.com/questions/3209233/how-to-replace-an-instance-in-init-with-a-different-object&#xa;        # Based on upgrade header, websocket request handler must be used&#xa;        try:&#xa;            if request.headers['Upgrade'].lower() == 'websocket':&#xa;                return CustomWebSocketHandler(application, request, **kwargs)&#xa;        except KeyError:&#xa;            pass&#xa;        return tornado.web.RequestHandler.__new__(cls, application, request, **kwargs)&#xa;&#xa;    def set_default_headers(self):&#xa;        # Automatically called by Tornado,&#xa;        # Used to remove ""Server"" header set by tornado&#xa;        del self._headers[""Server""]&#xa;&#xa;    def set_status(self, status_code, reason=None):&#xa;        """"""Sets the status code for our response.&#xa;&#xa;        Overriding is done so as to handle unknown response codes gracefully.&#xa;        """"""&#xa;        self._status_code = status_code&#xa;        if reason is not None:&#xa;            self._reason = tornado.escape.native_str(reason)&#xa;        else:&#xa;            try:&#xa;                self._reason = tornado.httputil.responses[status_code]&#xa;            except KeyError:&#xa;                self._reason = tornado.escape.native_str(""Server Not Found"")&#xa;&#xa;    def finish_response(self, response):&#xa;        """"""Write a new response and cache it.""""""&#xa;        self.set_status(response.code)&#xa;        for header, value in response.headers.get_all():&#xa;            if header == ""Set-Cookie"":&#xa;                self.add_header(header, value)&#xa;            else:&#xa;                if header not in ProxyHandler.restricted_response_headers:&#xa;                    self.set_header(header, value)&#xa;        self.finish()&#xa;&#xa;    def handle_data_chunk(self, data):&#xa;        """"""Callback when a small chunk is received.""""""&#xa;        if data:&#xa;            self.write(data)&#xa;            self.request.response_buffer += data&#xa;&#xa;    @tornado.web.asynchronous&#xa;    @tornado.gen.coroutine&#xa;    def get(self):&#xa;        """"""Handle all requests except the connect request.&#xa;&#xa;        Once ssl stream is formed between browser and proxy, the requests are then processed by this function.&#xa;        """"""&#xa;        # The flow starts here&#xa;        self.request.local_timestamp = datetime.datetime.now()&#xa;        self.request.response_buffer = ''&#xa;&#xa;        # The requests that come through ssl streams are relative requests, so transparent proxying is required. The&#xa;        # following snippet decides the url that should be passed to the async client&#xa;        if self.request.uri.startswith(self.request.protocol, 0):  # Normal Proxy Request.&#xa;            self.request.url = self.request.uri&#xa;        else:  # Transparent Proxy Request.&#xa;            self.request.url = ""%s://%s"" % (self.request.protocol, self.request.host)&#xa;            if self.request.uri != '/':  # Add uri only if needed.&#xa;                self.request.url += self.request.uri&#xa;&#xa;        # This block here checks for already cached response and if present returns one&#xa;        self.cache_handler = CacheHandler(self.application.cache_dir, self.request, self.application.cookie_regex,&#xa;                                          self.application.cookie_blacklist)&#xa;        request_hash = yield tornado.gen.Task(self.cache_handler.calculate_hash)&#xa;        self.cached_response = self.cache_handler.load()&#xa;&#xa;        if self.cached_response:&#xa;            if self.cached_response.body:&#xa;                self.write(self.cached_response.body)&#xa;            self.finish_response(self.cached_response)&#xa;        else:&#xa;            # Request header cleaning&#xa;            for header in ProxyHandler.restricted_request_headers:&#xa;                try:&#xa;                    del self.request.headers[header]&#xa;                except:&#xa;                    continue&#xa;            # HTTP auth if exists&#xa;            http_auth_username = None&#xa;            http_auth_password = None&#xa;            http_auth_mode = None&#xa;            if self.application.http_auth:&#xa;                host = self.request.host&#xa;                # If default ports are not provided, they are added&#xa;                if ':' not in self.request.host:&#xa;                    default_ports = {'http': '80', 'https': '443'}&#xa;                    if self.request.protocol in default_ports:&#xa;                        host = '%s:%s' % (self.request.host, default_ports[self.request.protocol])&#xa;                # Check if auth is provided for that host&#xa;                try:&#xa;                    index = self.application.http_auth_hosts.index(host)&#xa;                    http_auth_username = self.application.http_auth_usernames[index]&#xa;                    http_auth_password = self.application.http_auth_passwords[index]&#xa;                    http_auth_mode = self.application.http_auth_modes[index]&#xa;                except ValueError:&#xa;                    pass&#xa;&#xa;            # pycurl is needed for curl client&#xa;            async_client = tornado.curl_httpclient.CurlAsyncHTTPClient()&#xa;            # httprequest object is created and then passed to async client with a callback&#xa;            success_response = False  # is used to check the response in the botnet mode&#xa;&#xa;            while not success_response:&#xa;                # Proxy Switching (botnet_mode) code&#xa;                if self.application.proxy_manager:&#xa;                    proxy = self.application.proxy_manager.get_next_available_proxy()&#xa;                    self.application.outbound_ip = proxy[""proxy""][0]&#xa;                    self.application.outbound_port = int(proxy[""proxy""][1])&#xa;                # httprequest object is created and then passed to async client with a callback&#xa;                callback = None&#xa;                if self.application.outbound_proxy_type == 'socks':&#xa;                    callback = prepare_curl_callback  # socks callback function.&#xa;                body = self.request.body or None&#xa;                request = tornado.httpclient.HTTPRequest(&#xa;                    url=self.request.url,&#xa;                    method=self.request.method,&#xa;                    body=body,&#xa;                    headers=self.request.headers,&#xa;                    auth_username=http_auth_username,&#xa;                    auth_password=http_auth_password,&#xa;                    auth_mode=http_auth_mode,&#xa;                    follow_redirects=False,&#xa;                    use_gzip=True,&#xa;                    streaming_callback=self.handle_data_chunk,&#xa;                    header_callback=None,&#xa;                    proxy_host=self.application.outbound_ip,&#xa;                    proxy_port=self.application.outbound_port,&#xa;                    proxy_username=self.application.outbound_username,&#xa;                    proxy_password=self.application.outbound_password,&#xa;                    allow_nonstandard_methods=True,&#xa;                    prepare_curl_callback=callback,&#xa;                    validate_cert=False&#xa;                )&#xa;                try:&#xa;                    response = yield tornado.gen.Task(async_client.fetch, request)&#xa;                except Exception:&#xa;                    response = None&#xa;                    pass&#xa;                # Request retries&#xa;                for i in range(0, 3):&#xa;                    if (response is None) or response.code in [408, 599]:&#xa;                        self.request.response_buffer = ''&#xa;                        response = yield tornado.gen.Task(async_client.fetch, request)&#xa;                    else:&#xa;                        success_response = True&#xa;                        break&#xa;&#xa;                # Botnet mode code (proxy switching).&#xa;                # Checking the status of the proxy (asynchronous).&#xa;                if self.application.proxy_manager and not success_response:&#xa;                    proxy_check_req = tornado.httpclient.HTTPRequest(&#xa;                        url=self.application.proxy_manager.testing_url,  # testing url is google.com.&#xa;                        use_gzip=True,&#xa;                        proxy_host=self.application.outbound_ip,&#xa;                        proxy_port=self.application.outbound_port,&#xa;                        proxy_username=self.application.outbound_username,&#xa;                        proxy_password=self.application.outbound_password,&#xa;                        prepare_curl_callback=callback,  # socks callback function.&#xa;                        validate_cert=False&#xa;                    )&#xa;                    try:&#xa;                        proxy_check_resp = yield tornado.gen.Task(async_client.fetch, proxy_check_req)&#xa;                    except Exception:&#xa;                        pass&#xa;&#xa;                    if proxy_check_resp.code != 200:&#xa;                        self.application.proxy_manager.remove_proxy(proxy[""index""])&#xa;                    else:&#xa;                        success_response = True&#xa;                else:&#xa;                    success_response = True&#xa;&#xa;            self.finish_response(response)&#xa;            # Cache the response after finishing the response, so caching time is not included in response time&#xa;            self.cache_handler.dump(response)&#xa;&#xa;    # The following 5 methods can be handled through the above implementation.&#xa;    @tornado.web.asynchronous&#xa;    def post(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def head(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def put(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def delete(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def options(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def trace(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def connect(self):&#xa;        """"""Gets called when a connect request is received.&#xa;&#xa;        * The host and port are obtained from the request uri&#xa;        * A socket is created, wrapped in ssl and then added to SSLIOStream&#xa;        * This stream is used to connect to speak to the remote host on given port&#xa;        * If the server speaks ssl on that port, callback start_tunnel is called&#xa;        * An OK response is written back to client&#xa;        * The client side socket is wrapped in ssl&#xa;        * If the wrapping is successful, a new SSLIOStream is made using that socket&#xa;        * The stream is added back to the server for monitoring&#xa;        """"""&#xa;        host, port = self.request.uri.split(':')&#xa;&#xa;        def start_tunnel():&#xa;            try:&#xa;                self.request.connection.stream.write(b""HTTP/1.1 200 Connection established\r\n\r\n"")&#xa;                wrap_socket(&#xa;                    self.request.connection.stream.socket,&#xa;                    host,&#xa;                    self.application.ca_cert,&#xa;                    self.application.ca_key,&#xa;                    self.application.ca_key_pass,&#xa;                    self.application.certs_folder,&#xa;                    success=ssl_success&#xa;                )&#xa;            except tornado.iostream.StreamClosedError:&#xa;                pass&#xa;&#xa;        def ssl_success(client_socket):&#xa;            client = tornado.iostream.SSLIOStream(client_socket)&#xa;            ProxyHandler.server.handle_stream(client, self.application.inbound_ip)&#xa;&#xa;        # Tiny Hack to satisfy proxychains CONNECT request to HTTP port.&#xa;        # HTTPS fail check has to be improvised&#xa;        def ssl_fail():&#xa;            try:&#xa;                self.request.connection.stream.write(b""HTTP/1.1 200 Connection established\r\n\r\n"")&#xa;            except tornado.iostream.StreamClosedError:&#xa;                pass&#xa;            ProxyHandler.server.handle_stream(self.request.connection.stream, self.application.inbound_ip)&#xa;&#xa;        # Hacking to be done here, so as to check for ssl using proxy and auth&#xa;        try:&#xa;            # Adds a fix for check_hostname errors in Tornado 4.3.0&#xa;            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)&#xa;            context.check_hostname = False&#xa;            context.load_default_certs()&#xa;            # When connecting through a new socket, no need to wrap the socket before passing&#xa;            # to SSIOStream&#xa;            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)&#xa;            upstream = tornado.iostream.SSLIOStream(s, ssl_options=context)&#xa;            upstream.set_close_callback(ssl_fail)&#xa;            upstream.connect((host, int(port)), start_tunnel)&#xa;        except Exception:&#xa;            self.finish()&#xa;&#xa;&#xa;class CustomWebSocketHandler(tornado.websocket.WebSocketHandler):&#xa;&#xa;    """"""Class is used for handling websocket traffic.&#xa;&#xa;    * Object of this class replaces the main request handler for a request with header => ""Upgrade: websocket""&#xa;    * wss:// - CONNECT request is handled by main handler&#xa;    """"""&#xa;&#xa;    def upstream_connect(self, io_loop=None, callback=None):&#xa;        """"""Custom alternative to tornado.websocket.websocket_connect.&#xa;&#xa;        Returns a future.&#xa;        """"""&#xa;        # io_loop is needed or it won't work with Tornado.&#xa;        if io_loop is None:&#xa;            io_loop = tornado.ioloop.IOLoop.current()&#xa;&#xa;        # During secure communication, we get relative URI, so make them absolute&#xa;        if self.request.uri.startswith(self.request.protocol, 0):  # Normal Proxy Request.&#xa;            self.request.url = self.request.uri&#xa;        # Transparent Proxy Request&#xa;        else:&#xa;            self.request.url = ""%s://%s%s"" % (self.request.protocol, self.request.host, self.request.uri)&#xa;        self.request.url = self.request.url.replace(""http"", ""ws"", 1)&#xa;&#xa;        # Have to add cookies and stuff&#xa;        request_headers = tornado.httputil.HTTPHeaders()&#xa;        for name, value in self.request.headers.items():&#xa;            if name not in ProxyHandler.restricted_request_headers:&#xa;                request_headers.add(name, value)&#xa;        # Build a custom request&#xa;        request = tornado.httpclient.HTTPRequest(&#xa;            url=self.request.url,&#xa;            headers=request_headers,&#xa;            proxy_host=self.application.outbound_ip,&#xa;            proxy_port=self.application.outbound_port,&#xa;            proxy_username=self.application.outbound_username,&#xa;            proxy_password=self.application.outbound_password&#xa;        )&#xa;        self.upstream_connection = CustomWebSocketClientConnection(io_loop, request)&#xa;        if callback is not None:&#xa;            io_loop.add_future(self.upstream_connection.connect_future, callback)&#xa;        return self.upstream_connection.connect_future&#xa;&#xa;    def _execute(self, transforms, *args, **kwargs):&#xa;        """"""Overriding of a method of WebSocketHandler.""""""&#xa;&#xa;        def start_tunnel(future):&#xa;            """"""A callback which is called when connection to url is successful.""""""&#xa;            # We need upstream to write further messages&#xa;            self.upstream = future.result()&#xa;            # HTTPRequest needed for caching&#xa;            self.handshake_request = self.upstream_connection.request&#xa;            # Needed for websocket data & compliance with cache_handler stuff&#xa;            self.handshake_request.response_buffer = ''&#xa;            # Tiny hack to protect caching (according to websocket standards)&#xa;            self.handshake_request.version = 'HTTP/1.1'&#xa;            # XXX: I dont know why a None is coming&#xa;            self.handshake_request.body = self.handshake_request.body or ''&#xa;            # The regular procedures are to be done&#xa;            tornado.websocket.WebSocketHandler._execute(self, transforms, *args, **kwargs)&#xa;&#xa;        # We try to connect to provided URL & then we proceed with connection on client side.&#xa;        self.upstream = self.upstream_connect(callback=start_tunnel)&#xa;&#xa;    def store_upstream_data(self, message):&#xa;        """"""Save websocket data sent from client to server.&#xa;&#xa;        i.e add it to HTTPRequest.response_buffer with direction (>>)&#xa;        """"""&#xa;        try:  # Cannot write binary content as a string, so catch it&#xa;            self.handshake_request.response_buffer += ("">>> %s\r\n"" % message)&#xa;        except TypeError:&#xa;            self.handshake_request.response_buffer += ("">>> May be binary\r\n"")&#xa;&#xa;    def store_downstream_data(self, message):&#xa;        """"""Save websocket data sent from client to server.&#xa;&#xa;        i.e add it to HTTPRequest.response_buffer with direction (<<)&#xa;        """"""&#xa;        try:  # Cannot write binary content as a string, so catch it.&#xa;            self.handshake_request.response_buffer += (""<<< %s\r\n"" % message)&#xa;        except TypeError:&#xa;            self.handshake_request.response_buffer += (""<<< May be binary\r\n"")&#xa;&#xa;    def on_message(self, message):&#xa;        """"""Everytime a message is received from client side, this instance method is called.""""""&#xa;        self.upstream.write_message(message)  # The obtained message is written to upstream.&#xa;        self.store_upstream_data(message)&#xa;&#xa;        # The following check ensures that if a callback is added for reading message from upstream, another one is not&#xa;        # added.&#xa;        if not self.upstream.read_future:&#xa;            # A callback is added to read the data when upstream responds.&#xa;            self.upstream.read_message(callback=self.on_response)&#xa;&#xa;    def on_response(self, message):&#xa;        """"""A callback when a message is recieved from upstream.""""""&#xa;        # The following check ensures that if a callback is added for reading message from upstream, another one is not&#xa;        # added&#xa;        if not self.upstream.read_future:&#xa;            self.upstream.read_message(callback=self.on_response)&#xa;        if self.ws_connection:  # Check if connection still exists.&#xa;            if message.result():  # Check if it is not NULL (indirect checking of upstream connection).&#xa;                self.write_message(message.result())  # Write obtained message to client.&#xa;                self.store_downstream_data(message.result())&#xa;            else:&#xa;                self.close()&#xa;&#xa;    def on_close(self):&#xa;        """"""Called when websocket is closed.&#xa;&#xa;        So handshake request-response pair along with websocket data as response body is saved&#xa;        """"""&#xa;        # Required for cache_handler&#xa;        self.handshake_response = tornado.httpclient.HTTPResponse(&#xa;            self.handshake_request,&#xa;            self.upstream_connection.code,&#xa;            headers=self.upstream_connection.headers,&#xa;            request_time=0&#xa;        )&#xa;        # Procedure for dumping a tornado request-response&#xa;        self.cache_handler = CacheHandler(&#xa;            self.application.cache_dir,&#xa;            self.handshake_request,&#xa;            self.application.cookie_regex,&#xa;            self.application.cookie_blacklist&#xa;        )&#xa;        self.cached_response = self.cache_handler.load()&#xa;        self.cache_handler.dump(self.handshake_response)&#xa;&#xa;&#xa;class CustomWebSocketClientConnection(tornado.websocket.WebSocketClientConnection):&#xa;    def _handle_1xx(self, code):&#xa;        # Had to extract response code, so it is necessary to override.&#xa;        self.code = code&#xa;        super(CustomWebSocketClientConnection, self)._handle_1xx(code)&#xa;&#xa;&#xa;class CommandHandler(tornado.web.RequestHandler):&#xa;&#xa;    """"""This handles the python function calls issued with relative url ""/JSON/?cmd="".&#xa;&#xa;    Responses are in JSON&#xa;    """"""&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def get(self, relative_url):&#xa;        # Currently only get requests are sufficient for providing PnH service commands.&#xa;        command_list = self.get_arguments(""cmd"")&#xa;        info = {}&#xa;        for command in command_list:&#xa;            if command.startswith(""Core""):&#xa;                command = ""self.application.%s"" % command&#xa;                info[command] = eval(command)&#xa;            if command.startswith(""setattr""):&#xa;                info[command] = eval(command)&#xa;        self.write(info)&#xa;        self.finish()&#xa;&#xa;&#xa;class ProxyProcess(OWTFProcess, BaseComponent):&#xa;&#xa;    def initialize(self, outbound_options=[], outbound_auth=""""):&#xa;        # The tornado application, which is used to pass variables to request handler&#xa;        self.application = tornado.web.Application(handlers=[(r'.*', ProxyHandler)], debug=False, gzip=True,)&#xa;        self.config = self.get_component(""config"")&#xa;        self.db_config = self.get_component(""db_config"")&#xa;        # All required variables in request handler&#xa;        # Required variables are added as attributes to application, so that request handler can access these&#xa;        self.application.Core = self.get_component(""core"")&#xa;        try:&#xa;            self.proxy_manager = self.get_component(""proxy_manager"")&#xa;        except ComponentNotFoundException:&#xa;            self.proxy_manager = None&#xa;        self.application.proxy_manager = self.proxy_manager&#xa;        # ctypes object allocated from shared memory to verify if proxy must inject probe code or not&#xa;        # 'i' means ctypes type is integer, initialization value is 0&#xa;        # if lock is True then a new recursive lock object is created to&#xa;        # synchronize access to the value&#xa;        self.application.Core.pnh_inject = Value('i', 0, lock=True)&#xa;        self.application.inbound_ip = self.db_config.Get('INBOUND_PROXY_IP')&#xa;        self.application.inbound_port = int(self.db_config.Get('INBOUND_PROXY_PORT'))&#xa;&#xa;        if self.proxy_manager:&#xa;            self.instances = ""1""  # Botnet mode needs only one proxy process.&#xa;        else:&#xa;            self.instances = self.db_config.Get(""INBOUND_PROXY_PROCESSES"")&#xa;&#xa;        # Proxy CACHE&#xa;        # Cache related settings, including creating required folders according to cache folder structure&#xa;        self.application.cache_dir = self.db_config.Get(""INBOUND_PROXY_CACHE_DIR"")&#xa;        # Clean possible older cache directory.&#xa;        if os.path.exists(self.application.cache_dir):&#xa;            FileOperations.rm_tree(self.application.cache_dir)&#xa;        FileOperations.make_dirs(self.application.cache_dir)&#xa;&#xa;        # SSL MiTM&#xa;        # SSL certs, keys and other settings (os.path.expanduser because they are stored in users home directory&#xa;        # ~/.owtf/proxy)&#xa;        self.application.ca_cert = os.path.expanduser(self.db_config.Get('CA_CERT'))&#xa;        self.application.ca_key = os.path.expanduser(self.db_config.Get('CA_KEY'))&#xa;        # To stop OWTF from breaking for our beloved users :P&#xa;        try:&#xa;            self.application.ca_key_pass = FileOperations.open(os.path.expanduser(self.db_config.Get('CA_PASS_FILE')),&#xa;                                                               'r', owtf_clean=False).read().strip()&#xa;        except IOError:&#xa;            self.application.ca_key_pass = ""owtf""  # XXX: Legacy CA key pass for older versions.&#xa;        self.application.proxy_folder = os.path.dirname(self.application.ca_cert)&#xa;        self.application.certs_folder = os.path.expanduser(self.db_config.Get('CERTS_FOLDER'))&#xa;&#xa;        try:  # Ensure CA.crt and Key exist.&#xa;            assert os.path.exists(self.application.ca_cert)&#xa;            assert os.path.exists(self.application.ca_key)&#xa;        except AssertionError:&#xa;            self.get_component(""error_handler"").FrameworkAbort(""Files required for SSL MiTM are missing.""&#xa;                                                               "" Please run the install script"")&#xa;&#xa;        try:  # If certs folder missing, create that.&#xa;            assert os.path.exists(self.application.certs_folder)&#xa;        except AssertionError:&#xa;            FileOperations.make_dirs(self.application.certs_folder)&#xa;&#xa;        # Blacklist (or) Whitelist Cookies&#xa;        # Building cookie regex to be used for cookie filtering for caching&#xa;        if self.db_config.Get('WHITELIST_COOKIES') == 'None':&#xa;            cookies_list = self.db_config.Get('BLACKLIST_COOKIES').split(',')&#xa;            self.application.cookie_blacklist = True&#xa;        else:&#xa;            cookies_list = self.db_config.Get('WHITELIST_COOKIES').split(',')&#xa;            self.application.cookie_blacklist = False&#xa;        if self.application.cookie_blacklist:&#xa;            regex_cookies_list = [cookie + ""=([^;]+;?)"" for cookie in cookies_list]&#xa;        else:&#xa;            regex_cookies_list = [""("" + cookie + ""=[^;]+;?)"" for cookie in self.db_config.Get('COOKIES_LIST')]&#xa;        regex_string = '|'.join(regex_cookies_list)&#xa;        self.application.cookie_regex = re.compile(regex_string)&#xa;&#xa;        # Outbound Proxy&#xa;        # Outbound proxy settings to be used inside request handler&#xa;        if outbound_options:&#xa;            if len(outbound_options) == 3:&#xa;                self.application.outbound_proxy_type = outbound_options[0]&#xa;                self.application.outbound_ip = outbound_options[1]&#xa;                self.application.outbound_port = int(outbound_options[2])&#xa;            else:&#xa;                self.application.outbound_proxy_type = ""http""&#xa;                self.application.outbound_ip = outbound_options[0]&#xa;                self.application.outbound_port = int(outbound_options[1])&#xa;        else:&#xa;            self.application.outbound_ip = None&#xa;            self.application.outbound_port = None&#xa;            self.application.outbound_proxy_type = None&#xa;        if outbound_auth:&#xa;            self.application.outbound_username, self.application.outbound_password = outbound_auth.split("":"")&#xa;        else:&#xa;            self.application.outbound_username = None&#xa;            self.application.outbound_password = None&#xa;&#xa;        self.server = tornado.httpserver.HTTPServer(self.application)&#xa;        # server has to be a class variable, because it is used inside request handler to attach sockets for monitoring&#xa;        ProxyHandler.server = self.server&#xa;&#xa;        # Header filters&#xa;        # Restricted headers are picked from framework/config/framework_config.cfg&#xa;        # These headers are removed from the response obtained from webserver, before sending it to browser&#xa;        restricted_response_headers = self.config.FrameworkConfigGet(""PROXY_RESTRICTED_RESPONSE_HEADERS"").split("","")&#xa;        ProxyHandler.restricted_response_headers = restricted_response_headers&#xa;        # These headers are removed from request obtained from browser, before sending it to webserver&#xa;        restricted_request_headers = self.config.FrameworkConfigGet(""PROXY_RESTRICTED_REQUEST_HEADERS"").split("","")&#xa;        ProxyHandler.restricted_request_headers = restricted_request_headers&#xa;&#xa;        # HTTP Auth options&#xa;        if self.db_config.Get(""HTTP_AUTH_HOST"") != ""None"":&#xa;            self.application.http_auth = True&#xa;            # All the variables are lists&#xa;            self.application.http_auth_hosts = self.db_config.Get(""HTTP_AUTH_HOST"").strip().split(',')&#xa;            self.application.http_auth_usernames = self.db_config.Get(""HTTP_AUTH_USERNAME"").strip().split(',')&#xa;            self.application.http_auth_passwords = self.db_config.Get(""HTTP_AUTH_PASSWORD"").strip().split(',')&#xa;            self.application.http_auth_modes = self.db_config.Get(""HTTP_AUTH_MODE"").strip().split(',')&#xa;        else:&#xa;            self.application.http_auth = False&#xa;&#xa;    def pseudo_run(self):&#xa;        self.application.Core.disable_console_logging()&#xa;        try:&#xa;            self.server.bind(self.application.inbound_port, address=self.application.inbound_ip)&#xa;            # Useful for using custom loggers because of relative paths in secure requests&#xa;            # http://www.joet3ch.com/blog/2011/09/08/alternative-tornado-logging/&#xa;            tornado.options.parse_command_line(&#xa;                args=[""dummy_arg"", ""--log_file_prefix=%s"" % self.db_config.Get(""PROXY_LOG""), ""--logging=info""])&#xa;            # To run any number of instances&#xa;            # ""0"" equals the number of cores present in a machine&#xa;            self.server.start(int(self.instances))&#xa;            tornado.ioloop.IOLoop.instance().start()&#xa;        except:&#xa;            # Cleanup code&#xa;            self.clean_up()&#xa;&#xa;    def clean_up(self):&#xa;        self.server.stop()&#xa;        tornado.ioloop.IOLoop.instance().stop()&#xa;"
4494404|"# -*- coding: utf-8 -*-&#xa;&#xa;import os, sys&#xa;print(""CWD: "" + os.getcwd() )&#xa;&#xa;config_path = os.path.abspath('../matplotlib/')&#xa;sys.path.append(config_path)&#xa;lib_path = os.path.abspath('../../lib')&#xa;sys.path.append(lib_path)&#xa;&#xa;# Load configuration file (before pyplot)&#xa;import configuration as config&#xa;&#xa;&#xa;import numpy as np&#xa;import scipy.ndimage as ndi&#xa;import cv2&#xa;import matplotlib.pyplot as plt&#xa;&#xa;&#xa;import framemanager_python&#xa;&#xa;# Force reloading of external library (convenient during active development)&#xa;reload(framemanager_python)&#xa;&#xa;&#xa;# Taken from http://stackoverflow.com/questions/4494404/find-large-number-of-consecutive-values-fulfilling-condition-in-a-numpy-array&#xa;# Author: Joe Kington&#xa;def contiguous_regions(condition):&#xa;    """"""Finds contiguous True regions of the boolean array ""condition"". Returns&#xa;    a 2D array where the first column is the start index of the region and the&#xa;    second column is the end index.""""""&#xa;&#xa;    # Find the indicies of changes in ""condition""&#xa;    d = np.diff(condition)&#xa;    idx, = d.nonzero() &#xa;&#xa;    # We need to start things after the change in ""condition"". Therefore, &#xa;    # we'll shift the index by 1 to the right.&#xa;    idx += 1&#xa;&#xa;    if condition[0]:&#xa;        # If the start of condition is True prepend a 0&#xa;        idx = np.r_[0, idx]&#xa;&#xa;    if condition[-1]:&#xa;        # If the end of condition is True, append the length of the array&#xa;        idx = np.r_[idx, condition.size] # Edit&#xa;&#xa;    # Reshape the result into two columns&#xa;    idx.shape = (-1,2)&#xa;    return idx&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;profileName = os.path.abspath(""some_steps.dsa"")&#xa;frameManager = framemanager_python.FrameManagerWrapper()&#xa;frameManager.load_profile(profileName);&#xa;&#xa;&#xa;numTSFrames = frameManager.get_tsframe_count();&#xa;starttime = frameManager.get_tsframe_timestamp(0)&#xa;stoptime = frameManager.get_tsframe_timestamp(numTSFrames)&#xa;&#xa;max_matrix_1 = frameManager.get_max_matrix_list(1)&#xa;max_matrix_5 = frameManager.get_max_matrix_list(5)&#xa;&#xa;# Time stamps&#xa;timestamps = frameManager.get_tsframe_timestamp_list()&#xa;timestamps = (timestamps-timestamps[0]) / 1000.0 # Relative timestamps in seconds&#xa;&#xa;&#xa;&#xa;# Simple smoothing&#xa;#filtered_matrix_5 = ndi.filters.median_filter(max_matrix_5, size=5, mode='reflect')&#xa;#filtered_matrix_5 = ndi.uniform_filter1d(max_matrix_5, size=10, mode='reflect')&#xa;&#xa;# Edge detection&#xa;sobel = ndi.sobel(max_matrix_5, mode='reflect')  &#xa;#laplace = ndi.laplace(max_matrix_5, mode='reflect') # Too sensitive to noise&#xa;#gaussian_laplace = ndi.filters.gaussian_laplace(max_matrix_5, sigma=1.0, mode='reflect')&#xa;#gaussian_gradient_magnitude = ndi.filters.gaussian_gradient_magnitude(max_matrix_5, sigma=1.0, mode='reflect')&#xa;&#xa;max_matrix_5_cv = (max_matrix_5/(4096.0/255.0)).astype(np.uint8) # Scale [0..255], convert to CV_8U&#xa;canny = cv2.Canny(max_matrix_5_cv, 10, 20) # Hysteresis Thresholding: &#xa;canny = canny.astype(np.float64) * (sobel.max()/255.0) # Scale to comparable scale&#xa;&#xa;&#xa;#---------------------------------&#xa;# Simple step detection algorithm&#xa;#---------------------------------&#xa;# Find all non-zero sequences&#xa;# Throw small sequencs away. Actual grasps are remaining&#xa;# For more elaborated methods: http://en.wikipedia.org/wiki/Step_detection&#xa;&#xa;thresh_sequence = 10 # Minimum length of a sequence to be considered a ""grasp""&#xa;grasp_begin = []&#xa;grasp_end = []&#xa;for start, stop in contiguous_regions(max_matrix_5 != 0):&#xa;    if (stop-start) > thresh_sequence:&#xa;        grasp_begin.append([start, max_matrix_5[start]])&#xa;        grasp_end.append([stop-1, max_matrix_5[stop-1]])&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;############&#xa;# Plotting&#xa;############&#xa;text_width = 6.30045 # LaTeX text width in inches&#xa;golden_ratio = (1 + np.sqrt(5) ) / 2.0&#xa;&#xa;size_factor = 1.0&#xa;figure_width = size_factor*text_width&#xa;#figure_height = (figure_width / golden_ratio)&#xa;figure_height = 1.3 * figure_width&#xa;figure_size = [figure_width, figure_height]&#xa;config.load_config_large()&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;fig = plt.figure(figsize=figure_size)&#xa;&#xa;# Axis 1&#xa;ax1 = fig.add_subplot(2,1,1)&#xa;&#xa;ax1.plot(max_matrix_5, ""-"", label=""Max Matrix 5"")&#xa;#ax1.plot(filtered_matrix_5, ""-"", marker=""x"", markersize=4, label=""Median Matrix 5"")&#xa;&#xa;ax1.plot([p[0] for p in grasp_begin], [p[1] for p in grasp_begin], ""o"", markersize=8, color=""green"", label=""Grasp begin"")&#xa;ax1.plot([p[0] for p in grasp_end], [p[1] for p in grasp_end], ""o"", markersize=8, color=""red"", label=""Grasp end"")&#xa;&#xa;&#xa;#ax1.set_xlim([xmin,xmax])&#xa;ax1.set_ylim([0, 1.2*np.max(max_matrix_5)])&#xa;ax1.set_xlabel(""# Frames"")&#xa;ax1.set_ylabel(""Raw Sensor Value"", rotation=90)&#xa;ax1.set_title(""Step detection by finding long non-zero sequences in tactile sensor readings"", y=1.10)&#xa;&#xa;# Second axis for time&#xa;ax1_time = ax1.twiny()&#xa;dummy = ax1_time.plot(timestamps, np.ones([timestamps.size]))&#xa;dummy.pop(0).remove()&#xa;ax1_time.set_xlabel(""Time [s]"")&#xa;ax1.legend(loc = 'upper left')&#xa;&#xa;&#xa;&#xa;&#xa;# Axis 2&#xa;ax2 = fig.add_subplot(2,1,2, sharex=ax1)&#xa;&#xa;ax2.plot(sobel, label=""Sobel"")&#xa;#ax2.plot(laplace, label=""Laplace"")&#xa;#ax2.plot(gaussian_laplace, label=""Gaussian laplace"")&#xa;#ax2.plot(gaussian_gradient_magnitude, label=""Gaussian gradient magnitude"")&#xa;ax2.plot(canny, label=""Canny"")&#xa;&#xa;ax2.set_xlabel(""# Frames"")&#xa;ax2.set_ylabel(""Filtered"", rotation=90)&#xa;ax2.legend(loc = 'lower left')&#xa;&#xa;&#xa;fig.tight_layout()&#xa;#plt.show() &#xa;&#xa;&#xa;plotname = ""step_detection_tactile_sensors""&#xa;fig.savefig(plotname+"".pdf"", pad_inches=0, dpi=fig.dpi) # pdf&#xa;#fig.savefig(plotname+"".pgf"", pad_inches=0, dpi=fig.dpi) # pgf&#xa;plt.close()&#xa;"
10415215|"from SearchDistribute.Search import GoogleSearch&#xa;from SearchDistribute.Enums import SearchEngines&#xa;from SearchDistribute.SearchExtractorErrors import InvalidSearchParameterException&#xa;from SearchDistribute.SearchExtractorErrors import MissingSearchParameterException&#xa;from SearchDistribute.SearchExtractorErrors import SERPPageLoadingException&#xa;from SearchDistribute.SearchExtractorErrors import SERPParsingException&#xa;from SearchDistribute import Enums&#xa;import time&#xa;import datetime&#xa;from multiprocessing import Process, Queue&#xa;## Some sources for multiprocessing:&#xa;##      https://www.blog.pythonlibrary.org/2016/08/02/python-201-a-multiprocessing-tutorial/&#xa;##      http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing&#xa;##      https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Queue&#xa;##      https://stackoverflow.com/a/10415215/4900327&#xa;##      https://stackoverflow.com/a/7399423/4900327&#xa;##      https://stackoverflow.com/a/1541117/4900327&#xa;&#xa;class Distribute:&#xa;    ''' Each distribute object runs queries on one search engine, across multiple *Search objects (each of which have identical parameters).&#xa;        It essentially runs a 'map' function, distributing the fetching of the results of the query across several workers (*Search objects).&#xa;    '''&#xa;    search_engine = """"          ## (optional, defaults to SearchEngines.Google) A string of the search engine to use, must be present in SearchDistribute.Enums.SearchEngines&#xa;    country = """"                ## (optional, defaults to ""USA"") The country where we want to search. Handled in Search.py&#xa;    num_workers = -1            ## (optional, defaults to 1) The number of workers we instantiate and assign to fetch parts of the total result set.&#xa;    num_results = -1            ## (optional, defaults to 10) The total number of results we want for the query.&#xa;    num_results_per_page =- 1   ## (optional, defaults to 10) The number of results per SERP&#xa;    save_to_db = None           ## (optional, defaults to False) Whether to save the results in an SQLite database or not&#xa;    db_config = {}              ## (optional, defaults to None) A hashtable with these fields: db_path, db_table. See Search.py for how they are handled.&#xa;    proxy_browser_config = {}   ## A hashtable with these fields: proxy_browser_type, proxy_args. See ProxyBrowser.py for how they are handled.&#xa;    cooldown_time = -1          ## The time in seconds, that each worker MUST wait before it can fetch the next SERP. No more SERPs can be fetched before this time expires&#xa;    destroy_workers = None      ## (optional, defaults to True) Whether to destroy the workers when calling self.finish()&#xa;&#xa;    workers = []  ## An array of the *Search objects. Their `time_of_last_retrieved_query` allows us to choose the one which has cooled down the most, to assign to fetch the next SERP.&#xa;    serps_Queue = None&#xa;&#xa;    def __init__(self, config):&#xa;        '''For all the parameters in `config`:&#xa;            - Necessary parameters:&#xa;                - If missing, raise a MissingSearchParameterException.&#xa;                - If present but in incorrect format, raise an InvalidSearchParameterException.&#xa;            - Optional parameters:&#xa;                - If missing, try to use the value from self.default_values().&#xa;                - If present but in incorrect format, raise an InvalidSearchParameterException.&#xa;&#xa;        Example config with all parameters set:&#xa;        {&#xa;            ""search_engine"" : SearchEngines.Google,&#xa;            ""country"" : ""USA"",&#xa;            ""num_workers"" : 1,&#xa;            ""num_results"" : 10,&#xa;            ""num_results_per_page"" : 10,&#xa;            ""cooldown_time"" : 300,&#xa;            ""proxy_browser_config"" : {&#xa;                ""proxy_browser_type"" : Enums.ProxyBrowsers.PhantomJS,&#xa;                ""proxy_args"" : {&#xa;                    ""proxy_type"" : Enums.ProxyTypes.Socks5,&#xa;                    ""hostname"" : ""proxy-nl.privateinternetaccess.com"",&#xa;                    ""port"" : 1080,&#xa;                    ""username"" : ""x1237029"",&#xa;                    ""password"" : ""3iV3za46xD""&#xa;                }&#xa;            },&#xa;            ""save_to_db"" : True,&#xa;            ""db_config"" : {&#xa;                ""db_path"" : ""./SearchResults.db"",&#xa;                ""db_table"" : ""GoogleSearchResults""&#xa;            }&#xa;        }&#xa;&#xa;&#xa;        PARAMETERS CHECKED IN THIS FILE (Distribute.py):&#xa;         - search_engine&#xa;         - query&#xa;         - num_workers&#xa;         - num_results&#xa;         - num_results_per_page&#xa;         - cooldown_time&#xa;         - save_to_db&#xa;         - destroy_workers&#xa;&#xa;        PARAMETERS CHECKED OTHER FILES (Search.py, ProxyBrowser.py):&#xa;        '''&#xa;&#xa;        get_default_if_not_found_in_config = lambda param_name: config.get(param_name) if config.get(param_name) is not None else self.default_values(param_name)&#xa;&#xa;        ## Optional parameter `search_engine`&#xa;        self.search_engine = get_default_if_not_found_in_config(""search_engine"")&#xa;        if type(self.search_engine) != type("""") or len(self.search_engine) == 0:    ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""search_engine"", self.search_engine, ""must be a non-empty string"")&#xa;&#xa;        ## Optional parameter `search_engine`, handled in Search.py&#xa;        self.country = config.get(""country"")&#xa;&#xa;        ## Optional parameter `num_workers`&#xa;        self.num_workers = get_default_if_not_found_in_config(""num_workers"")&#xa;        if type(self.num_workers) != type(0) or self.num_workers <= 0:     ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""num_workers"", self.num_workers, ""must be an integer greater than zero"")&#xa;&#xa;        ## Optional parameter `num_results`&#xa;        self.num_results = get_default_if_not_found_in_config(""num_results"")&#xa;        if type(self.num_results) != type(0) or self.num_results <= 0:     ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""num_results"", self.num_results, ""must be an integer greater than zero"")&#xa;&#xa;        ## Optional parameter `num_results_per_page`&#xa;        self.num_results_per_page = get_default_if_not_found_in_config(""num_results_per_page"")&#xa;        if type(self.num_results_per_page) != type(0) or self.num_results_per_page <= 0:   ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""num_results_per_page"", self.num_results_per_page, ""must be an integer greater than zero"")&#xa;&#xa;        ## Optional parameter `cooldown_time`&#xa;        self.cooldown_time = get_default_if_not_found_in_config(""cooldown_time"")&#xa;        if type(self.cooldown_time) != type(0) or self.cooldown_time < 0:   ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""cooldown_time"", self.cooldown_time, ""must be an integer greater than or equal to zero"")&#xa;&#xa;        ## Optional parameter `save_to_db`&#xa;        self.save_to_db = get_default_if_not_found_in_config(""save_to_db"")&#xa;        if type(self.save_to_db) != type(False):&#xa;            raise InvalidSearchParameterException(self.search_engine, ""save_to_db"", self.save_to_db, ""must be either True or false"")&#xa;&#xa;        self.destroy_workers = get_default_if_not_found_in_config(""destroy_workers"")&#xa;        if type(self.destroy_workers) != type(False):&#xa;            raise InvalidSearchParameterException(self.search_engine, ""destroy_workers"", self.destroy_workers, ""must be either True or false"")&#xa;&#xa;        if self.save_to_db == False:&#xa;            self.db_config = None&#xa;        else:&#xa;            ## Optional parameter `db_config`, handled in Search.py&#xa;            self.db_config = get_default_if_not_found_in_config(""db_config"")  ## checked in Search.py&#xa;&#xa;        ## Optional parameter `proxy_browser_config`, handled in ProxyBrowser.py&#xa;        self.proxy_browser_config = get_default_if_not_found_in_config(""proxy_browser_config"")  ## checked in ProxyBrowser.py&#xa;&#xa;&#xa;&#xa;    def default_values(self, param):&#xa;        ## A concise set of defaults. Also update these values at the top of the function.&#xa;        default_config = {&#xa;            ""search_engine"" : SearchEngines.Google,&#xa;            ""country"" : ""USA"",&#xa;            ""num_workers"" : 1,&#xa;            ""num_results"" : 10,&#xa;            ""num_results_per_page"" : 10,&#xa;            ""save_to_db"" : False,&#xa;            ""proxy_browser_config"" : {""proxy_browser_type"" : Enums.ProxyBrowsers.PhantomJS},&#xa;            ""cooldown_time"" : 300,&#xa;            ""destroy_workers"" : True&#xa;        }&#xa;        return default_config.get(param)&#xa;&#xa;&#xa;    ## Ready, set, GO!&#xa;    def start(self, query, force_destroy_workers = False):&#xa;        ## Necessary parameter `query`&#xa;        if query == None:  ## will only run if the value is not present&#xa;            raise MissingSearchParameterException(self.search_engine, ""query"")&#xa;        if type(query) != type("""") or len(query) == 0:  ## will only run if the value is present, but in the wrong format.&#xa;            raise InvalidSearchParameterException(self.search_engine, ""query"", query, ""must be a non-empty string"")&#xa;&#xa;        ## Added multiprocessing, as per https://www.blog.pythonlibrary.org/2016/08/02/python-201-a-multiprocessing-tutorial/&#xa;        self.serps_Queue = Queue()  ## an array of parsed SERPs. This is the main output of the function and a shared variable passed to our process (stackoverflow.com/a/10415215/4900327)&#xa;        self.proc = Process(target = self.distribute_query, args=(query, self.serps_Queue, self.num_results, self.num_workers, self.num_results_per_page, self.cooldown_time, self.save_to_db, self.destroy_workers or  force_destroy_workers))&#xa;        self.proc.start()&#xa;&#xa;&#xa;    def finish(self, no_wait = False):&#xa;        '''join, terminate and cleanup browser instances'''&#xa;        print(""Starting cleanup process"")&#xa;        if no_wait == False:&#xa;            self.proc.join()    ## Wait for the job to fetch all the results. This happens when self.start() returns.&#xa;&#xa;        ## Clean up workers&#xa;        if self.destroy_workers:&#xa;            for worker in self.workers:&#xa;                worker.close()&#xa;&#xa;        self.serp_results = []&#xa;        if no_wait == False:&#xa;            for serp in iter(self.serps_Queue.get, 'STOP'):&#xa;                self.serp_results.append(serp)&#xa;        self.serps_Queue.close()&#xa;        self.serps_Queue.join_thread()&#xa;&#xa;        self.proc.terminate()&#xa;        print(""Done with cleanup process"")&#xa;&#xa;&#xa;&#xa;    def get_results(self):&#xa;        '''Gets results from the internal multiprocessing.Queue object'''&#xa;        ## Source: https://stackoverflow.com/a/1541117/4900327&#xa;        return self.serp_results&#xa;&#xa;&#xa;&#xa;    def distribute_query(self, query, serps_Queue, num_results, num_workers, num_results_per_page, cooldown_time, save_to_db, destroy_workers):&#xa;        def _spawn_worker(self):  ## Can be extended to use multithreading or multiprocessing.&#xa;            worker_config = {&#xa;                ""country"": self.country,&#xa;                ""proxy_browser_config"": self.proxy_browser_config,&#xa;                ""db_config"": self.db_config&#xa;            }&#xa;&#xa;            if self.search_engine == SearchEngines.Google:&#xa;                return GoogleSearch(worker_config)&#xa;&#xa;        def _get_index_of_coolest_worker(self):&#xa;            index_of_coolest_worker = -1&#xa;            time_of_last_retrieved_query_of_coolest_worker = time.time()&#xa;            for i in range(0, len(self.workers)):&#xa;                if self.workers[i].time_of_last_retrieved_query < time_of_last_retrieved_query_of_coolest_worker:&#xa;                    index_of_coolest_worker = i&#xa;                    time_of_last_retrieved_query_of_coolest_worker = self.workers[i].time_of_last_retrieved_query&#xa;            time_passed_since_last_fetched_from_coolest_worker = time.time() - self.workers[&#xa;                index_of_coolest_worker].time_of_last_retrieved_query&#xa;            return (index_of_coolest_worker, time_passed_since_last_fetched_from_coolest_worker)&#xa;&#xa;&#xa;        def _print_current_serp(self, query, start_datetime, parsed_serps):&#xa;            ## Print the current SERP list and its timestamp.&#xa;            parsed_serp = parsed_serps[-1]&#xa;            now = datetime.datetime.now()&#xa;            time_str = ""%s-%s-%s %s:%s:%s"" % (now.year, now.month, now.day, now.hour, now.minute, now.second)&#xa;            print(""\n\nQuery: `%s`""%query)&#xa;            print(""Results %s-%s, page #%s (obtained at %s)\n"" % (&#xa;            parsed_serp.start_offset, parsed_serp.start_offset + parsed_serp.num_results, parsed_serp.current_page_num, time_str))&#xa;            for url in parsed_serp.results:&#xa;                print(""\t%s""%url)&#xa;            print(""\nRate of retrieving results: %.1f URLs per hour."" % (&#xa;                sum([serp.num_results for serp in parsed_serps]) / ((datetime.datetime.now() - start_datetime).days * 24 + (datetime.datetime.now() - start_datetime).seconds / 3600)))&#xa;            print(""\nNumber of unique results so far: %s."" % (len(set([res for serp in parsed_serps for res in serp.results]))))  ## Source: https://stackoverflow.com/a/952952/4900327&#xa;&#xa;        parsed_serps = []&#xa;&#xa;        ## Print start time.&#xa;        start_datetime = datetime.datetime.now()&#xa;        time_str = ""%s-%s-%s %s:%s:%s""%(start_datetime.year, start_datetime.month, start_datetime.day, start_datetime.hour, start_datetime.minute, start_datetime.second)&#xa;        print(""\nStarting the %s search with query `%s`\nStart time: %s\n"" % (self.search_engine, query, time_str))&#xa;&#xa;        ## The first worker sets the stage for the other workers, getting the basic url which is then modified by each worker.&#xa;        worker = _spawn_worker(self)&#xa;        basic_url, basic_serp = worker.perform_search_from_main_page(query, num_results_per_page)&#xa;        actual_total_num_results_for_query = basic_serp.total_num_results_for_query&#xa;        print(""\nFound %s results for query `%s`, trying to get %s.""%(actual_total_num_results_for_query, query, num_results))&#xa;        self.workers.append(worker)&#xa;&#xa;        num_completed = 0&#xa;        next_page_num = len(parsed_serps) + 1&#xa;        ## Create all the other workers. On creation, make the worker fetch a SERP.&#xa;        for i in range(1, num_workers):&#xa;            time.sleep(3)   ## Added because internet was not loading, may remove later.&#xa;            worker = _spawn_worker(self)&#xa;            ## Try to get a SERP.&#xa;            try:&#xa;                parsed_serp = worker.get_SERP_results(basic_url,&#xa;                                                      num_completed,&#xa;                                                      next_page_num,&#xa;                                                      num_results_per_page,&#xa;                                                      save_to_db)&#xa;                if parsed_serp is None:&#xa;                    raise SERPPageLoadingException(self.search_engine,&#xa;                                                   self.proxy_browser_config.get(""proxy_browser_type""),&#xa;                                                   url = worker._update_url_number_of_results_per_page(worker._update_url_start(basic_url, num_completed), num_results_per_page))&#xa;                parsed_serps.append(parsed_serp)&#xa;                serps_Queue.put(parsed_serp)&#xa;                _print_current_serp(self, query, start_datetime, parsed_serps)&#xa;                num_completed = len([res for serp in parsed_serps for res in serp.results])&#xa;&#xa;                self.workers.append(worker)     ## Can be extended to use multithreading or multiprocessing.&#xa;&#xa;            ## If we are at the last page and there are no more results, return.&#xa;            except SERPParsingException:&#xa;                print(""\nObtained %s results in the last SERP. There are no more result pages."" % parsed_serp.num_results)&#xa;                serps_Queue.put(""STOP"") ## Sentinel, as per https://stackoverflow.com/a/1541117/4900327&#xa;                print(""Done fetching results"")&#xa;                return&#xa;            next_page_num = len(parsed_serps) + 1&#xa;            pass&#xa;&#xa;&#xa;        while num_completed < num_results:&#xa;            ## Get the coolest worker. If this one is not cooler than `cooldown_time`, wait for the remaining time.&#xa;            index_of_coolest_worker, time_passed_since_last_fetched_from_coolest_worker = _get_index_of_coolest_worker(self)&#xa;            if time_passed_since_last_fetched_from_coolest_worker < cooldown_time:&#xa;                sleep_for = cooldown_time - time_passed_since_last_fetched_from_coolest_worker ## Sleep for the remaining time.&#xa;                wakeup_datetime = datetime.datetime.now() + datetime.timedelta(seconds=sleep_for) ## Source: http://stackoverflow.com/a/3240493/4900327&#xa;                time_str = ""%s-%s-%s %s:%s:%s""%(wakeup_datetime.year, wakeup_datetime.month, wakeup_datetime.day, wakeup_datetime.hour, wakeup_datetime.minute, wakeup_datetime.second)&#xa;                print(""<-----All workers need to cooldown, sleeping till: %s----->"" % time_str)&#xa;                time.sleep(sleep_for)&#xa;            try:&#xa;                parsed_serp = self.workers[index_of_coolest_worker].get_SERP_results(basic_url,&#xa;                                                                                     num_completed,&#xa;                                                                                     next_page_num,&#xa;                                                                                     num_results_per_page,&#xa;                                                                                     save_to_db)&#xa;                parsed_serps.append(parsed_serp)&#xa;                serps_Queue.put(parsed_serp)&#xa;                _print_current_serp(self, query, start_datetime, parsed_serps)&#xa;                num_completed = len([res for serp in parsed_serps for res in serp.results])&#xa;&#xa;            ## If we are at the last page and there are no more results, return.&#xa;            except SERPParsingException:&#xa;                print(""\nObtained %s results in the last SERP. There are no more result pages.""%parsed_serp.num_results)&#xa;                serps_Queue.put(""STOP"")  ## Sentinel, as per https://stackoverflow.com/a/1541117/4900327&#xa;                return&#xa;                print(""Done fetching results"")&#xa;            next_page_num = len(parsed_serps) + 1&#xa;            pass&#xa;        serps_Queue.put(""STOP"")  ## Sentinel, as per https://stackoverflow.com/a/1541117/4900327&#xa;        print(""Done fetching results"")&#xa;        return&#xa;"
236861|"import os, sys&#xa;&#xa;from collections import OrderedDict, defaultdict&#xa;import fnmatch&#xa;import importlib.util&#xa;import json&#xa;import locale&#xa;import logging&#xa;import time&#xa;&#xa;from applyActions import executeActionList&#xa;from constants import *&#xa;# TODO: Fix json errors being incomprehensible, because the location specified does not match the minified json&#xa;import strip_comments_json as configjson&#xa;&#xa;class FileDirectory:&#xa;    def __init__(self, path, *, isDirectory, inSourceDir, inCompareDir):&#xa;        self.path = path&#xa;        self.inSourceDir = inSourceDir&#xa;        self.inCompareDir = inCompareDir&#xa;        self.isDirectory = isDirectory&#xa;&#xa;    def __str__(self):&#xa;        inStr = []&#xa;        if self.inSourceDir:&#xa;            inStr.append(""source dir"")&#xa;        if self.inCompareDir:&#xa;            inStr.append(""compare dir"")&#xa;        return self.path + (""(directory)"" if self.isDirectory else """") + "" ("" + "","".join(inStr) + "")""&#xa;&#xa;# os.walk is not used since files would always be processed separate from directories&#xa;# But os.walk will just ignore errors, if no error callback is given, scandir will not.&#xa;def relativeWalk(path, startPath = None):&#xa;    if startPath == None: startPath = path&#xa;    # strxfrm -> local aware sorting - https://docs.python.org/3/howto/sorting.html#odd-and-ends&#xa;    for entry in sorted(os.scandir(path), key = lambda x: locale.strxfrm(x.name)):&#xa;        try:&#xa;            #print(entry.path, "" ----- "",  entry.name)&#xa;            if entry.is_file():&#xa;                yield os.path.relpath(entry.path, startPath), False&#xa;            elif entry.is_dir():&#xa;                yield os.path.relpath(entry.path, startPath), True&#xa;                yield from relativeWalk(entry.path, startPath)&#xa;            else:&#xa;                logging.error(""Encountered an object which is neither directory nor file: "" + entry.path)&#xa;        except OSError as e:&#xa;            logging.error(e)&#xa;&#xa;# Possible actions:&#xa;# copy (always from source to target),&#xa;# delete (always in target)&#xa;# hardlink (always from compare directory to target directory)&#xa;# rename (always in target) (2-variate) (only needed for move detection)&#xa;# hardlink2 (alway from compare directory to target directory) (2-variate) (only needed for move detection)&#xa;def Action(type, **params):&#xa;    return OrderedDict(type=type, params=params)&#xa;&#xa;def fileBytewiseCmp(a, b):&#xa;    BUFSIZE = 8192 # http://stackoverflow.com/questions/236861/how-do-you-determine-the-ideal-buffer-size-when-using-fileinputstream&#xa;    with open(a, ""rb"") as file1, open(b, ""rb"") as file2:&#xa;        while True:&#xa;            buf1 = file1.read(BUFSIZE)&#xa;            buf2 = file2.read(BUFSIZE)&#xa;            if buf1 != buf2: return False&#xa;            if not buf1: return True&#xa;&#xa;def filesEq(a, b):&#xa;    try:&#xa;        aStat = os.stat(a)&#xa;        bStat = os.stat(b)&#xa;&#xa;        equal = True&#xa;        for method in config[""compare_method""]:&#xa;            if method == ""moddate"":&#xa;                if aStat.st_mtime != bStat.st_mtime:&#xa;                    break&#xa;            elif method == ""size"":&#xa;                if aStat.st_size != bStat.st_size:&#xa;                    break&#xa;            elif method == ""bytes"":&#xa;                if not fileBytewiseCmp(a, b):&#xa;                    break&#xa;            else:&#xa;                logging.critical(""Compare method '"" + method + ""' does not exist"")&#xa;                quit()&#xa;        else:&#xa;            return True&#xa;&#xa;        return False # This will be executed if break was called from the loop&#xa;    except Exception as e: # Why is there no proper list of exceptions that may be thrown by filecmp.cmp and os.stat?&#xa;        logging.error(""For files '"" + a + ""'' and '"" + b + ""'' either 'stat'-ing or comparing the files failed: "" + str(e))&#xa;        return False # If we don't know, it has to be assumed they are different, even if this might result in more file operatiosn being scheduled&#xa;&#xa;def dirEmpty(path):&#xa;    try:&#xa;        for entry in os.scandir(path):&#xa;            return False&#xa;        else:&#xa;            return True&#xa;    except Exception as e:&#xa;        logging.error(""Scanning directory '"" + path + ""' failed: "" + str(e))&#xa;        return True&#xa;&#xa;if __name__ == '__main__':&#xa;    logger = logging.getLogger()&#xa;&#xa;    stderrHandler = logging.StreamHandler(stream=sys.stderr)&#xa;    stderrHandler.setFormatter(LOGFORMAT)&#xa;    logger.addHandler(stderrHandler)&#xa;&#xa;    if len(sys.argv) < 2:&#xa;        logging.critical(""Please specify the configuration file for your backup."")&#xa;        quit()&#xa;&#xa;    if not os.path.isfile(sys.argv[1]):&#xa;        logging.critical(""Configuration '"" + sys.argv[1] + ""' does not exist."")&#xa;        quit()&#xa;&#xa;    with open(DEFAULT_CONFIG_FILENAME) as configFile:&#xa;        config = configjson.load(configFile)&#xa;&#xa;    userConfigPath = sys.argv[1]&#xa;    with open(userConfigPath) as userConfigFile:&#xa;        userConfig = configjson.load(userConfigFile)&#xa;&#xa;    for k, v in userConfig.items():&#xa;        if k not in config:&#xa;            logging.critical(""Unknown key '"" + k + ""' in the passed configuration file '"" + userConfigPath + ""'"")&#xa;            quit()&#xa;        else:&#xa;            config[k] = v&#xa;    for mandatory in [""source_dir"", ""backup_root_dir""]:&#xa;        if mandatory not in userConfig:&#xa;            logging.critical(""Please specify the mandatory key '"" + mandatory + ""' in the passed configuration file '"" + userConfigPath + ""'"")&#xa;            quit()&#xa;&#xa;    logger.setLevel(config[""log_level""])&#xa;&#xa;    if config[""mode""] == ""hardlink"":&#xa;        config[""versioned""] = True&#xa;        config[""compare_with_last_backup""] = True&#xa;&#xa;    # Setup target and metadata directories and metadata file&#xa;    os.makedirs(config[""backup_root_dir""], exist_ok = True)&#xa;    if config[""versioned""]:&#xa;        backupDirectory = os.path.join(config[""backup_root_dir""], time.strftime(config[""version_name""]))&#xa;&#xa;        suffixNumber = 1&#xa;        while True:&#xa;            try:&#xa;                path = backupDirectory&#xa;                if suffixNumber > 1: path = path + ""_"" + str(suffixNumber)&#xa;                os.makedirs(path)&#xa;                backupDirectory = path&#xa;                break&#xa;            except FileExistsError as e:&#xa;                suffixNumber += 1&#xa;                logging.error(""Target Backup directory '"" + path + ""' already exists. Appending suffix '_"" + str(suffixNumber) + ""'"")&#xa;    else:&#xa;        backupDirectory = config[""backup_root_dir""]&#xa;&#xa;    # Create backupDirectory and targetDirectory&#xa;    targetDirectory = os.path.join(backupDirectory, os.path.basename(config[""source_dir""]))&#xa;    compareDirectory = targetDirectory&#xa;    os.makedirs(targetDirectory, exist_ok = True)&#xa;&#xa;    # Init log file&#xa;    fileHandler = logging.FileHandler(os.path.join(backupDirectory, LOG_FILENAME))&#xa;    fileHandler.setFormatter(LOGFORMAT)&#xa;    logger.addHandler(fileHandler)&#xa;&#xa;    # update compare directory&#xa;    if config[""versioned""] and config[""compare_with_last_backup""]:&#xa;        oldBackups = []&#xa;        for entry in os.scandir(config[""backup_root_dir""]):&#xa;            if entry.is_dir() and os.path.join(config[""backup_root_dir""], entry.name) != backupDirectory:&#xa;                metadataFile = os.path.join(config[""backup_root_dir""], entry.name, METADATA_FILENAME)&#xa;                if os.path.isfile(metadataFile):&#xa;                    with open(metadataFile) as inFile:&#xa;                        oldBackups.append(json.load(inFile))&#xa;&#xa;        logging.debug(""Found "" + str(len(oldBackups)) + "" old backups: "" + str(oldBackups))&#xa;&#xa;        for backup in sorted(oldBackups, key = lambda x: x['started'], reverse = True):&#xa;            if backup[""successful""]:&#xa;                compareDirectory = os.path.join(config[""backup_root_dir""], backup['name'], os.path.basename(config[""source_dir""]))&#xa;                break&#xa;            else:&#xa;                logging.error(""It seems the last backup failed, so it will be skipped and the new backup will compare the source to the backup '"" +backup[""name""] + ""'. The failed backup should probably be deleted."")&#xa;        else:&#xa;            logging.warning(""No old backup found. Creating first backup."")&#xa;&#xa;    # Prepare metadata.json&#xa;    with open(os.path.join(backupDirectory, METADATA_FILENAME), ""w"") as outFile:&#xa;        json.dump({&#xa;            'name': os.path.basename(backupDirectory),&#xa;            'successful': False,&#xa;            'started': time.time(),&#xa;            'sourceDirectory': config[""source_dir""],&#xa;            'compareDirectory': compareDirectory,&#xa;            'targetDirectory': targetDirectory,&#xa;        }, outFile, indent=4)&#xa;&#xa;    logging.info(""Source directory: "" + config[""source_dir""])&#xa;    logging.info(""Backup directory: "" + backupDirectory)&#xa;    logging.info(""Target directory: "" + targetDirectory)&#xa;    logging.info(""Compare directory: "" + compareDirectory)&#xa;    logging.info(""Starting backup in "" + config[""mode""] + "" mode"")&#xa;&#xa;    # Build a list of all files in source directory and compare directory&#xa;    # TODO: Include/exclude empty folders&#xa;    logging.info(""Building file set."")&#xa;    logging.info(""Reading source directory"")&#xa;    fileDirSet = []&#xa;    for name, isDir in relativeWalk(config[""source_dir""]):&#xa;        for exclude in config[""exclude_paths""]:&#xa;            if fnmatch.fnmatch(name, exclude):&#xa;                break&#xa;        else:&#xa;            fileDirSet.append(FileDirectory(name, isDirectory = isDir, inSourceDir = True, inCompareDir = False))&#xa;&#xa;    logging.info(""Comparing with compare directory"")&#xa;    insertIndex = 0&#xa;    for name, isDir in relativeWalk(compareDirectory):&#xa;        while insertIndex < len(fileDirSet) and locale.strcoll(name, fileDirSet[insertIndex].path) > 0:&#xa;            insertIndex += 1&#xa;&#xa;        if insertIndex < len(fileDirSet) and locale.strcoll(name, fileDirSet[insertIndex].path) == 0:&#xa;            fileDirSet[insertIndex].inCompareDir = True&#xa;        else:&#xa;            fileDirSet.insert(insertIndex, FileDirectory(name, isDirectory = isDir, inSourceDir = False, inCompareDir = True))&#xa;&#xa;        insertIndex += 1&#xa;&#xa;    for file in fileDirSet:&#xa;        logging.debug(file)&#xa;&#xa;    # Determine what to do with these files&#xa;    actions = []&#xa;&#xa;    # ============== SAVE&#xa;    # Write all files that are in source, but are not already existing in compare (in that version)&#xa;    # source\compare: copy&#xa;    # source&compare:&#xa;    #   same: ignore&#xa;    #   different: copy&#xa;    # compare\source: ignore&#xa;&#xa;    # --- move detection:&#xa;    # The same, except if files in source\compare and compare\source are equal, don't copy,&#xa;    # but rather rename compare\source (old backup) to source\compare (new backup)&#xa;&#xa;    # ============== MIRROR&#xa;    # End up with a complete copy of source in compare&#xa;    # source\compare: copy&#xa;    # source&compare:&#xa;    #   same: ignore&#xa;    #   different: copy&#xa;    # compare\source: delete&#xa;&#xa;    # --- move detection:&#xa;    # The same, except if files in source\compare and compare\source are equal, don't delete and copy, but rename&#xa;&#xa;&#xa;    # ============== HARDLINK&#xa;    # (Attention: here the source is compared against an older backup!)&#xa;    # End up with a complete copy of source in compare, but have hardlinks to already existing versions in other backups, if it exists&#xa;    # source\compare: copy&#xa;    #   same: hardlink to new backup from old backup&#xa;    #   different: copy&#xa;    # compare\source: ignore&#xa;&#xa;    # --- move detection:&#xa;    # The same, except if files in source\compare and compare\source are equal, don't copy,&#xa;    # but rather hardlink from compare\source (old backup) to source\compare (new backup)&#xa;&#xa;    logging.info(""Generating actions for "" + str(len(fileDirSet)) + "" files.. "")&#xa;    lastProgress = 0&#xa;    percentSteps = 5&#xa;    inNewDir = None&#xa;    for i, element in enumerate(fileDirSet):&#xa;        progress = int(i/len(fileDirSet)*100.0/percentSteps + 0.5) * percentSteps&#xa;        if lastProgress != progress:&#xa;            print(str(progress) + ""%  "", end="""", flush = True)&#xa;        lastProgress = progress&#xa;&#xa;        # source\compare&#xa;        if element.inSourceDir and not element.inCompareDir:&#xa;            if inNewDir != None and element.path.startswith(inNewDir):&#xa;                actions.append(Action(""copy"", name=element.path, htmlFlags=""inNewDir""))&#xa;            else:&#xa;                actions.append(Action(""copy"", name=element.path))&#xa;                if element.isDirectory:&#xa;                    inNewDir = element.path&#xa;&#xa;&#xa;&#xa;        # source&compare&#xa;        elif element.inSourceDir and element.inCompareDir:&#xa;            if element.isDirectory:&#xa;                if config[""versioned""] and config[""compare_with_last_backup""]:&#xa;                    # only explicitly create empty directories, so the action list is not cluttered with every directory in the source&#xa;                    if dirEmpty(os.path.join(config[""source_dir""], element.path)):&#xa;                        actions.append(Action(""copy"", name=element.path, htmlFlags=""emptyFolder""))&#xa;            else:&#xa;                # same&#xa;                if filesEq(os.path.join(config[""source_dir""], element.path), os.path.join(compareDirectory, element.path)):&#xa;                    if config[""mode""] == ""hardlink"":&#xa;                        actions.append(Action(""hardlink"", name=element.path))&#xa;&#xa;                # different&#xa;                else:&#xa;                    actions.append(Action(""copy"", name=element.path))&#xa;&#xa;        # compare\source&#xa;        elif not element.inSourceDir and element.inCompareDir:&#xa;            if config[""mode""] == ""mirror"":&#xa;                if not config[""compare_with_last_backup""] or not config[""versioned""]:&#xa;                    actions.append(Action(""delete"", name=element.path))&#xa;    print("""") # so the progress output from before ends with a new line&#xa;&#xa;    if config[""save_actionfile""]:&#xa;        # Write the action file&#xa;        actionFilePath = os.path.join(backupDirectory, ACTIONS_FILENAME)&#xa;        logging.info(""Saving the action file to "" + actionFilePath)&#xa;        actionJson = ""[\n"" + "",\n"".join(map(json.dumps, actions)) + ""\n]""&#xa;        with open(actionFilePath, ""w"") as actionFile:&#xa;            actionFile.write(actionJson)&#xa;&#xa;        if config[""open_actionfile""]:&#xa;            os.startfile(actionFilePath)&#xa;&#xa;    if config[""save_actionhtml""]:&#xa;        # Write HTML actions&#xa;        actionHtmlFilePath = os.path.join(backupDirectory, ACTIONSHTML_FILENAME)&#xa;        logging.info(""Generating and writing action HTML file to "" + actionHtmlFilePath)&#xa;        templatePath = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""template.html"")&#xa;        with open(templatePath, ""r"") as templateFile:&#xa;            template = templateFile.read()&#xa;&#xa;        with open(actionHtmlFilePath, ""w"", encoding = ""utf-8"") as actionHTMLFile:&#xa;            templateParts = template.split(""<!-- ACTIONTABLE -->"")&#xa;&#xa;            actionHist = defaultdict(int)&#xa;            for action in actions:&#xa;                actionHist[action[""type""]] += 1&#xa;            actionOverviewHTML = "" | "".join(map(lambda k_v: k_v[0] + ""("" + str(k_v[1]) + "")"", actionHist.items()))&#xa;            actionHTMLFile.write(templateParts[0].replace(""<!-- OVERVIEW -->"", actionOverviewHTML))&#xa;&#xa;            # Writing this directly is a lot faster than concatenating huge strings&#xa;            for action in actions:&#xa;                if action[""type""] not in config[""exclude_actionhtml_actions""]:&#xa;                    # Insert zero width space, so that the line breaks at the backslashes&#xa;                    itemClass = action[""type""]&#xa;                    itemText = action[""type""]&#xa;                    if ""htmlFlags"" in action[""params""]:&#xa;                        flags = action[""params""][""htmlFlags""]&#xa;                        itemClass += ""_"" + flags&#xa;                        if flags == ""emptyFolder"":&#xa;                            itemText += "" (empty directory)""&#xa;                        elif flags == ""inNewDir"":&#xa;                            itemText += "" (in new directory)""&#xa;                        else:&#xa;                            logging.error(""Unknown html flags for action html: "" + str(flags))&#xa;                    actionHTMLFile.write(""\t\t<tr class=\"""" + itemClass + ""\""><td class=\""type\"">"" + itemText&#xa;                                         + ""</td><td class=\""name\"">"" + action[""params""][""name""].replace(""\\"", ""\\&#8203;"") + ""</td>\n"")&#xa;&#xa;            actionHTMLFile.write(templateParts[1])&#xa;&#xa;        if config[""open_actionhtml""]:&#xa;            os.startfile(actionHtmlFilePath)&#xa;&#xa;    if config[""apply_actions""]:&#xa;        executeActionList(backupDirectory, actions)&#xa;&#xa;"
29082268|"# Copyright 2017 Battelle Energy Alliance, LLC&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;# http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;""""""&#xa;Created on Mar 5, 2013&#xa;&#xa;@author: alfoa, cogljj, crisr&#xa;""""""&#xa;#for future compatibility with Python 3-----------------------------------------&#xa;from __future__ import division, print_function, unicode_literals, absolute_import&#xa;import warnings&#xa;warnings.simplefilter('default',DeprecationWarning)&#xa;if not 'xrange' in dir(__builtins__):&#xa;  xrange = range&#xa;#End compatibility block for Python 3-------------------------------------------&#xa;&#xa;#External Modules---------------------------------------------------------------&#xa;import time&#xa;import collections&#xa;import subprocess&#xa;&#xa;import os&#xa;import copy&#xa;import sys&#xa;import abc&#xa;import threading&#xa;import random&#xa;import socket&#xa;#External Modules End-----------------------------------------------------------&#xa;&#xa;#Internal Modules---------------------------------------------------------------&#xa;from utils import utils&#xa;from BaseClasses import BaseType&#xa;import MessageHandler&#xa;import Runners&#xa;import Models&#xa;# for internal parallel&#xa;if sys.version_info.major == 2:&#xa;  import pp&#xa;  import ppserver&#xa;else:&#xa;  print(""pp does not support python3"")&#xa;# end internal parallel module&#xa;#Internal Modules End-----------------------------------------------------------&#xa;&#xa;&#xa;## FIXME: Finished jobs can bog down the queue waiting for other objects to take&#xa;## them away. Can we shove them onto a different list and free up the job queue?&#xa;&#xa;class JobHandler(MessageHandler.MessageUser):&#xa;  """"""&#xa;    JobHandler class. This handles the execution of any job in the RAVEN&#xa;    framework&#xa;  """"""&#xa;  def __init__(self):&#xa;    """"""&#xa;      Init method&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    self.printTag         = 'Job Handler'&#xa;    self.runInfoDict      = {}&#xa;&#xa;    self.isParallelPythonInitialized = False&#xa;&#xa;    self.sleepTime  = 0.005&#xa;    self.completed = False&#xa;&#xa;    ## Determines whether to collect and print job timing summaries at the end of job runs.&#xa;    self.__profileJobs = False&#xa;&#xa;    ## Prevents the pending queue from growing indefinitely, but also allowing&#xa;    ## extra jobs to be queued to prevent starving parallelized environments of&#xa;    ## jobs.&#xa;    self.maxQueueSize = None&#xa;&#xa;    ############################################################################&#xa;    ## The following variables are protected by the __queueLock&#xa;&#xa;    ## Placeholders for each actively running job. When a job finishes, its&#xa;    ## spot in one of these lists will be reset to None and the next Runner will&#xa;    ## be placed in a free None spot, and set to start&#xa;    self.__running       = []&#xa;    self.__clientRunning = []&#xa;&#xa;    ## Queue of jobs to be run, when something on the list above opens up, the&#xa;    ## corresponding queue will pop a job (Runner) and put it into that location&#xa;    ## and set it to start&#xa;    self.__queue       = collections.deque()&#xa;    self.__clientQueue = collections.deque()&#xa;&#xa;    ## A counter used for uniquely identifying the next id for an ExternalRunner&#xa;    ## InternalRunners will increment this counter, but do not use it currently&#xa;    self.__nextId = 0&#xa;&#xa;    ## List of finished jobs. When a job finishes, it is placed here until&#xa;    ## something from the main thread can remove them.&#xa;    self.__finished = []&#xa;&#xa;    ## End block of __queueLock protected variables&#xa;    ############################################################################&#xa;&#xa;    self.__queueLock = threading.RLock()&#xa;&#xa;    ## List of submitted job identifiers, includes jobs that have completed as&#xa;    ## this list is not cleared until a new step is entered&#xa;    self.__submittedJobs = []&#xa;    ## Dict of failed jobs of the form { identifer: metadata }&#xa;    self.__failedJobs = {}&#xa;&#xa;    #self.__noResourcesJobs = []&#xa;&#xa;  def initialize(self, runInfoDict, messageHandler):&#xa;    """"""&#xa;      Method to initialize the JobHandler&#xa;      @ In, runInfoDict, dict, dictionary of run info settings&#xa;      @ In, messageHandler, MessageHandler object, instance of the global RAVEN&#xa;        message handler&#xa;      @ Out, None&#xa;    """"""&#xa;    self.runInfoDict = runInfoDict&#xa;    self.messageHandler = messageHandler&#xa;    # set the maximum queue size (number of jobs to queue past the running number)&#xa;    self.maxQueueSize = runInfoDict['maxQueueSize']&#xa;    # defaults to None; if None, then use batchSize instead&#xa;    if self.maxQueueSize is None:&#xa;      self.maxQueueSize = runInfoDict['batchSize']&#xa;    # if requsted max size less than 1, we can't do that, so take 1 instead&#xa;    if self.maxQueueSize < 1:&#xa;      self.raiseAWarning('maxQueueSize was set to be less than 1!  Setting to 1...')&#xa;      self.maxQueueSize = 1&#xa;    self.raiseADebug('Setting maxQueueSize to',self.maxQueueSize)&#xa;&#xa;    #initialize PBS&#xa;    with self.__queueLock:&#xa;      self.__running       = [None]*self.runInfoDict['batchSize']&#xa;      self.__clientRunning = [None]*self.runInfoDict['batchSize']&#xa;&#xa;  def __checkAndRemoveFinished(self, running):&#xa;    """"""&#xa;      Method to check if a run is finished and remove it from the queque&#xa;      @ In, running, instance, the job instance (InternalRunner or ExternalRunner)&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      returnCode = running.getReturnCode()&#xa;      if returnCode != 0:&#xa;        metadataFailedRun = running.getMetadata()&#xa;        metadataToKeep = metadataFailedRun&#xa;        if metadataFailedRun is not None:&#xa;          metadataKeys      = metadataFailedRun.keys()&#xa;          if 'jobHandler' in metadataKeys:&#xa;            metadataKeys.pop(metadataKeys.index(""jobHandler""))&#xa;            metadataToKeep = { keepKey: metadataFailedRun[keepKey] for keepKey in metadataKeys }&#xa;        ## FIXME: The running.command was always internal now, so I removed it.&#xa;        ## We should probably find a way to give more pertinent information.&#xa;        self.raiseAMessage("" Process Failed "" + str(running) + "" internal returnCode "" + str(returnCode))&#xa;        self.__failedJobs[running.identifier]=(returnCode,copy.deepcopy(metadataToKeep))&#xa;&#xa;  def __initializeParallelPython(self):&#xa;    """"""&#xa;      Internal method that is aimed to initialize the internal parallel system.&#xa;      It initilizes the paralle python implementation (with socketing system) in&#xa;      case RAVEN is run in a cluster with multiple nodes or the NumMPI > 1,&#xa;      otherwise multi-threading is used.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    ## Check if the list of unique nodes is present and, in case, initialize the&#xa;    ## socket&#xa;    if self.runInfoDict['internalParallel']:&#xa;      if len(self.runInfoDict['Nodes']) > 0:&#xa;        availableNodes = [nodeId.strip() for nodeId in self.runInfoDict['Nodes']]&#xa;&#xa;        ## Set the initial port randomly among the user accessible ones&#xa;        ## Is there any problem if we select the same port as something else?&#xa;        randomPort = random.randint(1024,65535)&#xa;&#xa;        ## Get localHost and servers&#xa;        localHostName, ppservers = self.__runRemoteListeningSockets(randomPort)&#xa;        self.raiseADebug(""Local host is ""+ localHostName)&#xa;&#xa;        if len(ppservers) == 0:&#xa;          ## We are on a single node&#xa;          self.ppserver = pp.Server(ncpus=len(availableNodes))&#xa;        else:&#xa;          ## We are using multiple nodes&#xa;          self.raiseADebug(""Servers found are "" + ','.join(ppservers))&#xa;          self.raiseADebug(""Server port in use is "" + str(randomPort))&#xa;          self.ppserver = pp.Server(ncpus=0, ppservers=tuple(ppservers))&#xa;      else:&#xa;         ## We are using the parallel python system&#xa;        self.ppserver = pp.Server(ncpus=int(self.runInfoDict['totalNumCoresUsed']))&#xa;    else:&#xa;      ## We are just using threading&#xa;      self.ppserver = None&#xa;&#xa;    self.isParallelPythonInitialized = True&#xa;&#xa;  def __getLocalAndRemoteMachineNames(self):&#xa;    """"""&#xa;      Method to get the qualified host and remote nodes' names&#xa;      @ In, None&#xa;      @ Out, hostNameMapping, dict, dictionary containing the qualified names&#xa;        {'local':hostName,'remote':{nodeName1:IP1,nodeName2:IP2,etc}}&#xa;    """"""&#xa;    hostNameMapping = {'local':"""",'remote':{}}&#xa;&#xa;    ## Store the local machine name as its fully-qualified domain name (FQDN)&#xa;    hostNameMapping['local'] = str(socket.getfqdn()).strip()&#xa;    self.raiseADebug(""Local Host is "" + hostNameMapping['local'])&#xa;&#xa;    ## collect the qualified hostnames for each remote node&#xa;    for nodeId in list(set(self.runInfoDict['Nodes'])):&#xa;      hostNameMapping['remote'][nodeId.strip()] = socket.gethostbyname(nodeId.strip())&#xa;      self.raiseADebug(""Remote Host identified "" + hostNameMapping['remote'][nodeId.strip()])&#xa;&#xa;    return hostNameMapping&#xa;&#xa;  def __runRemoteListeningSockets(self,newPort):&#xa;    """"""&#xa;      Method to activate the remote sockets for parallel python&#xa;      @ In, newPort, integer, the comunication port to use&#xa;      @ Out, (qualifiedHostName, ppservers), tuple, tuple containining:&#xa;             - in position 0 the host name and&#xa;             - in position 1 the list containing the nodes in which the remote&#xa;               sockets have been activated&#xa;    """"""&#xa;    ## Get the local machine name and the remote nodes one&#xa;    hostNameMapping = self.__getLocalAndRemoteMachineNames()&#xa;    qualifiedHostName =  hostNameMapping['local']&#xa;    remoteNodesIP = hostNameMapping['remote']&#xa;&#xa;    ## Strip out the nodes' names&#xa;    availableNodes = [node.strip() for node in self.runInfoDict['Nodes']]&#xa;&#xa;    ## Get unique nodes&#xa;    uniqueNodes    = list(set(availableNodes))&#xa;    ppservers      = []&#xa;&#xa;    if len(uniqueNodes) > 1:&#xa;      ## There are remote nodes that need to be activated&#xa;&#xa;      ## Locate the ppserver script to be executed&#xa;      ppserverScript = os.path.join(self.runInfoDict['FrameworkDir'],""contrib"",""pp"",""ppserver.py"")&#xa;&#xa;      ## Modify the python path used by the local environment&#xa;      localenv = os.environ.copy()&#xa;      pathSeparator = os.pathsep&#xa;      localenv[""PYTHONPATH""] = pathSeparator.join(sys.path)&#xa;&#xa;      for nodeId in uniqueNodes:&#xa;        ## Build the filename&#xa;        outFileName = nodeId.strip()+""_port:""+str(newPort)+""_server_out.log""&#xa;        outFileName = os.path.join(self.runInfoDict['WorkingDir'], outFileName)&#xa;&#xa;        outFile = open(outFileName, 'w')&#xa;&#xa;        ## Check how many processors are available in the node&#xa;        ntasks = availableNodes.count(nodeId)&#xa;        remoteHostName =  remoteNodesIP[nodeId]&#xa;&#xa;        ## Activate the remote socketing system&#xa;&#xa;        ## Next line is a direct execute of a ppserver:&#xa;        #subprocess.Popen(['ssh', nodeId, ""python2.7"", ppserverScript,""-w"",str(ntasks),""-i"",remoteHostName,""-p"",str(newPort),""-t"",""1000"",""-g"",localenv[""PYTHONPATH""],""-d""],shell=False,stdout=outFile,stderr=outFile,env=localenv)&#xa;&#xa;        ## Instead, let's build the command and then call the os-agnostic version&#xa;        command="" "".join([""python"",ppserverScript,""-w"",str(ntasks),""-i"",remoteHostName,""-p"",str(newPort),""-t"",""50000"",""-g"",localenv[""PYTHONPATH""],""-d""])&#xa;        utils.pickleSafeSubprocessPopen(['ssh',nodeId,""COMMAND='""+command+""'"",self.runInfoDict['RemoteRunCommand']],shell=False,stdout=outFile,stderr=outFile,env=localenv)&#xa;        ## e.g., ssh nodeId COMMAND='python ppserverScript -w stuff'&#xa;&#xa;        ## update list of servers&#xa;        ppservers.append(nodeId+"":""+str(newPort))&#xa;&#xa;    return qualifiedHostName, ppservers&#xa;&#xa;  def startLoop(self):&#xa;    """"""&#xa;    This function begins the polling loop for the JobHandler where it will&#xa;    constantly fill up its running queue with jobs in its pending queue and&#xa;    unload finished jobs into its finished queue to be extracted by&#xa;    """"""&#xa;    while not self.completed:&#xa;      self.fillJobQueue()&#xa;      self.cleanJobQueue()&#xa;      ## TODO May want to revisit this:&#xa;      ## http://stackoverflow.com/questions/29082268/python-time-sleep-vs-event-wait&#xa;      ## probably when we move to Python 3.&#xa;      time.sleep(self.sleepTime)&#xa;&#xa;  def addJob(self, args, functionToRun, identifier, metadata=None, modulesToImport = [], forceUseThreads = False, uniqueHandler=""any"", clientQueue = False):&#xa;    """"""&#xa;      Method to add an internal run (function execution)&#xa;      @ In, args, dict, this is a list of arguments that will be passed as&#xa;        function parameters into whatever method is stored in functionToRun.&#xa;        e.g., functionToRun(*args)&#xa;      @ In, functionToRun,function or method, the function that needs to be&#xa;        executed&#xa;      @ In, identifier, string, the job identifier&#xa;      @ In, metadata, dict, optional, dictionary of metadata associated to this&#xa;        run&#xa;      @ In, modulesToImport, list, optional, list of modules that need to be&#xa;        imported for internal parallelization (parallel python). This list&#xa;        should be generated with the method returnImportModuleString in utils.py&#xa;      @ In, forceUseThreads, bool, optional, flag that, if True, is going to&#xa;        force the usage of multi-threading even if parallel python is activated&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        this runner. For example, if present, to retrieve this runner using the&#xa;        method jobHandler.getFinished, the uniqueHandler needs to be provided.&#xa;        If uniqueHandler == 'any', every ""client"" can get this runner&#xa;      @ In, clientQueue, boolean, optional, if this run needs to be added in the&#xa;        clientQueue&#xa;      @ Out, None&#xa;    """"""&#xa;    ## internal server is initialized only in case an internal calc is requested&#xa;    if not self.isParallelPythonInitialized:&#xa;      self.__initializeParallelPython()&#xa;&#xa;    if self.ppserver is None or forceUseThreads:&#xa;      internalJob = Runners.SharedMemoryRunner(self.messageHandler, args,&#xa;                                               functionToRun,&#xa;                                               identifier, metadata,&#xa;                                               uniqueHandler,&#xa;                                               profile=self.__profileJobs)&#xa;    else:&#xa;      skipFunctions = [utils.metaclass_insert(abc.ABCMeta,BaseType)]&#xa;      internalJob = Runners.DistributedMemoryRunner(self.messageHandler,&#xa;                                                    self.ppserver, args,&#xa;                                                    functionToRun,&#xa;                                                    modulesToImport, identifier,&#xa;                                                    metadata, skipFunctions,&#xa;                                                    uniqueHandler,&#xa;                                                    profile=self.__profileJobs)&#xa;&#xa;    # set the client info&#xa;    internalJob.clientRunner = clientQueue&#xa;    # add the runner in the Queue&#xa;    self.reAddJob(internalJob)&#xa;&#xa;  def reAddJob(self, runner):&#xa;    """"""&#xa;      Method to add a runner object in the queue&#xa;      @ In, runner, Runner Instance, this is the instance of the runner that we want to readd in the queque&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      if not runner.clientRunner:&#xa;        self.__queue.append(runner)&#xa;      else:&#xa;        self.__clientQueue.append(runner)&#xa;      if self.__profileJobs:&#xa;        runner.trackTime('queue')&#xa;      self.__submittedJobs.append(runner.identifier)&#xa;&#xa;  def addClientJob(self, args, functionToRun, identifier, metadata=None, modulesToImport = [], uniqueHandler=""any""):&#xa;    """"""&#xa;      Method to add an internal run (function execution), without consuming&#xa;      resources (free spots). This can be used for client handling (see&#xa;      metamodel)&#xa;      @ In, args, dict, this is a list of arguments that will be passed as&#xa;        function parameters into whatever method is stored in functionToRun.&#xa;        e.g., functionToRun(*args)&#xa;      @ In, functionToRun,function or method, the function that needs to be&#xa;        executed&#xa;      @ In, identifier, string, the job identifier&#xa;      @ In, metadata, dict, optional, dictionary of metadata associated to this&#xa;        run&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        this runner. For example, if present, to retrieve this runner using the&#xa;        method jobHandler.getFinished, the uniqueHandler needs to be provided.&#xa;        If uniqueHandler == 'any', every ""client"" can get this runner.&#xa;      @ Out, None&#xa;    """"""&#xa;    self.addJob(args, functionToRun, identifier, metadata, modulesToImport,&#xa;                forceUseThreads = True, uniqueHandler = uniqueHandler,&#xa;                clientQueue = True)&#xa;&#xa;  def isFinished(self):&#xa;    """"""&#xa;      Method to check if all the runs in the queue are finished&#xa;      @ In, None&#xa;      @ Out, isFinished, bool, True all the runs in the queue are finished&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      ## If there is still something left in the queue, we are not done yet.&#xa;      if len(self.__queue) > 0 or len(self.__clientQueue) > 0:&#xa;        return False&#xa;&#xa;      ## Otherwise, let's look at our running lists and see if there is a job&#xa;      ## that is not done.&#xa;      for run in self.__running+self.__clientRunning:&#xa;        if run:&#xa;          return False&#xa;&#xa;    ## Are there runs that need to be claimed? If so, then I cannot say I am&#xa;    ## done.&#xa;    if len(self.getFinishedNoPop()) > 0:&#xa;      return False&#xa;&#xa;    return True&#xa;&#xa;  def availability(self, client=False):&#xa;    """"""&#xa;      Returns the number of runs that can be added until we consider our queue&#xa;      saturated&#xa;      @ In, client, bool, if true, then return the values for the&#xa;      __clientQueue, otherwise use __queue&#xa;      @ Out, availability, int the number of runs that can be added until we&#xa;      reach saturation&#xa;    """"""&#xa;    ## Due to possibility of memory explosion, we should include the finished&#xa;    ## queue when considering whether we should add a new job. There was an&#xa;    ## issue when running on a distributed system where we saw that this list&#xa;    ## seemed to be growing indefinitely as the main thread was unable to clear&#xa;    ## that list within a reasonable amount of time. The issue on the main thread&#xa;    ## should also be addressed, but at least we can prevent it on this end since&#xa;    ## the main thread's issue may be legitimate.&#xa;&#xa;    maxCount = self.maxQueueSize&#xa;    finishedCount = len(self.__finished)&#xa;&#xa;    if client:&#xa;      if maxCount is None:&#xa;        maxCount = self.__clientRunning.count(None)&#xa;      queueCount = len(self.__clientQueue)&#xa;    else:&#xa;      if maxCount is None:&#xa;        maxCount = self.__running.count(None)&#xa;      queueCount = len(self.__queue)&#xa;&#xa;    availability = maxCount - queueCount - finishedCount&#xa;    return availability&#xa;&#xa;  def isThisJobFinished(self, identifier):&#xa;    """"""&#xa;      Method to check if the run identified by ""identifier"" is finished&#xa;      @ In, identifier, string, identifier&#xa;      @ Out, isFinished, bool, True if the job identified by ""identifier"" is&#xa;        finished&#xa;    """"""&#xa;    identifier = identifier.strip()&#xa;    with self.__queueLock:&#xa;      ## Look through the finished jobs and attempt to find a matching&#xa;      ## identifier. If the job exists here, it is finished&#xa;      for run in self.__finished:&#xa;        if run.identifier == identifier:&#xa;          return True&#xa;&#xa;      ## Look through the pending jobs and attempt to find a matching identifier&#xa;      ## If the job exists here, it is not finished&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        for run in queue:&#xa;          if run.identifier == identifier:&#xa;            return False&#xa;&#xa;      ## Look through the running jobs and attempt to find a matching identifier&#xa;      ## If the job exists here, it is not finished&#xa;      for run in self.__running+self.__clientRunning:&#xa;        if run is not None and run.identifier == identifier:&#xa;          return False&#xa;&#xa;    ##  If you made it here and we still have not found anything, we have got&#xa;    ## problems.&#xa;    self.raiseAnError(RuntimeError,""Job ""+identifier+"" is unknown!"")&#xa;&#xa;  def areTheseJobsFinished(self, uniqueHandler=""any""):&#xa;    """"""&#xa;      Method to check if all the runs in the queue are finished&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        each runner. If provided, just the jobs that have the uniqueIdentifier&#xa;        will be retrieved. By default uniqueHandler = 'any' => all the jobs for&#xa;        which no uniqueIdentifier has been set up are going to be retrieved&#xa;      @ Out, isFinished, bool, True all the runs in the queue are finished&#xa;    """"""&#xa;    uniqueHandler = uniqueHandler.strip()&#xa;    with self.__queueLock:&#xa;      for run in self.__finished:&#xa;        if run.uniqueHandler == uniqueHandler:&#xa;          return False&#xa;&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        for run in queue:&#xa;          if run.uniqueHandler == uniqueHandler:&#xa;            return False&#xa;&#xa;      for run in self.__running + self.__clientRunning:&#xa;        if run is not None and run.uniqueHandler == uniqueHandler:&#xa;          return False&#xa;&#xa;    self.raiseADebug(""The jobs with uniqueHandler "", uniqueHandler, ""are finished"")&#xa;&#xa;    return True&#xa;&#xa;  def getFailedJobs(self):&#xa;    """"""&#xa;      Method to get list of failed jobs&#xa;      @ In, None&#xa;      @ Out, __failedJobs, list, list of the identifiers (jobs) that failed&#xa;    """"""&#xa;    return self.__failedJobs&#xa;&#xa;  def getFinished(self, removeFinished=True, jobIdentifier = '', uniqueHandler = ""any""):&#xa;    """"""&#xa;      Method to get the list of jobs that ended (list of objects)&#xa;      @ In, removeFinished, bool, optional, flag to control if the finished jobs&#xa;        need to be removed from the queue&#xa;      @ In, jobIdentifier, string, optional, if specified, only collects&#xa;        finished runs that start with this text. If not specified collect all.&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        each runner. If provided, just the jobs that have the uniqueIdentifier&#xa;        will be retrieved. By default uniqueHandler = 'any' => all the jobs for&#xa;        which no uniqueIdentifier has been set up are going to be retrieved&#xa;      @ Out, finished, list, list of finished jobs (InternalRunner or&#xa;        ExternalRunner objects) (if jobIdentifier is None), else the finished&#xa;        jobs matching the base case jobIdentifier&#xa;    """"""&#xa;    finished = []&#xa;&#xa;    ## If the user does not specify a jobIdentifier, then set it to the empty&#xa;    ## string because every job will match this starting string.&#xa;    if jobIdentifier is None:&#xa;      jobIdentifier = ''&#xa;&#xa;    with self.__queueLock:&#xa;      runsToBeRemoved = []&#xa;      for i,run in enumerate(self.__finished):&#xa;        ## If the jobIdentifier does not match or the uniqueHandler does not&#xa;        ## match, then don't bother trying to do anything with it&#xa;        if not run.identifier.startswith(jobIdentifier) \&#xa;        or uniqueHandler != run.uniqueHandler:&#xa;          continue&#xa;&#xa;        finished.append(run)&#xa;        if removeFinished:&#xa;          runsToBeRemoved.append(i)&#xa;          self.__checkAndRemoveFinished(run)&#xa;&#xa;      ##Since these indices are sorted, reverse them to ensure that when we&#xa;      ## delete something it will not shift anything to the left (lower index)&#xa;      ## than it.&#xa;      for i in reversed(runsToBeRemoved):&#xa;        self.__finished[i].trackTime('collected')&#xa;        del self.__finished[i]&#xa;    ## end with self.__queueLock&#xa;&#xa;    return finished&#xa;&#xa;  def getFinishedNoPop(self):&#xa;    """"""&#xa;      Method to get the list of jobs that ended (list of objects) without&#xa;      removing them from the queue&#xa;      @ In, None&#xa;      @ Out, finished, list, list of finished jobs (InternalRunner or&#xa;        ExternalRunner objects)&#xa;    """"""&#xa;    finished = self.getFinished(False)&#xa;    return finished&#xa;&#xa;  ## Deprecating this function because I don't think it is doing the right thing&#xa;  ## People using the job handler should be asking for what is available not the&#xa;  ## number of free spots in the running block. Only the job handler should be&#xa;  ## able to internally alter or query the running and clientRunning queues.&#xa;  ## The outside environment can only access the queue and clientQueue variables.&#xa;  # def numFreeSpots(self, client=False):&#xa;&#xa;  def numRunning(self):&#xa;    """"""&#xa;      Returns the number of runs currently running.&#xa;      @ In, None&#xa;      @ Out, activeRuns, int, number of active runs&#xa;    """"""&#xa;    #with self.__queueLock:&#xa;    ## The size of the list does not change, only its contents, so I don't&#xa;    ## think there should be any conflict if we are reading a variable from&#xa;    ## one thread and updating it on the other thread.&#xa;    activeRuns = sum(run is not None for run in self.__running)&#xa;    return activeRuns&#xa;&#xa;  def numSubmitted(self):&#xa;    """"""&#xa;      Method to get the number of submitted jobs&#xa;      @ In, None&#xa;      @ Out, len(self.__submittedJobs), int, number of submitted jobs&#xa;    """"""&#xa;    return len(self.__submittedJobs)&#xa;&#xa;  def fillJobQueue(self):&#xa;    """"""&#xa;      Method to start running the jobs in queue.  If there are empty slots&#xa;      takes jobs out of the queue and starts running them.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;&#xa;    ## Only the jobHandler's startLoop thread should have write access to the&#xa;    ## self.__running variable, so we should be able to safely query this outside&#xa;    ## of the lock given that this function is called only on that thread as well.&#xa;    emptySlots = [i for i,run in enumerate(self.__running) if run is None]&#xa;&#xa;    ## Don't bother acquiring the lock if there are no empty spots or nothing&#xa;    ## in the queue (this could be simultaneously added to by the main thread,&#xa;    ## but I will be back here after a short wait on this thread so I am not&#xa;    ## concerned about this potential inconsistency)&#xa;    if len(emptySlots) > 0 and len(self.__queue) > 0:&#xa;      with self.__queueLock:&#xa;        for i in emptySlots:&#xa;          ## The queue could be emptied during this loop, so we will to break&#xa;          ## out as soon as that happens so we don't hog the lock.&#xa;          if len(self.__queue) > 0:&#xa;            item = self.__queue.popleft()&#xa;&#xa;            ## Okay, this is a little tricky, but hang with me here. Whenever&#xa;            ## a code model is run, we need to replace some of its command&#xa;            ## parameters. The way we do this is by looking at the job instance&#xa;            ## and checking if the first argument (the self in&#xa;            ## self.evaluateSample) is an instance of Code, if so, then we need&#xa;            ## to replace the execution command. Is this fragile? Possibly. We may&#xa;            ## want to revisit this on the next iteration of this code.&#xa;            if len(item.args) > 0 and isinstance(item.args[0], Models.Code):&#xa;              kwargs = {}&#xa;              kwargs['INDEX'] = str(i)&#xa;              kwargs['INDEX1'] = str(i+i)&#xa;              kwargs['CURRENT_ID'] = str(self.__nextId)&#xa;              kwargs['CURRENT_ID1'] = str(self.__nextId+1)&#xa;              kwargs['SCRIPT_DIR'] = self.runInfoDict['ScriptDir']&#xa;              kwargs['FRAMEWORK_DIR'] = self.runInfoDict['FrameworkDir']&#xa;              ## This will not be used since the Code will create a new&#xa;              ## directory for its specific files and will spawn a process there&#xa;              ## so we will let the Code fill that in. Note, the line below&#xa;              ## represents the WRONG directory for an instance of a code!&#xa;              ## It is however the correct directory for a MultiRun step&#xa;              ## -- DPM 5/4/17&#xa;              kwargs['WORKING_DIR'] = item.args[0].workingDir&#xa;              kwargs['BASE_WORKING_DIR'] = self.runInfoDict['WorkingDir']&#xa;              kwargs['METHOD'] = os.environ.get(""METHOD"",""opt"")&#xa;              kwargs['NUM_CPUS'] = str(self.runInfoDict['NumThreads'])&#xa;              item.args[3].update(kwargs)&#xa;&#xa;            self.__running[i] = item&#xa;            self.__running[i].start()&#xa;            self.__running[i].trackTime('started')&#xa;            self.__nextId += 1&#xa;          else:&#xa;            break&#xa;&#xa;    ## Repeat the same process above, only for the clientQueue&#xa;    emptySlots = [i for i,run in enumerate(self.__clientRunning) if run is None]&#xa;    if len(emptySlots) > 0 and len(self.__clientQueue) > 0:&#xa;      with self.__queueLock:&#xa;        for i in emptySlots:&#xa;          if len(self.__clientQueue) > 0:&#xa;            self.__clientRunning[i] = self.__clientQueue.popleft()&#xa;            self.__clientRunning[i].start()&#xa;            self.__clientRunning[i].trackTime('jobHandler_started')&#xa;            self.__nextId += 1&#xa;          else:&#xa;            break&#xa;&#xa;  def cleanJobQueue(self):&#xa;    """"""&#xa;    Method that will remove finished jobs from the queue and place them into the&#xa;    finished queue to be read by some other thread.&#xa;    @ In, None&#xa;    @ Out, None&#xa;    """"""&#xa;    ## The code handling these two lists was the exact same, I have taken the&#xa;    ## liberty of condensing these loops into one and removing some of the&#xa;    ## redundant checks to make this code a bit simpler.&#xa;    for runList in [self.__running, self.__clientRunning]:&#xa;      for i,run in enumerate(runList):&#xa;        if run is not None and run.isDone():&#xa;          ## We should only need the lock if we are touching the finished queue&#xa;          ## which is cleared by the main thread. Again, the running queues&#xa;          ## should not be modified by the main thread, however they may inquire&#xa;          ## it by calling numRunning.&#xa;          with self.__queueLock:&#xa;            self.__finished.append(run)&#xa;            self.__finished[-1].trackTime('jobHandler_finished')&#xa;            runList[i] = None&#xa;&#xa;  def setProfileJobs(self,profile=False):&#xa;    """"""&#xa;      Sets whether profiles for jobs are printed or not.&#xa;      @ In, profile, bool, optional, if True then print timings for jobs when they are garbage collected&#xa;      @ Out, None&#xa;    """"""&#xa;    self.__profileJobs = profile&#xa;&#xa;  def startingNewStep(self):&#xa;    """"""&#xa;      Method to reset the __submittedJobs to an empty list.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      self.__submittedJobs = []&#xa;&#xa;  def shutdown(self):&#xa;    """"""&#xa;    This function will mark the job handler as done, so it can shutdown its&#xa;    polling thread.&#xa;    @ In, None&#xa;    @ Out, None&#xa;    """"""&#xa;    self.completed = True&#xa;&#xa;  def terminateAll(self):&#xa;    """"""&#xa;      Method to clear out the queue by killing all running processes.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        queue.clear()&#xa;&#xa;      for runList in [self.__running, self.__clientRunning]:&#xa;        unfinishedRuns = [run for run in runList if run is not None]&#xa;        for run in unfinishedRuns:&#xa;          run.kill()&#xa;"
5599254|"# -*- coding: utf-8 -*-&#xa;&#xa;import os&#xa;import sys&#xa;import sphinx_bootstrap_theme&#xa;&#xa;# only needed for Autobahn|Python&#xa;sys.path.insert(0, os.path.abspath('./_extensions'))&#xa;sys.path.insert(0, os.path.abspath('..'))&#xa;&#xa;extensions = [&#xa;   'sphinx.ext.autodoc',&#xa;   'sphinx.ext.doctest',&#xa;   'sphinx.ext.intersphinx',&#xa;   'sphinx.ext.viewcode',&#xa;   'sphinx.ext.ifconfig',&#xa;   'sphinx.ext.todo',&#xa;   'sphinxcontrib.spelling',&#xa;   'txsphinx' # only needed for Autobahn|Python&#xa;]&#xa;&#xa;spelling_lang = 'en_US'&#xa;spelling_show_suggestions = False&#xa;spelling_word_list_filename = 'spelling_wordlist.txt'&#xa;&#xa;# Add any paths that contain templates here, relative to this directory.&#xa;templates_path = ['_templates']&#xa;&#xa;# The suffix of source filenames.&#xa;source_suffix = '.rst'&#xa;&#xa;# The master toctree document.&#xa;master_doc = 'index'&#xa;&#xa;# General information about the project.&#xa;project = u'AutobahnPython'&#xa;copyright = u'Tavendo GmbH'&#xa;&#xa;# The version info for the project you're documenting, acts as replacement for&#xa;# |version| and |release|, also used in various other places throughout the&#xa;# built documents.&#xa;#&#xa;base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))&#xa;init = {}&#xa;with open(os.path.join(base_dir, ""autobahn"", ""__init__.py"")) as f:&#xa;   exec(f.read(), init)&#xa;&#xa;version = release = init[""__version__""]&#xa;&#xa;&#xa;# The language for content autogenerated by Sphinx. Refer to documentation&#xa;# for a list of supported languages.&#xa;#language = None&#xa;&#xa;# There are two options for replacing |today|: either, you set today to some&#xa;# non-false value, then it is used:&#xa;#today = ''&#xa;# Else, today_fmt is used as the format for a strftime call.&#xa;#today_fmt = '%B %d, %Y'&#xa;&#xa;# List of patterns, relative to source directory, that match files and&#xa;# directories to ignore when looking for source files.&#xa;exclude_patterns = ['_build', 'work']&#xa;&#xa;# The name of the Pygments (syntax highlighting) style to use.&#xa;pygments_style = 'sphinx'&#xa;&#xa;## Sphinx-Bootstrap Theme&#xa;##&#xa;## http://sphinx-bootstrap-theme.readthedocs.org/en/latest/README.html&#xa;##&#xa;html_theme = 'bootstrap'&#xa;html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()&#xa;&#xa;html_theme_options = {&#xa;    # Navigation bar title. (Default: ``project`` value)&#xa;    'navbar_title': "" "",&#xa;&#xa;    # Tab name for entire site. (Default: ""Site"")&#xa;    'navbar_site_name': ""Site"",&#xa;&#xa;    # A list of tuples containing pages or urls to link to.&#xa;    # Valid tuples should be in the following forms:&#xa;    #    (name, page)                 # a link to a page&#xa;    #    (name, ""/aa/bb"", 1)          # a link to an arbitrary relative url&#xa;    #    (name, ""http://example.com"", True) # arbitrary absolute url&#xa;    # Note the ""1"" or ""True"" value above as the third argument to indicate&#xa;    # an arbitrary url.&#xa;    'navbar_links': [&#xa;        #(""Examples"", ""examples""),&#xa;        #(""Link"", ""http://example.com"", True),&#xa;    ],&#xa;&#xa;    # Render the next and previous page links in navbar. (Default: true)&#xa;    'navbar_sidebarrel': True,&#xa;&#xa;    # Render the current pages TOC in the navbar. (Default: true)&#xa;    'navbar_pagenav': True,&#xa;&#xa;    # Tab name for the current pages TOC. (Default: ""Page"")&#xa;    #'navbar_pagenav_name': ""Page"",&#xa;&#xa;    # Global TOC depth for ""site"" navbar tab. (Default: 1)&#xa;    # Switching to -1 shows all levels.&#xa;    'globaltoc_depth': 1,&#xa;&#xa;    # Include hidden TOCs in Site navbar?&#xa;    #&#xa;    # Note: If this is ""false"", you cannot have mixed ``:hidden:`` and&#xa;    # non-hidden ``toctree`` directives in the same page, or else the build&#xa;    # will break.&#xa;    #&#xa;    # Values: ""true"" (default) or ""false""&#xa;    'globaltoc_includehidden': ""true"",&#xa;&#xa;    # HTML navbar class (Default: ""navbar"") to attach to <div> element.&#xa;    # For black navbar, do ""navbar navbar-inverse""&#xa;    #'navbar_class': ""navbar navbar-inverse"",&#xa;    'navbar_class': ""navbar"",&#xa;&#xa;    # Fix navigation bar to top of page?&#xa;    # Values: ""true"" (default) or ""false""&#xa;    'navbar_fixed_top': ""true"",&#xa;&#xa;    # Location of link to source.&#xa;    # Options are ""nav"" (default), ""footer"" or anything else to exclude.&#xa;    'source_link_position': ""nav"",&#xa;&#xa;    # Bootswatch (http://bootswatch.com/) theme.&#xa;    #&#xa;    # Options are nothing with """" (default) or the name of a valid theme&#xa;    # such as ""amelia"" or ""cosmo"".&#xa;    'bootswatch_theme': """",&#xa;&#xa;    # Choose Bootstrap version.&#xa;    # Values: ""3"" (default) or ""2"" (in quotes)&#xa;    'bootstrap_version': ""3"",&#xa;}&#xa;&#xa;# Add any paths that contain custom themes here, relative to this directory.&#xa;#html_theme_path = []&#xa;&#xa;# The name for this set of Sphinx documents.  If None, it defaults to&#xa;# ""<project> v<release> documentation"".&#xa;#html_title = None&#xa;&#xa;# A shorter title for the navigation bar.  Default is the same as html_title.&#xa;#html_short_title = None&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top&#xa;# of the sidebar.&#xa;#html_logo = None&#xa;&#xa;# The name of an image file (within the static path) to use as favicon of the&#xa;# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32&#xa;# pixels large.&#xa;#html_favicon = None&#xa;&#xa;# Add any paths that contain custom static files (such as style sheets) here,&#xa;# relative to this directory. They are copied after the builtin static files,&#xa;# so a file named ""default.css"" will overwrite the builtin ""default.css"".&#xa;html_static_path = ['_static']&#xa;&#xa;&#xa;## additional variables which become accessible in RST (e.g. .. ifconfig:: not no_network)&#xa;##&#xa;def setup(app):&#xa;   app.add_config_value('no_network', False, True)&#xa;&#xa;no_network = None&#xa;&#xa;## additional variables which become accessible in the template engine's&#xa;## context for all pages&#xa;##&#xa;html_context = {&#xa;   #'widgeturl': 'https://demo.crossbar.io/clandeckwidget'&#xa;   #'widgeturl': 'http://127.0.0.1:8090/widget'&#xa;   'widgeturl': None,&#xa;   'no_network': False,&#xa;   #'cstatic': 'http://127.0.0.1:8888',&#xa;   'cstatic': '//tavendo-common-static.s3-eu-west-1.amazonaws.com',&#xa;}&#xa;&#xa;# (Optional) Logo. Should be small enough to fit the navbar (ideally 24x24).&#xa;# Path should be relative to the ``_static`` files directory.&#xa;html_logo = None&#xa;&#xa;&#xa;# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,&#xa;# using the given strftime format.&#xa;#html_last_updated_fmt = '%b %d, %Y'&#xa;&#xa;# If true, SmartyPants will be used to convert quotes and dashes to&#xa;# typographically correct entities.&#xa;#html_use_smartypants = True&#xa;&#xa;# Custom sidebar templates, maps document names to template names.&#xa;html_sidebars = {&#xa;   '**': [&#xa;      'side-primary.html'&#xa;   ]&#xa;}&#xa;&#xa;# Additional templates that should be rendered to pages, maps page names to&#xa;# template names.&#xa;#html_additional_pages = {}&#xa;&#xa;# If false, no module index is generated.&#xa;#html_domain_indices = True&#xa;&#xa;# If false, no index is generated.&#xa;#html_use_index = True&#xa;&#xa;# If true, the index is split into individual pages for each letter.&#xa;#html_split_index = False&#xa;&#xa;# If true, links to the reST sources are added to the pages.&#xa;#html_show_sourcelink = True&#xa;&#xa;# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.&#xa;#html_show_sphinx = True&#xa;&#xa;# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.&#xa;#html_show_copyright = True&#xa;&#xa;# If true, an OpenSearch description file will be output, and all pages will&#xa;# contain a <link> tag referring to it.  The value of this option must be the&#xa;# base URL from which the finished HTML is served.&#xa;#html_use_opensearch = ''&#xa;&#xa;# This is the file name suffix for HTML files (e.g. "".xhtml"").&#xa;#html_file_suffix = None&#xa;&#xa;# Output file base name for HTML help builder.&#xa;htmlhelp_basename = 'AutobahnPython'&#xa;&#xa;&#xa;# http://sphinx-doc.org/ext/intersphinx.html&#xa;intersphinx_mapping = {&#xa;   'py2': ('http://docs.python.org/2', None),&#xa;   'py3': ('http://docs.python.org/3', None),&#xa;   'six': ('https://pythonhosted.org/six/', None),&#xa;}&#xa;&#xa;rst_epilog = """"""&#xa;.. |ab| replace:: Autobahn&#xa;.. |Ab| replace:: **Autobahn**&#xa;.. |abL| replace:: Autobahn|Python&#xa;.. |AbL| replace:: **Autobahn**\|Python&#xa;.. _Autobahn: http://autobahn.ws&#xa;.. _AutobahnJS: http://autobahn.ws/js&#xa;.. _AutobahnPython: **Autobahn**\|Python&#xa;.. _WebSocket: http://tools.ietf.org/html/rfc6455&#xa;.. _RFC6455: http://tools.ietf.org/html/rfc6455&#xa;.. _WAMP: http://wamp.ws/&#xa;.. _Twisted: http://twistedmatrix.com/&#xa;.. _asyncio: http://docs.python.org/3.4/library/asyncio.html&#xa;.. _CPython: http://python.org/&#xa;.. _PyPy: http://pypy.org/&#xa;.. _Jython: http://jython.org/&#xa;.. _WAMP: http://wamp.ws/&#xa;.. _WAMPv1: http://wamp.ws/spec/wamp1/&#xa;.. _WAMPv2: https://github.com/tavendo/WAMP/blob/master/spec/README.md&#xa;.. _AutobahnTestsuite: http://autobahn.ws/testsuite&#xa;.. _trollius: https://pypi.python.org/pypi/trollius/&#xa;.. _tulip: https://pypi.python.org/pypi/asyncio/&#xa;""""""&#xa;&#xa;rst_prolog = """"""&#xa;""""""&#xa;&#xa;# http://stackoverflow.com/questions/5599254/how-to-use-sphinxs-autodoc-to-document-a-classs-init-self-method&#xa;autoclass_content = 'both'&#xa;&#xa;autodoc_member_order = 'bysource'&#xa;"
4575588|"# -*- coding: utf-8 -*-&#xa;""""""&#xa;&#xa;Edited by Christian Münker, 2013&#xa;""""""&#xa;from __future__ import print_function, division, unicode_literals, absolute_import&#xa;from PyQt4 import QtGui #, QtCore&#xa;import numpy as np&#xa;from numpy import pi, ones, zeros, sin, cos, log10&#xa;import scipy.signal as sig&#xa;&#xa;&#xa;import pyfda.filterbroker as fb&#xa;import pyfda.pyfda_rc as rc&#xa;from pyfda.pyfda_lib import H_mag&#xa;from pyfda.plot_widgets.plot_utils import MplWidget&#xa;&#xa;from mpl_toolkits.mplot3d.axes3d import Axes3D&#xa;from matplotlib import cm # Colormap&#xa;&#xa;class Plot3D(QtGui.QMainWindow):&#xa;    """"""&#xa;    Class for various 3D-plots:&#xa;    - lin / log line plot of H(f)&#xa;    - lin / log surf plot of H(z)&#xa;    - optional display of poles / zeros&#xa;    """"""&#xa;&#xa;    def __init__(self, parent=None, DEBUG=False): # default parent = None -> top Window&#xa;        super(Plot3D, self).__init__(parent) # initialize QWidget base class&#xa;#        QtGui.QMainWindow.__init__(self) # alternative syntax&#xa;&#xa;        self.DEBUG = DEBUG&#xa;        self.zmin = 0&#xa;        self.zmax = 4&#xa;        self.zmin_dB = -80&#xa;&#xa;        self.lblLog = QtGui.QLabel(""Log."")&#xa;        self.chkLog = QtGui.QCheckBox(self)&#xa;        self.chkLog.setObjectName(""chkLog"")&#xa;        self.chkLog.setToolTip(""Logarithmic scale"")&#xa;        self.chkLog.setChecked(False)&#xa;&#xa;        self.lblBottom = QtGui.QLabel(""Bottom:"")&#xa;&#xa;        self.ledBottom = QtGui.QLineEdit(self)&#xa;        self.ledBottom.setObjectName(""ledBottom"")&#xa;        self.ledBottom.setText(str(self.zmin))&#xa;        self.ledBottom.setToolTip(""Minimum display value."")&#xa;&#xa;        self.lblTop = QtGui.QLabel(""Top:"")&#xa;&#xa;        self.ledTop = QtGui.QLineEdit(self)&#xa;        self.ledTop.setObjectName(""ledTop"")&#xa;        self.ledTop.setText(str(self.zmax))&#xa;        self.ledTop.setToolTip(""Maximum display value."")&#xa;#        self.ledTop.setSizePolicy(QtGui.QSizePolicy.Maximum, QtGui.QSizePolicy.Maximum)&#xa;&#xa;        self.lblUC = QtGui.QLabel(self)&#xa;        self.lblUC.setText(""UC"")&#xa;        self.chkUC = QtGui.QCheckBox(self)&#xa;        self.chkUC.setObjectName(""chkUC"")&#xa;        self.chkUC.setToolTip(""Plot unit circle"")&#xa;        self.chkUC.setChecked(True)&#xa;&#xa;        self.lblPZ = QtGui.QLabel(self)&#xa;        self.lblPZ.setText(""P/Z"")&#xa;        self.chkPZ = QtGui.QCheckBox(self)&#xa;        self.chkPZ.setObjectName(""chkPZ"")&#xa;        self.chkPZ.setToolTip(""Plot poles and zeros"")&#xa;        self.chkPZ.setChecked(True)&#xa;&#xa;        self.lblHf = QtGui.QLabel(self)&#xa;        self.lblHf.setText(""H(f)"")&#xa;        self.chkHf = QtGui.QCheckBox(self)&#xa;        self.chkHf.setObjectName(""chkHf"")&#xa;        self.chkHf.setToolTip(""Plot H(f) along the unit circle"")&#xa;        self.chkHf.setChecked(True)&#xa;&#xa;        modes = ['None', 'Mesh', 'Surf', 'Contour']&#xa;        self.cmbMode3D = QtGui.QComboBox(self)&#xa;        self.cmbMode3D.addItems(modes)&#xa;        self.cmbMode3D.setObjectName(""cmbShow3D"")&#xa;        self.cmbMode3D.setToolTip(""Select 3D-plot mode."")&#xa;        self.cmbMode3D.setCurrentIndex(0)&#xa;        self.cmbMode3D.setSizeAdjustPolicy(QtGui.QComboBox.AdjustToContents)&#xa;&#xa;        self.lblColBar = QtGui.QLabel(self)&#xa;        self.lblColBar.setText(""Colorbar"")&#xa;        self.chkColBar = QtGui.QCheckBox(self)&#xa;        self.chkColBar.setObjectName(""chkColBar"")&#xa;        self.chkColBar.setToolTip(""Show colorbar"")&#xa;        self.chkColBar.setChecked(False)&#xa;&#xa;        self.diaAlpha = QtGui.QDial(self)&#xa;        self.diaAlpha.setRange(0., 10.)&#xa;        self.diaAlpha.setValue(5)&#xa;        self.diaAlpha.setTracking(False) # produce less events when turning&#xa;        self.diaAlpha.setFixedHeight(30)&#xa;        self.diaAlpha.setFixedWidth(30)&#xa;        self.diaAlpha.setWrapping(False)&#xa;        self.diaAlpha.setToolTip(""Set transparency for surf and contour plot."")&#xa;&#xa;        self.diaHatch = QtGui.QDial(self)&#xa;        self.diaHatch.setRange(0., 9.)&#xa;        self.diaHatch.setValue(5)&#xa;        self.diaHatch.setTracking(False) # produce less events when turning&#xa;        self.diaHatch.setFixedHeight(30)&#xa;        self.diaHatch.setFixedWidth(30)&#xa;        self.diaHatch.setWrapping(False)&#xa;        self.diaHatch.setToolTip(""Set hatching for H(jw)."")&#xa;&#xa;        self.lblContour2D = QtGui.QLabel(self)&#xa;        self.lblContour2D.setText(""Contour2D"")&#xa;        self.chkContour2D = QtGui.QCheckBox(self)&#xa;        self.chkContour2D.setObjectName(""chkContour2D"")&#xa;        self.chkContour2D.setToolTip(""Plot 2D-contours for real and imaginary part"")&#xa;        self.chkContour2D.setChecked(False)&#xa;&#xa;        self.layHChkBoxes = QtGui.QHBoxLayout()&#xa;        self.layHChkBoxes.addStretch(10)&#xa;&#xa;        self.layHChkBoxes.addWidget(self.lblLog)&#xa;        self.layHChkBoxes.addWidget(self.chkLog)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblBottom)&#xa;        self.layHChkBoxes.addWidget(self.ledBottom)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblTop)&#xa;        self.layHChkBoxes.addWidget(self.ledTop)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblUC)&#xa;        self.layHChkBoxes.addWidget(self.chkUC)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblPZ)&#xa;        self.layHChkBoxes.addWidget(self.chkPZ)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblHf)&#xa;        self.layHChkBoxes.addWidget(self.chkHf)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.cmbMode3D)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblColBar)&#xa;        self.layHChkBoxes.addWidget(self.chkColBar)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.diaAlpha)&#xa;        self.layHChkBoxes.addWidget(self.diaHatch)&#xa;        self.layHChkBoxes.addStretch(1)&#xa;        self.layHChkBoxes.addWidget(self.lblContour2D)&#xa;        self.layHChkBoxes.addWidget(self.chkContour2D)&#xa;&#xa;        self.layHChkBoxes.addStretch(1)&#xa;&#xa;&#xa;        self.mplwidget = MplWidget()&#xa;#        self.mplwidget.setParent(self)&#xa;&#xa;        self.mplwidget.layVMainMpl.addLayout(self.layHChkBoxes)&#xa;&#xa;#        self.mplwidget.setFocus()&#xa;        # make this the central widget, taking all available space:&#xa;        self.setCentralWidget(self.mplwidget)&#xa;&#xa;        self.initAxes()&#xa;&#xa;        self.initCoord()&#xa;&#xa;        self.draw() # calculate and draw phi(f)&#xa;&#xa;#        #=============================================&#xa;#        # Signals & Slots&#xa;#        #=============================================&#xa;        self.chkLog.clicked.connect(self.logClicked)&#xa;        self.ledBottom.editingFinished.connect(self.logClicked)&#xa;        self.ledTop.editingFinished.connect(self.logClicked)&#xa;        self.chkUC.clicked.connect(self.draw)&#xa;        self.chkHf.clicked.connect(self.draw)&#xa;        self.chkPZ.clicked.connect(self.draw)&#xa;        self.cmbMode3D.currentIndexChanged.connect(self.draw)&#xa;        self.chkColBar.clicked.connect(self.draw)&#xa;        self.diaAlpha.valueChanged.connect(self.draw)&#xa;        self.diaHatch.valueChanged.connect(self.draw)&#xa;        self.chkContour2D.clicked.connect(self.draw)&#xa;&#xa;&#xa;    def initCoord(self):&#xa;        """""" Initialize coordinates for the unit circle """"""&#xa;        # TODO: move creation of x,y-grid here as well&#xa;        self.phi_EK = np.linspace(0, 2*pi, 400, endpoint=True)&#xa;        self.xy_UC = np.exp(1j * self.phi_EK) # x,y coordinates of unity circle&#xa;&#xa;    def initAxes(self):&#xa;        """"""Initialize and clear the axes&#xa;        see http://stackoverflow.com/questions/4575588/matplotlib-3d-plot-with-pyqt4-in-qtabwidget-mplwidget&#xa;        """"""&#xa;        self.mplwidget.fig.clf() # needed to get rid of colormap&#xa;        self.ax3d = self.mplwidget.fig.add_subplot(111, projection='3d')&#xa;&#xa;&#xa;    def logClicked(self):&#xa;        """""" Change scale and settings to log / lin """"""&#xa;        self.log = self.chkLog.isChecked()&#xa;        if self.sender().objectName() == 'chkLog': # origin of signal that triggered the slot&#xa;            if self.log:&#xa;&#xa;                self.ledBottom.setText(str(self.zmin_dB))&#xa;                self.zmax_dB = np.round(20 * log10(self.zmax), 2)&#xa;                self.ledTop.setText(str(self.zmax_dB))&#xa;            else:&#xa;                self.ledBottom.setText(str(self.zmin))&#xa;                self.zmax = np.round(10**(self.zmax_dB / 20), 2)&#xa;                self.ledTop.setText(str(self.zmax))&#xa;        else:&#xa;            if self.log:&#xa;                self.zmin_dB = float(self.ledBottom.text())&#xa;                self.zmax_dB = float(self.ledTop.text())&#xa;            else:&#xa;                self.zmin = float(self.ledBottom.text())&#xa;                self.zmax = float(self.ledTop.text())&#xa;&#xa;        self.draw()&#xa;&#xa;&#xa;&#xa;    def draw(self):&#xa;        if self.mplwidget.mplToolbar.enable_update:&#xa;            self.draw_3d()&#xa;&#xa;&#xa;    def draw_3d(self):&#xa;        """"""&#xa;        Draw various 3D plots&#xa;        """"""&#xa;        self.initAxes() # needed to get rid of colormap&#xa;&#xa;&#xa;        bb = fb.fil[0]['ba'][0]&#xa;        aa = fb.fil[0]['ba'][1]&#xa;&#xa;        zz = np.array(fb.fil[0]['zpk'][0])&#xa;        pp = np.array(fb.fil[0]['zpk'][1])&#xa;&#xa;        wholeF = fb.fil[0]['freqSpecsRangeType'] != 'half'&#xa;        f_S = fb.fil[0]['f_S']&#xa;        N_FFT = rc.params['N_FFT']&#xa;        alpha = self.diaAlpha.value()/10.&#xa;&#xa;        #-----------------------------------------------------------------------------&#xa;        # Define 3D-Plotting Options&#xa;        #-----------------------------------------------------------------------------&#xa;&#xa;        OPT_3D_POLAR_SPEC = True # Plot circular range in 3D-Plot&#xa;        OPT_3D_FORCE_ZMAX = True # Enforce absolute limit for 3D-Plot&#xa;        OPT_3D_MSTRIDE = 1 # Schrittweite für MESH und CONT3D&#xa;        OPT_3D_ALPHA = alpha#0.5 # Transparency for surface plot&#xa;        cmap = cm.jet&#xa;        # Colormaps: 'hsv', 'jet_r', 'bone', 'prism' 'gray', 'prism', 'coolwarm'&#xa;        # *_r colormaps reverse the color order&#xa;        #&#xa;        steps = 100               # number of steps for x, y, r, phi&#xa;        rmin = 0;    rmax = 1.2  # polar range definition&#xa;        #&#xa;        xmin = -1.5; xmax = 1.5  # cartesian range definition&#xa;        ymin = -1.5; ymax = 1.5&#xa;        #&#xa;        zmax_rel = 5 # Max. displayed z - value relative to max|H(f)|&#xa;&#xa;        # Calculate limits etc. for 3D-Plots&#xa;        dr = rmax / steps * 2&#xa;        dphi = pi / steps # grid size for polar range&#xa;        dx = (xmax - xmin) / steps&#xa;        dy = (ymax - ymin) / steps # grid size cartesian range&#xa;        if OPT_3D_POLAR_SPEC == True: # polar grid&#xa;            [r, phi] = np.meshgrid(np.arange(rmin, rmax, dr),&#xa;                            np.linspace(0, 2 * pi, steps, endpoint=True))&#xa;            x = r * cos(phi)&#xa;            y = r * sin(phi)&#xa;        else: # cartesian grid&#xa;            [x, y] = np.meshgrid(np.arange(xmin, xmax, dx), np.arange(ymin, ymax, dy))&#xa;&#xa;        z = x + 1j*y # create coordinate grid for complex plane&#xa;&#xa;        [w, H] = sig.freqz(bb, aa, N_FFT, wholeF) # calculate H(w) along the&#xa;                                                # upper half of unity circle&#xa;                                                # w runs from 0 ... pi, length = N_FFT&#xa;        f = w / (2 * pi) * f_S                  # translate w to absolute frequencies&#xa;&#xa;        H_abs = abs(H)&#xa;        H_max = max(H_abs)&#xa;        H_max_dB = 20*log10(H_max)&#xa;        F_max = f[np.argmax(H_abs)]&#xa;&#xa;&#xa;        H_min = min(H_abs)&#xa;        H_min_dB = 20*log10(H_min)&#xa;        F_min = f[np.argmin(H_abs)]&#xa;&#xa;        plevel_rel = 1.05 # height of plotted pole position relative to zmax&#xa;        zlevel_rel = 0.1 # height of plotted zero position relative to zmax&#xa;&#xa;&#xa;        # calculate H(jw)| along the unity circle and |H(z)|, each clipped&#xa;        # between bottom and top&#xa;        if self.chkLog.isChecked():&#xa;            bottom = np.floor(max(self.zmin_dB, H_min_dB) / 10) * 10&#xa;            top = self.zmax_dB&#xa;            plevel_top = top + (top - bottom) * (plevel_rel - 1)&#xa;            plevel_btm = top&#xa;            zlevel = bottom - (top - bottom) * (zlevel_rel)&#xa;            H_UC = H_mag(bb, aa, self.xy_UC, top, H_min=bottom, log=True)&#xa;            Hmag = H_mag(bb, aa, z, top, H_min=bottom, log=True)&#xa;&#xa;        else:&#xa;            bottom = max(self.zmin, H_min)&#xa;            top = self.zmax&#xa;        #   top = zmax_rel * H_max # calculate display top from max. of H(f)&#xa;            H_UC = H_mag(bb, aa, self.xy_UC, top, H_min=bottom)&#xa;            Hmag = H_mag(bb, aa, z, top, H_min=bottom)&#xa;&#xa;            zlevel = zlevel_rel * top # height of displayed zero position&#xa;&#xa;            if self.cmbMode3D.currentText() == 'None': # ""Poleposition"" for H(f) plot only&#xa;                plevel_top = H_max * 0.3 # plevel = H_max * 0.1 / zlevel = 0.1&#xa;                plevel_btm = bottom&#xa;            else:&#xa;                plevel_top = plevel_rel * top # height of displayed pole position&#xa;                plevel_btm = top&#xa;&#xa;        #===============================================================&#xa;        ## plot unit circle&#xa;        #===============================================================&#xa;        if self.chkUC.isChecked():&#xa;        # Plot unit circle and marker at (1,0):&#xa;            self.ax3d.plot(self.xy_UC.real, self.xy_UC.imag,&#xa;                           ones(len(self.xy_UC)) * bottom, lw=2, color='k')&#xa;            self.ax3d.plot([0.97, 1.03], [0, 0], [bottom, bottom], lw=2, color='k')&#xa;&#xa;        #===============================================================&#xa;        ## plot ||H(f)| along unit circle as 3D-lineplot&#xa;        #===============================================================&#xa;        if self.chkHf.isChecked():&#xa;            self.ax3d.plot(self.xy_UC.real, self.xy_UC.imag, H_UC)&#xa;            # draw once more as dashed white line to improve visibility&#xa;            self.ax3d.plot(self.xy_UC.real, self.xy_UC.imag, H_UC, 'w--')&#xa;&#xa;&#xa;            NL = 10 - self.diaHatch.value() # plot line every NL points on the UC&#xa;            if NL < 10:&#xa;                for k in range(len(self.xy_UC[::NL])):&#xa;                    self.ax3d.plot([self.xy_UC.real[::NL][k], self.xy_UC.real[::NL][k]],&#xa;                        [self.xy_UC.imag[::NL][k], self.xy_UC.imag[::NL][k]],&#xa;                        [np.ones(len(self.xy_UC[::NL]))[k]*bottom, H_UC[::NL][k]],&#xa;                         linewidth=1, color=(0.5, 0.5, 0.5))&#xa;        #===============================================================&#xa;        ## plot Poles and Zeros&#xa;        #===============================================================&#xa;        if self.chkPZ.isChecked():&#xa;&#xa;            PN_SIZE = 8 # size of P/N symbols&#xa;&#xa;            # Plot zero markers at |H(z_i)| = zlevel with ""stems"":&#xa;            self.ax3d.plot(zz.real, zz.imag, ones(len(zz)) * zlevel, 'o',&#xa;               markersize=PN_SIZE, markeredgecolor='blue', markeredgewidth=2.0,&#xa;                markerfacecolor='none')&#xa;            for k in range(len(zz)): # plot zero ""stems""&#xa;                self.ax3d.plot([zz[k].real, zz[k].real], [zz[k].imag, zz[k].imag],&#xa;                            [bottom, zlevel], linewidth=1, color='b')&#xa;&#xa;            # Plot the poles at |H(z_p)| = plevel with ""stems"":&#xa;            self.ax3d.plot(np.real(pp), np.imag(pp), plevel_top,&#xa;              'x', markersize=PN_SIZE, markeredgewidth=2.0, markeredgecolor='red')&#xa;            for k in range(len(pp)): # plot pole ""stems""&#xa;                self.ax3d.plot([pp[k].real, pp[k].real], [pp[k].imag, pp[k].imag],&#xa;                            [plevel_btm, plevel_top], linewidth=1, color='r')&#xa;&#xa;        #===============================================================&#xa;        ## 3D-Plots of |H(z)| clipped between |H(z)| = top&#xa;        #===============================================================&#xa;        #&#xa;        ## Mesh plot&#xa;        if self.cmbMode3D.currentText() == 'Mesh':&#xa;        #    fig_mlab = mlab.figure(fgcolor=(0., 0., 0.), bgcolor=(1, 1, 1))&#xa;        #    self.ax3d.set_zlim(0,2)&#xa;            self.ax3d.plot_wireframe(x, y, Hmag, rstride=5,&#xa;                    cstride=OPT_3D_MSTRIDE, linewidth=1, color='gray')&#xa;&#xa;        #---------------------------------------------------------------&#xa;        ## 3D-surface plot;&#xa;        elif self.cmbMode3D.currentText() == 'Surf':&#xa;            s = self.ax3d.plot_surface(x, y, Hmag,&#xa;                    alpha=OPT_3D_ALPHA, rstride=1, cstride=1, cmap=cmap,&#xa;                    linewidth=0, antialiased=False, shade=True) # facecolors= cmap ??&#xa;            s.set_edgecolor('gray')&#xa;&#xa;        #---------------------------------------------------------------&#xa;        ## 3D-Contour plot&#xa;        elif self.cmbMode3D.currentText() == 'Contour':&#xa;            s = self.ax3d.contourf3D(x, y, Hmag, 20, alpha=alpha,&#xa;                    rstride=OPT_3D_MSTRIDE, cstride=OPT_3D_MSTRIDE, cmap=cmap)&#xa;&#xa;        #---------------------------------------------------------------&#xa;        ## 2D-Contour plot&#xa;        # TODO: 2D contour plots do not plot correctly together with 3D plots in&#xa;        #       current matplotlib 1.4.3 -> disable them for now&#xa;        # TODO: zdir = x / y delivers unexpected results -> rather plot max(H)&#xa;        #       along the other axis?&#xa;        # TODO: colormap is created depending on the zdir = 'z' contour plot&#xa;        #       -> set limits of (all) other plots manually?&#xa;        if self.chkContour2D.isChecked():&#xa;#            self.ax3d.contourf(x, y, Hmag, 20, zdir='x', offset=xmin,&#xa;#                         cmap=cmap, alpha = alpha)#, vmin = bottom)#, vmax = top, vmin = bottom)&#xa;#            self.ax3d.contourf(x, y, Hmag, 20, zdir='y', offset=ymax,&#xa;#                         cmap=cmap, alpha = alpha)#, vmin = bottom)#, vmax = top, vmin = bottom)&#xa;            s = self.ax3d.contourf(x, y, Hmag, 20, zdir='z',&#xa;                               offset=bottom - (top - bottom) * 0.05,&#xa;                                cmap=cmap, alpha=alpha)&#xa;&#xa;        if self.cmbMode3D.currentText() in {'Contour', 'Surf'}\&#xa;                    or self.chkContour2D.isChecked():&#xa;                        if self.chkColBar.isChecked():&#xa;                            self.colb = self.mplwidget.fig.colorbar(s,&#xa;                                ax=self.ax3d, shrink=0.8, aspect=20,&#xa;                                pad=0.02, fraction=0.08)&#xa;&#xa;&#xa;        self.ax3d.set_xlim3d(xmin, xmax)&#xa;        self.ax3d.set_ylim3d(ymin, ymax)&#xa;        self.ax3d.set_zlim3d(bottom, top)&#xa;&#xa;        self.ax3d.set_xlabel('Re')#(fb.fil[0]['plt_fLabel'])&#xa;        self.ax3d.set_ylabel('Im') #(r'$ \tau_g(\mathrm{e}^{\mathrm{j} \Omega}) / T_S \; \rightarrow $')&#xa;#        self.ax3d.set_zlabel(r'$|H(z)|\; \rightarrow $')&#xa;        self.ax3d.set_title(r'3D-Plot of $|H(\mathrm{e}^{\mathrm{j} \Omega})|$ and $|H(z)|$')&#xa;&#xa;        self.mplwidget.redraw()&#xa;&#xa;#------------------------------------------------------------------------------&#xa;&#xa;def main():&#xa;    import sys&#xa;    app = QtGui.QApplication(sys.argv)&#xa;    form = Plot3D()&#xa;    form.show()&#xa;    app.exec_()&#xa;&#xa;if __name__ == ""__main__"":&#xa;    main()&#xa;"
5209087|"from __future__ import absolute_import, print_function&#xa;&#xa;from collections import OrderedDict&#xa;from six.moves.urllib import request as urllib2&#xa;import io&#xa;import pandas as pd&#xa;from .. import charts&#xa;from . import help_messages as hm&#xa;&#xa;&#xa;def keep_source_input_sync(filepath, callback, start=0):&#xa;    """""" Monitor file at filepath checking for new lines (similar to&#xa;    tail -f) and calls callback on every new line found.&#xa;&#xa;    Args:&#xa;        filepath (str): path to the series data file (&#xa;            i.e.: /source/to/my/data.csv)&#xa;        callback (callable): function to be called with the a DataFrame&#xa;            created from the new lines found from file at filepath&#xa;            starting byte start&#xa;        start (int): specifies where to start reading from the file at&#xa;            filepath.&#xa;            Default: 0&#xa;&#xa;    Returns:&#xa;        DataFrame created from data read from filepath&#xa;    """"""&#xa;    if filepath is None:&#xa;        msg = ""No Input! Please specify --source_filename or --buffer t""&#xa;        raise IOError(msg)&#xa;&#xa;    if filepath.lower().startswith('http'):&#xa;        # Create a request for the given URL.&#xa;&#xa;        while True:&#xa;            request = urllib2.Request(filepath)&#xa;            data = get_data_from_url(request, start)&#xa;&#xa;            f = io.BytesIO(data)&#xa;            f.seek(start)&#xa;            line = f.readline()     # See note below&#xa;&#xa;            if not line:&#xa;                continue   # No data, try again&#xa;&#xa;            callback(line)&#xa;            start = len(data)&#xa;    else:&#xa;        f = open(filepath, 'r')&#xa;        f.seek(start)&#xa;        while True:&#xa;            line = f.readline()     # See note below&#xa;            if not line:&#xa;                continue   # No data, try again&#xa;            callback(line)&#xa;&#xa;    source = pd.read_csv(filepath)&#xa;    return source&#xa;&#xa;# Try to get the response. This will raise a urllib2.URLError if there is a&#xa;# problem (e.g., invalid URL).&#xa;# Reference:&#xa;# - http://stackoverflow.com/questions/5209087/python-seek-in-http-response-stream&#xa;# - http://stackoverflow.com/questions/1971240/python-seek-on-remote-file-using-http&#xa;def get_data_from_url(request, start=0, length=0):&#xa;    """""" Read from request after adding headers to retrieve data from byte&#xa;    specified in start.&#xa;&#xa;    request (urllib2.Request): request object related to the data to read&#xa;    start (int, optional): byte to start reading from.&#xa;        Default: 0&#xa;    length: length of the data range to read from start. If 0 it reads&#xa;        until the end of the stream.&#xa;        Default: 0&#xa;&#xa;    Returns:&#xa;        String read from request&#xa;    """"""&#xa;    # Add the header to specify the range to download.&#xa;    if start and length:&#xa;        request.add_header(""Range"", ""bytes=%d-%d"" % (start, start + length - 1))&#xa;    elif start:&#xa;        request.add_header(""Range"", ""bytes=%s-"" % start)&#xa;&#xa;    response = urllib2.urlopen(request)&#xa;    # If a content-range header is present, partial retrieval worked.&#xa;    if ""content-range"" in response.headers:&#xa;        print(""Partial retrieval successful."")&#xa;&#xa;        # The header contains the string 'bytes', followed by a space, then the&#xa;        # range in the format 'start-end', followed by a slash and then the total&#xa;        # size of the page (or an asterix if the total size is unknown). Lets get&#xa;        # the range and total size from this.&#xa;        _range, total = response.headers['content-range'].split(' ')[-1].split('/')&#xa;        # Print a message giving the range information.&#xa;        if total == '*':&#xa;            print(""Bytes %s of an unknown total were retrieved."" % _range)&#xa;        else:&#xa;            print(""Bytes %s of a total of %s were retrieved."" % (_range, total))&#xa;&#xa;    # # No header, so partial retrieval was unsuccessful.&#xa;    # else:&#xa;    #     print ""Unable to use partial retrieval.""&#xa;    data = response.read()&#xa;&#xa;    return data&#xa;&#xa;def parse_output_config(output):&#xa;    """"""Parse the output specification string and return the related chart&#xa;    output attribute.&#xa;&#xa;    Attr:&#xa;        output (str): String with the syntax convention specified for the&#xa;            cli output option is as follows: <output_type>://<type_arg>&#xa;            Valid values:&#xa;                output_type: file or server&#xa;                type_arg:&#xa;                    file_path if output_type is file&#xa;                    serve path if output_type is server&#xa;&#xa;    Returns:&#xa;        dictionary containing the output arguments to pass to a chart object&#xa;    """"""&#xa;    output_type, output_options = output.split('://')&#xa;&#xa;    if output_type == 'file':&#xa;        return {'filename': output_options}&#xa;&#xa;    elif output_type == 'server':&#xa;        # TODO: check if server configuration is as flexible as with plotting&#xa;        #       interface and add support for url/name if so.&#xa;        out_opt = output_options.split(""@"")&#xa;        attrnames = ['server', 'url', 'name']&#xa;&#xa;        # unpack server output parametrs in order to pass them to the plot&#xa;        # creation function&#xa;        kws = dict((attrn, val) for attrn, val in zip( attrnames, out_opt))&#xa;        return {'server': kws['server']}&#xa;&#xa;    else:&#xa;        msg = ""Unknown output type %s found. Please use: file|server""&#xa;        print (msg % output_type)&#xa;        return {}&#xa;&#xa;&#xa;def get_chart_params(title, output, show_legend=False):&#xa;    """"""Parse output type and output options and return related chart&#xa;    parameters. For example: returns filename if output_type is file&#xa;    or server it output_type is server&#xa;&#xa;    Args:&#xa;        title (str): the title of your plot.&#xa;        output (str): selected output. Follows the following convention:&#xa;            <output_type>://<type_arg> where output_type can be&#xa;            `file` (in that case type_arg specifies the file path) or&#xa;            `server` (in that case type_arg specify the server name).&#xa;&#xa;&#xa;    Returns:&#xa;        dictionary containing the arguments to pass to a chart object&#xa;        related to title and output options&#xa;    """"""&#xa;    params = {'title': title, 'legend': show_legend}&#xa;    output_params = parse_output_config(output)&#xa;    if output_params:&#xa;        params.update(output_params)&#xa;&#xa;    return params&#xa;&#xa;&#xa;def get_data_series(series, source, indexes):&#xa;    """"""Generate an OrderedDict from the source series excluding index&#xa;    and all series not specified in series.&#xa;&#xa;    Args:&#xa;        series (list(str)): list of strings specifying the names of the&#xa;            series to keep from source&#xa;        source (DataFrame): pandas DataFrame with the data series to be&#xa;            plotted&#xa;        indexes (lst(str)): name of the series of source to be used as index.&#xa;&#xa;    Returns:&#xa;        OrderedDict with the data series from source&#xa;    """"""&#xa;    series = define_series(series, source, indexes)&#xa;    # generate charts data&#xa;    data_series = OrderedDict()&#xa;    for i, colname in enumerate(series+indexes):&#xa;        try:&#xa;            data_series[colname] = source[colname]&#xa;        except KeyError:&#xa;            raise KeyError(hm.ERR_MSG_SERIES_NOT_FOUND % (colname, source.keys()))&#xa;&#xa;    return data_series&#xa;&#xa;&#xa;def define_series(series, source, indexes):&#xa;    """"""If series is empty returns source_columns excluding the column&#xa;    where column == index. Otherwise returns the series.split(',')&#xa;&#xa;    Args:&#xa;        series (str): string that contains the names of the&#xa;            series to keep from source, separated by `,`&#xa;        source (DataFrame): pandas DataFrame with the data series to be&#xa;            plotted&#xa;        indexes (lst(str)): name of the series of source to be used as index.&#xa;&#xa;    Returns:&#xa;        list of the names (as str) of the series except index&#xa;    """"""&#xa;    if not series:&#xa;        return [c for c in source.columns if c not in indexes]&#xa;    else:&#xa;        return series.split(',')&#xa;&#xa;&#xa;def get_charts_mapping():&#xa;    """"""Return a dict with chart classes names (lower case) as keys and&#xa;    their related class as values.&#xa;&#xa;    Returns:&#xa;        dict mapping chart classes names to chart classes&#xa;    """"""&#xa;    mapping = {}&#xa;    for (clsname, cls) in charts.__dict__.items():&#xa;        try:&#xa;            # TODO: We may need to restore the objects filtering&#xa;            # when charts creators (or builders registration) is added&#xa;            # to the charts API&#xa;            mapping[clsname.lower()] = cls&#xa;        except TypeError:&#xa;            pass&#xa;    return mapping&#xa;"
5347065|"#!/bin/env python&#xa;&#xa;""""""&#xa;Copyright (c) 2016 Christopher M. Bruns&#xa;&#xa;Permission is hereby granted, free of charge, to any person obtaining a copy&#xa;of this software and associated documentation files (the ""Software""), to deal&#xa;in the Software without restriction, including without limitation the rights&#xa;to use, copy, modify, merge, publish, distribute, sublicense, and/or sell&#xa;copies of the Software, and to permit persons to whom the Software is&#xa;furnished to do so, subject to the following conditions:&#xa;&#xa;The above copyright notice and this permission notice shall be included in all&#xa;copies or substantial portions of the Software.&#xa;&#xa;THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR&#xa;IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,&#xa;FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE&#xa;AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER&#xa;LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,&#xa;OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE&#xa;SOFTWARE.&#xa;""""""&#xa;&#xa;# built-in python modules&#xa;import sys&#xa;import io&#xa;import time&#xa;from glob import glob&#xa;import os&#xa;import math&#xa;import datetime&#xa;&#xa;# third party python modules&#xa;from OpenGL import GL&#xa;from tifffile import TiffFile&#xa;import tifffile&#xa;import numpy&#xa;&#xa;# local python modules&#xa;import ktx&#xa;from ktx.util import create_mipmaps, mipmap_dimension, interleave_channel_arrays, downsample_array_xy&#xa;&#xa;""""""&#xa;TODO: For converting rendered octree blocks, include the following precomputed:&#xa;  * all mipmap levels&#xa;  * optional intensity downsampling, with affine reestimation parameters&#xa;  * optional spatial downsampling&#xa;  * other metadata:&#xa;      * distance units e.g. ""micrometers"", for all the transforms below&#xa;      * transform from texture coordinates to Cartesian reference space&#xa;      * Optional transform from texture coordinates to Allen reference space&#xa;      * center xyz in reference space&#xa;      * bounding radius&#xa;      * nominal spatial resolution range at this level&#xa;      * specimen ID, e.g. ""2015-06-19-johan-full""&#xa;      * parent tile/block ID, e.g. ""/1/5/4/8/default.[0,1].tif""&#xa;      * relation to parent tile/block, e.g. ""downsampled 2X in XY; rescaled intensity to 8 bits; sub-block (1,2) of (6,6)&#xa;      * multiscale level ID (int)&#xa;          * of total multiscale level count (int)&#xa;      * per channel&#xa;          * affine parameters to approximate background level of first channel, for dynamic unmixing&#xa;          * min, max, average, median intensities&#xa;          * proportion of zero/NaN in this block&#xa;      * creation time&#xa;      * name of program used to create this block&#xa;      * version of program used to create this block&#xa;      * texture coordinate bounds for display (because there might be padding...)&#xa;""""""&#xa;&#xa;def test_downsample_xy(filter_='arthur'):&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data = tif.asarray()&#xa;    t0 = time.time()&#xa;    downsampled = downsample_array_xy(data, filter_=filter_)&#xa;    t1 = time.time()&#xa;    print (t1-t0, "" seconds elapsed time to downsample volume in XY"")&#xa;    tifffile.imsave(""downsampled.tif"", downsampled)&#xa;    t2 = time.time()&#xa;    print (t2-t1, "" seconds elapsed time to save downsampled volume in tiff format to disk"")&#xa;def test_interleave_channel_arrays():&#xa;    a = numpy.array( (1,2,3,4,5,), dtype='uint16' )&#xa;    b = numpy.array( (6,7,8,9,10,), dtype='uint16' )&#xa;    # print (a)&#xa;    # print (b)&#xa;    c = interleave_channel_arrays( (a,b,) )&#xa;    # print (c)&#xa;    assert numpy.array_equal(c, numpy.array(&#xa;        [[ 1,  6],&#xa;         [ 2,  7],&#xa;         [ 3,  8],&#xa;         [ 4,  9],&#xa;         [ 5, 10]]))&#xa;&#xa;def test_create_mipmaps(filter_='arthur'):&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data = tif.asarray()&#xa;    data = downsample_array_xy(data, filter_=filter_)&#xa;    t0 = time.time()&#xa;    mipmaps = create_mipmaps(data, filter_=filter_)&#xa;    t1 = time.time()&#xa;    print (t1-t0, "" seconds elapsed time to compute mipmaps"")&#xa;    for i in range(len(mipmaps)):&#xa;        tifffile.imsave(""test_mipmap%02d.tif"" % i, mipmaps[i])&#xa;    t2 = time.time()&#xa;    print (t2-t1, "" seconds elapsed time to save mipmaps in tiff format to disk"")&#xa;&#xa;def test_create_tiff():&#xa;    # https://pypi.python.org/pypi/tifffile&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data1 = tif.asarray()&#xa;        # tifffile.imsave('test1.tif', data1)&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.1.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data2 = tif.asarray()&#xa;        # tifffile.imsave('test2.tif', data2)&#xa;    # TODO unmixing test&#xa;    # compute channel 1/2 unmixing parameters&#xa;    # For lower end of mapping, just use lower quartile intensity (non-zero!)&#xa;    lower1 = numpy.percentile(data1[data1 != 0], 40)&#xa;    lower2 = numpy.percentile(data2[data2 != 0], 40)&#xa;    print (lower1, lower2)&#xa;    # For upper end of mapping, use voxels that are bright in BOTH channels&#xa;    m_a = numpy.median(data1[data1 != 0])&#xa;    m_b = numpy.median(data2[data2 != 0])&#xa;    s_a = numpy.std(data1[data1 != 0])&#xa;    s_b = numpy.std(data2[data2 != 0])&#xa;    upper1 = numpy.median(data1[(data1 > m_a + 2*s_a) & (data2 > m_b + 2*s_b)])&#xa;    upper2 = numpy.median(data2[(data1 > m_a + 2*s_a) & (data2 > m_b + 2*s_b)])&#xa;    print (upper1, upper2)&#xa;    # transform data2 to match data1&#xa;    scale = (upper1 - lower1) / (upper2 - lower2)&#xa;    offset = upper1 - upper2 * scale&#xa;    scale2 = (upper2 - lower2) / (upper1 - lower1)&#xa;    offset2 = upper2 - upper1 * scale2&#xa;    data2b = numpy.array(data2, dtype='float32')&#xa;    data2b *= scale&#xa;    data2b += offset&#xa;    data2b[data2 == 0] = 0&#xa;    data2b[data2b <= 0] = 0&#xa;    data2b = numpy.array(data2b, dtype=data1.dtype)&#xa;    # TODO ktx to tiff&#xa;    # Needs 1 or 3 channels for Fiji to load it OK&#xa;    # data3 = numpy.zeros_like(data1)&#xa;    tissue = numpy.minimum(data1, data2)&#xa;    tissue_base = numpy.percentile(tissue[tissue != 0], 4) - 1&#xa;    tissue = numpy.array(tissue, dtype='float32') # so we can handle negative numbers&#xa;    print (tissue_base)&#xa;    tissue -= tissue_base&#xa;    tissue[tissue <= 0] = 0&#xa;    tissue = numpy.array(tissue, dtype=data1.dtype)&#xa;    #&#xa;    unmixed1 = numpy.array(data1, dtype='float32')&#xa;    unmixed1 -= data2b&#xa;    # unmixed1 += s_a # tweak background up to show more stuff&#xa;    unmixed1[unmixed1 <= 0] = 0&#xa;    unmixed1 = numpy.array(unmixed1, dtype=data1.dtype)&#xa;    #&#xa;    data1b = numpy.array(data1, dtype='float32')&#xa;    data1b *= scale2&#xa;    data1b += offset2&#xa;    data1b[data1 == 0] = 0&#xa;    data1b[data1b <= 0] = 0&#xa;    data1b = numpy.array(data1b, dtype=data1.dtype)&#xa;    unmixed2 = numpy.array(data2, dtype='float32')&#xa;    unmixed2 -= data1b&#xa;    # unmixed2 += s_b # tweak background up to show more stuff&#xa;    unmixed2[unmixed2 <= 0] = 0&#xa;    unmixed2 = numpy.array(unmixed2, dtype=data1.dtype)&#xa;    #&#xa;    print (tissue.shape)&#xa;    data123 = interleave_channel_arrays( (data2, data1b, unmixed2) )&#xa;    # print (data123.shape)&#xa;    tifffile.imsave('test123.tif', data123)&#xa;&#xa;def ktx_from_mouselight_octree_folder(input_folder_name,&#xa;                              output_folder_name,&#xa;                              num_levels=1, # '0' means 'all'&#xa;                              mipmap_filter='max', &#xa;                              downsample_xy=True, &#xa;                              downsample_intensity=False):&#xa;    # Parse geometry data from top level transform.txt&#xa;    metadata = dict()&#xa;    with io.open(os.path.join(input_folder_name, ""transform.txt""), 'r') as transform_file:&#xa;        for line in transform_file:&#xa;            fields = line.split("": "")&#xa;            if len(fields) != 2:&#xa;                continue&#xa;            metadata[fields[0].strip()] = fields[1].strip()&#xa;    # Get original tiff file dimensions, to help compute geometry correctly&#xa;    with tifffile.TiffFile(os.path.join(input_folder_name, ""default.0.tif"")) as tif:&#xa;        original_tiff_dimensions = tif.asarray().shape&#xa;    # for k, v in metadata.items():&#xa;    #     print (k, v)&#xa;    if num_levels == 0:&#xa;        num_levels = int(metadata[""nl""])&#xa;    assert num_levels > 0&#xa;    folder = input_folder_name&#xa;    for level in range(num_levels):&#xa;        tiffs = glob(os.path.join(folder, ""default.*.tif""))&#xa;        ktx_obj = ktx_from_tiff_channel_files(tiffs, mipmap_filter, downsample_xy, downsample_intensity)&#xa;        # Populate custom block metadata&#xa;        kh = ktx_obj.header&#xa;        # kv = ktx_obj.header.key_value_metadata&#xa;        # kv[b'distance_units'] = b'micrometers'&#xa;        kh[""distance_units""] = ""micrometers""&#xa;        umFromNm = 1.0/1000.0&#xa;        # Origin of volume (corner of corner voxel)&#xa;        ox = umFromNm*float(metadata['ox'])&#xa;        oy = umFromNm*float(metadata['oy'])&#xa;        oz = umFromNm*float(metadata['oz'])&#xa;        # Size of entire volume&#xa;        # Use original dimensions, to account for downsampling...&#xa;        sx = umFromNm * original_tiff_dimensions[2] * float(metadata['sx']) &#xa;        sy = umFromNm * original_tiff_dimensions[1] * float(metadata['sy'])&#xa;        sz = umFromNm * original_tiff_dimensions[0] * float(metadata['sz'])&#xa;        xform = numpy.array([&#xa;                [sx, 0, 0, ox],&#xa;                [0, sy, 0, oy],&#xa;                [0, 0, sz, oz],&#xa;                [0, 0, 0, 1],], dtype='float32')&#xa;        # print(xform)&#xa;        kh[""xyz_from_texcoord_xform""] = xform&#xa;        # print (kh[""xyz_from_texcoord_xform""])&#xa;        #&#xa;        center = numpy.array( (ox + 0.5*sx, oy + 0.5*sy, oz + 0.5*sz,), )&#xa;        radius = math.sqrt(sx*sx + sy*sy + sz*sz)/16.0&#xa;        kh['bounding_sphere_center'] = center&#xa;        kh['bounding_sphere_radius'] = radius&#xa;        # Nominal resolution&#xa;        resX = sx / ktx_obj.header.pixel_width&#xa;        resY = sy / ktx_obj.header.pixel_height&#xa;        resZ = sz / ktx_obj.header.pixel_depth&#xa;        rms = math.sqrt(numpy.mean(numpy.square([resX, resY, resZ],)))&#xa;        kh['nominal_resolution'] = rms&#xa;        # print (kh['nominal_resolution'])&#xa;        # Specimen ID&#xa;        kh['specimen_id'] = os.path.split(input_folder_name)[-1]&#xa;        # print (kh['specimen_id'])&#xa;        # TODO: octree block ID&#xa;        # Relation to parent tile/block&#xa;        kh['mipmap_filter'] = mipmap_filter&#xa;        relations = list()&#xa;        if downsample_xy:&#xa;            relations.append(""downsampled 2X in X & Y"")&#xa;        if downsample_intensity:&#xa;            relations.append(""rescaled intensity to 8 bits"")&#xa;        if len(relations) == 0:&#xa;            relations.append(""unchanged"")&#xa;        kh['relation_to_parent'] = "";"".join(relations)&#xa;        # print (kh['relation_to_parent'])&#xa;        kh['multiscale_level_id'] = level&#xa;        kh['multiscale_total_levels'] = metadata['nl']&#xa;        # TODO: Per channel statistics&#xa;        kh['ktx_file_creation_date'] = datetime.datetime.now()&#xa;        # print (kh['ktx_file_creation_date'])&#xa;        import __main__ #@UnresolvedImport&#xa;        kh['ktx_file_creation_program'] = __main__.__file__&#xa;        # print (kh['ktx_file_creation_program'])&#xa;        kh['pyktx_version'] = ktx.__version__&#xa;        # print (kh['ktx_package_version'])&#xa;        # TODO: Texture coordinate bounds for display&#xa;        # Write LZ4-compressed KTX file&#xa;        t1 = time.time()&#xa;        with io.open('test.ktx', 'wb') as ktx_out:&#xa;            temp = io.BytesIO()&#xa;            ktx_obj.write_stream(temp)&#xa;            ktx_out.write(temp.getvalue())        # Create tiff file for sanity check testing&#xa;        t2 = time.time()&#xa;        print (""Creating uncompressed ktx file took %.3f seconds"" % (t2 - t1))&#xa;        # TODO: create tiffFromKtx.py as a separate tool&#xa;        # tifffile.imsave('test.tif', ktx_obj.asarray(0))&#xa;&#xa;def ktx_from_tiff_channel_files(channel_tiff_names, mipmap_filter='max', downsample_xy=True, downsample_intensity=False):&#xa;    """"""&#xa;    Load multiple single-channel tiff files, and create a multichannel Ktx object.&#xa;    """"""&#xa;    t0 = time.time()&#xa;    channels = list()&#xa;    for fname in channel_tiff_names:&#xa;        with TiffFile(fname) as tif:&#xa;            arr = tif.asarray()&#xa;            if downsample_xy:&#xa;                arr = downsample_array_xy(arr, mipmap_filter)&#xa;            channels.append(arr)&#xa;    t1 = time.time()&#xa;    print (""loading tiff files took %.3f seconds"" % (t1 - t0))&#xa;    if downsample_intensity:&#xa;        new_channels = list()&#xa;        channel_transforms = list()&#xa;        for channel in channels:&#xa;            min_ = numpy.min(channel[channel != 0])&#xa;            max_ = numpy.max(channel[channel != 0])&#xa;            scale = 1.0&#xa;            offset = min_ - 1&#xa;            if max_ - min_ > 255: # need a lossy contraction of intensities&#xa;                # Discard dimmest 2% of intensities&#xa;                min_ = numpy.percentile(channel[channel != 0], 2)&#xa;                median = numpy.median(channel[channel != 0])&#xa;                max_ = numpy.max(channel[channel != 0])&#xa;                # Discard intensities above 90% of max&#xa;                max_ = median + 0.90 * (max_ - median)&#xa;                print(min_, median, max_)&#xa;                scale = (max_ - min_) / 255.0&#xa;                offset = min_ - 1&#xa;            if channel.dtype.itemsize == 2:&#xa;                c = numpy.array(channel, dtype='float32')&#xa;                c -= offset&#xa;                c /= scale&#xa;                c[c<0] = 0&#xa;                if channel.dtype == numpy.uint16:&#xa;                    dt = numpy.uint8&#xa;                else:&#xa;                    raise # TODO: more cases&#xa;                c = numpy.array(c, dtype=dt)&#xa;                new_channels.append(c)&#xa;                channel_transforms.append( tuple([scale, offset]) )&#xa;            else:&#xa;                raise # TODO:&#xa;        channels = new_channels&#xa;    combined = interleave_channel_arrays(channels)&#xa;    ktx_obj = ktx.Ktx.from_ndarray(combined, mipmap_filter=mipmap_filter)&#xa;    # Include metadata for reconstructing original intensities&#xa;    if downsample_intensity:&#xa;        c = 0&#xa;        for ct in channel_transforms:&#xa;            ktx_obj.header['intensity_transform_%d'%c] = ct&#xa;            c += 1&#xa;    t2 = time.time()&#xa;    print (""creating swizzled mipmapped ktx data took %.3f seconds"" % (t2 - t1))    &#xa;    return ktx_obj&#xa;&#xa;def main():&#xa;    test_mipmap_dimension()&#xa;    test_downsample_xy()&#xa;    return&#xa;    ""Interleave multiple single channel tiff files into a multi-channel KTX file""&#xa;    arrays = list()&#xa;    for arg in sys.argv[1:]:&#xa;        for fname in glob(arg):&#xa;            print (fname)&#xa;            with TiffFile(fname) as tif:&#xa;                print (len(tif.pages))&#xa;                data = tif.asarray()&#xa;                print (data.shape)&#xa;                arrays.append(data)&#xa;                # print (numpy.percentile(data[data != 0], [25, 99]))&#xa;    a = arrays[0]&#xa;    b = arrays[1]&#xa;    # TODO: generate linear unmixing parameters appropriate for both dim and bright sections&#xa;    # Use only non-zero locations for basic statistics&#xa;    m_a = numpy.median(a[a != 0])&#xa;    s_a = numpy.std(a[a != 0])&#xa;    m_b = numpy.median(b[b != 0])&#xa;    s_b = numpy.std(b[b != 0])&#xa;    # Statistic for locations where both channels are bright at the same location&#xa;    h_a = numpy.median(a[(a > m_a + 2*s_a) & (b > m_b + 2*s_b)])&#xa;    print (m_a, s_a, h_a)&#xa;    h_b = numpy.median(b[(a > m_a + 2*s_a) & (b > m_b + 2*s_b)])&#xa;    print (m_b, s_b, h_b)&#xa;    # Interleave two channels&#xa;    # http://stackoverflow.com/questions/5347065/interweaving-two-numpy-arrays&#xa;    c = numpy.empty( shape=(a.shape[0], a.shape[1], a.shape[2], len(arrays)), dtype=a.dtype)&#xa;    for i in range(len(arrays)):&#xa;        c[:,:,:,i] = arrays[i]&#xa;    print (c.shape)&#xa;    dt = c.dtype&#xa;    ktx_obj = ktx.Ktx()&#xa;    kh = ktx_obj.header&#xa;    if dt.byteorder == '<':&#xa;        kh.little_endian = True&#xa;    elif dt.byteorder == '=':&#xa;        kh.little_endian = sys.byteorder == 'little'&#xa;    else:&#xa;        raise # TODO&#xa;    print (dt.byteorder)&#xa;    print (kh.little_endian)&#xa;    if dt.kind == 'u':&#xa;        if dt.itemsize == 2:&#xa;            kh.gl_type = GL.GL_UNSIGNED_SHORT&#xa;        elif dt.itemsize == 1:&#xa;            kh.gl_type = GL.GL_UNSIGNED_BYTE&#xa;        else:&#xa;            raise # TODO&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    kh.gl_type_size = dt.itemsize&#xa;    #&#xa;    if c.shape[3] == 1:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RED&#xa;    elif c.shape[3] == 2:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RG&#xa;    elif c.shape[3] == 3:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RGB&#xa;    elif c.shape[3] == 4:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RGBA&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    if kh.gl_base_internal_format == GL.GL_RG and kh.gl_type == GL.GL_UNSIGNED_SHORT:&#xa;        kh.gl_internal_format = GL.GL_RG16UI&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    kh.pixel_width = c.shape[2]&#xa;    kh.pixel_height = c.shape[1]&#xa;    kh.pixel_depth = c.shape[0]&#xa;    kh.number_of_array_elements = 0&#xa;    kh.number_of_faces = 0&#xa;    kh.number_of_mipmap_levels = 1 # TODO zero for autogenerate?&#xa;    # TODO - key/value pairs for provenance&#xa;    ktx_obj.image_data.mipmaps.clear()&#xa;    ktx_obj.image_data.mipmaps.append(c.tostring())&#xa;    &#xa;if __name__ == ""__main__"":&#xa;    """"""&#xa;    ktx_from_tiff_channel_files(&#xa;            (""E:/brunsc/projects/ktxtiff/octree_tip/default.1.tif"",&#xa;            ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif"",&#xa;            ), )&#xa;    """"""&#xa;    """"""&#xa;    ktx_from_mouselight_octree_folder(&#xa;            input_folder_name='//fxt/nobackup2/mouselight/2015-06-19-johan-full', &#xa;            output_folder_name='',&#xa;            mipmap_filter='arthur', &#xa;            downsample_xy=True,&#xa;            downsample_intensity=True)&#xa;    """"""&#xa;    test_create_mipmaps('arthur')&#xa;"
1550560|"import datetime&#xa;import struct&#xa;import logging&#xa;import numpy as np&#xa;import os&#xa;import time&#xa;import re&#xa;import pytz&#xa;import scipy.spatial.distance as distance&#xa;from tzlocal import get_localzone&#xa;import warnings&#xa;&#xa;&#xa;def get_filename_meta_data(fn):&#xa;    """"""&#xa;    This function retrieves the meta information from a filename given the typical formatting in the Time Travel Task.&#xa;&#xa;    :param fn: a filename to parse for meta-data&#xa;    :return: a dictionary containing keys 'subID', 'trial', 'phase', 'inverse' and 'datetime' of types string&#xa;             with the exception of datetime which is of type datetime&#xa;    """"""&#xa;    parts = fn.split('_')&#xa;    dt = datetime.datetime.strptime(parts[4] + '_' + parts[5].split('.')[0], '%Y-%m-%d_%H-%M-%S')&#xa;    return {""subID"": parts[0], ""trial"": parts[1], ""phase"": parts[2], ""inverse"": parts[3], ""datetime"": dt}&#xa;&#xa;&#xa;def phase_num_to_str(phase):&#xa;    """"""&#xa;    This function converts a phase integer into a nameable phase string.&#xa;&#xa;    :param phase: an integer which represents the phase type to be converted to a string&#xa;    :return: 'VR Practice', 'VR Study', 'VR Test', 'VE Practice', 'VE Study', 'VE Test', '2D Practice', '2D Study',&#xa;             '2D Test', in order, from 0 to 8.&#xa;    """"""&#xa;    names = ['VR Practice', 'VR Study', 'VR Test', 'VE Practice', 'VE Study', 'VE Test',&#xa;             '2D Practice', '2D Study', '2D Test']&#xa;    lookup = phase&#xa;    # noinspection PyCompatibility&#xa;    if isinstance(lookup, str):&#xa;        lookup = int(lookup)&#xa;    return names[lookup]&#xa;&#xa;&#xa;def decode_7bit_int_length(fp):&#xa;    """"""&#xa;    From:&#xa;    http://stackoverflow.com/questions/1550560/encoding-an-integer-in-7-bit-format-of-c-sharp-binaryreader-readstring&#xa;&#xa;    This function takes a file pointer and extracts the appropriate next information which is expected to contain a&#xa;    .NET 7bit binary datetime encoded value and extracts the length of that datetime value.&#xa;&#xa;    :param fp: a file pointer whose next expected element is a 7bit integer length of a binary datetime in .NET&#xa;    :return: a length value representing the string length of a binary datetime in .NET&#xa;    """"""&#xa;    string_length = 0&#xa;    string_length_parsed = False&#xa;    step = 0&#xa;    while not string_length_parsed:&#xa;        part = ord(fp.read(1))&#xa;        string_length_parsed = ((part >> 7) == 0)&#xa;        part_cutter = part & 127&#xa;        to_add = part_cutter << (step * 7)&#xa;        string_length += to_add&#xa;        step += 1&#xa;    return string_length&#xa;&#xa;&#xa;def datetime_from_dot_net_binary(data):&#xa;    """"""&#xa;    From http://stackoverflow.com/questions/15919598/serialize-datetime-as-binary&#xa;&#xa;    This function converts data from a .NET datetime binary representation to a python datetime object&#xa;&#xa;    :param data: some binary data which is expected to convert to a datetime value&#xa;&#xa;    :return: a datetime value corresponding to the binary .NET datetime representation from the input data&#xa;    """"""&#xa;    kind = (data % 2 ** 64) >> 62  # This says about UTC and stuff...&#xa;    ticks = data & 0x3FFFFFFFFFFFFFFF&#xa;    seconds = float(ticks) / 10000000.0&#xa;    tz = pytz.utc&#xa;    if kind == 0:&#xa;        tz = get_localzone()&#xa;    return datetime.datetime(1, 1, 1, tzinfo=tz) + datetime.timedelta(seconds=seconds)&#xa;&#xa;&#xa;def read_binary_file(path):&#xa;    """"""&#xa;    This function reads a Time Travel Task binary file in its entirety and converts it into a list of iterations which&#xa;    can be parsed independently.&#xa;&#xa;    :param path: a string absolute path to a Time Travel Task binary file&#xa;&#xa;    :return: a list of iterations (dictionaries containing values:&#xa;             'version' - an integer version number&#xa;&#xa;             'datetime' - a python datetime object for this iteration&#xa;&#xa;             'time_val' - a time for this iteration&#xa;&#xa;             'timescale' - a timescale which the current time is proceding through&#xa;&#xa;             'x', 'y', 'z' - x, y, and z spatial coordinates in which the participant resides&#xa;&#xa;             'rx', 'ry', 'rz', 'rw' - x, y, z, and w rotation quaternion coordinates in which the participant resides&#xa;             'keys', 'buttons', 'keylabels', 'buttonlabels' 0 the key states, button states, key labels and button&#xa;             labels for every key and button which is registered to be logged&#xa;&#xa;             'itemsx', 'itemsy', 'itemsz', 'itemsactive', 'itemsclicked', 'itemsevent', 'itemstime' - the item x, y, z&#xa;             spatial coordinates, the item active state (enabled or disabled in the environment), the item event,&#xa;             and time&#xa;             'boundarystate', 'br', 'bg', 'bb' - the boundary state and Red, Green, and Blue color intensities of the&#xa;             boundary&#xa;&#xa;             'inventoryitemnumber'- the item numbers in the inventory this iteration&#xa;&#xa;             'activeinventoryitemnumber','activeinventoryeventinder' - the active item number and event number this&#xa;             iteration)&#xa;    """"""&#xa;    iterations = []&#xa;    with open(path, 'rb') as f:&#xa;        header_length = decode_7bit_int_length(f)&#xa;        header = f.read(header_length)&#xa;        split_header = header.split(',')&#xa;        if split_header[0] != 'version':  # Beta version with new version prefix&#xa;            num_keys = header.count('key')&#xa;            num_buttons = header.count('button')&#xa;            num_items = header.count('itemXYZAC')&#xa;&#xa;            while f.read(1):  # Look ahead for end of file&#xa;                f.seek(-1, 1)  # Go back one to undo the look-ahead&#xa;&#xa;                # Extract time_val information&#xa;                date_time = datetime_from_dot_net_binary(struct.unpack_from('q', f.read(8))[0])&#xa;                time_val = struct.unpack_from('f', f.read(4))[0]&#xa;                time_scale = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract position information&#xa;                x = struct.unpack_from('f', f.read(4))[0]&#xa;                y = struct.unpack_from('f', f.read(4))[0]&#xa;                z = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract rotation information&#xa;                rx = struct.unpack_from('f', f.read(4))[0]&#xa;                ry = struct.unpack_from('f', f.read(4))[0]&#xa;                rz = struct.unpack_from('f', f.read(4))[0]&#xa;                rw = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract key, button, and item information according to expected numbers of each&#xa;                keys = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_keys):&#xa;                    keys.append(struct.unpack_from('?', f.read(1))[0])&#xa;                buttons = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_buttons):&#xa;                    buttons.append(struct.unpack_from('?', f.read(1))[0])&#xa;                ix = []&#xa;                iy = []&#xa;                iz = []&#xa;                i_active = []&#xa;                i_clicked = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_items):&#xa;                    ix.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iy.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iz.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    i_active.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_clicked.append(struct.unpack_from('?', f.read(1))[0])&#xa;&#xa;                # Extract boundary information&#xa;                boundary_state = struct.unpack_from('i', f.read(4))[0]&#xa;                br = struct.unpack_from('f', f.read(4))[0]&#xa;                bg = struct.unpack_from('f', f.read(4))[0]&#xa;                bb = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Store all information in simple dictionary and add to list of iterations&#xa;                iterations.append({""version"": 0,&#xa;                                   ""datetime"": date_time, ""time_val"": time_val, ""timescale"": time_scale,&#xa;                                   ""x"": x, ""y"": y, ""z"": z,&#xa;                                   ""rx"": rx, ""ry"": ry, ""rz"": rz, ""rw"": rw,&#xa;                                   ""keys"": keys, ""buttons"": buttons,&#xa;                                   ""itemsx"": ix, ""itemsy"": iy, ""itemsz"": iz, ""itemsactive"": i_active,&#xa;                                   ""itemsclicked"": i_clicked,&#xa;                                   ""boundarystate"": boundary_state, ""br"": br, ""bg"": bg, ""bb"": bb})&#xa;        elif split_header[1] == '2':  # Version 2&#xa;            num_keys = header.count('key')&#xa;            num_buttons = header.count('button')&#xa;            num_items = header.count('itemXYZActiveClickedEventTime')&#xa;            key_labels = []&#xa;            key_split = header.split('key')&#xa;            for i in range(1, len(key_split)):&#xa;                key_labels.append(key_split[i].split('_')[0])&#xa;            button_labels = []&#xa;            button_split = header.split('button')&#xa;            for i in range(1, len(button_split)):&#xa;                button_labels.append(button_split[i].split('_')[0])&#xa;            while f.read(1):  # Look ahead for end of file&#xa;                f.seek(-1, 1)  # Go back one to undo the look-ahead&#xa;&#xa;                # Extract time_val information&#xa;                date_time = datetime_from_dot_net_binary(struct.unpack_from('q', f.read(8))[0])&#xa;                time_val = struct.unpack_from('f', f.read(4))[0]&#xa;                time_scale = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract position information&#xa;                x = struct.unpack_from('f', f.read(4))[0]&#xa;                y = struct.unpack_from('f', f.read(4))[0]&#xa;                z = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract rotation information&#xa;                rx = struct.unpack_from('f', f.read(4))[0]&#xa;                ry = struct.unpack_from('f', f.read(4))[0]&#xa;                rz = struct.unpack_from('f', f.read(4))[0]&#xa;                rw = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract key, button, and item information according to expected numbers of each&#xa;                keys = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_keys):&#xa;                    keys.append(struct.unpack_from('?', f.read(1))[0])&#xa;                buttons = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_buttons):&#xa;                    buttons.append(struct.unpack_from('?', f.read(1))[0])&#xa;                ix = []&#xa;                iy = []&#xa;                iz = []&#xa;                i_active = []&#xa;                i_clicked = []&#xa;                i_event_type = []&#xa;                i_event_time = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_items):&#xa;                    ix.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iy.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iz.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    i_active.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_clicked.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_event_type.append(struct.unpack_from('i', f.read(4))[0])&#xa;                    i_event_time.append(struct.unpack_from('f', f.read(4))[0])&#xa;&#xa;                # Extract boundary information&#xa;                boundary_state = struct.unpack_from('i', f.read(4))[0]&#xa;                br = struct.unpack_from('f', f.read(4))[0]&#xa;                bg = struct.unpack_from('f', f.read(4))[0]&#xa;                bb = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract inventory state&#xa;                inventory_item_numbers = []&#xa;                for i in range(0, num_items):&#xa;                    inventory_item_numbers.append(struct.unpack_from('i', f.read(4))[0])&#xa;                active_inventory_item_number = struct.unpack_from('i', f.read(4))[0]&#xa;                active_inventory_event_index = struct.unpack_from('i', f.read(4))[0]&#xa;&#xa;                # Store all information in simple dictionary and add to list of iterations&#xa;                iterations.append({""version"": 2,&#xa;                                   ""datetime"": date_time, ""time_val"": time_val, ""timescale"": time_scale,&#xa;                                   ""x"": x, ""y"": y, ""z"": z,&#xa;                                   ""rx"": rx, ""ry"": ry, ""rz"": rz, ""rw"": rw,&#xa;                                   ""keys"": keys, ""buttons"": buttons,&#xa;                                   'keylabels': key_labels, 'buttonlabels': button_labels,&#xa;                                   ""itemsx"": ix, ""itemsy"": iy, ""itemsz"": iz, ""itemsactive"": i_active,&#xa;                                   ""itemsclicked"": i_clicked, 'itemsevent': i_event_type, 'itemstime': i_event_time,&#xa;                                   ""boundarystate"": boundary_state, ""br"": br, ""bg"": bg, ""bb"": bb,&#xa;                                   'inventoryitemnumbers': inventory_item_numbers,&#xa;                                   'activeinventoryitemnumber': active_inventory_item_number,&#xa;                                   'activeinventoryeventindex': active_inventory_event_index})&#xa;&#xa;        return iterations&#xa;&#xa;&#xa;def find_last(lst, sought_elt):&#xa;    """"""&#xa;    This function finds the last index that an element of a particular value appears in a list.&#xa;&#xa;    :param lst: the list to search&#xa;    :param sought_elt: the element for which we search&#xa;    :return: the index at which the last element matching sought_elt resides&#xa;    """"""&#xa;    for r_idx, elt in enumerate(reversed(lst)):&#xa;        if elt == sought_elt:&#xa;            return len(lst) - 1 - r_idx&#xa;&#xa;&#xa;def parse_test_items(iterations, cols, item_number_label, event_state_labels):&#xa;    """"""&#xa;    This function takes in a set of iterations parsed from read_binary_file, a set of color values, a set of item&#xa;    labels, and a set of event labels and produces the items, reconstruction items and the order description for&#xa;    each item in the reconstruction.&#xa;&#xa;    :param iterations: the iterations output from read_binary_file&#xa;    :param cols: the colors of each item in canonical ordering&#xa;    :param item_number_label: the labels for each item in canonical ordering&#xa;    :param event_state_labels: the event labels for each item in canonical ordering&#xa;&#xa;    :return: a dictionary containing:&#xa;             ""direction"" - a numeric value representing the up, down or stationary state&#xa;             ""pos"" - the x, z, and t values representing the 2D position and time coordinates&#xa;             ""color"" - the color value representing the indexed color value from cols associated with the item&#xa;    """"""&#xa;    descrambler = [1, 2, 4, 7, 0, 3, 5, 6, 8, 9]&#xa;    descrambler_type = [2, 2, 2, 2, 1, 1, 1, 1, 0, 0]&#xa;    reconstruction_items = [None] * len(item_number_label)&#xa;    if iterations[0]['version'] == 0:&#xa;        # pos = np.empty((len(items), 3))&#xa;        # size = np.empty((len(items)))&#xa;        # color = np.empty((len(items), 4))&#xa;        # end_time = 60  # End time for convenience&#xa;&#xa;        ################################################################################################################&#xa;        # BEGIN DESCRAMBLER&#xa;        ################################################################################################################&#xa;&#xa;        # start_state = iterations[0]  # Store first iteration&#xa;        # end_state = iterations[len(iterations) - 1]  # Store last iteration&#xa;        # prev_active = start_state['itemsactive']  # Create activity array&#xa;&#xa;        # Event state tracker variables (this works great)&#xa;        number_placed = 0&#xa;        event_state = 0&#xa;        prev_event_btn_state = False&#xa;        prev_drop_button_state = False&#xa;        prev_inventory_button_state = False&#xa;        inventory_index = 0&#xa;        inventory = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&#xa;        numbers_placed = [-1] * 10&#xa;        flag_for_same_place_override = False&#xa;        flag_for_same_place_location = None&#xa;        for iterations_idx, i in enumerate(iterations):&#xa;            if iterations_idx == len(iterations) - 1:&#xa;                break&#xa;            # Get the event state button (keys, 4, buttons, 4)&#xa;            event_button_state = i['buttons'][4]&#xa;            if event_button_state and not prev_event_btn_state:  # On rising edge&#xa;                event_state = (event_state + 1) % 3  # Set the event state appropriately&#xa;            prev_event_btn_state = event_button_state  # Update prev state for edge detection&#xa;            # Get the item drop button state (keys 1, buttons 1)&#xa;            drop_button_state = i['buttons'][1]&#xa;            inventory_button_state = i['buttons'][3]&#xa;            # Find the value of the index of this item in the descrambler, this is the correct item value (I think)&#xa;            # Coordinate comparison&#xa;            current_item_coords = []&#xa;            for xx, zz in zip(i['itemsx'], i['itemsz']):&#xa;                current_item_coords.append((xx, zz))&#xa;            next_iteration = iterations[iterations_idx + 1]&#xa;            next_item_coords = []&#xa;            for xx, zz in zip(next_iteration['itemsx'], next_iteration['itemsz']):&#xa;                next_item_coords.append((xx, zz))&#xa;&#xa;            # Check for changed state (at this point, active and button press simultaneously)&#xa;            if (current_item_coords != next_item_coords) and (not iterations_idx == 0):&#xa;                logging.debug(','.join(map(str, current_item_coords)))&#xa;                logging.debug(','.join(map(str, next_item_coords)))&#xa;                present_checklist = [False] * len(current_item_coords)&#xa;                missing_item_index = -1&#xa;                for idxx, (first, second) in enumerate(zip(current_item_coords, next_item_coords)):&#xa;                    if first in next_item_coords:&#xa;                        present_checklist[idxx] = True&#xa;                    if next_item_coords.count(first) > 1 or current_item_coords.count(second) > 1:&#xa;                        logging.warning('Items were found to be placed on top of each other. ' +&#xa;                                        'This will likely make the item identities during reconstruction inaccurate.')&#xa;                        logging.debug('CASE: Multiple items in same location, ' +&#xa;                                      'consulting Active list for differentiation.')&#xa;                        active_list = i['itemsactive']&#xa;                        next_active_list = iterations[iterations_idx + 1]['itemsactive']&#xa;                        logging.debug(','.join(map(str, active_list)))&#xa;                        logging.debug(','.join(map(str, next_active_list)))&#xa;                        logging.debug(event_state)&#xa;                        logging.debug(','.join(map(str, descrambler_type)))&#xa;                        for idxxx, (a, b) in enumerate(zip(current_item_coords, next_item_coords)):&#xa;                            if not a == b:&#xa;                                present_checklist[idxxx] = False&#xa;                                logging.debug('{0} found as move index'.format(idxxx))&#xa;                                break&#xa;                for idxx, check in enumerate(present_checklist):&#xa;                    if not check:&#xa;                        if current_item_coords.count(current_item_coords[idxx]) > 1:&#xa;                            flag_for_same_place_override = True&#xa;                            flag_for_same_place_location = current_item_coords[idxx]&#xa;                        missing_item_index = idxx&#xa;                logging.debug('{0} is missing item index'.format(missing_item_index))&#xa;                if drop_button_state and not prev_drop_button_state:  # Rising edge, item dropped&#xa;                    logging.info('item dropped/picked up.')&#xa;                    if current_item_coords == next_item_coords:  # item picked up&#xa;                        # noinspection PyTypeChecker&#xa;                        inventory.insert(inventory_index, '?')&#xa;                        logging.info(&#xa;                            'item picked up to inventory index {0}: inventory state: {1}'.format(inventory_index,&#xa;                                                                                                 inventory))&#xa;                    else:&#xa;                        if inventory:&#xa;                            inventory.pop(inventory_index)&#xa;                        numbers_placed[descrambler[missing_item_index]] = number_placed&#xa;                        number_placed += 1&#xa;                        logging.info(&#xa;                            'item dropped from inventory index {0}: inventory state: {1}'.format(inventory_index,&#xa;                                                                                                 inventory))&#xa;                if inventory_button_state and not prev_inventory_button_state:&#xa;                    inventory_index = (inventory_index + 1) % len(inventory)&#xa;&#xa;                # Now we know which item (according to the previous descrambler state) had its location change&#xa;                # and which index it WAS in. We also know the event type it was placed as so we can compute its NEW&#xa;                # position in the list.&#xa;                # missing_item_index is the index of the placed item (according to current descrambler)&#xa;                # event_state is the type of event which was placed&#xa;                # Edge cases include if an item is placed precisely back where it was and if multiple items are&#xa;                # placed in the same place... unfortunately this happens in 44.6% of test files...&#xa;                # MORE NOTES:&#xa;                # I think it is possible to completely descramble because if an item is inserted into a&#xa;                # list of consecutive&#xa;                # identical coordinates, the items form a stack (where the latest placed item is picked up first)...&#xa;                # when picked up, we now know it's relative index in the inventory (and we can be relatively sure it's&#xa;                # picked up because it should become inactive). Since it must be picked up to be placed correctly,&#xa;                # if we track its position in the inventory until it is again placed, we can disentangle it from its&#xa;                # identical partners. To track it in inventory, it is necessary to track inventory clicks as well as&#xa;                # the number of items in the inventory so a proper modulus can be established.&#xa;                # Ugh.&#xa;                # DESCRAMBLER LOGIC&#xa;                if flag_for_same_place_override:&#xa;                    tmp_max = -1&#xa;                    tmp_max_index = -1&#xa;                    for x, y in enumerate(numbers_placed):&#xa;                        if current_item_coords[descrambler[x]] == flag_for_same_place_location:&#xa;                            if y > tmp_max:&#xa;                                tmp_max = y&#xa;                                tmp_max_index = x&#xa;                    override_index = tmp_max_index&#xa;                    # override_index = [x for x, y in enumerate(numbers_placed) if y == max(numbers_placed)][0]&#xa;                    override_index_descrambled = [x for x, y in enumerate(descrambler) if y == override_index][0]&#xa;                    logging.info('same place override, most recent placed descramble index is {0} compared to '&#xa;                                 'original missing index of {1}'.format(override_index_descrambled,&#xa;                                                                        missing_item_index))&#xa;                    missing_item_index = override_index_descrambled&#xa;                    flag_for_same_place_override = False&#xa;                # If the current event state is 0 (stationary), move the current item to the end of the list&#xa;                insertion_index = -1&#xa;                val = descrambler[missing_item_index]&#xa;                del descrambler[missing_item_index]&#xa;                del descrambler_type[missing_item_index]&#xa;                if event_state == 0:&#xa;                    descrambler.append(val)&#xa;                    descrambler_type.append(event_state)&#xa;                    insertion_index = len(descrambler) - 1&#xa;                # If the current event state is 1 (up/fly), move the current item to the last fly position&#xa;                elif event_state == 1 or event_state == 2:&#xa;                    last = find_last(descrambler_type, event_state)&#xa;                    if last is None and event_state == 1:&#xa;                        last = find_last(descrambler_type, 2)&#xa;                    elif last is None and event_state == 2:&#xa;                        last = 0&#xa;                    logging.debug('inserting into {0}'.format((last + 1)))&#xa;                    descrambler.insert(last + 1, val)&#xa;                    descrambler_type.insert(last + 1, event_state)&#xa;                    insertion_index = last + 1&#xa;&#xa;                # Generate projected values (time is the important one, the space ones are replaced at the end&#xa;                # according to the descrambler order)&#xa;                placed_x = next_item_coords[insertion_index][0]&#xa;                placed_z = next_item_coords[insertion_index][1]&#xa;                placed_t = i['time_val']&#xa;                # If the event is stationary, the time of placement doesn't matter, ignore it and set to 0&#xa;                if event_state == 0:&#xa;                    placed_t = 0&#xa;                # Add the item to the list using the correct IDX to look up the color&#xa;                reconstruction_items[val] = {'direction': event_state,&#xa;                                             'pos': (placed_x, placed_z, placed_t),&#xa;                                             'color': cols[val]}&#xa;                # Log debug information&#xa;                logging.debug(','.join(map(str, descrambler)))&#xa;                logging.debug(""{0}, {1}, ({2}, {3}, {4})"".format(&#xa;                    item_number_label[val].ljust(11, ' '),&#xa;                    event_state_labels[event_state], placed_x, placed_z, placed_t))&#xa;            prev_drop_button_state = drop_button_state&#xa;            prev_inventory_button_state = inventory_button_state&#xa;&#xa;            # Replace all of the position values with the descrambled position values at the final time point.&#xa;            # Keep the time point the same as it should've been corrected earlier&#xa;            # for idx in range(0, len(reconstruction_items)):&#xa;            #    reconstruction_items[idx]['pos'] = (end_state['itemsx'][descrambler[idx]],&#xa;            #                                        end_state['itemsz'][descrambler[idx]],&#xa;            #                                        reconstruction_items[idx]['pos'][2])&#xa;            #    reconstruction_items[idx]['color'] = cols[idx]&#xa;&#xa;            ############################################################################################################&#xa;            # END DESCRAMBLER&#xa;            ############################################################################################################&#xa;&#xa;    order = [[] for _ in range(0, len(reconstruction_items))]&#xa;    if iterations[0]['version'] == 2:&#xa;&#xa;        end_state = iterations[len(iterations) - 1]&#xa;        for idx, (x, y, z, active, clicked, event, time_val) in \&#xa;                enumerate(zip(end_state['itemsx'], end_state['itemsy'], end_state['itemsz'], end_state['itemsactive'],&#xa;                              end_state['itemsclicked'], end_state['itemsevent'], end_state['itemstime'])):&#xa;            reconstruction_items[idx] = {'direction': event, 'pos': (x, z, time_val), 'color': cols[idx]}&#xa;        order_num = 0&#xa;        for iter_idx, i in enumerate(iterations):&#xa;            for idx, (x, y, z, active, clicked, event, time_val) in enumerate(zip(i['itemsx'], i['itemsy'], i['itemsz'],&#xa;                                                                                  i['itemsactive'], i['itemsclicked'],&#xa;                                                                                  i['itemsevent'], i['itemstime'])):&#xa;                if active and not iterations[iter_idx - 1]['itemsactive'][idx]:&#xa;                    order[idx].append(order_num)&#xa;                    order_num += 1&#xa;    return reconstruction_items, order&#xa;&#xa;&#xa;def get_click_locations_and_indicies(iterations, items, meta):&#xa;    # If Study/Practice, label click events&#xa;    """"""&#xa;    This function takes the iterations from read_binary_file, the items to be searched and the meta information from&#xa;    the file and returns the clicked positions, indices in the iterations, size and colors for visualization.&#xa;&#xa;    :param iterations: the iterations from read_binary_file&#xa;    :param items: the items to be visualized&#xa;    :param meta: the meta information from the filename&#xa;    :return: a dictionary containing:&#xa;             click_pos - the x, z, time coordinates of the click position&#xa;             click_idx - the index in iterations at which time the click happened&#xa;             click_size - the size the click should be visualized as&#xa;             click_color - the color with which the click should be visualized&#xa;    """"""&#xa;    click_idx = np.empty(len(items))&#xa;    click_pos = np.empty((len(items), 3))&#xa;    click_size = np.zeros((len(iterations), len(items)))&#xa;    click_color = np.empty((len(items), 4))&#xa;    if meta['phase'] in ['0', '1', '3', '4', '6', '7']:&#xa;        for idx, i in enumerate(iterations):&#xa;            if idx + 1 < len(iterations):&#xa;                for idxx, (i1, i2) in enumerate(zip(i['itemsclicked'], iterations[idx + 1]['itemsclicked'])):&#xa;                    if i['itemsclicked'][idxx]:&#xa;                        click_size[idx][idxx] = 0.5&#xa;                    if not i1 == i2:&#xa;                        click_idx[idxx] = idx&#xa;                        click_pos[idxx] = (i['x'], i['z'], i['time_val'])&#xa;                        click_color[idxx] = (128, 128, 128, 255)&#xa;            else:&#xa;                for idxx, i1 in enumerate(i['itemsclicked']):&#xa;                    if i['itemsclicked'][idxx]:&#xa;                        click_size[idx][idxx] = 0.5&#xa;    return {'position': click_pos, 'index': click_idx, 'size': click_size, 'color': click_color}&#xa;&#xa;&#xa;def get_items_solutions(meta):&#xa;    """"""&#xa;    This function returns the solution values given a particular meta-file information configuration.&#xa;&#xa;    :param meta: the meta information from get_filename_meta_data&#xa;    :return: a tuple with items, times and directions where times contains numeric time constants, directions contains&#xa;             numeric labels such that 2 is Fall, 1 is Fly, and 0 is Stationary/Stay, and items containts a list of&#xa;             dictionaries containing values:&#xa;             ""direction"" - the 0, 1, or 2 direction value&#xa;             ""pos"" - the x, z, time coordinate of the item&#xa;             ""color"" - the RGB color tuple for the item&#xa;    """"""&#xa;    if meta['phase'] == '0' or meta['phase'] == '3' or meta['phase'] == '6':&#xa;        times = [2, 12, 18, 25]&#xa;        directions = [2, 1, 2, 1]  # Fall = 2, Fly = 1, Stay = 0&#xa;        if meta['inverse'] == '1':&#xa;            times.reverse()&#xa;            directions.reverse()&#xa;        items = [{'direction': directions[0], 'pos': (2, -12, times[0]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[1], 'pos': (2, 13, times[1]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[2], 'pos': (-13, 2, times[2]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[3], 'pos': (-12, -17, times[3]), 'color': (0, 0, 255)},&#xa;                 {'direction': 0, 'pos': (13, 5, 0), 'color': (128, 0, 128)}]&#xa;    # elif meta['phase'] == '7' or meta['phase'] == '8':&#xa;    #    times = [2, 8, 17, 23]&#xa;    #    directions = [2, 1, 1, 2]  # Fall = 2, Fly = 1, Stay = 0&#xa;    #    if meta['inverse'] == '1':&#xa;    #        times.reverse()&#xa;    #        directions.reverse()&#xa;    #    items = [{'direction': directions[0], 'pos': (16, -14, times[0]), 'color': (255, 255, 0)},&#xa;    #             {'direction': directions[1], 'pos': (-10, -2, times[1]), 'color': (255, 0, 0)},&#xa;    #             {'direction': directions[2], 'pos': (15, -8, times[2]), 'color': (0, 255, 0)},&#xa;    #             {'direction': directions[3], 'pos': (-15, -15, times[3]), 'color': (0, 0, 255)},&#xa;    #             {'direction': 0, 'pos': (-2, 10, 0), 'color': (128, 0, 128)}]&#xa;    else:&#xa;        times = [4, 10, 16, 25, 34, 40, 46, 51]&#xa;        directions = [2, 1, 1, 2, 1, 2, 2, 1]  # Fall = 2, Fly = 1, Stay = 0&#xa;        if meta['inverse'] == '1':&#xa;            times.reverse()&#xa;            directions.reverse()&#xa;        items = [{'direction': directions[0], 'pos': (18, -13, times[0]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[1], 'pos': (-13, 9, times[1]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[2], 'pos': (-10, -2, times[2]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[3], 'pos': (6, -2, times[3]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[4], 'pos': (17, -8, times[4]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[5], 'pos': (-2, -7, times[5]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[6], 'pos': (-15, -15, times[6]), 'color': (0, 0, 255)},&#xa;                 {'direction': directions[7], 'pos': (6, 18, times[7]), 'color': (0, 0, 255)},&#xa;                 {'direction': 0, 'pos': (14, 6, 0), 'color': (128, 0, 128)},&#xa;                 {'direction': 0, 'pos': (-2, 10, 0), 'color': (128, 0, 128)}]&#xa;    return items, times, directions&#xa;&#xa;&#xa;def find_data_files_in_directory(directory, file_regex=""""):&#xa;    """"""&#xa;    This function acts as a helper to search a directory for files that match a regular expression. The function&#xa;    will raise an IOError if the input path is not found.&#xa;&#xa;    :param directory: the directory to search&#xa;    :param file_regex: the regular expression to match for files&#xa;&#xa;    :return: a list of files which match the regular expression&#xa;    """"""&#xa;    if not os.path.exists(directory):&#xa;        raise IOError('The input path was not found.')&#xa;&#xa;    start_time = time.time()&#xa;    data_files = []&#xa;    file_index = []&#xa;    file_roots_index = []&#xa;    for root, dirs, files in os.walk(directory):&#xa;        for f in files:&#xa;            file_index.append(f)&#xa;            file_roots_index.append(root)&#xa;&#xa;    regex = re.compile(file_regex)&#xa;    for root, f in zip(file_roots_index, file_index):&#xa;        if regex.search(os.path.basename(f)):&#xa;            logging.debug('Found data file ({0}).'.format(f))&#xa;            data_files.append(os.path.join(root, f))&#xa;    logging.info('Found {0} data files in {1} seconds.'.format(len(data_files), time.time() - start_time))&#xa;    return data_files&#xa;&#xa;&#xa;def get_exploration_metrics(iterations):&#xa;    """"""&#xa;    This function gets the common exploration metrics from an iterations list returned by read_binary_file.&#xa;&#xa;    :param iterations: the iterations from read_binary_file&#xa;    :return: a tuple containing total_time, space_travelled, time_travelled, and space_time_travelled&#xa;    """"""&#xa;    total_time = (iterations[-1]['datetime'] - iterations[0]['datetime']).total_seconds()&#xa;    space_travelled = 0&#xa;    time_travelled = 0&#xa;    space_time_travelled = 0&#xa;    for idx, i in enumerate(iterations):&#xa;        if idx == len(iterations) - 1:&#xa;            break&#xa;        t = iterations[idx]['time_val']&#xa;        xy = [iterations[idx]['x'], iterations[idx]['y']]&#xa;        xyt = xy + [t]&#xa;        t_next = iterations[idx + 1]['time_val']&#xa;        xy_next = [iterations[idx + 1]['x'], iterations[idx + 1]['y']]&#xa;        xyt_next = xy_next + [t_next]&#xa;        space_travelled += distance.euclidean(xy, xy_next)&#xa;        space_time_travelled += distance.euclidean(xyt, xyt_next)&#xa;        time_travelled += distance.euclidean(t, t_next)&#xa;&#xa;    return total_time, space_travelled, time_travelled, space_time_travelled&#xa;&#xa;&#xa;def is_correct_color(t, solution_t, bins=15.0):&#xa;    """"""&#xa;    This function determines if a particular item time is correct given a time, solution time and bins in which a&#xa;    timeline is divided.&#xa;&#xa;    :param bins: the float representing the bins into which the timeline is divided&#xa;    :param t: the time of the item&#xa;    :param solution_t: the correct time of the time&#xa;    :return: a boolean value, true if the item is in the correct time region, false otherwise&#xa;    """"""&#xa;    lower = float(np.floor(float(solution_t) / bins) * bins)&#xa;    upper = float(np.ceil(float(solution_t) / bins) * bins)&#xa;    # noinspection PyTypeChecker&#xa;    return float(lower) < float(t) < float(upper)&#xa;&#xa;&#xa;def compute_accuracy(meta, items):&#xa;    """"""&#xa;    Given some meta information from the file via get_filename_meta_data and the item information, compute the accuracy&#xa;    of the items within and across contexts.&#xa;&#xa;    :param meta: the meta information from get_filename_meta_data&#xa;    :param items: the items from parse_test_items&#xa;    :return: a tuple containing:&#xa;             space_misplacement - the amount of space-only misplacement&#xa;             time_misplacement - the amount of time-only misplacement&#xa;             space_time_misplacement - the total space and time misplacement (treating the values equally)&#xa;             direction_correct_count - the number of correct direction labels&#xa;             mean_context_crossing_excluding_wrong_context_pairs - the mean of the distance between context&#xa;             crossing pairs&#xa;             excluding those which are in the wrong context&#xa;             mean_context_noncrossing_excluding_wrong_context_pairs - the mean of the distance between non-context&#xa;             crossing pairs&#xa;             excluding those which are in the wrong context&#xa;             mean_context_crossing - the mean distance between context crossing pairs with no exclusions&#xa;             mean_noncontext_crossing - the mean distance between non-context-crossing pairs with no exclusions&#xa;    """"""&#xa;    solution_items, times_solution, directions_solution = get_items_solutions(meta)&#xa;    xs = [item['pos'][0] for item in items]&#xa;    zs = [item['pos'][1] for item in items]&#xa;    times = [item['pos'][2] for item in items]&#xa;    directions = [item['direction'] for item in items]&#xa;    xs_solution = [item['pos'][0] for item in solution_items]&#xa;    zs_solution = [item['pos'][1] for item in solution_items]&#xa;&#xa;    space_misplacement = 0&#xa;    time_misplacement = 0&#xa;    space_time_misplacement = 0&#xa;    direction_correct_count = 0&#xa;    for x, z, t, d, solx, solz, solt, sold in zip(xs, zs, times, directions, xs_solution, zs_solution, times_solution,&#xa;                                                  directions_solution):&#xa;        space_misplacement += distance.euclidean((x, z), (solx, solz))&#xa;        time_misplacement += np.abs(t - solt)&#xa;        space_time_misplacement += distance.euclidean((x, z, t), (solx, solz, solt))&#xa;        direction_correct_count += int(d == sold)&#xa;&#xa;    context_crossing_dist_exclude_wrong_colors_pairs = []&#xa;    context_noncrossing_dist_exclude_wrong_colors_pairs = []&#xa;    context_crossing_dist_pairs = []&#xa;    context_noncrossing_dist_pairs = []&#xa;&#xa;    pairs = [(1, 1, 2), (1, 3, 4), (1, 5, 6), (0, 0, 1), (0, 2, 3), (0, 4, 5), (0, 6, 7)]&#xa;    for pair in pairs:&#xa;        crossing = pair[0] != 0&#xa;        idx0 = pair[1]&#xa;        idx1 = pair[2]&#xa;        x0, z0, t0, d0 = (xs[idx0], zs[idx0], times[idx0], directions[idx0])&#xa;        solx0, solz0, solt0, sold0 = (&#xa;            xs_solution[idx0], zs_solution[idx0], times_solution[idx0], directions_solution[idx0])&#xa;        x1, z1, t1, d1 = (xs[idx1], zs[idx1], times[idx1], directions[idx1])&#xa;        solx1, solz1, solt1, sold1 = (&#xa;            xs_solution[idx1], zs_solution[idx1], times_solution[idx1], directions_solution[idx1])&#xa;        dist = np.abs(t0 - t1) / np.abs(solt0 - solt1)&#xa;        if crossing:&#xa;            context_crossing_dist_pairs.append(dist)&#xa;        else:&#xa;            context_noncrossing_dist_pairs.append(dist)&#xa;        if is_correct_color(t0, solt0) and is_correct_color(t1, solt1):&#xa;            if crossing:&#xa;                context_crossing_dist_exclude_wrong_colors_pairs.append(dist)&#xa;            else:&#xa;                context_noncrossing_dist_exclude_wrong_colors_pairs.append(dist)&#xa;&#xa;    with warnings.catch_warnings():&#xa;        warnings.simplefilter(""ignore"", category=RuntimeWarning)&#xa;        return space_misplacement, time_misplacement, space_time_misplacement, direction_correct_count, \&#xa;               np.mean(context_crossing_dist_exclude_wrong_colors_pairs), \&#xa;               np.mean(context_noncrossing_dist_exclude_wrong_colors_pairs), \&#xa;               np.mean(context_crossing_dist_pairs), np.mean(context_noncrossing_dist_pairs), \&#xa;               np.nan, np.nan, np.nan, np.nan  # Patch to fix issue with space return values which are missing...&#xa;&#xa;&#xa;def get_item_details(pastel_factor=127):&#xa;    """"""&#xa;    This function returns detailed information about the item solutions including strings representing the event&#xa;    state, strings with the item labels, and filename image strings (JPG), as well as RGB color tuples for each item.&#xa;&#xa;    :param pastel_factor: a factor to render the RGB values via pastel shades (default 127)&#xa;    :return: a tuple containing:&#xa;             event_state_labels - a set of strings containing the labels for event states given an integer&#xa;             item_number_label - a set of strings containing the labels for items given an integer&#xa;             item_label_filenames - a set of strings containing the filename for JPGs containing the images of items&#xa;             given an integer&#xa;             cols - a set of RGB colors influenced by the input pastel_factor representing the item colors&#xa;    """"""&#xa;    event_state_labels = ['stationary', 'up', 'down']&#xa;    item_number_label = ['bottle', 'icecubetray', 'clover', 'basketball', 'boot', 'crown', 'bandana', 'hammer',&#xa;                         'fireext', 'guitar']&#xa;    item_label_filename = ['bottle.jpg', 'icecubetray.jpg', 'clover.jpg', 'basketball.jpg',&#xa;                           'boot.jpg', 'crown.jpg', 'bandana.jpg', 'hammer.jpg',&#xa;                           'fireextinguisher.jpg', 'guitar.jpg']&#xa;&#xa;    cols = [(255, 255, pastel_factor), (255, 255, pastel_factor),&#xa;            (255, pastel_factor, pastel_factor), (255, pastel_factor, pastel_factor),&#xa;            (pastel_factor, 255, pastel_factor), (pastel_factor, 255, pastel_factor),&#xa;            (pastel_factor, pastel_factor, 255),&#xa;            (pastel_factor, pastel_factor, 255),&#xa;            (128, pastel_factor / 2, 128), (128, pastel_factor / 2, 128)]&#xa;    return event_state_labels, item_number_label, item_label_filename, cols&#xa;"
600612|"#! /usr/bin/env python&#xa;&#xa;import errno&#xa;import os&#xa;&#xa;from subprocess import Popen, PIPE&#xa;&#xa;&#xa;def popen(cmd):&#xa;    """"""&#xa;    Fork the specified command, returning a tuple of (stdout, stderr)&#xa;    """"""&#xa;    return Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE).communicate()&#xa;&#xa;&#xa;def get_stdout(cmd):&#xa;    """"""&#xa;    Fork the specified command, returning stdout&#xa;    """"""&#xa;    return popen(cmd)[0]&#xa;&#xa;&#xa;def get_stderr(cmd):&#xa;    """"""&#xa;    Fork the specified command, returning stderr&#xa;    """"""&#xa;    return popen(cmd)[1]&#xa;&#xa;&#xa;def mkdir_p(path):&#xa;    """"""&#xa;    Python version of shell `mkdir -p`&#xa;&#xa;    Taken from http://stackoverflow.com/a/600612/450858&#xa;    """"""&#xa;    try:&#xa;        os.makedirs(path)&#xa;    except OSError as exc:  # Python >2.5&#xa;        if exc.errno == errno.EEXIST and os.path.isdir(path):&#xa;            pass&#xa;        else:&#xa;            raise&#xa;"
4939734|"#!/usr/bin/env python&#xa;#try:&#xa;try:&#xa;	import gi, loggy, player&#xa;	gi.require_version('Gst', '1.0')&#xa;	from gi.repository import GObject, Gst, Gtk, GdkPixbuf, GdkX11, GstVideo&#xa;except:&#xa;	loggy.warn('Gui could not import required libraries')&#xa;# import soundblizzard&#xa;# import player, loggy, gst, cairo, aspectimage&#xa;# from gi.repository import Gtk&#xa;# from gi.repository import GdkPixbuf&#xa;# from gi.repository import GdkX11&#xa;# from gi.repository import GObject&#xa;#TODO: look at kiwi for pygtk lists etc.&#xa;#pyGtk.require(""2.0"")&#xa;#except:&#xa;#	print ""gui - Required libraries not found - pyGtk, Gtk, player, loggy, gst, sbdb! Please install\n""&#xa;#TODO: while gtk.gtk_events_pending(): gtk.gtk_main_iteration() - does this work?&#xa;class GTKGui(object):&#xa;	def __init__(self, sb):&#xa;		loggy.log('Gui loading...')&#xa;		#self.play_buttons = []&#xa;		#self.progress_bars    = []&#xa;		#self.position_labels    = []&#xa;		self.volume_scales =[]&#xa;		#self.info_labels = []&#xa;		self.album_arts = []&#xa;		self.main_trees = []&#xa;		self.slave_windows = []&#xa;		self.main_tree_modes = {}&#xa;		self.WIDGETS = 3&#xa;&#xa;&#xa;		self.widgets={'consume_toggles':[],'repeat_toggles':[],'single_toggles':[],'random_toggles':[],'play_buttons':[], 'progress_bars':[], 'position_labels':[], 'info_labels':[], 'fullscreen_widgets':[]}&#xa;		#self.sb = soundblizzard.soundblizzard # fakes for tab completion&#xa;		self.sb = sb&#xa;		self.sb.player.connect('async-done', self.on_async_done)&#xa;		self.master_tree_load()&#xa;&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file(""glade/gui.glade"")&#xa;		self.builder.connect_signals(self)&#xa;		#widgets = self.builder.get_objects()&#xa;		self.window = self.builder.get_object(""window1"")&#xa;		self.window.set_title(""SoundBlizzard"")&#xa;		self.window.connect('delete-event', Gtk.main_quit)&#xa;		#pixbuf = GdkPixbuf.Pixbuf.new_from_file('/home/sam/Code/Eclipse workspace/soundblizzard/logo.png')&#xa;		self.window.set_icon_from_file('logo.png')&#xa;		self.get_widgets('window')&#xa;		for window in self.widgets['window']:&#xa;			window.show()&#xa;			self.window.show()&#xa;		#self.window.fullscreen() #TODO: fullscreen&#xa;&#xa;		#self.get_widgets('albumartdrawingarea')&#xa;		#for alb in self.widgets['albumartdrawingarea']:&#xa;			#self.album_arts.append(alb)&#xa;			#print alb.window&#xa;			#self.sb.player.videosink.set_xwindow_id(alb.window.xid)&#xa;			#print(alb.get_window_xid())&#xa;&#xa;		#self.sb.player.on_update_play_state.append(self.on_play_state_change)&#xa;		#self.sb.player.on_update_volume.append(self.on_volume_change)&#xa;		#self.sb.player.on_update_tags.append(self.on_update_tags)&#xa;		self.sb.player.connect('async-done', self.on_update_tags)&#xa;		self.sb.player.connect('volume-changed', self.on_volume_change)&#xa;		self.sb.player.connect('position-changed', self.on_position_change)&#xa;		self.sb.player.connect('play-state-changed', self.on_play_state_change)&#xa;	def debug(self, data=None):&#xa;		print ('debug got')&#xa;		print (data)&#xa;&#xa;	def get_widgets(self, name):&#xa;		'''Searches Gtkbuilder for widgets with name1, name2, name3 etc and adds them to self.widgets[name]'''&#xa;		self.widgets[name]=[]&#xa;		for x in range(self.WIDGETS):&#xa;			if self.builder.get_object(name + str(x)):&#xa;				self.widgets[name].append(self.builder.get_object(name + str(x)))&#xa;	def get_next(self, widget):&#xa;		self.sb.playlist.get_next()&#xa;	def get_prev(self, widget):&#xa;		self.sb.playlist.get_prev()&#xa;	def on_playbutton_click(self, widget):&#xa;		loggy.debug('gui.on_playbutton_click')&#xa;		widget.set_relief(Gtk.ReliefStyle.NONE)&#xa;		self.sb.player.playpause()&#xa;	def on_position_change(self, player):&#xa;		'''What to do when position change signal recieved'''&#xa;		#loggy.debug('gui.on_position_change')&#xa;		for progress_bar in self.widgets['progress_bars']:&#xa;			progress_bar.set_range(0, self.sb.player.durns)&#xa;			progress_bar.set_value(self.sb.player.posns)&#xa;		label = self.sb.player.posstr + ' / ' + self.sb.player.durstr&#xa;		for position_label in self.widgets['position_labels']:&#xa;			position_label.set_label(label)&#xa;	def on_play_state_change(self, player):&#xa;		#TODO: - make player only emit one signal for each event&#xa;		#state = self.sb.player.getstate()&#xa;		loggy.debug('gui.on_play_state_change ' + self.sb.player.state)&#xa;		if (self.sb.player.state == 'play'):&#xa;			for playbutton in self.widgets['play_buttons']:&#xa;				playbutton.set_label(Gtk.STOCK_MEDIA_PAUSE)&#xa;		elif (self.sb.player.state == 'pause' or self.sb.player.state == 'stop'):&#xa;			for playbutton in self.widgets['play_buttons']:&#xa;				playbutton.set_label(Gtk.STOCK_MEDIA_PLAY)&#xa;		else:&#xa;			loggy.warn('gui.on_play_state_change got unknown state: ' + self.sb.player.state)&#xa;	def is_play_button(self, widget):&#xa;		#print 'playbutton found'&#xa;		self.widgets['play_buttons'].append(widget)#TODO: check for duplicates&#xa;	def is_fullscreen_toggle(self,widget):&#xa;		self.widgets['fullscreen_widgets'].append(widget)&#xa;		widget.get_parent_window()&#xa;		widget.connect('toggled',self.on_fullscreen_toggle)&#xa;		#def change_fullscreen_toggle(self):&#xa;		#widget.get_parent_window().connect('window-state-event',self.on_window_state_event)&#xa;	def on_window_state_event(self):&#xa;		print (widget)&#xa;		print ('LoL')&#xa;	def on_fullscreen_toggle(self, widget):&#xa;		if (widget.get_label() == 'gtk-fullscreen'):&#xa;			widget.get_parent_window().fullscreen()&#xa;			widget.set_label('gtk-leave-fullscreen')&#xa;		else:&#xa;			widget.get_parent_window().unfullscreen()&#xa;			widget.set_label('gtk-fullscreen')&#xa;	def on_progress_bar_change_value (self, value_range, scroll, value, data=None):&#xa;		self.sb.player.setpos(value)&#xa;		#self.sb.player.player.seek_simple(Gst.Format.TIME, Gst.SeekFlags.FLUSH, value)&#xa;	def is_progress_bar(self, widget):&#xa;		#print 'progress bar found'&#xa;		self.widgets['progress_bars'].append(widget)&#xa;	def is_position_label(self, widget):&#xa;		self.widgets['position_labels'].append(widget)&#xa;	def gst_time_string(self, nanosecs):&#xa;		# This method was submitted by Sam Mason.&#xa;		# It's much shorter than the original one.&#xa;		s,ns = divmod(nanosecs, self.sb.player.SECOND)&#xa;		m,s = divmod(s, 60)&#xa;		if m < 60:&#xa;			return ""%02i:%02i"" %(m,s)&#xa;		else:&#xa;			h,m = divmod(m, 60)&#xa;			return ""%i:%02i:%02i"" %(h,m,s)&#xa;	def is_volume_scale(self, widget):&#xa;		self.volume_scales.append(widget)&#xa;		widget.set_adjustment(Gtk.Adjustment(value=self.sb.player.vol, lower=0, upper=100, step_incr=5, page_incr=10, page_size=0))&#xa;		widget.connect('value-changed', self.on_volume_scale_change)&#xa;		#widget.set_value(self.sb.player.getvol())&#xa;		#widget.set_from_icon_name(Gtk.STOCK_OPEN, 36)&#xa;	def on_volume_change(self, caller):&#xa;		for volume_scale in self.volume_scales:&#xa;			if (int(round(volume_scale.get_value()))!=self.sb.player.vol): &#xa;				volume_scale.set_value(self.sb.player.vol)&#xa;	def on_volume_scale_change(self, widget, value):&#xa;		if (int(round(value)) == self.sb.player.vol):&#xa;			return True&#xa;		self.sb.player.setvol(int(round(value)))&#xa;	def is_info_label(self, widget):&#xa;		self.widgets['info_labels'].append(widget)&#xa;	def is_single_toggle(self, widget):&#xa;		self.widgets['single_toggles'].append(widget)&#xa;		self.single_toggle_update(self.sb.playlist.single, self.sb.playlist.single.get(), widget)&#xa;		widget.connect('toggled', self.on_single_toggle)&#xa;		self.sb.playlist.single.connect('changed', self.single_toggle_update, widget)&#xa;	def on_single_toggle(self, widget):&#xa;		if self.sb.playlist.single.get() != widget.get_active():&#xa;			self.sb.playlist.single.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	#TODO: combine this into one function not four&#xa;	def single_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_consume_toggle(self, widget):&#xa;		self.widgets['consume_toggles'].append(widget)&#xa;		self.consume_toggle_update(self.sb.playlist.consume, self.sb.playlist.consume.get(), widget)&#xa;		widget.connect('toggled', self.on_consume_toggle)&#xa;		self.sb.playlist.consume.connect('changed', self.consume_toggle_update, widget)&#xa;	def on_consume_toggle(self, widget):&#xa;		if self.sb.playlist.consume.get != widget.get_active():&#xa;			self.sb.playlist.consume.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def consume_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_repeat_toggle(self, widget):&#xa;		self.widgets['repeat_toggles'].append(widget)&#xa;		self.repeat_toggle_update(self.sb.playlist.repeat, self.sb.playlist.repeat.get(), widget)&#xa;		widget.connect('toggled', self.on_repeat_toggle)&#xa;		self.sb.playlist.repeat.connect('changed', self.repeat_toggle_update, widget)&#xa;	def on_repeat_toggle(self, widget):&#xa;		if self.sb.playlist.repeat.get() != widget.get_active():&#xa;			self.sb.playlist.repeat.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def repeat_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_random_toggle(self, widget):&#xa;		self.widgets['random_toggles'].append(widget)&#xa;		self.random_toggle_update(self.sb.playlist.random, self.sb.playlist.random.get(), widget)&#xa;		widget.connect('toggled', self.on_random_toggle)&#xa;		self.sb.playlist.random.connect('changed', self.random_toggle_update, widget)&#xa;	def on_random_toggle(self, widget):&#xa;		if self.sb.playlist.random.get() != widget.get_active():&#xa;			self.sb.playlist.random.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def random_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def on_async_done(self, player):&#xa;		loggy.debug('gui.on_async_done')&#xa;		self.on_update_tags()&#xa;	def on_update_tags(self, *player):&#xa;		text = ''&#xa;		if 'title' in self.sb.player.tags: #TODO: do this after async done, not every time&#xa;			text += self.sb.player.tags['title']&#xa;		if 'artist' in self.sb.player.tags:&#xa;			text += '\n by ' + self.sb.player.tags['artist']&#xa;		if 'album' in self.sb.player.tags:&#xa;			text += ' from ' + self.sb.player.tags['album'] #TODO: make font italic&#xa;		#print text&#xa;		for label in self.widgets['info_labels']:&#xa;			label.set_label(text)&#xa;		#TODO: - get this on timer not on async&#xa;		if 'image' in self.sb.player.tags:&#xa;			filename = '/home/sam/.temp.img'#TODO: replace file to temp file or better interface&#xa;			img = open(filename, 'w')&#xa;			img.write(self.sb.player.tags['image'])&#xa;			img.close()&#xa;			for album_art in self.album_arts:&#xa;				pass&#xa;				#album_art.connect('draw', self.draw_album_art, album_art)&#xa;				#cairo.ImageSurface.create_from_png(filename)&#xa;				#album_art.set_from_file(file)&#xa;				#self.on_image_resize(album_art, None)&#xa;#    def draw_album_art(self, widget):&#xa;#        print (widget + 'fart')&#xa;&#xa;	def is_album_art(self, widget):&#xa;		#image = aspectimage.AspectImage('logo.png')&#xa;		image = Gtk.Image()&#xa;		image.set_from_file('logo.png')&#xa;		widget.pack_start(image,True, True, 0)&#xa;		widget.show_all()&#xa;		#widget.add(art)&#xa;		widget.show()&#xa;		self.album_arts.append(widget)&#xa;		#print('got image '+str(widget))&#xa;		#widget.set_from_file('logo16.png')&#xa;		&#xa;		#self.on_image_resize(widget, None)&#xa;#	def redraw_album_art(self, widget, event):&#xa;#		print 'image redraw'&#xa;#		x , y, width, height = event.area&#xa;#		file = self.sbdb.config.get('Main', 'imagefile') #TODO: - does this result in file read?&#xa;#		pixbuf = Gtk.gdk.pixbuf_new_from_file_at_size(file, width , height)&#xa;#		widget.draw_pixbuf(widget.get_style().fg_gc[STATE_NORMAL],pixbuf, x, y, x, y, width, height, Gtk.gdk.RGB_DITHER_NONE, 0 , 0)&#xa;#&#xa;	def on_image_resize(self, widgetty, event): #TODO: on_image_resize&#xa;		None&#xa;#        print 'image resize'&#xa;#        #src_width, src_height = widget.get_pixmap().get_width(), widget.get_pixmap().get_height()&#xa;#        #allocation = widget.get_allocation()# thanks to http://stackoverflow.com/questions/4939734/automatic-image-scaling-on-resize-with-pyGtk&#xa;#        #ben = widget.get_pixbuf()&#xa;#        for widget in self.album_arts:&#xa;#            file = '/home/sam/temp.img' #TODO: - does this result in file read?&#xa;#            pixbuf = Gtk.gdk.pixbuf_new_from_file_at_size(file,widget.allocation.height , widget.allocation.width)&#xa;#            #pixbuf = widget.get_pixbuf().scale_simple(widget.allocation.width, widget.allocation.height, Gtk.gdk.INTERP_BILINEAR)&#xa;#            widget.set_from_pixbuf(pixbuf)&#xa;	def is_video_out(self, widget):&#xa;		loggy.debug('is_video_out')&#xa;		self.sb.player.vidout['xid'] = widget.get_property('window').get_xid() # don't forget to import GdkX11!&#xa;		self.sb.player.videosink.set_xwindow_id(self.sb.player.vidout['xid'])&#xa;		#print (widget)&#xa;		#self.sb.player.add_vid(widget.window.xid)&#xa;	def is_master_tree(self, widget):&#xa;		self.main_trees.append(widget)&#xa;		#self.main_tree_load()&#xa;		widget.set_model(self.main_tree_store)&#xa;		widget.tv_column = Gtk.TreeViewColumn('Spambox')&#xa;		widget.append_column(widget.tv_column)&#xa;		widget.cell = Gtk.CellRendererText()&#xa;		widget.tv_column.pack_start(widget.cell, True)&#xa;		widget.tv_column.add_attribute(widget.cell, 'text', 0)&#xa;		widget.set_reorderable(True)&#xa;		widget.connect('cursor-changed', self.master_tree_cursor_changed)&#xa;	def master_tree_cursor_changed(self, widget):&#xa;		#TODO: set position in all trees&#xa;		loggy.debug('gui.master_tree_cursor_changed')&#xa;		(model, iterat) = widget.get_selection().get_selected()&#xa;		#print self.slave_windows[0].get_children()&#xa;		try:&#xa;			self.slave_view.destroy()&#xa;		except:&#xa;			None&#xa;&#xa;		if iter:&#xa;			self.main_tree_modes[model.get_value(iterat,0)]['open_func']()&#xa;	def master_tree_add(self, name, open_func):&#xa;		self.main_tree_store.append(None, [name])&#xa;		self.main_tree_modes[name] = {'open_func' : open_func}&#xa;&#xa;	def master_tree_load(self):&#xa;		self.main_tree_store = Gtk.TreeStore(str)&#xa;		self.master_tree_add('Now Playing', self.slave_enter_now_playing_view)&#xa;		self.master_tree_add('Media', self.slave_enter_media_view)&#xa;		self.master_tree_add('Preferences', self.slave_enter_preferences_view)&#xa;		self.master_tree_playlist_iter = self.main_tree_store.append(None, ['Playlists'])&#xa;		for playlist in self.sb.playlist.playlists.keys():&#xa;			self.main_tree_store.append(self.master_tree_playlist_iter, [playlist])&#xa;		self.sb.player.connect('new-playlist', self.master_tree_new_playlist)&#xa;		self.sb.player.connect('deleted-playlist', self.master_tree_deleted_playlist)&#xa;	def master_tree_deleted_playlist(self,player, name):&#xa;		while name in self.main_tree_store[self.master_tree_playlist_iter]: self.main_tree_store[self.master_tree_playlist_iter].remove(name)&#xa;		&#xa;		return True #What a line of code!&#xa;	def master_tree_new_playlist(self,player, name):&#xa;		if name in self.main_tree_store[self.master_tree_playlist_iter][0]:&#xa;			return True&#xa;		else:&#xa;			self.main_tree_store.append(self.master_tree_playlist_iter, [name])&#xa;			return True&#xa;		&#xa;		&#xa;#TODO: connect signals other than map automatically&#xa;	def is_slave_area(self, widget):&#xa;		loggy.debug('gui.is_slave_area')&#xa;		self.slave_windows.append(widget) #TODO: allow multiple separate master/slave combos&#xa;	def slave_enter_media_view(self):&#xa;		loggy.debug('gui.slave_enter_media_view')&#xa;		#TODO: check slave_window is a hbox&#xa;		self.slave_view = GTK_media_view(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;	def slave_enter_now_playing_view(self):&#xa;		self.slave_view = GTK_now_playing(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;		loggy.debug('gui.slave_enter_now_playing_view')&#xa;	def slave_enter_preferences_view(self):&#xa;		self.slave_view = GTK_preferences(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;class GTK_media_view(Gtk.HBox):&#xa;	def __init__(self, sb):&#xa;		self.sb = sb&#xa;		self.gui = sb.gtkgui&#xa;		Gtk.HBox.__init__(self) # load glade&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file('glade/media_view.glade')&#xa;		widget = self.builder.get_object(""vbox1"")&#xa;		widget.reparent(self)&#xa;		self.builder.connect_signals(self)&#xa;		self.keystoshow = ('artist', 'title', 'album', 'date', 'genre', 'duration', 'rating','mimetype', 'atime', 'mtime', 'ctime', 'dtime', 'size')&#xa;		self.keystoshowdict = {} # dic of name, col position&#xa;		for i, a in enumerate(self.keystoshow):&#xa;			self.keystoshowdict[a] = i&#xa;		#print self.keystoshowdict&#xa;		#print 'GOOB' + str( player.sbdb.keytypelist )&#xa;		#creates list store with as many string columns as there are keys for&#xa;		#print player.sbdb.get_uri_db_info(""file:///home/sam/Music/POPCORN.MP3"")&#xa;		#self.list_store.append(self.sb.sbdb.get_uri_db_info(""file:///home/sam/Music/POPCORN.MP3""))&#xa;&#xa;		arsy = (GObject.TYPE_STRING,)*len(self.sb.sbdb.keys)&#xa;		self.list_store = Gtk.ListStore(*arsy)&#xa;		print( ' row length = ' + str(self.list_store.get_n_columns()))&#xa;		self.sb.sbdb.iter(self.list_store.append)&#xa;	def is_listview(self, widget):&#xa;		loggy.debug('gui.GTK_media_view.is_listview')&#xa;		self.treeview = widget&#xa;		widget.connect('row-activated', self.treeview_activated)&#xa;		widget.set_model(self.list_store) # init treeview&#xa;&#xa;		widget.tv_columns = {}&#xa;		for i, name in enumerate(self.sb.sbdb.keys): # go through all keys&#xa;			if name in self.keystoshowdict: #check if column is to display&#xa;				widget.tv_columns[name] = Gtk.TreeViewColumn(name)&#xa;				widget.insert_column(widget.tv_columns[name], self.keystoshowdict[name]) # inserts column in order from keystoshow&#xa;				widget.tv_columns[name].cell = Gtk.CellRendererText()&#xa;				widget.tv_columns[name].pack_start(widget.tv_columns[name].cell, True)&#xa;				widget.tv_columns[name].add_attribute(widget.tv_columns[name].cell, 'text', i)&#xa;				widget.tv_columns[name].set_resizable(True)&#xa;				#widget.tv_columns[name].set_clickable(True)&#xa;				widget.tv_columns[name].connect('clicked', self.tv_clicked, i)&#xa;				#widget.tv_columns[name].set_sort_indicator(True)&#xa;		widget.columns_autosize()&#xa;		#widget.set_headers_clickable(True)&#xa;		#widget.tv_column = Gtk.TreeViewColumn('Spambox')&#xa;		#widget.append_column(widget.tv_column)&#xa;		#widget.cell = Gtk.CellRendererText()&#xa;		#widget.tv_column.pack_start(widget.cell, True)&#xa;		#widget.tv_column.add_attribute(widget.cell, 'text', 0)&#xa;		#widget.set_reorderable(True)&#xa;	def treeview_activated(self, treeview, path, view_column):&#xa;		loggy.debug('gui.GTK_media_view.treeview_activated')&#xa;		(model, iterat) = treeview.get_selection().get_selected()&#xa;		if iterat:&#xa;			self.sb.playlist.load_uri(self.list_store.get_value(iterat,len(self.sb.sbdb.keys)-2))&#xa;			self.sb.playlist.playlist = [self.list_store.get_value(iterat,len(self.sb.sbdb.keys)-2)]&#xa;	def tv_clicked(self, widget, i):&#xa;		loggy.debug('gui.GTK_media_view.tv_clicked')&#xa;		widget.set_sort_column_id(i)&#xa;		#prevorder = widget.get_sort_order()&#xa;		#if prevorder = Gtk.SORT_ASCENDING:&#xa;		#	widget.set_sort_order(Gtk.SORT_DESCENDING)&#xa;		#else:&#xa;		#	widget.set_sort_order(Gtk.SORT_ASCENDING)&#xa;&#xa;class GTK_preferences(Gtk.HBox): # thanks to http://stackoverflow.com/questions/2129369/Gtk-builder-and-multiple-glade-files-breaks&#xa;	def __init__(self, sb):&#xa;		self.sb = sb&#xa;		Gtk.HBox.__init__(self)&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file('glade/preferences.glade')&#xa;		some_widget = self.builder.get_object(""notebook1"")&#xa;		some_widget.reparent(self)&#xa;		self.folderview = self.builder.get_object(""treeview1"")&#xa;		self.builder.connect_signals(self)&#xa;		self.folderstore = Gtk.ListStore(str)&#xa;		for folder in self.sb.config.config['libraryfolders']:&#xa;			self.folderstore.append([folder])&#xa;		self.folderview.set_model(self.folderstore)&#xa;		renderer = Gtk.CellRendererText()&#xa;		column = Gtk.TreeViewColumn(""Folders"", renderer, text=0)&#xa;		self.folderview.append_column(column)&#xa;		True&#xa;		#self.show_all()&#xa;		#self.add(some_widget)&#xa;		#some_widget.show()&#xa;		&#xa;#	def __destroy__(self):&#xa;#		loggy.log('GTK_media_view destroyed')&#xa;#		self.builder.destroy()&#xa;	def on_button4_clicked(self, button):&#xa;		loggy.debug('GTK_media_view.on_button4_clicked')&#xa;		self.sb.sbdb.recreate_db()&#xa;		True&#xa;	def on_button3_clicked(self, button):&#xa;		loggy.debug('GTK_media_view.on_button3_clicked')&#xa;		self.sb.sbdb.update_db()&#xa;		True&#xa;	def on_button1_clicked(self, button):&#xa;		Folderbox = Gtk.FileChooserDialog(""Please select a folder containing media"", self.sb.gtkgui.window, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OK, Gtk.ResponseType.OK))&#xa;		Folderbox.set_default_size(800,400)&#xa;		Folderbox.set_select_multiple(True)&#xa;		Folderbox.set_local_only(False)&#xa;		&#xa;		response = Folderbox.run()&#xa;		if response == Gtk.ResponseType.OK:&#xa;			loggy.log(""Gui adding media folder: "" + str(Folderbox.get_filenames()))&#xa;			self.sb.config.config['libraryfolders'] = self.sb.config.config['libraryfolders'] + Folderbox.get_filenames()&#xa;			self.sb.config.save_config()&#xa;			for folder in Folderbox.get_filenames():&#xa;				self.folderstore.append([folder])			&#xa;		Folderbox.destroy()&#xa;		True&#xa;	def on_button2_clicked(self, button):&#xa;		selection = self.folderview.get_selection()&#xa;		model, treeiter = selection.get_selected()&#xa;		if treeiter != None:&#xa;			loggy.log (""Removing "" + model[treeiter][0] + ""from folderlist"")&#xa;			self.sb.config.config['libraryfolders'].remove(model[treeiter][0])&#xa;			self.folderstore.remove(treeiter)&#xa;			self.sb.config.save_config()&#xa;			#now redo list store&#xa;		True&#xa;class GTK_now_playing(Gtk.DrawingArea):&#xa;	def __init__(self,sb):&#xa;		#print (""in the monkey"")&#xa;		Gtk.DrawingArea.__init__(self)&#xa;		#sb.gtkgui.is_video_out(self)&#xa;		True&#xa;&#xa;if __name__ == ""__main__"":&#xa;	temp = ''&#xa;	player1 = player.player&#xa;	#sbdb1 = sbdb.sbdb()&#xa;	temp.player = player1&#xa;	loggy.debug_setting = True&#xa;	app = GTKGui(temp)&#xa;	Gtk.main()&#xa;&#xa;"
16834861|"# Copyright (c) 2015 William Lees&#xa;&#xa;# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated&#xa;# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the&#xa;# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit&#xa;# persons to whom the Software is furnished to do so, subject to the following conditions:&#xa;&#xa;# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the&#xa;# Software.&#xa;&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE&#xa;# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR&#xa;# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR&#xa;# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;&#xa;&#xa;# Create just enough of a database file to work with Change-o's DefineClones&#xa;&#xa;__author__ = 'William Lees'&#xa;__docformat__ = ""restructuredtext en""&#xa;&#xa;import os&#xa;import sys&#xa;import argparse&#xa;import csv&#xa;import re&#xa;import numpy as np&#xa;import matplotlib.pyplot as plt&#xa;import matplotlib.colors as mcolors&#xa;import matplotlib.mlab as mlab&#xa;import itertools&#xa;&#xa;def main(argv):&#xa;    parser = argparse.ArgumentParser(description='Read an IgBlastPlus file and plot the contained CDR3 lengths.')&#xa;    parser.add_argument('infiles', help='input files, separated by commas (IgBLASTPlus format, nt or aa)')&#xa;    parser.add_argument('-b', '--barcolour', help='colour or list of colours for bars')&#xa;    parser.add_argument('-c', '--cols', help='Number of columns for plot')&#xa;    parser.add_argument('-d', '--dupheader', help='Prefix for duplicate count, eg ""DUPCOUNT="" for Presto')&#xa;    parser.add_argument('-g', '--gradientfill', help='fill bars with a gradiented colour', action='store_true')    &#xa;    parser.add_argument('-gh', '--grid_horizontal', help='horizontal grid lines', action='store_true')&#xa;    parser.add_argument('-gv', '--grid_vertical', help='vertical grid lines every n bars')&#xa;    parser.add_argument('-ga', '--gauss', help='plot best-fit Gaussian distribution', action='store_true')    &#xa;    parser.add_argument('-s', '--save', help='Save output to file (as opposed to interactive display)')&#xa;    parser.add_argument('-sz', '--size', help='Figure size (x,y)')&#xa;    parser.add_argument('-t', '--titles', help='titles for each plot, separated by commas')&#xa;    parser.add_argument('-u', '--unique', help='only count unique sequences', action='store_true')&#xa;    parser.add_argument('-w', '--width', help='relative bar width (number between 0 and 1)')&#xa;    parser.add_argument('-xmax', '--xmax', help='Max x-value to use on all charts')&#xa;    parser.add_argument('-xmin', '--xmin', help='Min x-value to use on all charts')&#xa;    parser.add_argument('-y', '--ymax', help='Max y-value to use on all charts')&#xa;    args = parser.parse_args()&#xa;    infiles = args.infiles.split(',')&#xa;    titles = args.titles.split(',') if args.titles else [""""]&#xa;    ncols = int(args.cols) if args.cols else 1&#xa;    xmin = int(args.xmin) if args.xmin else 1&#xa;    xmax = int(args.xmax) if args.xmax else None&#xa;    ymax = float(args.ymax) if args.ymax else None&#xa;    outfile = args.save if args.save else None&#xa;    dupheader = args.dupheader&#xa;    bar_width = float(args.width) if args.width else 1.0&#xa;    mapcolour = args.barcolour if args.barcolour else 'blue'&#xa;    mapcolour = mapcolour.split(',')&#xa;    grid_vertical = int(args.grid_vertical) if args.grid_vertical else False&#xa;    gauss = args.gauss&#xa;&#xa;    nrows = len(infiles) / ncols&#xa;    if len(infiles) % ncols != 0:&#xa;        nrows += 1&#xa;    (sizex, sizey) = args.size.split(',') if args.size else (8*ncols,4*nrows)&#xa;&#xa;    lengths = []&#xa;    for infile in infiles:&#xa;        lengths.append(determine_stats(dupheader, infile, args.unique))&#xa;        &#xa;    if not outfile or len(outfile) < 5 or outfile[-4:] != '.csv':&#xa;        plt.figure(figsize=(float(sizex),float(sizey)))&#xa;        plot_number = 1&#xa;        plotted = False&#xa;        for (length, title, colour) in zip(lengths, itertools.cycle(titles), itertools.cycle(mapcolour)):&#xa;            if length is not None:&#xa;                plot_file(length, xmin, xmax, ymax, title, nrows, ncols, plot_number, colour, bar_width, args.gradientfill, args.grid_horizontal, grid_vertical, gauss)&#xa;                plot_number += 1&#xa;                plotted = True&#xa;        if not plotted:&#xa;            quit()&#xa;&#xa;        plt.tight_layout()&#xa;        if outfile:&#xa;            plt.savefig(outfile)&#xa;        else:&#xa;            plt.show()&#xa;    else:&#xa;        minlength = 9999&#xa;        maxlength = 0&#xa;        for length in lengths:&#xa;            if length is not None:&#xa;                maxlength = max(maxlength, max([i for i in range(len(length)) if length[i] > 0]))&#xa;                minlength = min(minlength, min([i for i in range(len(length)) if length[i] > 0]))&#xa;&#xa;        with open(outfile, 'wb') as fo:&#xa;            writer = csv.writer(fo)&#xa;            writer.writerow(['Length'] + range(minlength, maxlength))&#xa;            ititle = iter(titles) if titles else iter(infiles)&#xa;            for length in lengths:&#xa;                if length is not None:&#xa;                    writer.writerow([next(ititle)] + list(length[minlength:maxlength]))&#xa;&#xa;&#xa;def plot_file(lengths, xmin, xmax, ymax, title, nrows, ncols, plot_number, mapcolour, bar_width, gradientfill, grid_horizontal, grid_vertical, gauss):&#xa;    if not xmax:&#xa;        for xmax in range(len(lengths)-1, 0, -1):&#xa;            if lengths[xmax] > 0:&#xa;                break&#xa;    &#xa;    ax = plt.subplot(nrows, ncols, plot_number)&#xa;    ax.tick_params(direction='out', top=False, right=False)&#xa;&#xa;    if title != """":&#xa;        plt.xlabel(title)&#xa;&#xa;    bar_pos = np.arange(xmax + 1)&#xa;    &#xa;    # if xmax is set, all the x axes will be the same, so only put labels on the bottom row&#xa;    # if we have a vertical grid, label just one point in each column&#xa;    &#xa;    if xmax is None or plot_number > (nrows-1)*ncols:&#xa;        if grid_vertical:&#xa;            labels = []&#xa;            if grid_vertical % 2 == 0:&#xa;                midpoint = grid_vertical/2&#xa;            else:&#xa;                midpoint = int(grid_vertical/2)&#xa;            for tick in bar_pos:&#xa;                if tick % grid_vertical == midpoint:&#xa;                    labels.append(str(tick))&#xa;                else:&#xa;                    labels.append(' ')&#xa;        else:&#xa;            labels = ['%d' % x for x in bar_pos]&#xa;    else:&#xa;        labels = []&#xa;            &#xa;    plt.xticks(bar_pos+0.5, labels)&#xa;&#xa;    if ymax:&#xa;        plt.ylim(0, ymax)&#xa;    if xmax or xmin != 0:&#xa;        plt.xlim(xmin, xmax)&#xa;&#xa;    if bar_width < 1.:&#xa;        bar_pos = bar_pos + (1 - bar_width) / 2.&#xa;&#xa;    if grid_horizontal:&#xa;        plt.grid(which='major', axis='y', c='black', linestyle='-', alpha=0.6, zorder=1)&#xa;&#xa;    if gradientfill:&#xa;        gbar(bar_pos, lengths, mapcolour, width=bar_width)&#xa;    else:&#xa;        plt.bar(bar_pos, lengths[:xmax+1], width=bar_width, color=mapcolour, zorder=10)&#xa;&#xa;    if grid_vertical:&#xa;        pos = 0&#xa;        while pos < xmax:&#xa;            ymin, ymax = plt.ylim()&#xa;            plt.plot([bar_pos[pos] - (1 - bar_width)/2, bar_pos[pos] - (1 - bar_width)/2], [ymin, ymax], c='black', linestyle='-', alpha=0.6, zorder=1)&#xa;            pos += grid_vertical&#xa;            &#xa;    # Remove every other y label because we get far too many by default&#xa;    &#xa;    locs, labels = plt.yticks()&#xa;    newlocs = []&#xa;    newlabels = []&#xa;    &#xa;    for i in range(0, len(labels)):&#xa;        if i % 2 != 0:&#xa;            newlocs.append(locs[i])&#xa;            newlabels.append(str(int(locs[i])))&#xa;            &#xa;    plt.yticks(newlocs, newlabels)&#xa;            &#xa;    if gauss:&#xa;        values = []&#xa;        for i in range(len(lengths)-1):&#xa;            if lengths[i] > 0:&#xa;                foo = [i]*lengths[i]&#xa;                values += ([i] * lengths[i])&#xa;        values = np.array(values)&#xa;        mean = np.mean(values)&#xa;        variance = np.var(values)&#xa;        sigma = np.sqrt(variance)&#xa;        x = np.linspace(min(values), max(values), 100)&#xa;        plt.plot(x, mlab.normpdf(x, mean, sigma)*len(values), zorder=20)&#xa;&#xa;    ax.set_aspect('auto')&#xa;    plt.tight_layout()&#xa;&#xa;&#xa;def gbar(x, y, mapcolour, width=1, bottom=0):&#xa;    X = [[.6, .6], [.7, .7]]&#xa;    c = mcolors.ColorConverter().to_rgb&#xa;    cm = make_colormap([c('white'), c(mapcolour)])&#xa;    for left, top in zip(x, y):&#xa;        if top != bottom:&#xa;            right = left + width&#xa;            plt.imshow(X, interpolation='bicubic', cmap=cm, extent=(left, right, bottom, top), alpha=1, zorder=10)&#xa;            plt.plot([left, left], [bottom, top], color='black', linestyle='-', zorder=20)&#xa;            plt.plot([right, right], [bottom, top], color='black', linestyle='-', zorder=20)&#xa;            plt.plot([right, left], [top, top], color='black', linestyle='-', zorder=20)&#xa;&#xa;&#xa;# From http://stackoverflow.com/questions/16834861/create-own-colormap-using-matplotlib-and-plot-color-scale&#xa;def make_colormap(seq):&#xa;    """"""Return a LinearSegmentedColormap&#xa;    seq: a sequence of floats and RGB-tuples. The floats should be increasing&#xa;    and in the interval (0,1).&#xa;    """"""&#xa;    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]&#xa;    cdict = {'red': [], 'green': [], 'blue': []}&#xa;    for i, item in enumerate(seq):&#xa;        if isinstance(item, float):&#xa;            r1, g1, b1 = seq[i - 1]&#xa;            r2, g2, b2 = seq[i + 1]&#xa;            cdict['red'].append([item, r1, r2])&#xa;            cdict['green'].append([item, g1, g2])&#xa;            cdict['blue'].append([item, b1, b2])&#xa;    return mcolors.LinearSegmentedColormap('CustomMap', cdict)&#xa;&#xa;&#xa;def determine_stats(dupheader, infile, unique):&#xa;    seen = {}&#xa;    with open(infile, 'r') as fi:&#xa;        ln = fi.readline()&#xa;        sep = (""\t"" if ""\t"" in ln else "","")&#xa;        fi.seek(0)&#xa;        reader = csv.DictReader(fi, delimiter=sep)&#xa;        lengths = np.zeros(600)&#xa;        first_row = True&#xa;        seen_data = False&#xa;        for row in reader:&#xa;            if first_row:&#xa;                for f in ('Functionality', 'CDR3-IMGT', 'Sequence ID'):&#xa;                    if f not in row:&#xa;                        print 'Error: required field %s not in %s.' % (f, infile)&#xa;                        quit()&#xa;                first_row = False&#xa;            if 'unproductive' not in row['Functionality'] and row['CDR3-IMGT'] and (&#xa;                not unique or row['CDR3-IMGT'] not in seen):&#xa;                lengths[len(row['CDR3-IMGT'])] += (get_size(row['Sequence ID'], dupheader) if dupheader else 1)&#xa;                seen[row['CDR3-IMGT']] = 1&#xa;                seen_data = True&#xa;&#xa;    if not seen_data:&#xa;        print 'Warning: no functional CDR3 records found in file %s.' % infile&#xa;        return None&#xa;&#xa;    return lengths&#xa;&#xa;&#xa;# Find duplicate size, or return 1&#xa;&#xa;def get_size(s, dupheader):&#xa;    count = None&#xa;    if dupheader in s:&#xa;        spl = s.split(dupheader)&#xa;        for i in range(1, len(spl[1])):&#xa;            if spl[1][0:i].isdigit():&#xa;                count = int(spl[1][0:i])&#xa;            else:&#xa;                break&#xa;&#xa;    return count if count else 1    &#xa;&#xa;if __name__==""__main__"":&#xa;    main(sys.argv)&#xa;&#xa;"
4913653|"""""""Module with location helpers.""""""&#xa;import collections&#xa;from math import radians, cos, sin, asin, sqrt&#xa;&#xa;import requests&#xa;&#xa;&#xa;LocationInfo = collections.namedtuple(&#xa;    ""LocationInfo"",&#xa;    ['ip', 'country_code', 'country_name', 'region_code', 'region_name',&#xa;     'city', 'zip_code', 'time_zone', 'latitude', 'longitude',&#xa;     'use_fahrenheit'])&#xa;&#xa;&#xa;def detect_location_info():&#xa;    """""" Detect location information. """"""&#xa;    try:&#xa;        raw_info = requests.get(&#xa;            'https://freegeoip.net/json/', timeout=5).json()&#xa;    except requests.RequestException:&#xa;        return&#xa;&#xa;    data = {key: raw_info.get(key) for key in LocationInfo._fields}&#xa;&#xa;    # From Wikipedia: Fahrenheit is used in the Bahamas, Belize,&#xa;    # the Cayman Islands, Palau, and the United States and associated&#xa;    # territories of American Samoa and the U.S. Virgin Islands&#xa;    data['use_fahrenheit'] = data['country_code'] in (&#xa;        'BS', 'BZ', 'KY', 'PW', 'US', 'AS', 'VI')&#xa;&#xa;    return LocationInfo(**data)&#xa;&#xa;&#xa;# From: http://stackoverflow.com/a/4913653/646416&#xa;def distance(lon1, lat1, lon2, lat2):&#xa;    """"""&#xa;    Calculate the great circle distance in meters between two points specified&#xa;    in decimal degrees on the earth using the Haversine algorithm.&#xa;    """"""&#xa;    # convert decimal degrees to radians&#xa;    lon1, lat1, lon2, lat2 = (radians(val) for val in (lon1, lat1, lon2, lat2))&#xa;&#xa;    dlon = lon2 - lon1&#xa;    dlat = lat2 - lat1&#xa;    angle = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2&#xa;    # Radius of earth in meters.&#xa;    radius = 6371000&#xa;    return 2 * radius * asin(sqrt(angle))&#xa;"
246128|"#!/usr/bin/env python&#xa;#&#xa;# Wrapper script for Java Conda packages that ensures that the java runtime&#xa;# is invoked with the right options. Adapted from the bash script (http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in/246128#246128).&#xa;&#xa;#&#xa;# Program Parameters&#xa;#&#xa;import os&#xa;import subprocess&#xa;import sys&#xa;import shutil&#xa;from os import access&#xa;from os import getenv&#xa;from os import X_OK&#xa;jar_file = 'PeptideShaker-1.11.0.jar'&#xa;&#xa;default_jvm_mem_opts = ['-Xms512m', '-Xmx1g']&#xa;&#xa;# !!! End of parameter section. No user-serviceable code below this line !!!&#xa;&#xa;&#xa;def real_dirname(path):&#xa;    """"""Return the symlink-resolved, canonicalized directory-portion of path.""""""&#xa;    return os.path.dirname(os.path.realpath(path))&#xa;&#xa;&#xa;def java_executable():&#xa;    """"""Return the executable name of the Java interpreter.""""""&#xa;    java_home = getenv('JAVA_HOME')&#xa;    java_bin = os.path.join('bin', 'java')&#xa;&#xa;    if java_home and access(os.path.join(java_home, java_bin), X_OK):&#xa;        return os.path.join(java_home, java_bin)&#xa;    else:&#xa;        return 'java'&#xa;&#xa;&#xa;def jvm_opts(argv):&#xa;    """"""Construct list of Java arguments based on our argument list.&#xa;&#xa;    The argument list passed in argv must not include the script name.&#xa;    The return value is a 3-tuple lists of strings of the form:&#xa;      (memory_options, prop_options, passthrough_options)&#xa;    """"""&#xa;    mem_opts = []&#xa;    prop_opts = []&#xa;    pass_args = []&#xa;    exec_dir = None&#xa;&#xa;    for arg in argv:&#xa;        if arg.startswith('-D'):&#xa;            prop_opts.append(arg)&#xa;        elif arg.startswith('-XX'):&#xa;            prop_opts.append(arg)&#xa;        elif arg.startswith('-Xm'):&#xa;            mem_opts.append(arg)&#xa;        elif arg.startswith('--exec_dir='):&#xa;            exec_dir = arg.split('=')[1].strip('""').strip(""'"")&#xa;            if not os.path.exists(exec_dir):&#xa;                shutil.copytree(real_dirname(sys.argv[0]), exec_dir, symlinks=False, ignore=None)&#xa;        else:&#xa;            pass_args.append(arg)&#xa;&#xa;    # In the original shell script the test coded below read:&#xa;    #   if [ ""$jvm_mem_opts"" == """" ] && [ -z ${_JAVA_OPTIONS+x} ]&#xa;    # To reproduce the behaviour of the above shell code fragment&#xa;    # it is important to explictly check for equality with None&#xa;    # in the second condition, so a null envar value counts as True!&#xa;&#xa;    if mem_opts == [] and getenv('_JAVA_OPTIONS') is None:&#xa;        mem_opts = default_jvm_mem_opts&#xa;&#xa;    return (mem_opts, prop_opts, pass_args, exec_dir)&#xa;&#xa;&#xa;def main():&#xa;    java = java_executable()&#xa;    """"""&#xa;    PeptideShaker updates files relative to the path of the jar file.&#xa;    In a multiuser setting, the option --exec_dir=""exec_dir""&#xa;    can be used as the location for the peptide-shaker distribution.&#xa;    If the exec_dir dies not exist,&#xa;    we copy the jar file, lib, and resources to the exec_dir directory.&#xa;    """"""&#xa;    (mem_opts, prop_opts, pass_args, exec_dir) = jvm_opts(sys.argv[1:])&#xa;    jar_dir = exec_dir if exec_dir else real_dirname(sys.argv[0])&#xa;&#xa;    if pass_args != [] and pass_args[0].startswith('eu'):&#xa;        jar_arg = '-cp'&#xa;    else:&#xa;        jar_arg = '-jar'&#xa;&#xa;    jar_path = os.path.join(jar_dir, jar_file)&#xa;&#xa;    java_args = [java] + mem_opts + prop_opts + [jar_arg] + [jar_path] + pass_args&#xa;&#xa;    sys.exit(subprocess.call(java_args))&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    main()&#xa;"
37661854|"from __future__ import print_function&#xa;import os&#xa;import sys&#xa;import datetime&#xa;import os.path as op&#xa;import glob&#xa;import pandas as pd&#xa;import numpy as np&#xa;import contextlib&#xa;import logging&#xa;import distutils.spawn&#xa;import subprocess&#xa;import shlex&#xa;import requests&#xa;import json&#xa;import warnings&#xa;import gzip&#xa;from collections import OrderedDict&#xa;from collections import Callable&#xa;import operator&#xa;&#xa;log = logging.getLogger(__name__)&#xa;&#xa;&#xa;def is_ipynb():&#xa;    """"""Return True if the module is running in IPython kernel,&#xa;    False if in IPython shell or other Python shell.&#xa;&#xa;    Copied from: http://stackoverflow.com/a/37661854/1592810&#xa;    There are other methods there too&#xa;&#xa;    >>> is_ipynb()&#xa;    False&#xa;&#xa;    """"""&#xa;    try:&#xa;        shell = get_ipython().__class__.__name__&#xa;        if shell == 'ZMQInteractiveShell':  # Jupyter notebook or qtconsole?&#xa;            return True&#xa;        elif shell == 'TerminalInteractiveShell':  # Terminal running IPython?&#xa;            return False&#xa;        else:&#xa;            return False  # Other type (?)&#xa;    except NameError:&#xa;        return False      # Probably standard Python interpreter&#xa;&#xa;&#xa;class Date():&#xa;    """"""Various methods to return formatted dates for today.&#xa;    """"""&#xa;    def __init__(self):&#xa;        self.today = datetime.date.today()&#xa;        self.short_date = self.today.strftime('%y%m%d')&#xa;        self.long_date = self.today.strftime('%Y-%m-%d')&#xa;&#xa;&#xa;def todays_short_date():&#xa;    today = Date()&#xa;    return today.short_date&#xa;&#xa;&#xa;def todays_long_date():&#xa;    today = Date()&#xa;    return today.long_date&#xa;&#xa;&#xa;class DefaultOrderedDict(OrderedDict):&#xa;    """"""Class to combine defaultdict and OrderedDict.&#xa;    Source: http://stackoverflow.com/a/6190500/562769&#xa;&#xa;    """"""&#xa;    def __init__(self, default_factory=None, *a, **kw):&#xa;        if (default_factory is not None and&#xa;                not isinstance(default_factory, Callable)):&#xa;            raise TypeError('first argument must be callable')&#xa;        OrderedDict.__init__(self, *a, **kw)&#xa;        self.default_factory = default_factory&#xa;&#xa;    def __getitem__(self, key):&#xa;        try:&#xa;            return OrderedDict.__getitem__(self, key)&#xa;        except KeyError:&#xa;            return self.__missing__(key)&#xa;&#xa;    def __missing__(self, key):&#xa;        if self.default_factory is None:&#xa;            raise KeyError(key)&#xa;        self[key] = value = self.default_factory()&#xa;        return value&#xa;&#xa;    def __reduce__(self):&#xa;        if self.default_factory is None:&#xa;            args = tuple()&#xa;        else:&#xa;            args = self.default_factory,&#xa;        return type(self), args, None, None, self.items()&#xa;&#xa;    def copy(self):&#xa;        return self.__copy__()&#xa;&#xa;    def __copy__(self):&#xa;        return type(self)(self.default_factory, self)&#xa;&#xa;    def __deepcopy__(self, memo):&#xa;        import copy&#xa;        return type(self)(self.default_factory,&#xa;                          copy.deepcopy(self.items()))&#xa;&#xa;    def __repr__(self):&#xa;        return 'OrderedDefaultDict(%s, %s)' % (self.default_factory,&#xa;                                               OrderedDict.__repr__(self))&#xa;&#xa;&#xa;@contextlib.contextmanager&#xa;def suppress_stdout():&#xa;    """"""Suppress the stdout of any function.&#xa;&#xa;    Usage:&#xa;    with ssbio.utils.suppress_stdout():&#xa;        my_function_here()&#xa;    """"""&#xa;    with open(os.devnull, ""w"") as devnull:&#xa;        old_stdout = sys.stdout&#xa;        sys.stdout = devnull&#xa;        try:&#xa;            yield&#xa;        finally:&#xa;            sys.stdout = old_stdout&#xa;&#xa;&#xa;def clean_df(df, fill_nan=True, drop_empty_columns=True):&#xa;    """"""Clean a pandas dataframe by:&#xa;        1. Filling empty values with Nan&#xa;        2. Dropping columns with all empty values&#xa;&#xa;    Args:&#xa;        df: Pandas DataFrame&#xa;        fill_nan (bool): If any empty values (strings, None, etc) should be replaced with NaN&#xa;        drop_empty_columns (bool): If columns whose values are all empty should be dropped&#xa;&#xa;    Returns:&#xa;        DataFrame: cleaned DataFrame&#xa;&#xa;    """"""&#xa;    if fill_nan:&#xa;        df = df.fillna(value=np.nan)&#xa;    if drop_empty_columns:&#xa;        df = df.dropna(axis=1, how='all')&#xa;    return df.sort_index()&#xa;&#xa;&#xa;def clean_single_dict(indict, prepend_to_keys=None, remove_keys_containing=None):&#xa;    """"""Clean a dict with values that contain single item iterators to single items&#xa;&#xa;    Args:&#xa;        indict (dict): Dictionary to be cleaned&#xa;        prepend_to_keys (str): String to prepend to all keys&#xa;        remove_keys_containing (str): Text to check for in keys to ignore&#xa;&#xa;    Returns:&#xa;         dict: Cleaned dictionary&#xa;&#xa;    Examples:&#xa;        >>> clean_single_dict(indict={'test1': [1], 'test2': ['H']})&#xa;        {'test1': 1, 'test2': 'H'}&#xa;&#xa;        >>> clean_single_dict(indict={'test1': [1], 'test2': ['H']}, prepend_to_keys='struct_')&#xa;        {'struct_test1': 1, 'struct_test2': 'H'}&#xa;&#xa;        >>> clean_single_dict(indict={'test1': [1], 'ignore': ['H']}, prepend_to_keys='struct_', remove_keys_containing='ignore')&#xa;        {'struct_test1': 1}&#xa;&#xa;    """"""&#xa;    if not prepend_to_keys:&#xa;        prepend_to_keys = ''&#xa;&#xa;    outdict = {}&#xa;    for k, v in indict.items():&#xa;        if remove_keys_containing:&#xa;            if remove_keys_containing in k:&#xa;                continue&#xa;        outdict[prepend_to_keys + k] = v[0]&#xa;&#xa;    return outdict&#xa;&#xa;&#xa;def double_check_attribute(object, setter, backup_attribute, custom_error_text=None):&#xa;    """"""Check if a parameter to be used is None, if it is, then check the specified backup attribute and throw&#xa;    an error if it is also None.&#xa;&#xa;    Args:&#xa;        object: The original object&#xa;        setter: Any input object&#xa;        backup_attribute (str): Attribute in <object> to be double checked&#xa;        custom_error_text (str): If a custom string for the error should be raised&#xa;&#xa;    Raises:&#xa;         ValueError: If both setter and backup_attribute are None&#xa;&#xa;    """"""&#xa;    if not setter:&#xa;        next_checker = getattr(object, backup_attribute)&#xa;        if not next_checker:&#xa;            if custom_error_text:&#xa;                raise ValueError(custom_error_text)&#xa;            else:&#xa;                raise ValueError('Attribute replacing ""{}"" must be specified'.format(backup_attribute))&#xa;&#xa;&#xa;def split_folder_and_path(filepath):&#xa;    """"""Split a file path into its folder, filename, and extension&#xa;&#xa;    Args:&#xa;        path (str): Path to a file&#xa;&#xa;    Returns:&#xa;        tuple: of (folder, filename (without extension), extension)&#xa;&#xa;    """"""&#xa;    dirname = op.dirname(filepath)&#xa;    filename = op.basename(filepath)&#xa;    splitext = op.splitext(filename)&#xa;    filename_without_extension = splitext[0]&#xa;    extension = splitext[1]&#xa;&#xa;    return dirname, filename_without_extension, extension&#xa;&#xa;&#xa;def is_non_zero_file(fpath):&#xa;    """"""Check if a file exists, and that it contains some kind of contents&#xa;&#xa;    Args:&#xa;        fpath (str): Path to file&#xa;&#xa;    Returns:&#xa;        bool: If file exists and contains contents&#xa;&#xa;    """"""&#xa;    return op.isfile(fpath) and os.path.getsize(fpath) > 0&#xa;&#xa;&#xa;def outfile_maker(inname, outext='.out', outname='', outdir='', append_to_name=''):&#xa;    """"""Create a default name for an output file based on the inname name, unless a output name is specified.&#xa;&#xa;    Args:&#xa;        inname: Path to input file&#xa;        outext: Optional specified extension for output file (with the "".""). Default is "".out"".&#xa;        outfile: Optional specified name of output file.&#xa;        outdir: Optional path to output directory.&#xa;&#xa;    Returns:&#xa;        str: Path to final output destination.&#xa;&#xa;    Examples:&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta')&#xa;        'P00001.out'&#xa;&#xa;        >>> outfile_maker(inname='P00001')&#xa;        'P00001.out'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', append_to_name='_new')&#xa;        'P00001_new.out'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outext='.mao')&#xa;        'P00001.mao'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outext='.mao', append_to_name='_new')&#xa;        'P00001_new.mao'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outext='.new', outname='P00001_aligned')&#xa;        'P00001_aligned.new'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outname='P00001_aligned')&#xa;        'P00001_aligned.out'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outname='P00001_aligned', append_to_name='_new')&#xa;        'P00001_aligned_new.out'&#xa;&#xa;        >>> outfile_maker(inname='P00001.fasta', outname='P00001_aligned', outdir='/my/dir/')&#xa;        '/my/dir/P00001_aligned.out'&#xa;&#xa;        >>> outfile_maker(inname='/test/other/dir/P00001.fasta', append_to_name='_new')&#xa;        '/test/other/dir/P00001_new.out'&#xa;&#xa;        >>> outfile_maker(inname='/test/other/dir/P00001.fasta', outname='P00001_aligned')&#xa;        '/test/other/dir/P00001_aligned.out'&#xa;&#xa;        >>> outfile_maker(inname='/test/other/dir/P00001.fasta', outname='P00001_aligned', outdir='/my/dir/')&#xa;        '/my/dir/P00001_aligned.out'&#xa;&#xa;    """"""&#xa;&#xa;    # TODO: CHECK IF OUTNAME IS A VALID FILE NAME!&#xa;    orig_dir, orig_name, orig_ext = split_folder_and_path(inname)&#xa;&#xa;    # If output filename not provided, default is to take name of inname&#xa;    if not outname:&#xa;        outname = orig_name&#xa;&#xa;    # Create new path in the same directory of old path if a new one isn't specified&#xa;    if not outdir:&#xa;        outdir = orig_dir&#xa;&#xa;    # Append additional stuff to the filename if specified&#xa;    if append_to_name:&#xa;        outname += append_to_name&#xa;&#xa;    # Join the output filename and output extension&#xa;    final_outfile = op.join(outdir, '{}{}'.format(outname, outext))&#xa;&#xa;    return final_outfile&#xa;&#xa;&#xa;def force_rerun(flag, outfile):&#xa;    """"""Check if we should force rerunning of a command if an output file exists.&#xa;&#xa;    Args:&#xa;        flag (bool): Flag to force rerun.&#xa;        outfile (str): Path to output file which may already exist.&#xa;&#xa;    Returns:&#xa;        bool: If we should force rerunning of a command&#xa;&#xa;    Examples:&#xa;        >>> force_rerun(flag=True, outfile='/not/existing/file.txt')&#xa;        True&#xa;&#xa;        >>> force_rerun(flag=False, outfile='/not/existing/file.txt')&#xa;        True&#xa;&#xa;        >>> force_rerun(flag=True, outfile='./utils.py')&#xa;        True&#xa;&#xa;        >>> force_rerun(flag=False, outfile='./utils.py')&#xa;        False&#xa;&#xa;    """"""&#xa;    # If flag is True, always run&#xa;    if flag:&#xa;        return True&#xa;    # If flag is False but file doesn't exist, also run&#xa;    elif not flag and not op.exists(outfile):&#xa;        return True&#xa;    # If flag is False but filesize of output is 0, also run&#xa;    elif not flag and not is_non_zero_file(outfile):&#xa;        return True&#xa;    # Otherwise, do not run&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;def gunzip_file(infile, outfile=None, outdir=None, delete_original=False, force_rerun_flag=False):&#xa;    """"""Decompress a gzip file and optionally set output values.&#xa;&#xa;    Args:&#xa;        infile: Path to .gz file&#xa;        outfile: Name of output file&#xa;        outdir: Path to output directory&#xa;        delete_original: If original .gz file should be deleted&#xa;        force_rerun_flag: If file should be decompressed if outfile already exists&#xa;&#xa;    Returns:&#xa;        str: Path to decompressed file&#xa;&#xa;    """"""&#xa;    if not outfile:&#xa;        outfile = infile.replace('.gz', '')&#xa;&#xa;    if not outdir:&#xa;        outdir = ''&#xa;    else:&#xa;        outdir = op.dirname(infile)&#xa;    outfile = op.join(outdir, op.basename(outfile))&#xa;&#xa;    if force_rerun(flag=force_rerun_flag, outfile=outfile):&#xa;        gz = gzip.open(infile, ""rb"")&#xa;        decoded = gz.read()&#xa;&#xa;        with open(outfile, ""wb"") as new_file:&#xa;            new_file.write(decoded)&#xa;&#xa;        gz.close()&#xa;        log.debug('{}: file unzipped'.format(outfile))&#xa;    else:&#xa;        log.debug('{}: file already unzipped'.format(outfile))&#xa;&#xa;    if delete_original:&#xa;        os.remove(infile)&#xa;&#xa;    return outfile&#xa;&#xa;&#xa;def request_file(link, outfile, force_rerun_flag=False):&#xa;    """"""Download a file given a URL if the outfile does not exist already.&#xa;&#xa;    Args:&#xa;        link (str): Link to download file.&#xa;        outfile (str): Path to output file, will make a new file if it does not exist. Will not download if it does&#xa;            exist, unless force_rerun_flag is True.&#xa;        force_rerun_flag (bool): Flag to force re-downloading of the file if it exists already.&#xa;&#xa;    Returns:&#xa;        str: Path to downloaded file.&#xa;&#xa;    """"""&#xa;    if force_rerun(flag=force_rerun_flag, outfile=outfile):&#xa;        req = requests.get(link)&#xa;        if req.status_code == 200:&#xa;            with open(outfile, 'w') as f:&#xa;                f.write(req.text)&#xa;            log.debug('Loaded and saved {} to {}'.format(link, outfile))&#xa;        else:&#xa;            log.error('{}: request error {}'.format(link, req.status_code))&#xa;    return outfile&#xa;&#xa;&#xa;def request_json(link, outfile, force_rerun_flag, outdir=None):&#xa;    """"""Download a file in JSON format from a web request&#xa;&#xa;    Args:&#xa;        link: Link to web request&#xa;        outfile: Name of output file&#xa;        outdir: Directory of output file&#xa;        force_rerun_flag: If true, redownload the file&#xa;&#xa;    Returns:&#xa;        dict: contents of the JSON request&#xa;&#xa;    """"""&#xa;    if not outdir:&#xa;        outdir = ''&#xa;    outfile = op.join(outdir, outfile)&#xa;&#xa;    if force_rerun(flag=force_rerun_flag, outfile=outfile):&#xa;        text_raw = requests.get(link)&#xa;        my_dict = text_raw.json()&#xa;        with open(outfile, 'w') as f:&#xa;            json.dump(my_dict, f)&#xa;&#xa;        log.debug('Loaded and saved {} to {}'.format(link, outfile))&#xa;    else:&#xa;        with open(outfile, 'r') as f:&#xa;            my_dict = json.load(f)&#xa;        log.debug('Loaded {}'.format(outfile))&#xa;&#xa;    return my_dict&#xa;&#xa;&#xa;def program_exists(prog_name):&#xa;    """"""Check if a program is available as a command line executable on a system.&#xa;&#xa;    Args:&#xa;        prog_name: Name of the program.&#xa;&#xa;    Returns:&#xa;        bool: True if the program is available.&#xa;&#xa;    """"""&#xa;    if distutils.spawn.find_executable(prog_name):&#xa;        return True&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;def command_runner(shell_command, force_rerun_flag, outfile_checker, cwd=None, silent=False):&#xa;    """"""Run a shell command with subprocess, with additional options to check if output file exists and printing stdout.&#xa;&#xa;    Args:&#xa;        shell_command (str): Command as it would be formatted in the command-line (ie. ""program -i test.in -o test.out"").&#xa;        force_rerun_flag: If the program should be rerun even if the output file exists.&#xa;        outfile_checker (str): Name out the output file which may have been generated. This does not specify what the outfile&#xa;            will be, that should be done in the program's args or predetermined.&#xa;        cwd (str): Path to working directory where command will be executed.&#xa;        silent (bool): If program STDOUT should be printed to the current shell.&#xa;&#xa;    Returns:&#xa;        bool: If the program ran successfully.&#xa;&#xa;    """"""&#xa;    program_and_args = shlex.split(shell_command)&#xa;&#xa;    # Check if program is installed&#xa;    if not program_exists(program_and_args[0]):&#xa;        raise OSError('{}: program not installed'.format(program_and_args[0]))&#xa;&#xa;    # Format outfile if working in cwd&#xa;    if cwd:&#xa;        # TODO: should this be done, or should user explicitly define whole outfile path?&#xa;        outfile_checker = op.join(cwd, op.basename(outfile_checker))&#xa;&#xa;    # Check for force rerunning&#xa;    if force_rerun(flag=force_rerun_flag, outfile=outfile_checker):&#xa;        if silent:&#xa;            command = subprocess.Popen(program_and_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd)&#xa;            out, err = command.communicate()&#xa;            ret = command.returncode&#xa;        else:&#xa;            # Prints output&#xa;            for path in execute(cmd=program_and_args, cwd=cwd):&#xa;                print(path, end="""")&#xa;&#xa;        # TODO: check return code and log properly&#xa;        log.debug('{}: Ran program, output to {}'.format(program_and_args[0], outfile_checker))&#xa;    else:&#xa;        log.debug('{}: Output already exists'.format(outfile_checker))&#xa;&#xa;&#xa;def execute(cmd, cwd=None):&#xa;    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, universal_newlines=True)&#xa;    # TODO: should also print stderr or give an option for that&#xa;    for stdout_line in iter(popen.stdout.readline, """"):&#xa;        yield stdout_line&#xa;    popen.stdout.close()&#xa;    return_code = popen.wait()&#xa;    if return_code:&#xa;        raise subprocess.CalledProcessError(return_code, cmd)&#xa;&#xa;&#xa;def write_torque_script(command, outfile, walltime, queue, name, out, err, print_exec=True):&#xa;&#xa;    with open(outfile, 'w') as script:&#xa;        script.write('#PBS -l walltime={}\n'.format(walltime))&#xa;        script.write('#PBS -q regular\n')&#xa;        script.write('#PBS -N {}\n'.format(name))&#xa;        script.write('#PBS -o {}.o\n'.format(out))&#xa;        script.write('#PBS -e {}.e\n'.format(err))&#xa;        script.write('cd ${PBS_O_WORKDIR}\n')&#xa;        script.write(command)&#xa;&#xa;    os.chmod(outfile, 0o755)&#xa;&#xa;    if print_exec:&#xa;        print('qsub {};'.format(outfile))&#xa;&#xa;    return outfile&#xa;&#xa;&#xa;def dict_head(d, N=5):&#xa;    """"""Return the head of a dictionary. It will be random!&#xa;&#xa;    Default is to return the first 5 key/value pairs in a dictionary.&#xa;&#xa;    Args:&#xa;        d: Dictionary to get head.&#xa;        N: Number of elements to display.&#xa;&#xa;    Returns:&#xa;        dict: the first N items of the dictionary.&#xa;&#xa;    """"""&#xa;    return {k: d[k] for k in list(d.keys())[:N]}&#xa;&#xa;&#xa;def rank_dated_files(pattern, dir, descending=True):&#xa;    """"""Search a directory for files that match a pattern. Return an ordered list of these files by filename.&#xa;&#xa;    Args:&#xa;        pattern: The glob pattern to search for.&#xa;        dir: Path to directory where the files will be searched for.&#xa;        descending: Default True, will sort alphabetically by descending order.&#xa;&#xa;    Returns:&#xa;        list: Rank-ordered list by filename.&#xa;&#xa;    """"""&#xa;    files = glob.glob(op.join(dir, pattern))&#xa;    return sorted(files, reverse=descending)&#xa;&#xa;&#xa;def find(lst, a, case_sensitive=True):&#xa;    """"""Return indices of a list which have elements that match an object or list of objects&#xa;&#xa;    Args:&#xa;        lst: list of values&#xa;        a: object(s) to check equality&#xa;        case_sensitive: if the search should be case sensitive&#xa;&#xa;    Returns:&#xa;        list: list of indicies of lst which equal a&#xa;&#xa;    """"""&#xa;    a = force_list(a)&#xa;&#xa;    if not case_sensitive:&#xa;        lst = [x.lower() for x in lst]&#xa;        a = [y.lower() for y in a]&#xa;&#xa;    return [i for i, x in enumerate(lst) if x in a]&#xa;&#xa;&#xa;def not_find(lst, a, case_sensitive=True):&#xa;    """"""Return indices of a list which have elements that DO NOT match an object or list of objects&#xa;&#xa;    Args:&#xa;        lst: list of values&#xa;        a: object(s) to check inequality&#xa;        case_sensitive: if the search should be case sensitive&#xa;&#xa;    Returns:&#xa;        list: list of indices of lst which do not equal a&#xa;&#xa;    """"""&#xa;    a = force_list(a)&#xa;&#xa;    if not case_sensitive:&#xa;        lst = [x.lower() for x in lst]&#xa;        a = [y.lower() for y in a]&#xa;&#xa;    return [i for i, x in enumerate(lst) if x not in a]&#xa;&#xa;&#xa;def filter_list(lst, takeout, case_sensitive=True):&#xa;    """"""Return a modified list removing items specified.&#xa;&#xa;    Args:&#xa;        lst: Original list of values&#xa;        takeout: Object or objects to remove from lst&#xa;        case_sensitive: if the search should be case sensitive&#xa;&#xa;    Returns:&#xa;        list: Filtered list of values&#xa;&#xa;    """"""&#xa;    takeout = force_list(takeout)&#xa;&#xa;    if not case_sensitive:&#xa;        lst = [x.lower() for x in lst]&#xa;        takeout = [y.lower() for y in takeout]&#xa;&#xa;    return [x for x in lst if x not in takeout]&#xa;&#xa;&#xa;def filter_list_by_indices(lst, indices):&#xa;    """"""Return a modified list containing only the indices indicated.&#xa;&#xa;    Args:&#xa;        lst: Original list of values&#xa;        indices: List of indices to keep from the original list&#xa;&#xa;    Returns:&#xa;        list: Filtered list of values&#xa;&#xa;    """"""&#xa;    return [x for i, x in enumerate(lst) if i in indices]&#xa;&#xa;&#xa;def force_string(val=None):&#xa;    """"""Force a string representation of an object&#xa;&#xa;    Args:&#xa;        val: object to parse into a string&#xa;&#xa;    Returns:&#xa;        str: String representation&#xa;&#xa;    """"""&#xa;    if val is None:&#xa;        return ''&#xa;    if isinstance(val, list):&#xa;        newval = [str(x) for x in val]&#xa;        return ';'.join(newval)&#xa;    if isinstance(val, str):&#xa;        return val&#xa;    else:&#xa;        return str(val)&#xa;&#xa;&#xa;def force_list(val=None):&#xa;    """"""Force a list representation of an object&#xa;&#xa;    Args:&#xa;        val: object to parse into a list&#xa;&#xa;    Returns:&#xa;&#xa;    """"""&#xa;    if val is None:&#xa;        return []&#xa;    if isinstance(val, pd.Series):&#xa;        return val.tolist()&#xa;    return val if isinstance(val, list) else [val]&#xa;&#xa;&#xa;def force_lower_list(val=None):&#xa;    """"""Force a lowercase list representation of strings&#xa;&#xa;    Args:&#xa;        val: string or strings to parse into a list&#xa;&#xa;    Returns:&#xa;        list: with lowercase values&#xa;&#xa;    """"""&#xa;    return [x.lower() for x in force_list(val)]&#xa;&#xa;&#xa;def force_upper_list(val=None):&#xa;    """"""Force a UPPERCASE list representation of strings&#xa;&#xa;    Args:&#xa;        val: string or strings to parse into a list&#xa;&#xa;    Returns:&#xa;        list: with UPPERCASE values&#xa;&#xa;    """"""&#xa;    return [x.upper() for x in force_list(val)]&#xa;&#xa;&#xa;def split_list_by_n(l, n):&#xa;    """"""Split a list into lists of size n.&#xa;&#xa;    Args:&#xa;        l: List of stuff.&#xa;        n: Size of new lists.&#xa;&#xa;    Returns:&#xa;        list: List of lists each of size n derived from l.&#xa;&#xa;    """"""&#xa;    n = max(1, n)&#xa;    return list(l[i:i+n] for i in range(0, len(l), n))&#xa;&#xa;&#xa;def split_list_into_n_lists(l, n):&#xa;    """"""Split a list into n lists.&#xa;&#xa;    Args:&#xa;        l: List of stuff.&#xa;        n: Number of new lists to generate.&#xa;&#xa;    Returns:&#xa;        list: List of n lists.&#xa;&#xa;    """"""&#xa;    return [l[i::n] for i in range(n)]&#xa;&#xa;&#xa;def input_list_parser(infile_list):&#xa;    """"""Always return a list of files with varying input.&#xa;&#xa;    >>> input_list_parser(['/path/to/folder/'])&#xa;    ['/path/to/folder/file1.txt', '/path/to/folder/file2.txt', '/path/to/folder/file3.txt']&#xa;&#xa;    >>> input_list_parser(['/path/to/file.txt'])&#xa;    ['/path/to/file.txt']&#xa;&#xa;    >>> input_list_parser(['file1.txt'])&#xa;    ['file1.txt']&#xa;&#xa;    Args:&#xa;        infile_list: List of arguments&#xa;&#xa;    Returns:&#xa;        list: Standardized list of files&#xa;&#xa;    """"""&#xa;&#xa;    final_list_of_files = []&#xa;&#xa;    for x in infile_list:&#xa;&#xa;        # If the input is a folder&#xa;        if op.isdir(x):&#xa;            os.chdir(x)&#xa;            final_list_of_files.extend(glob.glob('*'))&#xa;&#xa;        # If the input is a file&#xa;        if op.isfile(x):&#xa;            final_list_of_files.append(x)&#xa;&#xa;    return final_list_of_files&#xa;&#xa;&#xa;def flatlist_dropdup(list_of_lists):&#xa;    """"""Make a single list out of a list of lists, and drop all duplicates.&#xa;&#xa;    Args:&#xa;        list_of_lists: List of lists.&#xa;&#xa;    Returns:&#xa;        list: List of single objects.&#xa;&#xa;    """"""&#xa;    return list(set([str(item) for sublist in list_of_lists for item in sublist]))&#xa;&#xa;&#xa;def combinations(iterable, r):&#xa;    """"""Calculate combinations&#xa;&#xa;    >>> list(combinations('ABCD',2))&#xa;    [['A', 'B'], ['A', 'C'], ['A', 'D'], ['B', 'C'], ['B', 'D'], ['C', 'D']]&#xa;&#xa;    >>> list(combinations(range(4), 3))&#xa;    [[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]]&#xa;&#xa;    Args:&#xa;        iterable: Any iterable object.&#xa;        r: Size of combination.&#xa;&#xa;    Yields:&#xa;        list: Combination of size r.&#xa;&#xa;    """"""&#xa;    pool = tuple(iterable)&#xa;    n = len(pool)&#xa;    if r > n:&#xa;        return&#xa;&#xa;    indices = list(range(r))&#xa;    yield list(pool[i] for i in indices)&#xa;&#xa;    while True:&#xa;        for i in reversed(range(r)):&#xa;            if indices[i] != i + n - r:&#xa;                break&#xa;        else:&#xa;            return&#xa;        indices[i] += 1&#xa;        for j in range(i + 1, r):&#xa;            indices[j] = indices[j - 1] + 1&#xa;        yield list(pool[i] for i in indices)&#xa;&#xa;&#xa;def percentage_to_float(x):&#xa;    """"""Convert a string representation of a percentage to float.&#xa;&#xa;    >>> percentage_to_float('55%')&#xa;    0.55&#xa;&#xa;    Args:&#xa;        x: String representation of a percentage&#xa;&#xa;    Returns:&#xa;        float: Percentage in decimal form&#xa;&#xa;    """"""&#xa;    return float(x.strip('%')) / 100&#xa;&#xa;&#xa;def conv_to_float(indata, inf_str=''):&#xa;    """"""Try to convert an arbitrary string to a float. Specify what will be replaced with ""Inf"".&#xa;    &#xa;    Args:&#xa;        indata (str): String which contains a float&#xa;        inf_str (str): If string contains something other than a float, and you want to replace it with float(""Inf""), &#xa;            specify that string here.&#xa;&#xa;    Returns:&#xa;        float: Converted string representation&#xa;&#xa;    """"""&#xa;    if indata.strip() == inf_str:&#xa;        outdata = float('Inf')&#xa;    else:&#xa;        try:&#xa;            outdata = float(indata)&#xa;        except:&#xa;            raise ValueError('Unable to convert {} to float'.format(indata))&#xa;&#xa;    return outdata&#xa;&#xa;&#xa;def make_dir(path):&#xa;    """"""Make a directory if it does not already exist&#xa;&#xa;    Args:&#xa;        path: Path to new directory&#xa;&#xa;    """"""&#xa;    if not op.exists(path):&#xa;        os.mkdir(path)&#xa;    return path&#xa;&#xa;&#xa;def remap( x, oMin, oMax, nMin, nMax ):&#xa;    """"""Map to a 0 to 1 scale&#xa;        http://stackoverflow.com/questions/929103/convert-a-number-range-to-another-range-maintaining-ratio&#xa;&#xa;    """"""&#xa;&#xa;    #range check&#xa;    if oMin == oMax:&#xa;        log.warning(""Zero input range, unable to rescale"")&#xa;        return x&#xa;&#xa;    if nMin == nMax:&#xa;        log.warning(""Zero output range, unable to rescale"")&#xa;        return x&#xa;&#xa;    #check reversed input range&#xa;    reverseInput = False&#xa;    oldMin = min( oMin, oMax )&#xa;    oldMax = max( oMin, oMax )&#xa;    if not oldMin == oMin:&#xa;        reverseInput = True&#xa;&#xa;    #check reversed output range&#xa;    reverseOutput = False&#xa;    newMin = min( nMin, nMax )&#xa;    newMax = max( nMin, nMax )&#xa;    if not newMin == nMin :&#xa;        reverseOutput = True&#xa;&#xa;    portion = (x-oldMin)*(newMax-newMin)/(oldMax-oldMin)&#xa;    if reverseInput:&#xa;        portion = (oldMax-x)*(newMax-newMin)/(oldMax-oldMin)&#xa;&#xa;    result = portion + newMin&#xa;    if reverseOutput:&#xa;        result = newMax - portion&#xa;&#xa;    return result&#xa;&#xa;&#xa;def scale_calculator(multiplier, elements, rescale=None):&#xa;    """"""Get a dictionary of scales for each element in elements.&#xa;&#xa;    Examples:&#xa;        >>> scale_calculator(1, [2,7,8])&#xa;        {8: 1, 2: 1, 7: 1}&#xa;&#xa;        >>> scale_calculator(1, [2,2,2,3,4,5,5,6,7,8])&#xa;        {2: 3, 3: 1, 4: 1, 5: 2, 6: 1, 7: 1, 8: 1}&#xa;&#xa;        >>> scale_calculator(1, [2,2,2,3,4,5,5,6,7,8], rescale=(0.5,1))&#xa;        {2: 1.0, 3: 0.5, 4: 0.5, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.5}&#xa;&#xa;        >>> scale_calculator(1, {2:3, 3:1, 4:1, 5:2, 6:1, 7:1, 8:1}, rescale=(0.5,1))&#xa;        {2: 1.0, 3: 0.5, 4: 0.5, 5: 0.75, 6: 0.5, 7: 0.5, 8: 0.5}&#xa;&#xa;        >>> scale_calculator(1, [(2,2,2),(3,),(4,),(5,),(5,),(6,7,8)], rescale=(0.5,1))&#xa;        {(2, 2, 2): 0.5, (5,): 1.0, (3,): 0.5, (6, 7, 8): 0.5, (4,): 0.5}&#xa;&#xa;    Args:&#xa;        mutiplier (int, float): Base float to be multiplied&#xa;        elements (list, dict): Dictionary which contains object:count&#xa;            or list of objects that may have repeats which will be counted&#xa;        rescale (tuple): Min and max values to rescale to&#xa;&#xa;    Returns:&#xa;        dict: Scaled values of mutiplier for each element in elements&#xa;&#xa;    """"""&#xa;&#xa;    # TODO: think about what happens when:&#xa;    # TODO: 1. there is only one (or n) of each element, and rescale is set to seomthing. what is the original min/max to scale from?&#xa;    # TODO: 2. can we normalize the scale based on other counts? (ie. other gene mutation frequencies)&#xa;&#xa;    if isinstance(elements, list):&#xa;        unique_elements = list(set(elements))&#xa;        scales = {}&#xa;        for x in unique_elements:&#xa;            count = elements.count(x)&#xa;            scales[x] = multiplier * count&#xa;    elif isinstance(elements, dict):&#xa;        scales = {}&#xa;        for k,count in elements.items():&#xa;            scales[k] = multiplier * int(count)&#xa;    else:&#xa;        raise ValueError('Input list of elements or dictionary of elements & counts')&#xa;&#xa;    if not rescale:&#xa;        return scales&#xa;    else:&#xa;        new_scales = {}&#xa;        for k,v in scales.items():&#xa;            new_scales[k] = remap(v, min(scales.values()), max(scales.values()), rescale[0], rescale[1])&#xa;        return new_scales&#xa;&#xa;&#xa;def label_sequential_regions(inlist):&#xa;    """"""Input a list of labeled tuples and return a dictionary of sequentially labeled regions.&#xa;&#xa;    Args:&#xa;        inlist (list): A list of tuples with the first number representing the index and the second the index label.&#xa;&#xa;    Returns:&#xa;        dict: Dictionary of labeled regions.&#xa;&#xa;    Examples:&#xa;&#xa;        >>> label_sequential_regions([(1, 'O'), (2, 'O'), (3, 'O'), (4, 'M'), (5, 'M'), (6, 'I'), (7, 'M'), (8, 'O'), (9, 'O')])&#xa;        {'O1': [1, 2, 3], 'M1': [4, 5], 'I1': [6], 'M2': [7], 'O2': [8, 9]}&#xa;&#xa;    """"""&#xa;    import more_itertools as mit&#xa;&#xa;    df = pd.DataFrame(inlist).set_index(0)&#xa;&#xa;    labeled = {}&#xa;    for label in df[1].unique():&#xa;        iterable = df[df[1] == label].index.tolist()&#xa;        labeled.update({'{}{}'.format(label, i + 1): items for i, items in&#xa;                        enumerate([list(group) for group in mit.consecutive_groups(iterable)])})&#xa;&#xa;    return labeled&#xa;&#xa;&#xa;def check_condition(left, condition, right):&#xa;    ops = {'>': operator.gt,&#xa;           '<': operator.lt,&#xa;           '>=': operator.ge,&#xa;           '<=': operator.le,&#xa;           '=': operator.eq}&#xa;    if condition not in ops:&#xa;        raise KeyError('{}: condition not supported'.format(condition))&#xa;    return ops[condition](left, right)&#xa;"
21458387|"import time&#xa;&#xa;from django.db import transaction&#xa;from django.db.utils import IntegrityError&#xa;from django.test import TestCase&#xa;&#xa;from main.models import (&#xa;    User, UserFile, UserDir, Chunk, VersionChunk, Option, Storage,&#xa;    ChunkStorage,&#xa;)&#xa;from main.fs.clouds.base import BaseOAuth2APIClient&#xa;from main.fs.array import ArrayClient&#xa;&#xa;&#xa;class UserDirTestCase(TestCase):&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create(email='foo@bar.org')&#xa;&#xa;    def test_create(self):&#xa;        with self.assertRaises(ValueError):&#xa;            UserDir.objects.create(path='/foobar')&#xa;&#xa;        dir1 = UserDir.objects.create(path='/foobar', user=self.user)&#xa;        self.assertEqual('/foobar', dir1.path)&#xa;&#xa;        dir2 = UserDir.objects.create(path='/foobar/foo/bar', user=self.user)&#xa;        self.assertEqual('/foobar/foo/bar', dir2.path)&#xa;&#xa;        dir1.delete()&#xa;&#xa;&#xa;class UserFileTestCase(TestCase):&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create(email='foo@bar.org')&#xa;&#xa;    def test_create(self):&#xa;        with self.assertRaises(ValueError):&#xa;            UserFile.objects.create(path='/foo/bar')&#xa;&#xa;        dir1 = UserDir.objects.create(path='/foo', user=self.user)&#xa;        file1 = UserFile.objects.create(path='/foo/bar.txt', user=self.user)&#xa;&#xa;        self.assertEqual(dir1, UserDir.objects.get(uid=dir1.uid))&#xa;        self.assertEqual(file1, UserFile.objects.get(uid=file1.uid))&#xa;&#xa;        self.assertEqual('.txt', file1.extension)&#xa;&#xa;        # Two to account for ""root""&#xa;        self.assertEqual(2, UserDir.objects.all().count())&#xa;        self.assertEqual(1, UserFile.objects.all().count())&#xa;&#xa;        dir1.delete()&#xa;&#xa;        self.assertEqual(1, UserDir.objects.all().count())&#xa;        self.assertEqual(0, UserFile.objects.all().count())&#xa;&#xa;    def test_chunks(self):&#xa;        file = UserFile.objects.create(&#xa;            path='/foo/bar', user=self.user,&#xa;            parent=UserDir.objects.create(path='/foo', user=self.user))&#xa;&#xa;        chunk1 = Chunk.objects.create(size=1024, user=self.user)&#xa;        chunk2 = Chunk.objects.create(size=1024, user=self.user)&#xa;        versionchunk = file.file.version.add_chunk(chunk1)&#xa;&#xa;        self.assertEqual(1, versionchunk.serial)&#xa;&#xa;        storage = Storage.objects.create(&#xa;            user=self.user, type=Storage.TYPE_DROPBOX)&#xa;        ChunkStorage.objects.create(chunk=chunk1, storage=storage)&#xa;&#xa;        self.assertTrue(&#xa;            VersionChunk.objects.filter(&#xa;                version=file.file.version, chunk=chunk1).exists())&#xa;&#xa;        file.file.version.add_chunk(chunk2)&#xa;&#xa;        self.assertEqual(&#xa;            2, VersionChunk.objects.filter(version=file.file.version).count())&#xa;&#xa;        for i, chunk in enumerate(&#xa;            VersionChunk.objects.filter(&#xa;                version=file.file.version).order_by('serial')):&#xa;            self.assertEqual(i + 1, chunk.serial)&#xa;&#xa;        file.delete()&#xa;&#xa;&#xa;class UserTestCase(TestCase):&#xa;    def test_create(self):&#xa;        user = User.objects.create_user('foo@bar.org', full_name='Foo Bar')&#xa;        self.assertEqual(False, user.is_admin)&#xa;        self.assertEqual(False, user.is_staff)&#xa;        self.assertEqual('Foo', user.first_name)&#xa;&#xa;        # http://stackoverflow.com/questions/21458387/transactionmanagementerror-you-cant-execute-queries-until-the-end-of-the-atom  # noqa&#xa;        with transaction.atomic():&#xa;            with self.assertRaises(IntegrityError):&#xa;                User.objects.create_user('foo@bar.org')&#xa;&#xa;        superuser = User.objects.create_superuser('bar@foo.org', 'foobar',&#xa;                                                  full_name='Bar Foo')&#xa;        self.assertTrue(superuser.is_admin)&#xa;        self.assertTrue(superuser.is_staff)&#xa;        self.assertTrue(superuser.has_perm('any_perm'))&#xa;        self.assertTrue(superuser.has_module_perms('any_app'))&#xa;        self.assertEqual('Bar', superuser.first_name)&#xa;        self.assertEqual('Bar Foo', superuser.get_full_name())&#xa;        self.assertEqual('Bar', superuser.get_short_name())&#xa;&#xa;    def test_fail(self):&#xa;        with self.assertRaises(ValueError):&#xa;            User.objects.create_user('')&#xa;&#xa;    def test_options(self):&#xa;        user = User.objects.create_user('foo@bar.org', full_name='Foo Bar')&#xa;        options = Option.objects.create(user=user)&#xa;        self.assertEqual(1, options.raid_level)&#xa;        self.assertEqual(1, options.raid_replicas)&#xa;&#xa;&#xa;class UserStorageTestCase(TestCase):&#xa;    """"""&#xa;    Test user storage (usable storage instances).&#xa;    """"""&#xa;&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create_user('foo@bar.org',&#xa;                                            full_name='Foo Bar')&#xa;&#xa;    def test_oauth2(self):&#xa;        storage = Storage.objects.create(type=Storage.TYPE_DROPBOX,&#xa;                                         user=self.user)&#xa;&#xa;        client = storage.get_client()&#xa;        self.assertIsInstance(client, BaseOAuth2APIClient)&#xa;&#xa;        kwargs = {&#xa;            'access_token': 'AAAA',&#xa;            'refresh_token': 'BBBB',&#xa;            'expires_in': 10,&#xa;        }&#xa;        storage.auth.update(**kwargs)&#xa;        self.assertEqual('AAAA', storage.auth['access_token'])&#xa;        self.assertEqual('BBBB', storage.auth['refresh_token'])&#xa;        storage.auth.update({&#xa;            'access_token': 'CCCC',&#xa;            'expires_at': time.time()&#xa;        })&#xa;        self.assertEqual('CCCC', storage.auth['access_token'])&#xa;        self.assertEqual('BBBB', storage.auth['refresh_token'])&#xa;&#xa;    def test_array(self):&#xa;        array = Storage.objects.create(type=Storage.TYPE_ARRAY,&#xa;                                       user=self.user)&#xa;&#xa;        # Name should have usable default (is system-provided).&#xa;        self.assertIsNotNone(array.name)&#xa;&#xa;        obj = array.get_client()&#xa;        self.assertIsInstance(obj, ArrayClient)&#xa;&#xa;    def test_basic(self):&#xa;        storage = Storage.objects.create(&#xa;            type=Storage.TYPE_BASIC, user=self.user)&#xa;&#xa;        with self.assertRaises(NotImplementedError):&#xa;            storage.get_client()&#xa;"
136168|"#&#xa;# Licensed to the Apache Software Foundation (ASF) under one&#xa;# or more contributor license agreements.  See the NOTICE file&#xa;# distributed with this work for additional information&#xa;# regarding copyright ownership.  The ASF licenses this file&#xa;# to you under the Apache License, Version 2.0 (the&#xa;# ""License""); you may not use this file except in compliance&#xa;# with the License.  You may obtain a copy of the License at&#xa;#&#xa;#   http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing,&#xa;# software distributed under the License is distributed on an&#xa;# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY&#xa;# KIND, either express or implied.  See the License for the&#xa;# specific language governing permissions and limitations&#xa;# under the License.&#xa;#&#xa;&#xa;from __future__ import print_function&#xa;&#xa;import atexit as _atexit&#xa;import binascii as _binascii&#xa;import codecs as _codecs&#xa;import collections as _collections&#xa;import fnmatch as _fnmatch&#xa;import getpass as _getpass&#xa;import json as _json&#xa;import os as _os&#xa;import random as _random&#xa;import re as _re&#xa;import shlex as _shlex&#xa;import shutil as _shutil&#xa;import signal as _signal&#xa;import socket as _socket&#xa;import subprocess as _subprocess&#xa;import sys as _sys&#xa;import tarfile as _tarfile&#xa;import tempfile as _tempfile&#xa;import time as _time&#xa;import traceback as _traceback&#xa;import types as _types&#xa;import uuid as _uuid&#xa;&#xa;from subprocess import CalledProcessError&#xa;from subprocess import PIPE&#xa;&#xa;# See documentation at http://www.ssorj.net/projects/plano.html&#xa;&#xa;LINE_SEP = _os.linesep&#xa;PATH_SEP = _os.sep&#xa;PATH_VAR_SEP = _os.pathsep&#xa;ENV = _os.environ&#xa;ARGS = _sys.argv&#xa;&#xa;STD_IN = _sys.stdin&#xa;STD_OUT = _sys.stdout&#xa;STD_ERR = _sys.stderr&#xa;NULL_DEV = _os.devnull&#xa;&#xa;_message_levels = (&#xa;    ""debug"",&#xa;    ""notice"",&#xa;    ""warn"",&#xa;    ""error"",&#xa;)&#xa;&#xa;_debug = _message_levels.index(""debug"")&#xa;_notice = _message_levels.index(""notice"")&#xa;_warn = _message_levels.index(""warn"")&#xa;_error = _message_levels.index(""error"")&#xa;&#xa;_message_output = STD_ERR&#xa;_message_threshold = _notice&#xa;&#xa;def set_message_output(writeable):&#xa;    global _message_output&#xa;    _message_output = writeable&#xa;&#xa;def set_message_threshold(level):&#xa;    assert level in _message_levels&#xa;&#xa;    global _message_threshold&#xa;    _message_threshold = _message_levels.index(level)&#xa;&#xa;def fail(message, *args):&#xa;    error(message, *args)&#xa;&#xa;    if isinstance(message, BaseException):&#xa;        raise message&#xa;&#xa;    raise Exception(message.format(*args))&#xa;&#xa;def error(message, *args):&#xa;    _print_message(""Error"", message, args)&#xa;&#xa;def warn(message, *args):&#xa;    if _message_threshold <= _warn:&#xa;        _print_message(""Warning"", message, args)&#xa;&#xa;def notice(message, *args):&#xa;    if _message_threshold <= _notice:&#xa;        _print_message(None, message, args)&#xa;&#xa;def debug(message, *args):&#xa;    if _message_threshold <= _debug:&#xa;        _print_message(""Debug"", message, args)&#xa;&#xa;def exit(arg=None, *args):&#xa;    if arg in (0, None):&#xa;        _sys.exit()&#xa;&#xa;    if _is_string(arg):&#xa;        error(arg, args)&#xa;        _sys.exit(1)&#xa;    elif isinstance(arg, int):&#xa;        if arg > 0:&#xa;            error(""Exiting with code {0}"", arg)&#xa;        else:&#xa;            notice(""Exiting with code {0}"", arg)&#xa;&#xa;        _sys.exit(arg)&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;def _print_message(category, message, args):&#xa;    if _message_output is None:&#xa;        return&#xa;&#xa;    message = _format_message(category, message, args)&#xa;&#xa;    print(message, file=_message_output)&#xa;&#xa;    _message_output.flush()&#xa;&#xa;def _format_message(category, message, args):&#xa;    if not _is_string(message):&#xa;        message = str(message)&#xa;&#xa;    if args:&#xa;        message = message.format(*args)&#xa;&#xa;    if len(message) > 0 and message[0].islower():&#xa;        message = message[0].upper() + message[1:]&#xa;&#xa;    if category:&#xa;        message = ""{0}: {1}"".format(category, message)&#xa;&#xa;    program = program_name()&#xa;    message = ""{0}: {1}"".format(program, message)&#xa;&#xa;    return message&#xa;&#xa;def eprint(*args, **kwargs):&#xa;    print(*args, file=_sys.stderr, **kwargs)&#xa;&#xa;def flush():&#xa;    _sys.stdout.flush()&#xa;    _sys.stderr.flush()&#xa;&#xa;absolute_path = _os.path.abspath&#xa;normalize_path = _os.path.normpath&#xa;real_path = _os.path.realpath&#xa;exists = _os.path.exists&#xa;is_absolute = _os.path.isabs&#xa;is_dir = _os.path.isdir&#xa;is_file = _os.path.isfile&#xa;is_link = _os.path.islink&#xa;file_size = _os.path.getsize&#xa;&#xa;join = _os.path.join&#xa;split = _os.path.split&#xa;split_extension = _os.path.splitext&#xa;&#xa;current_dir = _os.getcwd&#xa;sleep = _time.sleep&#xa;&#xa;def home_dir(user=""""):&#xa;    return _os.path.expanduser(""~{0}"".format(user))&#xa;&#xa;def parent_dir(path):&#xa;    path = normalize_path(path)&#xa;    parent, child = split(path)&#xa;&#xa;    return parent&#xa;&#xa;def file_name(file):&#xa;    file = normalize_path(file)&#xa;    dir, name = split(file)&#xa;&#xa;    return name&#xa;&#xa;def name_stem(file):&#xa;    name = file_name(file)&#xa;&#xa;    if name.endswith("".tar.gz""):&#xa;        name = name[:-3]&#xa;&#xa;    stem, ext = split_extension(name)&#xa;&#xa;    return stem&#xa;&#xa;def name_extension(file):&#xa;    name = file_name(file)&#xa;    stem, ext = split_extension(name)&#xa;&#xa;    return ext&#xa;&#xa;def program_name(command=None):&#xa;    if command is None:&#xa;        args = ARGS&#xa;    else:&#xa;        args = command.split()&#xa;&#xa;    for arg in args:&#xa;        if ""="" not in arg:&#xa;            return file_name(arg)&#xa;&#xa;def which(program_name):&#xa;    for dir in ENV[""PATH""].split(PATH_VAR_SEP):&#xa;        program = join(dir, program_name)&#xa;&#xa;        if _os.access(program, _os.X_OK):&#xa;            return program&#xa;&#xa;def read(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return f.read()&#xa;&#xa;def write(file, string):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        f.write(string)&#xa;&#xa;    return file&#xa;&#xa;def append(file, string):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""a"") as f:&#xa;        f.write(string)&#xa;&#xa;    return file&#xa;&#xa;def prepend(file, string):&#xa;    orig = read(file)&#xa;    prepended = string + orig&#xa;&#xa;    return write(file, prepended)&#xa;&#xa;# XXX Should this work on directories?&#xa;def touch(file):&#xa;    return append(file, """")&#xa;&#xa;def tail(file, n):&#xa;    return """".join(tail_lines(file, n))&#xa;&#xa;def read_lines(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return f.readlines()&#xa;&#xa;def write_lines(file, lines):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        f.writelines(lines)&#xa;&#xa;    return file&#xa;&#xa;def append_lines(file, lines):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""a"") as f:&#xa;        f.writelines(string)&#xa;&#xa;    return file&#xa;&#xa;def prepend_lines(file, lines):&#xa;    orig_lines = read_lines(file)&#xa;&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        f.writelines(lines)&#xa;        f.writelines(orig_lines)&#xa;&#xa;    return file&#xa;&#xa;# Derived from http://stackoverflow.com/questions/136168/get-last-n-lines-of-a-file-with-python-similar-to-tail&#xa;def tail_lines(file, n):&#xa;    assert n >= 0&#xa;&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        pos = n + 1&#xa;        lines = list()&#xa;&#xa;        while len(lines) <= n:&#xa;                try:&#xa;                    f.seek(-pos, 2)&#xa;                except IOError:&#xa;                    f.seek(0)&#xa;                    break&#xa;                finally:&#xa;                    lines = f.readlines()&#xa;&#xa;                pos *= 2&#xa;&#xa;        return lines[-n:]&#xa;&#xa;def read_json(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return _json.load(f)&#xa;&#xa;def write_json(file, obj):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        return _json.dump(obj, f, indent=4, separators=("","", "": ""), sort_keys=True)&#xa;&#xa;def make_temp_file(suffix=""""):&#xa;    return _tempfile.mkstemp(prefix=""plano-"", suffix=suffix)[1]&#xa;&#xa;def make_temp_dir(suffix=""""):&#xa;    return _tempfile.mkdtemp(prefix=""plano-"", suffix=suffix)&#xa;&#xa;def make_user_temp_dir():&#xa;    temp_dir = _tempfile.gettempdir()&#xa;    user = _getpass.getuser()&#xa;    user_temp_dir = join(temp_dir, user)&#xa;&#xa;    return make_dir(user_temp_dir)&#xa;&#xa;class temp_file(object):&#xa;    def __init__(self, suffix=""""):&#xa;        self.file = make_temp_file(suffix=suffix)&#xa;&#xa;    def __enter__(self):&#xa;        return self.file&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        if exists(self.file):&#xa;            _os.remove(self.file)&#xa;&#xa;class temp_dir(object):&#xa;    def __init__(self, suffix=""""):&#xa;        self.dir = make_temp_dir(suffix=suffix)&#xa;&#xa;    def __enter__(self):&#xa;        return self.dir&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        if exists(self.dir):&#xa;            _shutil.rmtree(self.dir, ignore_errors=True)&#xa;&#xa;def unique_id(length=16):&#xa;    assert length >= 1&#xa;    assert length <= 16&#xa;&#xa;    uuid_bytes = _uuid.uuid4().bytes&#xa;    uuid_bytes = uuid_bytes[:length]&#xa;&#xa;    return _binascii.hexlify(uuid_bytes).decode(""utf-8"")&#xa;&#xa;def copy(from_path, to_path):&#xa;    notice(""Copying '{0}' to '{1}'"", from_path, to_path)&#xa;&#xa;    if is_dir(to_path):&#xa;        to_path = join(to_path, file_name(from_path))&#xa;    else:&#xa;        make_dir(parent_dir(to_path))&#xa;&#xa;    if is_dir(from_path):&#xa;        _copytree(from_path, to_path, symlinks=True)&#xa;    else:&#xa;        _shutil.copy(from_path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def move(from_path, to_path):&#xa;    notice(""Moving '{0}' to '{1}'"", from_path, to_path)&#xa;&#xa;    if is_dir(to_path):&#xa;        to_path = join(to_path, file_name(from_path))&#xa;    else:&#xa;        make_dir(parent_dir(to_path))&#xa;&#xa;    _shutil.move(from_path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def rename(path, expr, replacement):&#xa;    path = normalize_path(path)&#xa;    parent_dir, name = split(path)&#xa;    to_name = string_replace(name, expr, replacement)&#xa;    to_path = join(parent_dir, to_name)&#xa;&#xa;    notice(""Renaming '{0}' to '{1}'"", path, to_path)&#xa;&#xa;    move(path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def remove(path):&#xa;    notice(""Removing '{0}'"", path)&#xa;&#xa;    if not exists(path):&#xa;        return&#xa;&#xa;    if is_dir(path):&#xa;        _shutil.rmtree(path, ignore_errors=True)&#xa;    else:&#xa;        _os.remove(path)&#xa;&#xa;    return path&#xa;&#xa;def make_link(source_path, link_file):&#xa;    notice(""Making link '{0}' to '{1}'"", link_file, source_path)&#xa;&#xa;    if exists(link_file):&#xa;        assert read_link(link_file) == source_path&#xa;        return&#xa;&#xa;    link_dir = parent_dir(link_file)&#xa;&#xa;    if link_dir:&#xa;        make_dir(link_dir)&#xa;&#xa;    _os.symlink(source_path, link_file)&#xa;&#xa;    return link_file&#xa;&#xa;def read_link(file):&#xa;    return _os.readlink(file)&#xa;&#xa;def find(dir, *patterns):&#xa;    matched_paths = set()&#xa;&#xa;    if not patterns:&#xa;        patterns = (""*"",)&#xa;&#xa;    for root, dirs, files in _os.walk(dir):&#xa;        for pattern in patterns:&#xa;            matched_dirs = _fnmatch.filter(dirs, pattern)&#xa;            matched_files = _fnmatch.filter(files, pattern)&#xa;&#xa;            matched_paths.update([join(root, x) for x in matched_dirs])&#xa;            matched_paths.update([join(root, x) for x in matched_files])&#xa;&#xa;    return sorted(matched_paths)&#xa;&#xa;def find_any_one(dir, *patterns):&#xa;    paths = find(dir, *patterns)&#xa;&#xa;    if len(paths) == 0:&#xa;        return&#xa;&#xa;    return paths[0]&#xa;&#xa;def find_only_one(dir, *patterns):&#xa;    paths = find(dir, *patterns)&#xa;&#xa;    if len(paths) == 0:&#xa;        return&#xa;&#xa;    assert len(paths) == 1&#xa;&#xa;    return paths[0]&#xa;&#xa;# find_via_expr?&#xa;&#xa;def string_replace(string, expr, replacement, count=0):&#xa;    return _re.sub(expr, replacement, string, count)&#xa;&#xa;def make_dir(dir):&#xa;    if not exists(dir):&#xa;        _os.makedirs(dir)&#xa;&#xa;    return dir&#xa;&#xa;# Returns the current working directory so you can change it back&#xa;def change_dir(dir):&#xa;    notice(""Changing directory to '{0}'"", dir)&#xa;&#xa;    cwd = current_dir()&#xa;    _os.chdir(dir)&#xa;    return cwd&#xa;&#xa;def list_dir(dir, *patterns):&#xa;    assert is_dir(dir)&#xa;&#xa;    names = _os.listdir(dir)&#xa;&#xa;    if not patterns:&#xa;        return sorted(names)&#xa;&#xa;    matched_names = set()&#xa;&#xa;    for pattern in patterns:&#xa;        matched_names.update(_fnmatch.filter(names, pattern))&#xa;&#xa;    return sorted(matched_names)&#xa;&#xa;class working_dir(object):&#xa;    def __init__(self, dir):&#xa;        self.dir = dir&#xa;        self.prev_dir = None&#xa;&#xa;    def __enter__(self):&#xa;        self.prev_dir = change_dir(self.dir)&#xa;        return self.dir&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        change_dir(self.prev_dir)&#xa;&#xa;class temp_working_dir(working_dir):&#xa;    def __init__(self):&#xa;        super(temp_working_dir, self).__init__(make_temp_dir())&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        super(temp_working_dir, self).__exit__(exc_type, exc_value, traceback)&#xa;&#xa;        if exists(self.dir):&#xa;            _shutil.rmtree(self.dir, ignore_errors=True)&#xa;&#xa;def call(command, *args, **kwargs):&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    check_process(proc)&#xa;&#xa;def call_for_exit_code(command, *args, **kwargs):&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    return wait_for_process(proc)&#xa;&#xa;def call_for_output(command, *args, **kwargs):&#xa;    kwargs[""stdout""] = _subprocess.PIPE&#xa;&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    output = proc.communicate()[0]&#xa;    exit_code = proc.poll()&#xa;&#xa;    # XXX I don't know if None is possible here&#xa;    assert exit_code is not None&#xa;&#xa;    if exit_code not in (None, 0):&#xa;        error = CalledProcessError(exit_code, proc.command_string)&#xa;        error.output = output&#xa;&#xa;        raise error&#xa;&#xa;    return output&#xa;&#xa;def call_and_print_on_error(command, *args, **kwargs):&#xa;    with temp_file() as output_file:&#xa;        try:&#xa;            with open(output_file, ""w"") as out:&#xa;                kwargs[""output""] = out&#xa;                call(command, *args, **kwargs)&#xa;        except CalledProcessError:&#xa;            eprint(read(output_file), end="""")&#xa;            raise&#xa;&#xa;_child_processes = list()&#xa;&#xa;class _Process(_subprocess.Popen):&#xa;    def __init__(self, command, options, name, command_string):&#xa;        super(_Process, self).__init__(command, **options)&#xa;&#xa;        self.name = name&#xa;        self.command_string = command_string&#xa;&#xa;        _child_processes.append(self)&#xa;&#xa;    @property&#xa;    def exit_code(self):&#xa;        return self.returncode&#xa;&#xa;    def __enter__(self):&#xa;        return self&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        stop_process(self.proc)&#xa;&#xa;    def __repr__(self):&#xa;        return ""process {0} ({1})"".format(self.pid, self.name)&#xa;&#xa;def default_sigterm_handler(signum, frame):&#xa;    for proc in _child_processes:&#xa;        if proc.poll() is None:&#xa;            proc.terminate()&#xa;&#xa;    #_remove_temp_dir()&#xa;&#xa;    exit(-(_signal.SIGTERM))&#xa;&#xa;_signal.signal(_signal.SIGTERM, default_sigterm_handler)&#xa;&#xa;def _command_string(command, args):&#xa;    elems = [""\""{0}\"""".format(x) if "" "" in x else x for x in command]&#xa;    string = "" "".join(elems)&#xa;    string = string.format(*args)&#xa;&#xa;    return string&#xa;&#xa;def start_process(command, *args, **kwargs):&#xa;    if _is_string(command):&#xa;        command = command.format(*args)&#xa;        command_args = _shlex.split(command)&#xa;        command_string = command&#xa;    elif isinstance(command, _collections.Iterable):&#xa;        assert len(args) == 0, args&#xa;        command_args = command&#xa;        command_string = _command_string(command, [])&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;    notice(""Calling '{0}'"", command_string)&#xa;&#xa;    name = kwargs.get(""name"", command_args[0])&#xa;&#xa;    kwargs[""stdout""] = kwargs.get(""stdout"", _sys.stdout)&#xa;    kwargs[""stderr""] = kwargs.get(""stderr"", _sys.stderr)&#xa;&#xa;    if ""output"" in kwargs:&#xa;        out = kwargs.pop(""output"")&#xa;&#xa;        kwargs[""stdout""] = out&#xa;        kwargs[""stderr""] = out&#xa;&#xa;    if ""shell"" in kwargs and kwargs[""shell""] is True:&#xa;        proc = _Process(command_string, kwargs, name, command_string)&#xa;    else:&#xa;        proc = _Process(command_args, kwargs, name, command_string)&#xa;&#xa;    debug(""{0} started"", proc)&#xa;&#xa;    return proc&#xa;&#xa;def terminate_process(proc):&#xa;    notice(""Terminating {0}"", proc)&#xa;&#xa;    if proc.poll() is None:&#xa;        proc.terminate()&#xa;    else:&#xa;        debug(""{0} already exited"", proc)&#xa;&#xa;def stop_process(proc):&#xa;    notice(""Stopping {0}"", proc)&#xa;&#xa;    if proc.poll() is not None:&#xa;        if proc.returncode == 0:&#xa;            debug(""{0} already exited normally"", proc)&#xa;        elif proc.returncode == -(_signal.SIGTERM):&#xa;            debug(""{0} was already terminated"", proc)&#xa;        else:&#xa;            debug(""{0} already exited with code {1}"", proc, proc.returncode)&#xa;&#xa;        return&#xa;&#xa;    proc.terminate()&#xa;&#xa;    return wait_for_process(proc)&#xa;&#xa;def wait_for_process(proc):&#xa;    debug(""Waiting for {0} to exit"", proc)&#xa;&#xa;    proc.wait()&#xa;&#xa;    if proc.returncode == 0:&#xa;        debug(""{0} exited normally"", proc)&#xa;    elif proc.returncode == -(_signal.SIGTERM):&#xa;        debug(""{0} exited after termination"", proc)&#xa;    else:&#xa;        debug(""{0} exited with code {1}"", proc, proc.returncode)&#xa;&#xa;    return proc.returncode&#xa;&#xa;def check_process(proc):&#xa;    wait_for_process(proc)&#xa;&#xa;    if proc.returncode != 0:&#xa;        raise CalledProcessError(proc.returncode, proc.command_string)&#xa;&#xa;def exec_process(command, *args):&#xa;    if _is_string(command):&#xa;        command = command.format(*args)&#xa;        command_args = _shlex.split(command)&#xa;        command_string = command&#xa;    elif isinstance(command, _collections.Iterable):&#xa;        assert len(args) == 0, args&#xa;        command_args = command&#xa;        command_string = _command_string(command, [])&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;    notice(""Calling '{0}'"", command_string)&#xa;&#xa;    _os.execvp(command_args[0], command_args[1:])&#xa;&#xa;def make_archive(input_dir, output_dir, archive_stem):&#xa;    # XXX Cleanup temp dir&#xa;&#xa;    temp_dir = make_temp_dir()&#xa;    temp_input_dir = join(temp_dir, archive_stem)&#xa;&#xa;    copy(input_dir, temp_input_dir)&#xa;    make_dir(output_dir)&#xa;&#xa;    output_file = ""{0}.tar.gz"".format(join(output_dir, archive_stem))&#xa;    output_file = absolute_path(output_file)&#xa;&#xa;    with working_dir(temp_dir):&#xa;        call(""tar -czf {0} {1}"", output_file, archive_stem)&#xa;&#xa;    return output_file&#xa;&#xa;def extract_archive(archive_file, output_dir):&#xa;    assert is_file(archive_file)&#xa;&#xa;    if not exists(output_dir):&#xa;        make_dir(output_dir)&#xa;&#xa;    archive_file = absolute_path(archive_file)&#xa;&#xa;    with working_dir(output_dir):&#xa;        call(""tar -xf {0}"", archive_file)&#xa;&#xa;    return output_dir&#xa;&#xa;def rename_archive(archive_file, new_archive_stem):&#xa;    # XXX Cleanup temp dir&#xa;&#xa;    assert is_file(archive_file)&#xa;&#xa;    if name_stem(archive_file) == new_archive_stem:&#xa;        return archive_file&#xa;&#xa;    temp_dir = make_temp_dir()&#xa;&#xa;    extract_archive(archive_file, temp_dir)&#xa;&#xa;    input_name = list_dir(temp_dir)[0]&#xa;    input_dir = join(temp_dir, input_name)&#xa;    output_file = make_archive(input_dir, temp_dir, new_archive_stem)&#xa;    output_name = file_name(output_file)&#xa;    archive_dir = parent_dir(archive_file)&#xa;    new_archive_file = join(archive_dir, output_name)&#xa;&#xa;    move(output_file, new_archive_file)&#xa;    remove(archive_file)&#xa;&#xa;    return new_archive_file&#xa;&#xa;def random_port(min=49152, max=65535):&#xa;    return _random.randint(min, max)&#xa;&#xa;def wait_for_port(port, host="""", timeout=30):&#xa;    if _is_string(port):&#xa;        port = int(port)&#xa;&#xa;    sock = _socket.socket(_socket.AF_INET, _socket.SOCK_STREAM)&#xa;    sock.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, 1)&#xa;&#xa;    start = _time.time()&#xa;&#xa;    try:&#xa;        while True:&#xa;            if sock.connect_ex((host, port)) == 0:&#xa;                return&#xa;&#xa;            sleep(0.1)&#xa;&#xa;            if _time.time() - start > timeout:&#xa;                fail(""Timed out waiting for port {0} to open"", port)&#xa;    finally:&#xa;        sock.close()&#xa;&#xa;# Modified copytree impl that allows for already existing destination&#xa;# dirs&#xa;def _copytree(src, dst, symlinks=False, ignore=None):&#xa;    """"""Recursively copy a directory tree using copy2().&#xa;&#xa;    If exception(s) occur, an Error is raised with a list of reasons.&#xa;&#xa;    If the optional symlinks flag is true, symbolic links in the&#xa;    source tree result in symbolic links in the destination tree; if&#xa;    it is false, the contents of the files pointed to by symbolic&#xa;    links are copied.&#xa;&#xa;    The optional ignore argument is a callable. If given, it&#xa;    is called with the `src` parameter, which is the directory&#xa;    being visited by copytree(), and `names` which is the list of&#xa;    `src` contents, as returned by os.listdir():&#xa;&#xa;        callable(src, names) -> ignored_names&#xa;&#xa;    Since copytree() is called recursively, the callable will be&#xa;    called once for each directory that is copied. It returns a&#xa;    list of names relative to the `src` directory that should&#xa;    not be copied.&#xa;&#xa;    XXX Consider this example code rather than the ultimate tool.&#xa;&#xa;    """"""&#xa;    names = _os.listdir(src)&#xa;    if ignore is not None:&#xa;        ignored_names = ignore(src, names)&#xa;    else:&#xa;        ignored_names = set()&#xa;&#xa;    if not exists(dst):&#xa;        _os.makedirs(dst)&#xa;    errors = []&#xa;    for name in names:&#xa;        if name in ignored_names:&#xa;            continue&#xa;        srcname = _os.path.join(src, name)&#xa;        dstname = _os.path.join(dst, name)&#xa;        try:&#xa;            if symlinks and _os.path.islink(srcname):&#xa;                linkto = _os.readlink(srcname)&#xa;                _os.symlink(linkto, dstname)&#xa;            elif _os.path.isdir(srcname):&#xa;                _copytree(srcname, dstname, symlinks, ignore)&#xa;            else:&#xa;                # Will raise a SpecialFileError for unsupported file types&#xa;                _shutil.copy2(srcname, dstname)&#xa;        # catch the Error from the recursive copytree so that we can&#xa;        # continue with other files&#xa;        except _shutil.Error as err:&#xa;            errors.extend(err.args[0])&#xa;        except EnvironmentError as why:&#xa;            errors.append((srcname, dstname, str(why)))&#xa;    try:&#xa;        _shutil.copystat(src, dst)&#xa;    except OSError as why:&#xa;        if _shutil.WindowsError is not None and isinstance \&#xa;               (why, _shutil.WindowsError):&#xa;            # Copying file access times may fail on Windows&#xa;            pass&#xa;        else:&#xa;            errors.append((src, dst, str(why)))&#xa;    if errors:&#xa;        raise _shutil.Error(errors)&#xa;&#xa;def _is_string(obj):&#xa;    try:&#xa;        return isinstance(obj, basestring)&#xa;    except NameError:&#xa;        return isinstance(obj, str)&#xa;"
9619101|"#!/usr/bin/env python&#xa;# -*- coding: utf-8 -*-&#xa;&#xa;"""""" Convert Markdown file to html, which is embeded in html template.&#xa;""""""&#xa;&#xa;from __future__ import (print_function, with_statement, unicode_literals,&#xa;                        absolute_import)&#xa;&#xa;import os&#xa;import os.path&#xa;import io&#xa;import copy&#xa;import traceback&#xa;try:&#xa;    from collections import OrderedDict&#xa;except ImportError:&#xa;    from ordereddict import OrderedDict&#xa;&#xa;import markdown&#xa;import yaml&#xa;from jinja2 import (Environment, FileSystemLoader, TemplateError)&#xa;&#xa;from simiki import jinja_exts&#xa;from simiki.compat import is_py2, is_py3&#xa;&#xa;if is_py3:&#xa;    from functools import cmp_to_key&#xa;&#xa;PLAT_LINE_SEP = '\n'&#xa;&#xa;&#xa;class BaseGenerator(object):&#xa;    """"""Base generator class""""""&#xa;&#xa;    def __init__(self, site_config, base_path):&#xa;        '''&#xa;        :site_config: site global configuration parsed from _config.yml&#xa;        :base_path: root path of wiki directory&#xa;        '''&#xa;        self.site_config = copy.deepcopy(site_config)&#xa;        self.base_path = base_path&#xa;        _template_path = os.path.join(&#xa;            self.base_path,&#xa;            site_config[""themes_dir""],&#xa;            site_config[""theme""]&#xa;        )&#xa;        if not os.path.exists(_template_path):&#xa;            raise Exception(""Theme `{0}' not exists"".format(_template_path))&#xa;        self.env = Environment(&#xa;            loader=FileSystemLoader(_template_path)&#xa;        )&#xa;        self._jinja_load_exts()&#xa;&#xa;    def _jinja_load_exts(self):&#xa;        '''Load jinja custom filters and extensions'''&#xa;        for _filter in jinja_exts.filters:&#xa;            self.env.filters[_filter] = getattr(jinja_exts, _filter)&#xa;&#xa;&#xa;class PageGenerator(BaseGenerator):&#xa;&#xa;    def __init__(self, site_config, base_path, src_file_path):&#xa;        '''&#xa;        :src_file_path: path of a source file&#xa;        '''&#xa;        super(PageGenerator, self).__init__(site_config, base_path)&#xa;        self.src_file_path = src_file_path&#xa;        # source file path relative to base_path&#xa;        self.src_file_relpath = os.path.relpath(src_file_path, self.base_path)&#xa;        self.meta = None&#xa;        self.content = None&#xa;&#xa;    def to_html(self):&#xa;        """"""Load template, and generate html""""""&#xa;        self.meta, self.content = self.get_meta_and_content()&#xa;        if self.meta.get('draft', False):&#xa;            return None&#xa;        layout = self.get_layout(self.meta)&#xa;        template_file = ""{0}.html"".format(layout)&#xa;        template_vars = self.get_template_vars(self.meta, self.content)&#xa;        try:&#xa;            template = self.env.get_template(template_file)&#xa;            html = template.render(template_vars)&#xa;        except TemplateError:&#xa;            # jinja2.exceptions.TemplateNotFound will get blocked&#xa;            # in multiprocessing?&#xa;            exc_msg = ""unable to load template '{0}'\n{1}"" \&#xa;                      .format(template_file, traceback.format_exc())&#xa;            raise Exception(exc_msg)&#xa;&#xa;        return html&#xa;&#xa;    def get_meta_and_content(self):&#xa;        """"""Split the source file texts by triple-dashed lines, return the mata&#xa;        and content.&#xa;        meta is page's meta data, dict type.&#xa;        content is html parsed from markdown or other markup text.&#xa;        """"""&#xa;        meta_notation = ""---""&#xa;        with io.open(self.src_file_path, ""rt"", encoding=""utf-8"") as fd:&#xa;            textlist = fd.read().lstrip().splitlines()&#xa;            if textlist[0] != meta_notation:&#xa;                raise Exception(""disallow anything except newline ""&#xa;                                ""before begin meta notation '---'"")&#xa;            textlist = textlist[1:]&#xa;&#xa;        try:&#xa;            second_meta_notation_index = textlist.index(meta_notation)&#xa;        except ValueError:&#xa;            raise Exception(""can't find end meta notation '---'"")&#xa;        meta_textlist = textlist[:second_meta_notation_index]&#xa;        markup_textlist = textlist[second_meta_notation_index+1:]&#xa;&#xa;        meta = self._get_meta(PLAT_LINE_SEP.join(meta_textlist))&#xa;        markup_text = PLAT_LINE_SEP.join(markup_textlist)&#xa;        if meta.get('render', True):&#xa;            content = self._parse_markdown(markup_text)&#xa;        else:&#xa;            content = markup_text&#xa;&#xa;        return (meta, content)&#xa;&#xa;    @staticmethod&#xa;    def get_layout(meta):&#xa;        """"""Get layout config in meta, default is 'page'""""""&#xa;        if ""layout"" in meta:&#xa;            # Compatible with previous version, which default layout is ""post""&#xa;            # XXX Will remove this checker in v2.0&#xa;            if meta[""layout""] == ""post"":&#xa;                layout = ""page""&#xa;            else:&#xa;                layout = meta[""layout""]&#xa;        else:&#xa;            layout = ""page""&#xa;&#xa;        return layout&#xa;&#xa;    def get_template_vars(self, meta, content):&#xa;        """"""Get template variables, include site config and page config""""""&#xa;        category, src_fname = self.get_category_and_file()&#xa;        dst_fname = src_fname.replace(&#xa;            "".{0}"".format(self.site_config[""default_ext""]), "".html"")&#xa;        page = {&#xa;            ""category"": category,&#xa;            ""content"": content,&#xa;            ""filename"": dst_fname&#xa;        }&#xa;        page.update(meta)&#xa;        template_vars = {&#xa;            ""site"": self.site_config,&#xa;            ""page"": page,&#xa;        }&#xa;&#xa;        # if site.root endswith `/`, remove it.&#xa;        site_root = template_vars[""site""][""root""]&#xa;        if site_root.endswith(""/""):&#xa;            template_vars[""site""][""root""] = site_root[:-1]&#xa;&#xa;        return template_vars&#xa;&#xa;    def get_category_and_file(self):&#xa;        """"""Get the name of category and file(with extension)""""""&#xa;        src_file_relpath_to_source = \&#xa;            os.path.relpath(self.src_file_relpath, self.site_config['source'])&#xa;        category, filename = os.path.split(src_file_relpath_to_source)&#xa;        return (category, filename)&#xa;&#xa;    def _get_meta(self, meta_yaml):&#xa;        """"""Get meta and validate them&#xa;&#xa;        :param meta_yaml: meta in yaml format&#xa;        """"""&#xa;        try:&#xa;            meta = yaml.load(meta_yaml)&#xa;        except yaml.YAMLError as e:&#xa;            e.extra_msg = 'yaml format error'&#xa;            raise&#xa;&#xa;        if not self._check_meta(meta):&#xa;            raise Exception(""no 'title' in meta"")&#xa;&#xa;        return meta&#xa;&#xa;    def _check_meta(self, meta):&#xa;        """"""Check if meta is right""""""&#xa;        is_meta_right = True&#xa;        if ""title"" not in meta:&#xa;            is_meta_right = False&#xa;        return is_meta_right&#xa;&#xa;    def _parse_markdown(self, markdown_content):&#xa;        """"""Parse markdown text to html""""""&#xa;        markdown_extensions = self._set_markdown_extensions()&#xa;&#xa;        html_content = markdown.markdown(&#xa;            markdown_content,&#xa;            extensions=markdown_extensions,&#xa;        )&#xa;&#xa;        return html_content&#xa;&#xa;    def _set_markdown_extensions(self):&#xa;        """"""Set the extensions for markdown parser""""""&#xa;        # TODO: custom markdown extension in _config.yml&#xa;        # Base markdown extensions support ""fenced_code"".&#xa;        markdown_extensions = [""fenced_code""]&#xa;        if self.site_config[""pygments""]:&#xa;            markdown_extensions.extend([&#xa;                ""extra"",&#xa;                ""codehilite(css_class=hlcode)"",&#xa;                ""toc(title=Table of Contents)""&#xa;            ])&#xa;&#xa;        return markdown_extensions&#xa;&#xa;&#xa;class CatalogGenerator(BaseGenerator):&#xa;&#xa;    def __init__(self, site_config, base_path, pages):&#xa;        '''&#xa;        :pages: all pages' meta variables, dict type&#xa;        '''&#xa;        super(CatalogGenerator, self).__init__(site_config, base_path)&#xa;        self.pages = pages&#xa;&#xa;    def get_content_structure_and_meta(self):&#xa;        """"""Ref: http://stackoverflow.com/a/9619101/1276501""""""&#xa;        dct = {}&#xa;        ext = self.site_config[""default_ext""]&#xa;        for path, meta in self.pages.items():&#xa;            # Ignore other files&#xa;            if not path.endswith(ext):&#xa;                continue&#xa;            p = dct&#xa;            for x in path.split(os.sep):&#xa;                if ext in x:&#xa;                    meta[""name""] = os.path.splitext(x)[0]&#xa;                    p = p.setdefault(x, meta)&#xa;                else:&#xa;                    p = p.setdefault(x, {})&#xa;&#xa;        return dct.get(self.site_config['source'], {})&#xa;&#xa;    def sort_structure(self, structure):&#xa;        """"""Sort index structure in lower-case, alphabetical order&#xa;&#xa;        Compare argument is a key/value structure, if the compare argument is a&#xa;        leaf node, which has `title` key in its value, use the title value,&#xa;        else use the key to compare.&#xa;        """"""&#xa;&#xa;        def _cmp(arg1, arg2):&#xa;            arg1 = arg1[1][""title""] if ""title"" in arg1[1] else arg1[0]&#xa;            arg2 = arg2[1][""title""] if ""title"" in arg2[1] else arg2[0]&#xa;            # cmp not exists in py3&#xa;            # via <https://docs.python.org/3.0/whatsnew/3.0.html#ordering-comparisons>&#xa;            cmp = lambda x, y: (x > y) - (x < y)&#xa;            return cmp(arg1.lower(), arg2.lower())&#xa;&#xa;        if is_py2:&#xa;            sorted_opts = {'cmp': _cmp}&#xa;        elif is_py3:&#xa;            sorted_opts = {'key': cmp_to_key(_cmp)}&#xa;&#xa;        sorted_structure = copy.deepcopy(structure)&#xa;        for k, _ in sorted_structure.items():&#xa;            sorted_structure = OrderedDict(sorted(&#xa;                sorted_structure.items(),&#xa;                **sorted_opts&#xa;            ))&#xa;            if k.endswith("".{0}"".format(self.site_config[""default_ext""])):&#xa;                continue&#xa;            sorted_structure[k] = self.sort_structure(sorted_structure[k])&#xa;        return sorted_structure&#xa;&#xa;    def get_template_vars(self):&#xa;        self.site_config[""structure""] = \&#xa;            self.sort_structure(self.get_content_structure_and_meta())&#xa;        tpl_vars = {&#xa;            ""site"": self.site_config,&#xa;        }&#xa;&#xa;        # if site.root endwith `\`, remote it.&#xa;        site_root = tpl_vars[""site""][""root""]&#xa;        if site_root.endswith(""/""):&#xa;            tpl_vars[""site""][""root""] = site_root[:-1]&#xa;&#xa;        return tpl_vars&#xa;&#xa;    def generate_catalog_html(self):&#xa;        tpl_vars = self.get_template_vars()&#xa;        html = self.env.get_template(""index.html"").render(tpl_vars)&#xa;        return html&#xa;&#xa;&#xa;class FeedGenerator(BaseGenerator):&#xa;    def __init__(self, site_config, base_path, pages, feed_fn='atom.xml'):&#xa;        '''&#xa;        :pages: all pages' meta variables, dict type&#xa;        '''&#xa;        super(FeedGenerator, self).__init__(site_config, base_path)&#xa;        self.pages = pages&#xa;        self.feed_fn = feed_fn&#xa;&#xa;    def get_template_vars(self):&#xa;        tpl_vars = {&#xa;            ""site"": self.site_config,&#xa;            ""pages"": self.pages&#xa;        }&#xa;&#xa;        # if site.root endwith `\`, remote it.&#xa;        site_root = tpl_vars[""site""][""root""]&#xa;        if site_root.endswith(""/""):&#xa;            tpl_vars[""site""][""root""] = site_root[:-1]&#xa;&#xa;        return tpl_vars&#xa;&#xa;    def generate_feed(self):&#xa;        tpl_vars = self.get_template_vars()&#xa;        with open(os.path.join(self.base_path, self.feed_fn), 'r') as fd:&#xa;            template = self.env.from_string(fd.read())&#xa;            feed_content = template.render(tpl_vars)&#xa;        return feed_content&#xa;"
246128|"#!/opt/anaconda1anaconda2anaconda3/bin/python&#xa;#&#xa;# Wrapper script for Java Conda packages that ensures that the java runtime&#xa;# is invoked with the right options. Adapted from the bash script (http://stackoverflow.com/questions/59895/can-a-bash-script-tell-what-directory-its-stored-in/246128#246128).&#xa;&#xa;#&#xa;# Program Parameters&#xa;#&#xa;import os&#xa;import subprocess&#xa;import sys&#xa;from os import access&#xa;from os import getenv&#xa;from os import X_OK&#xa;jar_file = 'GenomeAnalysisTK.jar'&#xa;&#xa;default_jvm_mem_opts = ['-Xms512m', '-Xmx1g']&#xa;&#xa;# !!! End of parameter section. No user-serviceable code below this line !!!&#xa;&#xa;&#xa;def real_dirname(path):&#xa;    """"""Return the symlink-resolved, canonicalized directory-portion of path.""""""&#xa;    return os.path.dirname(os.path.realpath(path))&#xa;&#xa;&#xa;def java_executable():&#xa;    """"""Return the executable name of the Java interpreter.""""""&#xa;    java_home = getenv('JAVA_HOME')&#xa;    java_bin = os.path.join('bin', 'java')&#xa;    env_prefix = os.path.dirname(os.path.dirname(real_dirname(sys.argv[0])))&#xa;&#xa;    if java_home and access(os.path.join(java_home, java_bin), X_OK):&#xa;        return os.path.join(java_home, java_bin)&#xa;    else:&#xa;        # Use Java installed with Anaconda to ensure correct version&#xa;        return os.path.join(env_prefix, 'bin', 'java')&#xa;&#xa;&#xa;def jvm_opts(argv):&#xa;    """"""Construct list of Java arguments based on our argument list.&#xa;&#xa;    The argument list passed in argv must not include the script name.&#xa;    The return value is a 3-tuple lists of strings of the form:&#xa;      (memory_options, prop_options, passthrough_options)&#xa;    """"""&#xa;    mem_opts = []&#xa;    prop_opts = []&#xa;    pass_args = []&#xa;&#xa;    for arg in argv:&#xa;        if arg.startswith('-D'):&#xa;            prop_opts.append(arg)&#xa;        elif arg.startswith('-XX'):&#xa;            prop_opts.append(arg)&#xa;        elif arg.startswith('-Xm'):&#xa;            mem_opts.append(arg)&#xa;        else:&#xa;            pass_args.append(arg)&#xa;&#xa;    # In the original shell script the test coded below read:&#xa;    #   if [ ""$jvm_mem_opts"" == """" ] && [ -z ${_JAVA_OPTIONS+x} ]&#xa;    # To reproduce the behaviour of the above shell code fragment&#xa;    # it is important to explictly check for equality with None&#xa;    # in the second condition, so a null envar value counts as True!&#xa;&#xa;    if mem_opts == [] and getenv('_JAVA_OPTIONS') is None:&#xa;        mem_opts = default_jvm_mem_opts&#xa;&#xa;    return (mem_opts, prop_opts, pass_args)&#xa;&#xa;&#xa;def main():&#xa;    java = java_executable()&#xa;    jar_dir = real_dirname(sys.argv[0])&#xa;    (mem_opts, prop_opts, pass_args) = jvm_opts(sys.argv[1:])&#xa;&#xa;    if pass_args != [] and pass_args[0].startswith('org'):&#xa;        jar_arg = '-cp'&#xa;    else:&#xa;        jar_arg = '-jar'&#xa;&#xa;    jar_path = os.path.join(jar_dir, jar_file)&#xa;&#xa;    if not os.path.isfile(jar_path):&#xa;        sys.stderr.write('GATK jar file not found. Have you run \""gatk3-register\""?\n')&#xa;        sys.exit(1)&#xa;&#xa;    java_args = [java] + mem_opts + prop_opts + [jar_arg] + [jar_path] + pass_args&#xa;&#xa;    sys.exit(subprocess.call(java_args))&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    main()&#xa;"
764360|"# (c) 2014, Chris Church <chris@ninemoreminutes.com>&#xa;#&#xa;# This file is part of Ansible.&#xa;#&#xa;# Ansible is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Ansible is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.&#xa;from __future__ import (absolute_import, division, print_function)&#xa;__metaclass__ = type&#xa;&#xa;import base64&#xa;import os&#xa;import re&#xa;import random&#xa;import shlex&#xa;import time&#xa;&#xa;from ansible.utils.unicode import to_bytes, to_unicode&#xa;&#xa;_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']&#xa;&#xa;# Primarily for testing, allow explicitly specifying PowerShell version via&#xa;# an environment variable.&#xa;_powershell_version = os.environ.get('POWERSHELL_VERSION', None)&#xa;if _powershell_version:&#xa;    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]&#xa;&#xa;class ShellModule(object):&#xa;&#xa;    def env_prefix(self, **kwargs):&#xa;        return ''&#xa;&#xa;    def join_path(self, *args):&#xa;        parts = []&#xa;        for arg in args:&#xa;            arg = self._unquote(arg).replace('/', '\\')&#xa;            parts.extend([a for a in arg.split('\\') if a])&#xa;        path = '\\'.join(parts)&#xa;        if path.startswith('~'):&#xa;            return path&#xa;        return '""%s""' % path&#xa;&#xa;    def path_has_trailing_slash(self, path):&#xa;        # Allow Windows paths to be specified using either slash.&#xa;        path = self._unquote(path)&#xa;        return path.endswith('/') or path.endswith('\\')&#xa;&#xa;    def chmod(self, mode, path):&#xa;        return ''&#xa;&#xa;    def remove(self, path, recurse=False):&#xa;        path = self._escape(self._unquote(path))&#xa;        if recurse:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force -Recurse;''' % path)&#xa;        else:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force;''' % path)&#xa;&#xa;    def mkdtemp(self, basefile, system=False, mode=None):&#xa;        basefile = self._escape(self._unquote(basefile))&#xa;        # FIXME: Support system temp path!&#xa;        return self._encode_script('''(New-Item -Type Directory -Path $env:temp -Name ""%s"").FullName | Write-Host -Separator '';''' % basefile)&#xa;&#xa;    def expand_user(self, user_home_path):&#xa;        # PowerShell only supports ""~"" (not ""~username"").  Resolve-Path ~ does&#xa;        # not seem to work remotely, though by default we are always starting&#xa;        # in the user's home directory.&#xa;        user_home_path = self._unquote(user_home_path)&#xa;        if user_home_path == '~':&#xa;            script = 'Write-Host (Get-Location).Path'&#xa;        elif user_home_path.startswith('~\\'):&#xa;            script = 'Write-Host ((Get-Location).Path + ""%s"")' % self._escape(user_home_path[1:])&#xa;        else:&#xa;            script = 'Write-Host ""%s""' % self._escape(user_home_path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def checksum(self, path, *args, **kwargs):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path -PathType Leaf ""%(path)s"")&#xa;            {&#xa;                $sp = new-object -TypeName System.Security.Cryptography.SHA1CryptoServiceProvider;&#xa;                $fp = [System.IO.File]::Open(""%(path)s"", [System.IO.Filemode]::Open, [System.IO.FileAccess]::Read);&#xa;                [System.BitConverter]::ToString($sp.ComputeHash($fp)).Replace(""-"", """").ToLower();&#xa;                $fp.Dispose();&#xa;            }&#xa;            ElseIf (Test-Path -PathType Container ""%(path)s"")&#xa;            {&#xa;                Write-Host ""3"";&#xa;            }&#xa;            Else&#xa;            {&#xa;                Write-Host ""1"";&#xa;            }&#xa;        ''' % dict(path=path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def build_module_command(self, env_string, shebang, cmd, rm_tmp=None):&#xa;        cmd_parts = shlex.split(to_bytes(cmd), posix=False)&#xa;        cmd_parts = map(to_unicode, cmd_parts)&#xa;        if shebang and shebang.lower() == '#!powershell':&#xa;            if not self._unquote(cmd_parts[0]).lower().endswith('.ps1'):&#xa;                cmd_parts[0] = '""%s.ps1""' % self._unquote(cmd_parts[0])&#xa;            cmd_parts.insert(0, '&')&#xa;        elif shebang and shebang.startswith('#!'):&#xa;            cmd_parts.insert(0, shebang[2:])&#xa;        catch = '''&#xa;            $_obj = @{ failed = $true; $msg = $_ }&#xa;            echo $_obj | ConvertTo-Json -Compress -Depth 99&#xa;            Exit 1&#xa;        '''&#xa;        script = 'Try { %s }\nCatch { %s }' % (' '.join(cmd_parts), 'throw')&#xa;        if rm_tmp:&#xa;            rm_tmp = self._escape(self._unquote(rm_tmp))&#xa;            rm_cmd = 'Remove-Item ""%s"" -Force -Recurse -ErrorAction SilentlyContinue' % rm_tmp&#xa;            script = '%s\nFinally { %s }' % (script, rm_cmd)&#xa;        return self._encode_script(script)&#xa;&#xa;    def _unquote(self, value):&#xa;        '''Remove any matching quotes that wrap the given value.'''&#xa;        value = to_unicode(value or '')&#xa;        m = re.match(r'^\s*?\'(.*?)\'\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        m = re.match(r'^\s*?""(.*?)""\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        return value&#xa;&#xa;    def _escape(self, value, include_vars=False):&#xa;        '''Return value escaped for use in PowerShell command.'''&#xa;        # http://www.techotopia.com/index.php/Windows_PowerShell_1.0_String_Quoting_and_Escape_Sequences&#xa;        # http://stackoverflow.com/questions/764360/a-list-of-string-replacements-in-python&#xa;        subs = [('\n', '`n'), ('\r', '`r'), ('\t', '`t'), ('\a', '`a'),&#xa;                ('\b', '`b'), ('\f', '`f'), ('\v', '`v'), ('""', '`""'),&#xa;                ('\'', '`\''), ('`', '``'), ('\x00', '`0')]&#xa;        if include_vars:&#xa;            subs.append(('$', '`$'))&#xa;        pattern = '|'.join('(%s)' % re.escape(p) for p, s in subs)&#xa;        substs = [s for p, s in subs]&#xa;        replace = lambda m: substs[m.lastindex - 1]&#xa;        return re.sub(pattern, replace, value)&#xa;&#xa;    def _encode_script(self, script, as_list=False):&#xa;        '''Convert a PowerShell script to a single base64-encoded command.'''&#xa;        script = to_unicode(script)&#xa;        script = '\n'.join([x.strip() for x in script.splitlines() if x.strip()])&#xa;        encoded_script = base64.b64encode(script.encode('utf-16-le'))&#xa;        cmd_parts = _common_args + ['-EncodedCommand', encoded_script]&#xa;        if as_list:&#xa;            return cmd_parts&#xa;        return ' '.join(cmd_parts)&#xa;"
2371436|"from __future__ import division&#xa;from pyparsing import (Literal,CaselessLiteral,Word,Combine,Group,Optional,&#xa;                       ZeroOrMore,Forward,nums,alphas,oneOf)&#xa;import math&#xa;import operator&#xa;&#xa;__author__='Paul McGuire'&#xa;__version__ = '$Revision: 0.0 $'&#xa;__date__ = '$Date: 2009-03-20 $'&#xa;__source__='''http://pyparsing.wikispaces.com/file/view/fourFn.py&#xa;http://pyparsing.wikispaces.com/message/view/home/15549426&#xa;'''&#xa;__note__='''&#xa;Script from: http://stackoverflow.com/questions/2371436/evaluating-a-mathematical-expression-in-a-string&#xa;Original note:&#xa;&#xa;All I've done is rewrap Paul McGuire's fourFn.py as a class, so I can use it&#xa;more easily in other places.&#xa;'''&#xa;&#xa;class NumericStringParser(object):&#xa;    '''&#xa;    Most of this code comes from the fourFn.py pyparsing example&#xa;&#xa;    '''&#xa;    def pushFirst(self, strg, loc, toks ):&#xa;        self.exprStack.append( toks[0] )&#xa;    def pushUMinus(self, strg, loc, toks ):&#xa;        if toks and toks[0]=='-':&#xa;            self.exprStack.append( 'unary -' )&#xa;    def __init__(self):&#xa;        """"""&#xa;        expop   :: '^'&#xa;        multop  :: '*' | '/'&#xa;        addop   :: '+' | '-'&#xa;        integer :: ['+' | '-'] '0'..'9'+&#xa;        atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')'&#xa;        factor  :: atom [ expop factor ]*&#xa;        term    :: factor [ multop factor ]*&#xa;        expr    :: term [ addop term ]*&#xa;        """"""&#xa;        point = Literal( ""."" )&#xa;        e     = CaselessLiteral( ""E"" )&#xa;        fnumber = Combine( Word( ""+-""+nums, nums ) +&#xa;                           Optional( point + Optional( Word( nums ) ) ) +&#xa;                           Optional( e + Word( ""+-""+nums, nums ) ) )&#xa;        ident = Word(alphas, alphas+nums+""_$"")&#xa;        plus  = Literal( ""+"" )&#xa;        minus = Literal( ""-"" )&#xa;        mult  = Literal( ""*"" )&#xa;        div   = Literal( ""/"" )&#xa;        lpar  = Literal( ""("" ).suppress()&#xa;        rpar  = Literal( "")"" ).suppress()&#xa;        addop  = plus | minus&#xa;        multop = mult | div&#xa;        expop = Literal( ""^"" )&#xa;        pi    = CaselessLiteral( ""PI"" )&#xa;        expr = Forward()&#xa;        atom = ((Optional(oneOf(""- +"")) +&#xa;                 (pi|e|fnumber|ident+lpar+expr+rpar).setParseAction(self.pushFirst))&#xa;                | Optional(oneOf(""- +"")) + Group(lpar+expr+rpar)&#xa;                ).setParseAction(self.pushUMinus)&#xa;        # by defining exponentiation as ""atom [ ^ factor ]..."" instead of&#xa;        # ""atom [ ^ atom ]..."", we get right-to-left exponents, instead of left-to-right&#xa;        # that is, 2^3^2 = 2^(3^2), not (2^3)^2.&#xa;        factor = Forward()&#xa;        factor << atom + ZeroOrMore( ( expop + factor ).setParseAction( self.pushFirst ) )&#xa;        term = factor + ZeroOrMore( ( multop + factor ).setParseAction( self.pushFirst ) )&#xa;        expr << term + ZeroOrMore( ( addop + term ).setParseAction( self.pushFirst ) )&#xa;        # addop_term = ( addop + term ).setParseAction( self.pushFirst )&#xa;        # general_term = term + ZeroOrMore( addop_term ) | OneOrMore( addop_term)&#xa;        # expr <<  general_term&#xa;        self.bnf = expr&#xa;        # map operator symbols to corresponding arithmetic operations&#xa;        epsilon = 1e-12&#xa;        self.opn = { ""+"" : operator.add,&#xa;                ""-"" : operator.sub,&#xa;                ""*"" : operator.mul,&#xa;                ""/"" : operator.truediv,&#xa;                ""^"" : operator.pow }&#xa;        self.fn  = { ""sin"" : math.sin,&#xa;                ""cos"" : math.cos,&#xa;                ""tan"" : math.tan,&#xa;                ""abs"" : abs,&#xa;                ""trunc"" : lambda a: int(a),&#xa;                ""round"" : round,&#xa;                ""sgn"" : lambda a: abs(a)>epsilon and cmp(a,0) or 0}&#xa;    def evaluateStack(self, s ):&#xa;        op = s.pop()&#xa;        if op == 'unary -':&#xa;            return -self.evaluateStack( s )&#xa;        if op in ""+-*/^"":&#xa;            op2 = self.evaluateStack( s )&#xa;            op1 = self.evaluateStack( s )&#xa;            return self.opn[op]( op1, op2 )&#xa;        elif op == ""PI"":&#xa;            return math.pi # 3.1415926535&#xa;        elif op == ""E"":&#xa;            return math.e  # 2.718281828&#xa;        elif op in self.fn:&#xa;            return self.fn[op]( self.evaluateStack( s ) )&#xa;        elif op[0].isalpha():&#xa;            return 0&#xa;        else:&#xa;            return float( op )&#xa;    def eval(self,num_string,parseAll=True):&#xa;        self.exprStack=[]&#xa;        results=self.bnf.parseString(num_string,parseAll)&#xa;        val=self.evaluateStack( self.exprStack[:] )&#xa;        return val&#xa;"
16837946|""""""" pydevd_vars deals with variables:&#xa;    resolution/conversion to XML.&#xa;""""""&#xa;import pickle&#xa;from _pydevd_bundle.pydevd_constants import dict_contains, get_frame, get_thread_id, xrange&#xa;&#xa;from _pydevd_bundle.pydevd_custom_frames import get_custom_frame&#xa;from _pydevd_bundle.pydevd_xml import ExceptionOnEvaluate, get_type, var_to_xml&#xa;from _pydev_imps._pydev_saved_modules import thread&#xa;&#xa;try:&#xa;    from StringIO import StringIO&#xa;except ImportError:&#xa;    from io import StringIO&#xa;import sys  # @Reimport&#xa;&#xa;from _pydev_imps._pydev_saved_modules import threading&#xa;import traceback&#xa;from _pydevd_bundle import pydevd_save_locals&#xa;from _pydev_bundle.pydev_imports import Exec, quote, execfile&#xa;from _pydevd_bundle.pydevd_utils import to_string&#xa;&#xa;SENTINEL_VALUE = []&#xa;&#xa;# -------------------------------------------------------------------------- defining true and false for earlier versions&#xa;&#xa;try:&#xa;    __setFalse = False&#xa;except:&#xa;    import __builtin__&#xa;&#xa;    setattr(__builtin__, 'True', 1)&#xa;    setattr(__builtin__, 'False', 0)&#xa;&#xa;&#xa;# ------------------------------------------------------------------------------------------------------ class for errors&#xa;&#xa;class VariableError(RuntimeError): pass&#xa;&#xa;&#xa;class FrameNotFoundError(RuntimeError): pass&#xa;&#xa;&#xa;def _iter_frames(initialFrame):&#xa;    '''NO-YIELD VERSION: Iterates through all the frames starting at the specified frame (which will be the first returned item)'''&#xa;    # cannot use yield&#xa;    frames = []&#xa;&#xa;    while initialFrame is not None:&#xa;        frames.append(initialFrame)&#xa;        initialFrame = initialFrame.f_back&#xa;&#xa;    return frames&#xa;&#xa;&#xa;def dump_frames(thread_id):&#xa;    sys.stdout.write('dumping frames\n')&#xa;    if thread_id != get_thread_id(threading.currentThread()):&#xa;        raise VariableError(""find_frame: must execute on same thread"")&#xa;&#xa;    curFrame = get_frame()&#xa;    for frame in _iter_frames(curFrame):&#xa;        sys.stdout.write('%s\n' % pickle.dumps(frame))&#xa;&#xa;&#xa;# ===============================================================================&#xa;# AdditionalFramesContainer&#xa;# ===============================================================================&#xa;class AdditionalFramesContainer:&#xa;    lock = thread.allocate_lock()&#xa;    additional_frames = {}  # dict of dicts&#xa;&#xa;&#xa;def add_additional_frame_by_id(thread_id, frames_by_id):&#xa;    AdditionalFramesContainer.additional_frames[thread_id] = frames_by_id&#xa;&#xa;&#xa;addAdditionalFrameById = add_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def remove_additional_frame_by_id(thread_id):&#xa;    del AdditionalFramesContainer.additional_frames[thread_id]&#xa;&#xa;&#xa;removeAdditionalFrameById = remove_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def has_additional_frames_by_id(thread_id):&#xa;    return dict_contains(AdditionalFramesContainer.additional_frames, thread_id)&#xa;&#xa;&#xa;def get_additional_frames_by_id(thread_id):&#xa;    return AdditionalFramesContainer.additional_frames.get(thread_id)&#xa;&#xa;&#xa;def find_frame(thread_id, frame_id):&#xa;    """""" returns a frame on the thread that has a given frame_id """"""&#xa;    try:&#xa;        curr_thread_id = get_thread_id(threading.currentThread())&#xa;        if thread_id != curr_thread_id:&#xa;            try:&#xa;                return get_custom_frame(thread_id, frame_id)  # I.e.: thread_id could be a stackless frame id + thread_id.&#xa;            except:&#xa;                pass&#xa;&#xa;            raise VariableError(""find_frame: must execute on same thread (%s != %s)"" % (thread_id, curr_thread_id))&#xa;&#xa;        lookingFor = int(frame_id)&#xa;&#xa;        if AdditionalFramesContainer.additional_frames:&#xa;            if dict_contains(AdditionalFramesContainer.additional_frames, thread_id):&#xa;                frame = AdditionalFramesContainer.additional_frames[thread_id].get(lookingFor)&#xa;&#xa;                if frame is not None:&#xa;                    return frame&#xa;&#xa;        curFrame = get_frame()&#xa;        if frame_id == ""*"":&#xa;            return curFrame  # any frame is specified with ""*""&#xa;&#xa;        frameFound = None&#xa;&#xa;        for frame in _iter_frames(curFrame):&#xa;            if lookingFor == id(frame):&#xa;                frameFound = frame&#xa;                del frame&#xa;                break&#xa;&#xa;            del frame&#xa;&#xa;        # Important: python can hold a reference to the frame from the current context&#xa;        # if an exception is raised, so, if we don't explicitly add those deletes&#xa;        # we might have those variables living much more than we'd want to.&#xa;&#xa;        # I.e.: sys.exc_info holding reference to frame that raises exception (so, other places&#xa;        # need to call sys.exc_clear())&#xa;        del curFrame&#xa;&#xa;        if frameFound is None:&#xa;            msgFrames = ''&#xa;            i = 0&#xa;&#xa;            for frame in _iter_frames(get_frame()):&#xa;                i += 1&#xa;                msgFrames += str(id(frame))&#xa;                if i % 5 == 0:&#xa;                    msgFrames += '\n'&#xa;                else:&#xa;                    msgFrames += '  -  '&#xa;&#xa;            errMsg = '''find_frame: frame not found.&#xa;    Looking for thread_id:%s, frame_id:%s&#xa;    Current     thread_id:%s, available frames:&#xa;    %s\n&#xa;    ''' % (thread_id, lookingFor, curr_thread_id, msgFrames)&#xa;&#xa;            sys.stderr.write(errMsg)&#xa;            return None&#xa;&#xa;        return frameFound&#xa;    except:&#xa;        import traceback&#xa;        traceback.print_exc()&#xa;        return None&#xa;&#xa;&#xa;def getVariable(thread_id, frame_id, scope, attrs):&#xa;    """"""&#xa;    returns the value of a variable&#xa;&#xa;    :scope: can be BY_ID, EXPRESSION, GLOBAL, LOCAL, FRAME&#xa;&#xa;    BY_ID means we'll traverse the list of all objects alive to get the object.&#xa;&#xa;    :attrs: after reaching the proper scope, we have to get the attributes until we find&#xa;            the proper location (i.e.: obj\tattr1\tattr2)&#xa;&#xa;    :note: when BY_ID is used, the frame_id is considered the id of the object to find and&#xa;           not the frame (as we don't care about the frame in this case).&#xa;    """"""&#xa;    if scope == 'BY_ID':&#xa;        if thread_id != get_thread_id(threading.currentThread()):&#xa;            raise VariableError(""getVariable: must execute on same thread"")&#xa;&#xa;        try:&#xa;            import gc&#xa;            objects = gc.get_objects()&#xa;        except:&#xa;            pass  # Not all python variants have it.&#xa;        else:&#xa;            frame_id = int(frame_id)&#xa;            for var in objects:&#xa;                if id(var) == frame_id:&#xa;                    if attrs is not None:&#xa;                        attrList = attrs.split('\t')&#xa;                        for k in attrList:&#xa;                            _type, _typeName, resolver = get_type(var)&#xa;                            var = resolver.resolve(var, k)&#xa;&#xa;                    return var&#xa;&#xa;        # If it didn't return previously, we coudn't find it by id (i.e.: alrceady garbage collected).&#xa;        sys.stderr.write('Unable to find object with id: %s\n' % (frame_id,))&#xa;        return None&#xa;&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return {}&#xa;&#xa;    if attrs is not None:&#xa;        attrList = attrs.split('\t')&#xa;    else:&#xa;        attrList = []&#xa;&#xa;    for attr in attrList:&#xa;        attr.replace(""@_@TAB_CHAR@_@"", '\t')&#xa;&#xa;    if scope == 'EXPRESSION':&#xa;        for count in xrange(len(attrList)):&#xa;            if count == 0:&#xa;                # An Expression can be in any scope (globals/locals), therefore it needs to evaluated as an expression&#xa;                var = evaluate_expression(thread_id, frame_id, attrList[count], False)&#xa;            else:&#xa;                _type, _typeName, resolver = get_type(var)&#xa;                var = resolver.resolve(var, attrList[count])&#xa;    else:&#xa;        if scope == ""GLOBAL"":&#xa;            var = frame.f_globals&#xa;            del attrList[0]  # globals are special, and they get a single dummy unused attribute&#xa;        else:&#xa;            # in a frame access both locals and globals as Python does&#xa;            var = {}&#xa;            var.update(frame.f_globals)&#xa;            var.update(frame.f_locals)&#xa;&#xa;        for k in attrList:&#xa;            _type, _typeName, resolver = get_type(var)&#xa;            var = resolver.resolve(var, k)&#xa;&#xa;    return var&#xa;&#xa;&#xa;def resolve_compound_variable(thread_id, frame_id, scope, attrs):&#xa;    """""" returns the value of the compound variable as a dictionary""""""&#xa;&#xa;    var = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        _type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        sys.stderr.write('Error evaluating: thread_id: %s\nframe_id: %s\nscope: %s\nattrs: %s\n' % (&#xa;            thread_id, frame_id, scope, attrs,))&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def resolve_var(var, attrs):&#xa;    attrList = attrs.split('\t')&#xa;&#xa;    for k in attrList:&#xa;        type, _typeName, resolver = get_type(var)&#xa;&#xa;        var = resolver.resolve(var, k)&#xa;&#xa;    try:&#xa;        type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def custom_operation(thread_id, frame_id, scope, attrs, style, code_or_file, operation_fn_name):&#xa;    """"""&#xa;    We'll execute the code_or_file and then search in the namespace the operation_fn_name to execute with the given var.&#xa;&#xa;    code_or_file: either some code (i.e.: from pprint import pprint) or a file to be executed.&#xa;    operation_fn_name: the name of the operation to execute after the exec (i.e.: pprint)&#xa;    """"""&#xa;    expressionValue = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        namespace = {'__name__': '<custom_operation>'}&#xa;        if style == ""EXECFILE"":&#xa;            namespace['__file__'] = code_or_file&#xa;            execfile(code_or_file, namespace, namespace)&#xa;        else:  # style == EXEC&#xa;            namespace['__file__'] = '<customOperationCode>'&#xa;            Exec(code_or_file, namespace, namespace)&#xa;&#xa;        return str(namespace[operation_fn_name](expressionValue))&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def eval_in_context(expression, globals, locals):&#xa;    result = None&#xa;    try:&#xa;        result = eval(expression, globals, locals)&#xa;    except Exception:&#xa;        s = StringIO()&#xa;        traceback.print_exc(file=s)&#xa;        result = s.getvalue()&#xa;&#xa;        try:&#xa;            try:&#xa;                etype, value, tb = sys.exc_info()&#xa;                result = value&#xa;            finally:&#xa;                etype = value = tb = None&#xa;        except:&#xa;            pass&#xa;&#xa;        result = ExceptionOnEvaluate(result)&#xa;&#xa;        # Ok, we have the initial error message, but let's see if we're dealing with a name mangling error...&#xa;        try:&#xa;            if '__' in expression:&#xa;                # Try to handle '__' name mangling...&#xa;                split = expression.split('.')&#xa;                curr = locals.get(split[0])&#xa;                for entry in split[1:]:&#xa;                    if entry.startswith('__') and not hasattr(curr, entry):&#xa;                        entry = '_%s%s' % (curr.__class__.__name__, entry)&#xa;                    curr = getattr(curr, entry)&#xa;&#xa;                result = curr&#xa;        except:&#xa;            pass&#xa;    return result&#xa;&#xa;&#xa;def evaluate_expression(thread_id, frame_id, expression, doExec):&#xa;    '''returns the result of the evaluated expression&#xa;    @param doExec: determines if we should do an exec or an eval&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    # Not using frame.f_globals because of https://sourceforge.net/tracker2/?func=detail&aid=2541355&group_id=85796&atid=577329&#xa;    # (Names not resolved in generator expression in method)&#xa;    # See message: http://mail.python.org/pipermail/python-list/2009-January/526522.html&#xa;    updated_globals = {}&#xa;    updated_globals.update(frame.f_globals)&#xa;    updated_globals.update(frame.f_locals)  # locals later because it has precedence over the actual globals&#xa;&#xa;    try:&#xa;        expression = str(expression.replace('@LINE@', '\n'))&#xa;&#xa;        if doExec:&#xa;            try:&#xa;                # try to make it an eval (if it is an eval we can print it, otherwise we'll exec it and&#xa;                # it will have whatever the user actually did)&#xa;                compiled = compile(expression, '<string>', 'eval')&#xa;            except:&#xa;                Exec(expression, updated_globals, frame.f_locals)&#xa;                pydevd_save_locals.save_locals(frame)&#xa;            else:&#xa;                result = eval(compiled, updated_globals, frame.f_locals)&#xa;                if result is not None:  # Only print if it's not None (as python does)&#xa;                    sys.stdout.write('%s\n' % (result,))&#xa;            return&#xa;&#xa;        else:&#xa;            return eval_in_context(expression, updated_globals, frame.f_locals)&#xa;    finally:&#xa;        # Should not be kept alive if an exception happens and this frame is kept in the stack.&#xa;        del updated_globals&#xa;        del frame&#xa;&#xa;&#xa;def change_attr_expression(thread_id, frame_id, attr, expression, dbg, value=SENTINEL_VALUE):&#xa;    '''Changes some attribute in a given frame.&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    try:&#xa;        expression = expression.replace('@LINE@', '\n')&#xa;&#xa;        if dbg.plugin and value is SENTINEL_VALUE:&#xa;            result = dbg.plugin.change_variable(frame, attr, expression)&#xa;            if result:&#xa;                return result&#xa;&#xa;        if attr[:7] == ""Globals"":&#xa;            attr = attr[8:]&#xa;            if attr in frame.f_globals:&#xa;                if value is SENTINEL_VALUE:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_globals[attr] = value&#xa;                return frame.f_globals[attr]&#xa;        else:&#xa;            if pydevd_save_locals.is_save_locals_available():&#xa;                if value is SENTINEL_VALUE:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_locals[attr] = value&#xa;                pydevd_save_locals.save_locals(frame)&#xa;                return frame.f_locals[attr]&#xa;&#xa;            # default way (only works for changing it in the topmost frame)&#xa;            if value is SENTINEL_VALUE:&#xa;                value = eval(expression, frame.f_globals, frame.f_locals)&#xa;            result = value&#xa;            Exec('%s=%s' % (attr, expression), frame.f_globals, frame.f_locals)&#xa;            return result&#xa;&#xa;&#xa;    except Exception:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;MAXIMUM_ARRAY_SIZE = 100&#xa;&#xa;def table_like_struct_to_xml(array, name, roffset, coffset, rows, cols, format):&#xa;    _, type_name, _ = get_type(array)&#xa;    if type_name == 'ndarray':&#xa;        array, metaxml, r, c, f = array_to_meta_xml(array, name, format)&#xa;        xml = metaxml&#xa;        format = '%' + f&#xa;        if rows == -1 and cols == -1:&#xa;            rows = r&#xa;            cols = c&#xa;        xml += array_to_xml(array, roffset, coffset, rows, cols, format)&#xa;    elif type_name == 'DataFrame':&#xa;        xml = dataframe_to_xml(array, name, roffset, coffset, rows, cols, format)&#xa;    else:&#xa;        raise VariableError(""Do not know how to convert type %s to table"" % (type_name))&#xa;&#xa;    return ""<xml>%s</xml>"" % xml&#xa;&#xa;&#xa;def array_to_xml(array, roffset, coffset, rows, cols, format):&#xa;    xml = """"&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(cols, MAXIMUM_ARRAY_SIZE)&#xa;&#xa;    # there is no obvious rule for slicing (at least 5 choices)&#xa;    if len(array) == 1 and (rows > 1 or cols > 1):&#xa;        array = array[0]&#xa;    if array.size > len(array):&#xa;        array = array[roffset:, coffset:]&#xa;        rows = min(rows, len(array))&#xa;        cols = min(cols, len(array[0]))&#xa;        if len(array) == 1:&#xa;            array = array[0]&#xa;    elif array.size == len(array):&#xa;        if roffset == 0 and rows == 1:&#xa;            array = array[coffset:]&#xa;            cols = min(cols, len(array))&#xa;        elif coffset == 0 and cols == 1:&#xa;            array = array[roffset:]&#xa;            rows = min(rows, len(array))&#xa;&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>"" % to_string(row)&#xa;        for col in range(cols):&#xa;            value = array&#xa;            if rows == 1 or cols == 1:&#xa;                if rows == 1 and cols == 1:&#xa;                    value = array[0]&#xa;                else:&#xa;                    if rows == 1:&#xa;                        dim = col&#xa;                    else:&#xa;                        dim = row&#xa;                    value = array[dim]&#xa;                    if ""ndarray"" in str(type(value)):&#xa;                        value = value[0]&#xa;            else:&#xa;                value = array[row][col]&#xa;            value = format % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;&#xa;&#xa;def array_to_meta_xml(array, name, format):&#xa;    type = array.dtype.kind&#xa;    slice = name&#xa;    l = len(array.shape)&#xa;&#xa;    # initial load, compute slice&#xa;    if format == '%':&#xa;        if l > 2:&#xa;            slice += '[0]' * (l - 2)&#xa;            for r in range(l - 2):&#xa;                array = array[0]&#xa;        if type == 'f':&#xa;            format = '.5f'&#xa;        elif type == 'i' or type == 'u':&#xa;            format = 'd'&#xa;        else:&#xa;            format = 's'&#xa;    else:&#xa;        format = format.replace('%', '')&#xa;&#xa;    l = len(array.shape)&#xa;    reslice = """"&#xa;    if l > 2:&#xa;        raise Exception(""%s has more than 2 dimensions."" % slice)&#xa;    elif l == 1:&#xa;        # special case with 1D arrays arr[i, :] - row, but arr[:, i] - column with equal shape and ndim&#xa;        # http://stackoverflow.com/questions/16837946/numpy-a-2-rows-1-column-file-loadtxt-returns-1row-2-columns&#xa;        # explanation: http://stackoverflow.com/questions/15165170/how-do-i-maintain-row-column-orientation-of-vectors-in-numpy?rq=1&#xa;        # we use kind of a hack - get information about memory from C_CONTIGUOUS&#xa;        is_row = array.flags['C_CONTIGUOUS']&#xa;&#xa;        if is_row:&#xa;            rows = 1&#xa;            cols = len(array)&#xa;            if cols < len(array):&#xa;                reslice = '[0:%s]' % (cols)&#xa;            array = array[0:cols]&#xa;        else:&#xa;            cols = 1&#xa;            rows = len(array)&#xa;            if rows < len(array):&#xa;                reslice = '[0:%s]' % (rows)&#xa;            array = array[0:rows]&#xa;    elif l == 2:&#xa;        rows = array.shape[-2]&#xa;        cols = array.shape[-1]&#xa;        if cols < array.shape[-1] or rows < array.shape[-2]:&#xa;            reslice = '[0:%s, 0:%s]' % (rows, cols)&#xa;        array = array[0:rows, 0:cols]&#xa;&#xa;    # avoid slice duplication&#xa;    if not slice.endswith(reslice):&#xa;        slice += reslice&#xa;&#xa;    bounds = (0, 0)&#xa;    if type in ""biufc"":&#xa;        bounds = (array.min(), array.max())&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""%s\"" type=\""%s\"" max=\""%s\"" min=\""%s\""/>' % \&#xa;          (slice, rows, cols, format, type, bounds[1], bounds[0])&#xa;    return array, xml, rows, cols, format&#xa;&#xa;&#xa;def array_default_format(type):&#xa;    if type == 'f':&#xa;        return '.5f'&#xa;    elif type == 'i' or type == 'u':&#xa;        return 'd'&#xa;    else:&#xa;        return 's'&#xa;&#xa;&#xa;def dataframe_to_xml(df, name, roffset, coffset, rows, cols, format):&#xa;    """"""&#xa;    :type df: pandas.core.frame.DataFrame&#xa;    :type name: str&#xa;    :type coffset: int&#xa;    :type roffset: int&#xa;    :type rows: int&#xa;    :type cols: int&#xa;    :type format: str&#xa;&#xa;&#xa;    """"""&#xa;    num_rows = df.shape[0]&#xa;    num_cols = df.shape[1]&#xa;    if (num_rows, num_cols) != df.shape:&#xa;        df = df.iloc[0:num_rows, 0: num_cols]&#xa;        slice = '.iloc[0:%s, 0:%s]' % (num_rows, num_cols)&#xa;    else:&#xa;        slice = ''&#xa;    slice = name + slice&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""\"" type=\""\"" max=\""0\"" min=\""0\""/>\n' % \&#xa;          (slice, num_rows, num_cols)&#xa;&#xa;    if (rows, cols) == (-1, -1):&#xa;        rows, cols = num_rows, num_cols&#xa;&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(min(cols, MAXIMUM_ARRAY_SIZE), num_cols)&#xa;    # need to precompute column bounds here before slicing!&#xa;    col_bounds = [None] * cols&#xa;    for col in range(cols):&#xa;        dtype = df.dtypes.iloc[coffset + col].kind&#xa;        if dtype in ""biufc"":&#xa;            cvalues = df.iloc[:, coffset + col]&#xa;            bounds = (cvalues.min(), cvalues.max())&#xa;        else:&#xa;            bounds = (0, 0)&#xa;        col_bounds[col] = bounds&#xa;&#xa;    df = df.iloc[roffset: roffset + rows, coffset: coffset + cols]&#xa;    rows, cols = df.shape&#xa;&#xa;&#xa;    xml += ""<headerdata rows=\""%s\"" cols=\""%s\"">\n"" % (rows, cols)&#xa;    format = format.replace('%', '')&#xa;    col_formats = []&#xa;&#xa;    get_label = lambda label: str(label) if not isinstance(label, tuple) else '/'.join(map(str, label))&#xa;&#xa;    for col in range(cols):&#xa;        dtype = df.dtypes.iloc[col].kind&#xa;        fmt = format if (dtype == 'f' and format) else array_default_format(dtype)&#xa;        col_formats.append('%' + fmt)&#xa;        bounds = col_bounds[col]&#xa;&#xa;        xml += '<colheader index=\""%s\"" label=\""%s\"" type=\""%s\"" format=\""%s\"" max=\""%s\"" min=\""%s\"" />\n' % \&#xa;               (str(col), get_label(df.axes[1].values[col]), dtype, fmt, bounds[1], bounds[0])&#xa;    for row, label in enumerate(iter(df.axes[0])):&#xa;        xml += ""<rowheader index=\""%s\"" label = \""%s\""/>\n"" % \&#xa;               (str(row), get_label(label))&#xa;    xml += ""</headerdata>\n""&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>\n"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>\n"" % str(row)&#xa;        for col in range(cols):&#xa;            value = df.iat[row, col]&#xa;            value = col_formats[col] % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;"
9901862|"# nvPY: cross-platform note-taking app with simplenote syncing&#xa;# copyright 2012 by Charl P. Botha <cpbotha@vxlabs.com>&#xa;# new BSD license&#xa;&#xa;import logging&#xa;import os&#xa;import re&#xa;import search_entry&#xa;import tk&#xa;import tkFont&#xa;import tkMessageBox&#xa;import utils&#xa;import webbrowser&#xa;&#xa;&#xa;class WidgetRedirector:&#xa;&#xa;    """"""Support for redirecting arbitrary widget subcommands.""""""&#xa;&#xa;    def __init__(self, widget):&#xa;        self.dict = {}&#xa;        self.widget = widget&#xa;        self.tk = tk = widget.tk&#xa;        w = widget._w&#xa;        self.orig = w + ""_orig""&#xa;        tk.call(""rename"", w, self.orig)&#xa;        tk.createcommand(w, self.dispatch)&#xa;&#xa;    def __repr__(self):&#xa;        return ""WidgetRedirector(%s<%s>)"" % (self.widget.__class__.__name__,&#xa;                                             self.widget._w)&#xa;&#xa;    def close(self):&#xa;        for name in self.dict:&#xa;            self.unregister(name)&#xa;&#xa;        widget = self.widget&#xa;        del self.widget&#xa;        orig = self.orig&#xa;        del self.orig&#xa;        tk = widget.tk&#xa;        w = widget._w&#xa;        tk.deletecommand(w)&#xa;        tk.call(""rename"", orig, w)&#xa;&#xa;    def register(self, name, function):&#xa;        if name in self.dict:&#xa;            previous = self.dict[name]&#xa;&#xa;        else:&#xa;            previous = OriginalCommand(self, name)&#xa;&#xa;        self.dict[name] = function&#xa;        setattr(self.widget, name, function)&#xa;        return previous&#xa;&#xa;    def unregister(self, name):&#xa;        if name in self.dict:&#xa;            function = self.dict[name]&#xa;            del self.dict[name]&#xa;            if hasattr(self.widget, name):&#xa;                delattr(self.widget, name)&#xa;&#xa;            return function&#xa;&#xa;        else:&#xa;            return None&#xa;&#xa;    def dispatch(self, cmd, *args):&#xa;        m = self.dict.get(cmd)&#xa;        try:&#xa;            if m:&#xa;                return m(*args)&#xa;            else:&#xa;                return self.tk.call((self.orig, cmd) + args)&#xa;        except tk.TclError:&#xa;            return """"&#xa;&#xa;&#xa;class OriginalCommand:&#xa;&#xa;    def __init__(self, redir, name):&#xa;        self.redir = redir&#xa;        self.name = name&#xa;        self.tk = redir.tk&#xa;        self.orig = redir.orig&#xa;        self.tk_call = self.tk.call&#xa;        self.orig_and_name = (self.orig, self.name)&#xa;&#xa;    def __repr__(self):&#xa;        return ""OriginalCommand(%r, %r)"" % (self.redir, self.name)&#xa;&#xa;    def __call__(self, *args):&#xa;        return self.tk_call(self.orig_and_name + args)&#xa;&#xa;&#xa;#########################################################################&#xa;class RedirectedText(tk.Text):&#xa;    """"""We would like to know when the Text widget's contents change.  We can't&#xa;    just override the insert method, we have to make use of some Tk magic.&#xa;    This magic is encapsulated in the idlelib.WidgetRedirector class which&#xa;    we use here.&#xa;    """"""&#xa;&#xa;    def __init__(self, master=None, cnf={}, **kw):&#xa;        tk.Text.__init__(self, master, cnf, **kw)&#xa;&#xa;        # now attach the redirector&#xa;        self.redir = WidgetRedirector(self)&#xa;        self.orig_insert = self.redir.register(""insert"", self.new_insert)&#xa;        self.orig_delete = self.redir.register(""delete"", self.new_delete)&#xa;        self.fonts = [kw['font']]&#xa;&#xa;    def new_insert(self, *args):&#xa;        self.orig_insert(*args)&#xa;        self.event_generate('<<Change>>')&#xa;&#xa;    def new_delete(self, *args):&#xa;        self.orig_delete(*args)&#xa;        self.event_generate('<<Change>>')&#xa;&#xa;&#xa;class HelpBindings(tk.Toplevel):&#xa;    def __init__(self, parent=None):&#xa;        tk.Toplevel.__init__(self, parent)&#xa;        self.title(""Help | Bindings"")&#xa;&#xa;        import bindings&#xa;&#xa;        msg = tk.Text(self, width=80, wrap=tk.NONE)&#xa;        msg.insert(tk.END, bindings.description)&#xa;        msg.config(state=tk.DISABLED)&#xa;        msg.pack()&#xa;&#xa;        button = tk.Button(self, text=""Dismiss"", command=self.destroy)&#xa;        button.pack()&#xa;&#xa;&#xa;#########################################################################&#xa;class StatusBar(tk.Frame):&#xa;    """"""Adapted from the tkinterbook.&#xa;    """"""&#xa;&#xa;    # actions&#xa;    # global status&#xa;    # note status&#xa;&#xa;    # http://colorbrewer2.org/index.php?type=sequential&scheme=OrRd&n=3&#xa;    # from light to dark orange; colorblind-safe scheme&#xa;    #NOTE_STATUS_COLORS = [""#FEE8C8"", ""#FDBB84"", ""#E34A33""]&#xa;&#xa;    # http://colorbrewer2.org/index.php?type=diverging&scheme=RdYlBu&n=5&#xa;    # diverging red to blue; colorblind-safe scheme&#xa;    # red, lighter red, light yellow, light blue, dark blue&#xa;    NOTE_STATUS_COLORS = [""#D7191C"", ""#FDAE61"", ""#FFFFBF"", ""#ABD9E9"", ""#2C7BB6""]&#xa;    # 0 - saved and synced - light blue - 3&#xa;    # 1 - saved - light yellow - 2&#xa;    # 2 - modified - lighter red - 1&#xa;    NOTE_STATUS_LUT = {0: 3, 1: 2, 2: 1}&#xa;&#xa;    def __init__(self, master):&#xa;        tk.Frame.__init__(self, master)&#xa;&#xa;        self.status = tk.Label(self, relief=tk.SUNKEN, anchor=tk.W, width=40)&#xa;        self.status.pack(side=tk.LEFT, fill=tk.X, expand=1)&#xa;&#xa;        self.centre_status = tk.Label(self, relief=tk.SUNKEN, anchor=tk.W, width=35)&#xa;        self.centre_status.pack(side=tk.LEFT, fill=tk.X, padx=5)&#xa;&#xa;        self.note_status = tk.Label(self, relief=tk.SUNKEN, anchor=tk.W, width=25)&#xa;        self.note_status.pack(side=tk.LEFT, fill=tk.X)&#xa;&#xa;    def set_centre_status(self, fmt, *args):&#xa;        self.centre_status.config(text=fmt % args)&#xa;        self.centre_status.update_idletasks()&#xa;&#xa;    def set_note_status(self, fmt, *args):&#xa;        """""" *.. .s. .sS&#xa;        """"""&#xa;        self.note_status.config(text=fmt % args)&#xa;        self.note_status.update_idletasks()&#xa;&#xa;    def set_note_status_color(self, status_idx):&#xa;        """"""&#xa;        @param status_idx: 0 - saved and synced; 1 - saved; 2 - modified&#xa;        """"""&#xa;&#xa;        color_idx = self.NOTE_STATUS_LUT[status_idx]&#xa;        self.note_status.config(background=self.NOTE_STATUS_COLORS[color_idx])&#xa;&#xa;    def set_status(self, fmt, *args):&#xa;        self.status.config(text=fmt % args)&#xa;        self.status.update_idletasks()&#xa;&#xa;    def clear_status(self):&#xa;        self.status.config(text="""")&#xa;        self.status.update_idletasks()&#xa;&#xa;&#xa;class NotesList(tk.Frame):&#xa;    """"""&#xa;    @ivar note_headers: list containing tuples with each note's title, tags,&#xa;    modified date and so forth. Always in sync with what is displayed.&#xa;    """"""&#xa;&#xa;    TITLE_COL = 0&#xa;    TAGS_COL = 1&#xa;    MODIFYDATE_COL = 2&#xa;    PINNED_COL = 3&#xa;    CREATEDATE_COL = 4&#xa;&#xa;    def __init__(self, master, font_family, font_size, config):&#xa;        tk.Frame.__init__(self, master)&#xa;&#xa;        yscrollbar = tk.Scrollbar(self)&#xa;        yscrollbar.pack(side=tk.RIGHT, fill=tk.Y)&#xa;&#xa;        f = tkFont.Font(family=font_family, size=font_size)&#xa;        # tkFont.families(root) returns list of available font family names&#xa;        # this determines the width of the complete interface (yes)&#xa;        # size=-self.config.font_size&#xa;        self.text = tk.Text(self, height=25, width=30,&#xa;            wrap=tk.NONE,&#xa;            font=f,&#xa;            yscrollcommand=yscrollbar.set,&#xa;            undo=True,&#xa;            background=config.background_color)&#xa;        # change default font at runtime with:&#xa;        #text.config(font=f)&#xa;&#xa;        self.text.config(cursor=""arrow"")&#xa;        self.disable_text()&#xa;        self.text.pack(fill=tk.BOTH, expand=1)&#xa;&#xa;        # tags for all kinds of styling ############################&#xa;        ############################################################&#xa;&#xa;        self.text.tag_config(""selected"", background=""light blue"")&#xa;&#xa;        self.text.tag_config(""pinned"", foreground=""dark gray"")&#xa;&#xa;        # next two lines from:&#xa;        # http://stackoverflow.com/a/9901862/532513&#xa;        bold_font = tkFont.Font(self.text, self.text.cget(""font""))&#xa;        bold_font.configure(weight=""bold"")&#xa;        self.text.tag_config(""title"", font=bold_font)&#xa;&#xa;        italic_font = tkFont.Font(self.text, self.text.cget(""font""))&#xa;        italic_font.configure(slant=""italic"")&#xa;        self.text.tag_config(""tags"", font=italic_font, foreground=""dark gray"")&#xa;        self.text.tag_config(""found"", font=italic_font, foreground=""dark gray"", background=""lightyellow"")&#xa;&#xa;        self.text.tag_config(""modifydate"", foreground=""dark gray"")&#xa;&#xa;        yscrollbar.config(command=self.text.yview)&#xa;&#xa;        self._bind_events()&#xa;&#xa;        self.selected_idx = -1&#xa;        # list containing tuples with each note's title, tags,&#xa;        self.note_headers = []&#xa;&#xa;        self.layout = config.layout&#xa;        self.print_columns = config.print_columns&#xa;        if bold_font.measure(' ') > f.measure(' '):&#xa;            self.cwidth = bold_font.measure(' ')&#xa;        else:&#xa;            self.cwidth = f.measure(' ')&#xa;        self.fonts = [f, italic_font, bold_font]&#xa;&#xa;    def append(self, note, config):&#xa;        """"""&#xa;        @param note: The complete note dictionary.&#xa;        """"""&#xa;&#xa;        title = utils.get_note_title(note)&#xa;        tags = note.get('tags')&#xa;        modifydate = float(note.get('modifydate'))&#xa;        pinned = utils.note_pinned(note)&#xa;        createdate = float(note.get('createdate'))&#xa;        self.note_headers.append((title, tags, modifydate, pinned, createdate))&#xa;&#xa;        self.enable_text()&#xa;&#xa;        if self.layout == ""vertical"" and self.print_columns == 1:&#xa;            nrchars, rem = divmod((self.text.winfo_width()), self.cwidth)&#xa;            cellwidth = (int(nrchars) - 8) / 2&#xa;&#xa;            if pinned:&#xa;                title += ' *'&#xa;&#xa;            self.text.insert(tk.END, u'{0:<{w}}'.format(title[:cellwidth - 1], w=cellwidth), (""title,""))&#xa;&#xa;            if tags > 0:&#xa;                if config.tagfound:&#xa;                    self.text.insert(tk.END, u'{0:<{w}}'.format(','.join(tags)[:cellwidth - 1], w=cellwidth), (""found"",))&#xa;                else:&#xa;                    self.text.insert(tk.END, u'{0:<{w}}'.format(','.join(tags)[:cellwidth - 1], w=cellwidth), (""tags"",))&#xa;&#xa;            self.text.insert(tk.END, ' ' + utils.human_date(createdate), (""createdate"",))&#xa;&#xa;            # tags can be None (newly created note) or [] or ['tag1', 'tag2']&#xa;        else:&#xa;            self.text.insert(tk.END, title, (""title,""))&#xa;&#xa;            if pinned:&#xa;                self.text.insert(tk.END, ' *', (""pinned"",))&#xa;&#xa;            # latest modified first is the default mode&#xa;            # we could consider showing createddate here IF the sort mode&#xa;            # is configured to be latest created first&#xa;            self.text.insert(tk.END, ' ' + utils.human_date(modifydate), (""modifydate"",))&#xa;&#xa;            # tags can be None (newly created note) or [] or ['tag1', 'tag2']&#xa;            if tags > 0:&#xa;                if config.tagfound:&#xa;                    self.text.insert(tk.END, ' ' + ','.join(tags), (""found"",))&#xa;                else:&#xa;                    self.text.insert(tk.END, ' ' + ','.join(tags), (""tags"",))&#xa;&#xa;        self.text.insert(tk.END, '\n')&#xa;&#xa;        self.disable_text()&#xa;&#xa;    def _bind_events(self):&#xa;        # Text widget events ##########################################&#xa;&#xa;        self.text.bind(""<Button 1>"", self.cmd_text_button1)&#xa;&#xa;        # same deal as for pageup&#xa;        # we have to stop the text widget class event handler from firing&#xa;        def cmd_up(e):&#xa;            self.select_prev(silent=False)&#xa;            return ""break""&#xa;&#xa;        self.text.bind(""<Up>"", cmd_up)&#xa;        self.text.bind(""<Control-k>"", cmd_up)&#xa;&#xa;        # for pageup, event handler needs to return ""break"" so that&#xa;        # Text widget's default class handler for pageup does not trigger.&#xa;        def cmd_pageup(e):&#xa;            self.select_prev(silent=False, delta=10)&#xa;            return ""break""&#xa;&#xa;        self.text.bind(""<Prior>"", cmd_pageup)&#xa;&#xa;        def cmd_down(e):&#xa;            self.select_next(silent=False)&#xa;            return ""break""&#xa;&#xa;        self.text.bind(""<Down>"", cmd_down)&#xa;        self.text.bind(""<Control-j>"", cmd_down)&#xa;&#xa;        def cmd_pagedown(e):&#xa;            self.select_next(silent=False, delta=10)&#xa;            return ""break""&#xa;&#xa;        self.text.bind(""<Next>"", cmd_pagedown)&#xa;&#xa;    def cmd_text_button1(self, event):&#xa;        # find line that was clicked on&#xa;        text_index = self.text.index(""@%d,%d"" % (event.x, event.y))&#xa;        # go from event coordinate to tkinter text INDEX to note idx!&#xa;        idx = int(text_index.split('.')[0]) - 1&#xa;        self.select(idx, silent=False)&#xa;&#xa;    def clear(self):&#xa;        """"""&#xa;&#xa;        """"""&#xa;        self.enable_text()&#xa;        # clear everything from the display&#xa;        self.text.delete(1.0, tk.END)&#xa;        # and make sure our backing store is in sync&#xa;        del self.note_headers[:]&#xa;        self.disable_text()&#xa;&#xa;    def disable_text(self):&#xa;        self.text.config(state=tk.DISABLED)&#xa;&#xa;    def enable_text(self):&#xa;        self.text.config(state=tk.NORMAL)&#xa;&#xa;    def find_note_by_title(self, title):&#xa;        """"""&#xa;        Find note with given title.&#xa;&#xa;        @returns: Note index if found, -1 otherwise.&#xa;        """"""&#xa;&#xa;        idx = -1&#xa;        for i, nh in enumerate(self.note_headers):&#xa;            t = nh[NotesList.TITLE_COL]&#xa;            if t == title:&#xa;                idx = i&#xa;                break&#xa;&#xa;        return idx&#xa;&#xa;    def get_number_of_notes(self):&#xa;        # could also have used:&#xa;        # return int(self.text.index('end-1c').split('.')[0])&#xa;        # but we have the backing store!&#xa;        return len(self.note_headers)&#xa;&#xa;    def get_pinned(self, idx):&#xa;        return self.note_headers[idx][NotesList.PINNED_COL]&#xa;&#xa;    def get_tags(self, idx):&#xa;        """"""&#xa;        @returns: raw list of tag strings, e.g. ['work', 'howto']&#xa;        """"""&#xa;        return self.note_headers[idx][NotesList.TAGS_COL]&#xa;&#xa;    def get_title(self, idx):&#xa;        return self.note_headers[idx][NotesList.TITLE_COL]&#xa;&#xa;    def get_modifydate(self, idx):&#xa;        """"""&#xa;        Return modifydate of idx'th note.&#xa;&#xa;        @returns: modifydate as a floating point timestamp.&#xa;        """"""&#xa;        return self.note_headers[idx][NotesList.MODIFYDATE_COL]&#xa;&#xa;    def get_createdate(self, idx):&#xa;        """"""&#xa;        Return createdate of idx'th note.&#xa;&#xa;        @returns: createdate as a floating point timestamp.&#xa;        """"""&#xa;        return self.note_headers[idx][NotesList.CREATEDATE_COL]&#xa;&#xa;    def idx_to_index_range(self, idx):&#xa;        """"""&#xa;        Given a note index idx, return the Tkinter text index range for&#xa;        the start and end of that note.&#xa;        """"""&#xa;&#xa;        # tkinter text first line is 1, but first column is 0&#xa;        row = idx + 1&#xa;        start = ""%d.0"" % (row,)&#xa;        end = ""%d.end"" % (row,)&#xa;&#xa;        return (start, end)&#xa;&#xa;    def select(self, idx, silent=True):&#xa;        """"""&#xa;        @param idx: index of note to select. -1 if no selection.&#xa;        """"""&#xa;&#xa;        # remove tag selected from row 1 (first) and column 0 to the end of the buffer&#xa;        self.text.tag_remove(""selected"", ""1.0"", ""end"")&#xa;&#xa;        if idx >= 0 and idx < self.get_number_of_notes():&#xa;            # then add it to the requested note line(s)&#xa;            start, end = self.idx_to_index_range(idx)&#xa;            self.text.tag_add(""selected"", start, end)&#xa;            # ensure that this is visible&#xa;            self.text.see(start)&#xa;            # and store the current idx&#xa;            self.selected_idx = idx&#xa;&#xa;        else:&#xa;            self.selected_idx = -1&#xa;&#xa;        if not silent:&#xa;            self.event_generate('<<NotesListSelect>>')&#xa;&#xa;    def select_next(self, silent=True, delta=1):&#xa;        """"""&#xa;        Select note right after the current selection.&#xa;        """"""&#xa;&#xa;        new_idx = self.selected_idx + delta&#xa;        if new_idx >= 0 and new_idx < self.get_number_of_notes():&#xa;            self.select(new_idx, silent)&#xa;&#xa;        elif new_idx >= self.get_number_of_notes():&#xa;            self.select(self.get_number_of_notes() - 1, silent)&#xa;&#xa;    def select_prev(self, silent=True, delta=1):&#xa;        """"""&#xa;        Select note right after the current selection.&#xa;        """"""&#xa;&#xa;        new_idx = self.selected_idx - delta&#xa;        if new_idx >= 0 and new_idx <= self.get_number_of_notes():&#xa;            self.select(new_idx, silent)&#xa;&#xa;        elif new_idx < 0:&#xa;            self.select(0, silent)&#xa;&#xa;tkinter_umlauts = ['odiaeresis', 'adiaeresis', 'udiaeresis', 'Odiaeresis', 'Adiaeresis', 'Udiaeresis', 'ssharp']&#xa;&#xa;&#xa;class TriggeredcompleteEntry(tk.Entry):&#xa;    """"""&#xa;    Subclass of tk.Entry that features triggeredcompletion.&#xa;&#xa;    How this works: User types first part of tag, then triggers complete with&#xa;    ctrl-space. The first matching tag is shown. The user can either continue&#xa;    pressing ctrl-space to see more matching tags, or right arrow to select&#xa;    the current suggestion and continue typing. Backspace will delete the&#xa;    suggested part.&#xa;&#xa;    To enable triggeredcompletion use set_completion_list(list) to define&#xa;    a list of possible strings to hit.&#xa;    To cycle through hits use CTRL <space> keys.&#xa;&#xa;    @ivar cycle: if 1, then we're cycling through alternative completions.&#xa;    """"""&#xa;&#xa;    def __init__(self, master, case_sensitive, **kw):&#xa;        tk.Entry.__init__(self, master, **kw)&#xa;        self.case_sensitive = case_sensitive&#xa;        # make sure we're initialised, else the event handler could generate&#xa;        # exceptions checking for instance variables that don't exist yet.&#xa;        self.set_completion_list([])&#xa;        self.bind('<KeyRelease>', self.handle_keyrelease)&#xa;&#xa;    def set_completion_list(self, completion_list):&#xa;        self._completion_list = completion_list&#xa;        self._hits = []&#xa;        self._hit_index = 0&#xa;        self.wstart = 0&#xa;        self.position = 0&#xa;        self.cycle = 0&#xa;&#xa;    def triggeredcomplete(self):&#xa;        """"""triggeredcomplete the Entry, delta may be 0/1 to cycle through possible hits""""""&#xa;&#xa;        if self.cycle:  # need to delete selection otherwise we would fix the current position&#xa;            self.delete(self.position, tk.END)&#xa;            self._hit_index += 1&#xa;            if self._hit_index == len(self._hits):&#xa;                self._hit_index = 0&#xa;&#xa;        else:  # set position to end so selection starts where textentry ended&#xa;            self.position = len(self.get())&#xa;            wstartsc = self.get().rfind(':')&#xa;            wstartsp = self.get().rfind(' ')&#xa;            if wstartsc < 0 and wstartsp < 0:&#xa;                self.wstart = 0&#xa;            elif wstartsc > wstartsp:&#xa;                self.wstart = wstartsc + 1&#xa;            else:&#xa;                self.wstart = wstartsp + 1&#xa;&#xa;            # collect hits&#xa;            _hits = []&#xa;            for element in self._completion_list:&#xa;                if self.case_sensitive == 0:&#xa;                    if element.lower().startswith(self.get()[self.wstart:].lower()):&#xa;                        _hits.append(element)&#xa;                else:&#xa;                    if element.startswith(self.get()[self.wstart:]):&#xa;                        _hits.append(element)&#xa;&#xa;            self._hit_index = 0&#xa;            self._hits = _hits&#xa;&#xa;        # now finally perform the triggered completion&#xa;        if self._hits:&#xa;            self.delete(self.wstart, tk.END)&#xa;            self.insert(self.wstart, self._hits[self._hit_index])&#xa;            self.select_range(self.position, tk.END)&#xa;&#xa;    def handle_keyrelease(self, event):&#xa;        """"""event handler for the keyrelease event on this widget""""""&#xa;        ctrl = ((event.state & 0x0004) != 0)&#xa;&#xa;        # special case handling below only if we are in cycle mode.&#xa;        if self.cycle:&#xa;            if event.keysym == ""BackSpace"":&#xa;                self.cycle = 0&#xa;                self.delete(self.index(tk.INSERT), tk.END)&#xa;                self.position = self.index(tk.END)&#xa;&#xa;            if event.keysym == ""Right"":&#xa;                self.position = self.index(tk.END)  # go to end (no selection)&#xa;                self.cycle = 0&#xa;&#xa;            if event.keysym == ""Left"":&#xa;                self.cycle = 0&#xa;&#xa;        if event.keysym == ""space"" and ctrl:&#xa;            # cycle&#xa;            self.triggeredcomplete()&#xa;            if self.cycle == 0:&#xa;                self.cycle = 1&#xa;&#xa;&#xa;class View(utils.SubjectMixin):&#xa;    """"""Main user interface class.&#xa;    """"""&#xa;&#xa;    def __init__(self, config, notes_list_model):&#xa;        utils.SubjectMixin.__init__(self)&#xa;&#xa;        self.config = config&#xa;        self.taglist = None&#xa;&#xa;        notes_list_model.add_observer('set:list', self.observer_notes_list)&#xa;        self.notes_list_model = notes_list_model&#xa;&#xa;        self.root = None&#xa;&#xa;        self._create_ui()&#xa;        self._bind_events()&#xa;&#xa;        # set default font for dialog boxes on Linux&#xa;        # on Windows, tkinter uses system dialogs in any case&#xa;        self.root.option_add('*Dialog.msg.font', 'Helvetica 12')&#xa;&#xa;        self.text_tags_links = []&#xa;        self.text_tags_search = []&#xa;&#xa;        #self._current_text = None&#xa;        #self.user_text.focus_set()&#xa;&#xa;        self.search_entry.focus_set()&#xa;&#xa;    def askyesno(self, title, msg):&#xa;        return tkMessageBox.askyesno(title, msg)&#xa;&#xa;    def cmd_notes_list_select(self, evt=None):&#xa;        sidx = self.notes_list.selected_idx&#xa;        self.notify_observers('select:note', utils.KeyValueObject(sel=sidx))&#xa;&#xa;    def cmd_root_delete(self, evt=None):&#xa;        # double-check that the user really means delete&#xa;        # https://github.com/cpbotha/nvpy/issues/119&#xa;        if tkMessageBox.askyesno(""Really delete note?"",&#xa;                                 ""Are you sure you want to delete the current note?"",&#xa;                                 default=tkMessageBox.NO):&#xa;            sidx = self.notes_list.selected_idx&#xa;            self.notify_observers('delete:note', utils.KeyValueObject(sel=sidx))&#xa;&#xa;    def cmd_root_archive(self, evt=None):&#xa;        # double-check that the user really means archive&#xa;        if tkMessageBox.askyesno(""Really archive note?"",&#xa;                                 ""Are you sure you want to archive the current note?"",&#xa;                                 default=tkMessageBox.NO):&#xa;            sidx = self.notes_list.selected_idx&#xa;            self.notify_observers('archive:note', utils.KeyValueObject(sel=sidx))&#xa;&#xa;    def cmd_root_new(self, evt=None):&#xa;        # this'll get caught by a controller event handler&#xa;        self.notify_observers('create:note', utils.KeyValueObject(title=self.get_search_entry_text()))&#xa;        # the note will be created synchronously, so we can focus the text area already&#xa;        self.text_note.focus()&#xa;&#xa;    def cmd_select_all(self, evt=None):&#xa;        self.text_note.tag_add(""sel"", ""1.0"", ""end-1c"")&#xa;        # we don't want the text bind_class() handler for Ctrl-A to be fired.&#xa;        return ""break""&#xa;&#xa;    def cmd_delete_previous_word(self, evt=None):&#xa;        num_char_to_delete = 1&#xa;        while self.text_note.get(""insert - {} chars"".format(num_char_to_delete)) in ("" "", ""\t""):&#xa;            num_char_to_delete += 1&#xa;&#xa;        self.text_note.delete(""insert - {} chars wordstart"".format(num_char_to_delete), ""insert"")&#xa;        return ""break""&#xa;&#xa;    def cmd_delete_line(self, evt=None):&#xa;        if self.text_note.tag_ranges(tk.SEL):&#xa;            if self.text_note.index(""sel.last"").split(""."")[1] == ""0"":&#xa;                # end of selection is at beginning of line&#xa;                self.text_note.delete(""sel.first linestart"", ""sel.last"")&#xa;            else:&#xa;                self.text_note.delete(""sel.first linestart"", ""sel.last +1 line linestart"")&#xa;        else:&#xa;            self.text_note.delete(""insert linestart"", ""insert +1 line linestart"")&#xa;        return ""break""&#xa;&#xa;    def filter_lines(self, func):&#xa;        had_selection = False&#xa;        current_cursor = self.text_note.index(""insert"")&#xa;        if self.text_note.tag_ranges(tk.SEL):&#xa;            had_selection = True&#xa;            sel_first = self.text_note.index(""sel.first linestart"")&#xa;            if self.text_note.index(""sel.last"").split(""."")[1] == ""0"":&#xa;                sel_last = self.text_note.index(""sel.last -1 line lineend"")&#xa;            else:&#xa;                sel_last = self.text_note.index(""sel.last lineend"")&#xa;        else:&#xa;            sel_first = self.text_note.index(""insert linestart"")&#xa;            sel_last = self.text_note.index(""insert lineend"")&#xa;        for l in range(int(sel_first.split(""."")[0]), int(sel_last.split(""."")[0]) + 1):&#xa;            line = self.text_note.get(str(l) + "".0"", str(l) + "".end"")&#xa;            self.text_note.delete(str(l) + "".0"", str(l) + "".end"")&#xa;            self.text_note.insert(str(l) + "".0"", func(line))&#xa;&#xa;        if had_selection:&#xa;            self.text_note.tag_add(""sel"", sel_first, sel_last)&#xa;&#xa;        self.text_note.mark_set(""insert"", current_cursor)&#xa;&#xa;    def filter_toggle_checkbox(self, line):&#xa;        if ""[ ]"" in line:&#xa;            return line.replace(""[ ]"", ""[x]"")&#xa;        elif ""[x]"" in line:&#xa;            return line.replace(""[x]"", ""[ ]"")&#xa;        else:&#xa;            return line&#xa;&#xa;    def filter_indent(self, line):&#xa;        return ""\t"" + line&#xa;&#xa;    def filter_deindent(self, line):&#xa;        return re.sub(""^\t"", """", line, 1)&#xa;&#xa;    def cmd_toggle_checkbox(self, evt=None):&#xa;        self.filter_lines(self.filter_toggle_checkbox)&#xa;        return ""break""&#xa;&#xa;    def cmd_indent(self, evt=None):&#xa;        self.filter_lines(self.filter_indent)&#xa;        return ""break""&#xa;&#xa;    def cmd_deindent(self, evt=None):&#xa;        self.filter_lines(self.filter_deindent)&#xa;        return ""break""&#xa;&#xa;    def move(self, direction):&#xa;        had_selection = None&#xa;        current_cursor = self.text_note.index(""insert"")&#xa;        if self.text_note.tag_ranges(tk.SEL):&#xa;            had_selection = True&#xa;            sel_first = self.text_note.index(""sel.first linestart"")&#xa;            if self.text_note.index(""sel.last"").split(""."")[1] == ""0"":&#xa;                sel_last = self.text_note.index(""sel.last -1 line lineend"")&#xa;            else:&#xa;                sel_last = self.text_note.index(""sel.last lineend"")&#xa;        else:&#xa;            sel_first = self.text_note.index(""insert linestart"")&#xa;            sel_last = self.text_note.index(""insert lineend"")&#xa;&#xa;        selected_text = self.text_note.get(sel_first, sel_last)&#xa;        if direction == ""up"":&#xa;            if sel_first.split(""."")[0] == ""2"":&#xa;                return&#xa;&#xa;            prev_line_text = self.text_note.get(sel_first + "" -1 line linestart"", sel_first + "" -1 line lineend"")&#xa;            self.text_note.delete(sel_first + "" -1 line linestart"", sel_last)&#xa;            self.text_note.insert(sel_first + "" -1 line linestart"", selected_text + ""\n"" + prev_line_text)&#xa;&#xa;            if had_selection:&#xa;                self.text_note.tag_remove(""sel"", ""1.0"", ""end"")&#xa;                self.text_note.tag_add(""sel"", sel_first + "" -1 line linestart"", sel_last + "" -1 line lineend"")&#xa;&#xa;            self.text_note.mark_set(""insert"", sel_first + "" -1 line linestart"")&#xa;        elif direction == ""down"":&#xa;            if sel_last.split(""."")[0] == self.text_note.index(""end""):&#xa;                return&#xa;&#xa;            next_line_text = self.text_note.get(sel_last + "" +1 line linestart"", sel_last + "" +1 line lineend"")&#xa;            self.text_note.delete(sel_first, sel_last + "" +1 line lineend"")&#xa;            self.text_note.insert(sel_first, next_line_text + ""\n"" + selected_text)&#xa;&#xa;            if had_selection:&#xa;                self.text_note.tag_remove(""sel"", ""1.0"", ""end"")&#xa;                self.text_note.tag_add(""sel"", sel_first + "" +1 line linestart"", sel_last + "" +1 line lineend"")&#xa;&#xa;            self.text_note.mark_set(""insert"", sel_first + "" +1 line linestart"")&#xa;&#xa;    def cmd_moveup(self, evt=None):&#xa;        self.move(""up"")&#xa;        return ""break""&#xa;&#xa;    def cmd_movedown(self, evt=None):&#xa;        self.move(""down"")&#xa;        return ""break""&#xa;&#xa;    def set_note_editing(self, enable=True):&#xa;        """"""Enable or disable note editing controls.&#xa;&#xa;        This is used to disable the controls when no note has been selected.&#xa;        Disables note text widget, tag entry and pinned checkbutton.&#xa;&#xa;        @param enable: enable controls if True, else disable.&#xa;        @return: Nothing.&#xa;        """"""&#xa;&#xa;        state = tk.NORMAL if enable else tk.DISABLED&#xa;        self.text_note.config(state=state)&#xa;        self.tags_entry.config(state=state)&#xa;        self.pinned_checkbutton.config(state=state)&#xa;&#xa;    def get_continuous_rendering(self):&#xa;        return self.continuous_rendering.get()&#xa;&#xa;    def get_selected_text(self):&#xa;        """"""&#xa;        Return note text that has been selected by user.&#xa;        """"""&#xa;&#xa;        try:&#xa;            return self.text_note.selection_get()&#xa;        except tk.TclError:&#xa;            return ''&#xa;&#xa;    def get_text(self):&#xa;        # err, you have to specify 1.0 to END, and NOT 0 to END like I thought.&#xa;        # also, see the comment by Bryan Oakley to&#xa;        # http://stackoverflow.com/a/3137169&#xa;        # we need to get rid of newline that text adds automatically&#xa;        # at end.&#xa;        return self.text_note.get(1.0, ""end-1c"")&#xa;&#xa;    def get_search_entry_text(self):&#xa;        return self.search_entry_var.get()&#xa;&#xa;    def refresh_notes_list(self):&#xa;        """"""Trigger a complete refresh notes list by resetting search entry.&#xa;        """"""&#xa;        # store cursor position first! returns e.g. 8.32&#xa;        #cursor_pos = self.text_note.index(tk.INSERT)&#xa;&#xa;        # since 0.6, set_search_entry() tries to leave the currently selected&#xa;        # note untouched if it still exists in the newly returned list&#xa;        # so we don't have to do an explicit reselect.&#xa;        self.set_search_entry_text(self.get_search_entry_text())&#xa;&#xa;        #self.text_note.mark_set(tk.INSERT, cursor_pos)&#xa;&#xa;    def see_first_search_instance(self):&#xa;        """"""If there are instances of the search string in the current&#xa;        note, ensure that the first one is visible.&#xa;        """"""&#xa;&#xa;        if self.text_tags_search:&#xa;            self.text_note.see(self.text_tags_search[0] + '.first')&#xa;&#xa;    def select_note(self, idx, silent=False):&#xa;        """"""Programmatically select the note by idx&#xa;&#xa;        @param silent: If this is True, don't fire an event. VERY&#xa;        IMPORTANT: if you use silent, the controller won't set the&#xa;        selected_note_idx. You should make sure that it's in sync with&#xa;        what you've just selected.&#xa;        """"""&#xa;&#xa;        self.notes_list.select(idx, silent)&#xa;&#xa;    def select_note_by_name(self, name):&#xa;        idx = self.notes_list.find_note_by_title(name)&#xa;        if idx >= 0:&#xa;            self.select_note(idx, silent=False)&#xa;&#xa;        return idx&#xa;&#xa;    def set_note_status(self, status):&#xa;        """"""status is an object with ivars modified, saved and synced.&#xa;        """"""&#xa;&#xa;        if status.modified:&#xa;            s = 'modified'&#xa;            self.statusbar.set_note_status_color(2)&#xa;        elif status.saved and status.synced:&#xa;            s = 'saved + synced'&#xa;            self.statusbar.set_note_status_color(0)&#xa;        elif status.saved:&#xa;            s = 'saved'&#xa;            self.statusbar.set_note_status_color(1)&#xa;        else:&#xa;            s = 'synced'&#xa;            self.statusbar.set_note_status_color(0)&#xa;&#xa;        self.statusbar.set_note_status('Current note %s' % (s,))&#xa;&#xa;    def set_note_tally(self, filtered_notes, active_notes, total_notes):&#xa;        self.statusbar.set_centre_status('Listing %d / %d active notes (%d total)' % (filtered_notes, active_notes, total_notes))&#xa;&#xa;    def set_search_entry_text(self, text):&#xa;        self.search_entry_var.set(text)&#xa;&#xa;    def _bind_events(self):&#xa;        # make sure window close also goes through our handler&#xa;        self.root.protocol('WM_DELETE_WINDOW', self.handler_close)&#xa;&#xa;        self.root.bind_all(""<Control-g>"", lambda e: self.tags_entry.focus())&#xa;        self.root.bind_all(""<Control-question>"", lambda e: self.cmd_help_bindings())&#xa;        self.root.bind_all(""<Control-plus>"", lambda e: self.cmd_font_size(+1))&#xa;        self.root.bind_all(""<Control-minus>"", lambda e: self.cmd_font_size(-1))&#xa;&#xa;        self.notes_list.bind(""<<NotesListSelect>>"", self.cmd_notes_list_select)&#xa;        # same behaviour as when the user presses enter on search entry:&#xa;        # if something is selected, focus the text area&#xa;        # if nothing is selected, try to create new note with&#xa;        # search entry value as name&#xa;        self.notes_list.text.bind(""<Return>"", self.handler_search_enter)&#xa;&#xa;        self.search_entry.bind(""<Escape>"", lambda e:&#xa;                self.search_entry.delete(0, tk.END))&#xa;        self.search_entry.bind(""<Control-bracketleft>"", lambda e:&#xa;                self.search_entry.delete(0, tk.END))&#xa;        # this will either focus current content, or&#xa;        # if there's no selection, create a new note.&#xa;        self.search_entry.bind(""<Return>"", self.handler_search_enter)&#xa;&#xa;        self.search_entry.bind(""<Up>"", lambda e:&#xa;            self.notes_list.select_prev(silent=False))&#xa;        self.search_entry.bind(""<Control-k>"", lambda e:&#xa;            self.notes_list.select_prev(silent=False))&#xa;        self.search_entry.bind(""<Prior>"", lambda e:&#xa;            self.notes_list.select_prev(silent=False, delta=10))&#xa;&#xa;        self.search_entry.bind(""<Down>"", lambda e:&#xa;            self.notes_list.select_next(silent=False))&#xa;        self.search_entry.bind(""<Control-j>"", lambda e:&#xa;            self.notes_list.select_next(silent=False))&#xa;        self.search_entry.bind(""<Next>"", lambda e:&#xa;            self.notes_list.select_next(silent=False, delta=10))&#xa;&#xa;        self.text_note.bind(""<<Change>>"", self.handler_text_change)&#xa;&#xa;        # user presses escape in text area, they go back to notes list&#xa;        self.text_note.bind(""<Escape>"", lambda e: self.notes_list.text.focus())&#xa;        self.text_note.bind(""<Control-bracketleft>"", lambda e: self.notes_list.text.focus())&#xa;        # <Key>&#xa;&#xa;        self.text_note.bind(""<Control-a>"", self.cmd_select_all)&#xa;        self.text_note.bind(""<Control-BackSpace>"", self.cmd_delete_previous_word)&#xa;        self.text_note.bind(""<Control-d>"", self.cmd_delete_line)&#xa;        self.text_note.bind(""<Alt-c>"", self.cmd_toggle_checkbox)&#xa;        self.text_note.bind(""<Alt-Right>"", self.cmd_indent)&#xa;        self.text_note.bind(""<Alt-Left>"", self.cmd_deindent)&#xa;        self.text_note.bind(""<Alt-Up>"", self.cmd_moveup)&#xa;        self.text_note.bind(""<Alt-Down>"", self.cmd_movedown)&#xa;&#xa;        self.tags_entry.bind(""<Return>"", self.handler_add_tags_to_selected_note)&#xa;        self.tags_entry.bind(""<Escape>"", lambda e: self.text_note.focus())&#xa;&#xa;        self.pinned_checkbutton_var.trace('w', self.handler_pinned_checkbutton)&#xa;&#xa;        self.root.after(self.config.housekeeping_interval_ms, self.handler_housekeeper)&#xa;&#xa;    def _create_menu(self):&#xa;        """"""Utility function to setup main menu.&#xa;&#xa;        Called by _create_ui.&#xa;        """"""&#xa;&#xa;        # MAIN MENU ####################################################&#xa;        menu = tk.Menu(self.root)&#xa;        self.root.config(menu=menu)&#xa;&#xa;        file_menu = tk.Menu(menu, tearoff=False)&#xa;        menu.add_cascade(label=""File"", underline='0', menu=file_menu)&#xa;&#xa;        # FILE ##########################################################&#xa;        file_menu.add_command(label=""New note"", underline=0,&#xa;                              command=self.cmd_root_new, accelerator=""Ctrl+N"")&#xa;        self.root.bind_all(""<Control-n>"", self.cmd_root_new)&#xa;&#xa;        file_menu.add_command(label=""Delete note"", underline=0,&#xa;                              command=self.cmd_root_delete, accelerator=""Ctrl+Shift+D"")&#xa;        self.root.bind_all(""<Control-D>"", self.cmd_root_delete)&#xa;&#xa;        file_menu.add_command(label=""Archive note"", underline=0,&#xa;                command=self.cmd_root_archive, accelerator=""Ctrl+Shift+A"")&#xa;        self.root.bind_all(""<Control-A>"", self.cmd_root_archive)&#xa;&#xa;        file_menu.add_separator()&#xa;&#xa;        file_menu.add_command(label=""Sync full"", underline=5,&#xa;                              command=self.cmd_sync_full)&#xa;        file_menu.add_command(label=""Sync current note"",&#xa;                underline=0, command=self.cmd_sync_current_note)&#xa;        file_menu.add_command(label=""Sync local"",&#xa;                underline=0, command=self.cmd_sync_local,&#xa;                accelerator=""Ctrl+S"")&#xa;        self.root.bind_all(""<Control-s>"", self.cmd_sync_local)&#xa;&#xa;        file_menu.add_separator()&#xa;&#xa;        file_menu.add_command(label=""Render Markdown to HTML"", underline=7,&#xa;                command=self.cmd_markdown, accelerator=""Ctrl+M"")&#xa;        self.root.bind_all(""<Control-m>"", self.cmd_markdown)&#xa;&#xa;        self.continuous_rendering = tk.BooleanVar()&#xa;        self.continuous_rendering.set(False)&#xa;        file_menu.add_checkbutton(label=""Continuous Markdown to HTML rendering"",&#xa;                onvalue=True, offvalue=False,&#xa;                variable=self.continuous_rendering)&#xa;&#xa;        file_menu.add_command(label=""Render reST to HTML"", underline=7,&#xa;                command=self.cmd_rest, accelerator=""Ctrl+R"")&#xa;        self.root.bind_all(""<Control-r>"", self.cmd_rest)&#xa;&#xa;        file_menu.add_separator()&#xa;&#xa;        file_menu.add_command(label=""Exit"", underline=1,&#xa;                              command=self.handler_close, accelerator=""Ctrl+Q"")&#xa;        self.root.bind_all(""<Control-q>"", self.handler_close)&#xa;&#xa;        # EDIT ##########################################################&#xa;        edit_menu = tk.Menu(menu, tearoff=False)&#xa;        menu.add_cascade(label=""Edit"", underline=0, menu=edit_menu)&#xa;&#xa;        edit_menu.add_command(label=""Undo"", accelerator=""Ctrl+Z"",&#xa;                              underline=0, command=lambda: self.text_note.edit_undo())&#xa;        self.root.bind_all(""<Control-z>"", lambda e: self.text_note.edit_undo())&#xa;&#xa;        edit_menu.add_command(label=""Redo"", accelerator=""Ctrl+Y"",&#xa;                              underline=0, command=lambda: self.text_note.edit_undo())&#xa;        self.root.bind_all(""<Control-y>"", lambda e: self.text_note.edit_redo())&#xa;&#xa;        edit_menu.add_separator()&#xa;&#xa;        edit_menu.add_command(label=""Cut"", accelerator=""Ctrl+X"",&#xa;                              underline=2, command=self.cmd_cut)&#xa;        edit_menu.add_command(label=""Copy"", accelerator=""Ctrl+C"",&#xa;                              underline=0, command=self.cmd_copy)&#xa;        edit_menu.add_command(label=""Paste"", accelerator=""Ctrl+V"",&#xa;                              underline=0, command=self.cmd_paste)&#xa;&#xa;        edit_menu.add_command(label=""Select All"", accelerator=""Ctrl+A"",&#xa;                              underline=7, command=self.cmd_select_all)&#xa;        # FIXME: ctrl-a is usually bound to start-of-line. What's a&#xa;        # better binding for select all then?&#xa;&#xa;        edit_menu.add_separator()&#xa;&#xa;        edit_menu.add_command(label=""Find"", accelerator=""Ctrl+F"",&#xa;                              underline=0, command=lambda: self.search_entry.focus())&#xa;        self.root.bind_all(""<Control-f>"", self.search)&#xa;&#xa;        # TOOLS ########################################################&#xa;        tools_menu = tk.Menu(menu, tearoff=False)&#xa;        menu.add_cascade(label=""Tools"", underline=0, menu=tools_menu)&#xa;&#xa;        tools_menu.add_command(label=""Word Count"",&#xa;            underline=0, command=self.word_count)&#xa;&#xa;        tools_menu.add_command(label=""Toggle Checkbox"",&#xa;            underline=0, command=self.cmd_toggle_checkbox,&#xa;            accelerator=""Alt+c"")&#xa;&#xa;        # the internet thinks that multiple modifiers should work, but this didn't&#xa;        # want to.&#xa;        #self.root.bind_all(""<Control-Shift-c>"", lambda e: self.word_count())&#xa;&#xa;        # HELP ##########################################################&#xa;        help_menu = tk.Menu(menu, tearoff=False)&#xa;        menu.add_cascade(label=""Help"", underline='0', menu=help_menu)&#xa;&#xa;        help_menu.add_command(label=""About"", underline=0,&#xa;                              command=self.cmd_help_about)&#xa;        help_menu.add_command(label=""Bindings"", underline=0,&#xa;                              command=self.cmd_help_bindings,&#xa;                              accelerator=""Ctrl+?"")&#xa;&#xa;        # END MENU ######################################################&#xa;&#xa;    def _create_ui(self):&#xa;&#xa;        # set the correct class name. this helps your desktop environment&#xa;        # to identify the nvPY window.&#xa;        self.root = tk.Tk(className=""nvPY"")&#xa;&#xa;        # setup user-specified TTK theme&#xa;        # this HAS to happen after Tk() root has been instantiated, else&#xa;        # you'll see errors about PhotoImage not being PhotoImage when we&#xa;        # try to set the app icon.&#xa;        style = tk.Style()&#xa;        #print style.theme_names()&#xa;        #print style.theme_use()&#xa;        style.theme_use(self.config.theme)&#xa;&#xa;        self.root.title(""nvPY"")&#xa;        #self.root.configure(background=""#b2b2b2"")&#xa;&#xa;        # with iconphoto we have to use gif, also on windows&#xa;        icon_fn = 'nvpy.gif'&#xa;        iconpath = os.path.join(&#xa;            self.config.app_dir, 'icons', icon_fn)&#xa;&#xa;        self.icon = tk.PhotoImage(file=iconpath)&#xa;        self.root.tk.call('wm', 'iconphoto', self.root._w, self.icon)&#xa;&#xa;        # create menu ###################################################&#xa;        self._create_menu()&#xa;&#xa;        # separator after menu ##########################################&#xa;        #separator = tk.Frame(self.root, height=2, bd=1, relief=tk.SUNKEN)&#xa;        #separator.pack(fill=tk.X, padx=5, pady=2, side=tk.TOP)&#xa;&#xa;        # setup statusbar ###############################################&#xa;        # first pack this before panedwindow, else behaviour is unexpected&#xa;        # during sash moving and resizing&#xa;        self.statusbar = StatusBar(self.root)&#xa;        self.statusbar.set_status('%s', 'Welcome to nvPY!')&#xa;        self.statusbar.pack(fill=tk.X, side=tk.BOTTOM, padx=3, pady=3)&#xa;&#xa;        search_frame = tk.Frame(self.root)&#xa;&#xa;        search_entry.make_style()&#xa;        self.search_entry_var = tk.StringVar()&#xa;        self.search_entry = TriggeredcompleteEntry(search_frame, self.config.case_sensitive, textvariable=self.search_entry_var, style=""Search.entry"")&#xa;        self.search_entry_var.trace('w', self.handler_search_entry)&#xa;&#xa;        cs_label = tk.Label(search_frame, text=""CS "")&#xa;        self.cs_checkbutton_var = tk.IntVar()&#xa;        cs_checkbutton = tk.Checkbutton(search_frame, variable=self.cs_checkbutton_var)&#xa;        self.cs_checkbutton_var.trace('w', self.handler_cs_checkbutton)&#xa;&#xa;        self.search_mode_options = (""gstyle"", ""regexp"")&#xa;        self.search_mode_var = tk.StringVar()&#xa;        # I'm working with ttk.OptionVar, which has that extra default param!&#xa;        self.search_mode_cb = tk.OptionMenu(search_frame, self.search_mode_var,&#xa;            self.search_mode_options[0], *self.search_mode_options)&#xa;        self.search_mode_cb.config(width=6)&#xa;        self.search_mode_var.trace('w', self.handler_search_mode)&#xa;&#xa;        self.search_mode_cb.pack(side=tk.RIGHT, padx=5)&#xa;        cs_checkbutton.pack(side=tk.RIGHT)&#xa;        cs_label.pack(side=tk.RIGHT)&#xa;        self.search_entry.pack(fill=tk.X, padx=5, pady=5)&#xa;&#xa;        search_frame.pack(side=tk.TOP, fill=tk.X)&#xa;&#xa;        # the paned window ##############################################&#xa;&#xa;        if self.config.layout == ""horizontal"":&#xa;            paned_window = tk.PanedWindow(self.root, orient=tk.HORIZONTAL)&#xa;            paned_window.pack(fill=tk.BOTH, expand=1)&#xa;&#xa;            list_frame = tk.Frame(paned_window, width=100)&#xa;            paned_window.add(list_frame)&#xa;&#xa;            self.notes_list = NotesList(&#xa;                list_frame,&#xa;                self.config.list_font_family,&#xa;                self.config.list_font_size,&#xa;                utils.KeyValueObject(background_color=self.config.background_color,&#xa;                    layout=self.config.layout,&#xa;                    print_columns=self.config.print_columns))&#xa;            self.notes_list.pack(fill=tk.BOTH, expand=1)&#xa;&#xa;            note_frame = tk.Frame(paned_window, width=400)&#xa;&#xa;        else:&#xa;            paned_window = tk.PanedWindow(self.root, orient=tk.VERTICAL)&#xa;            paned_window.pack(fill=tk.BOTH, expand=1)&#xa;&#xa;            list_frame = tk.Frame(paned_window, height=150)&#xa;            list_frame.pack_propagate(0)&#xa;            paned_window.add(list_frame)&#xa;&#xa;            if self.config.print_columns == 1:&#xa;                font_family = self.config.list_font_family_fixed&#xa;            else:&#xa;                font_family = self.config.list_font_family&#xa;&#xa;            self.notes_list = NotesList(&#xa;                list_frame,&#xa;                font_family,&#xa;                self.config.list_font_size,&#xa;                utils.KeyValueObject(background_color=self.config.background_color,&#xa;                    layout=self.config.layout,&#xa;                    print_columns=self.config.print_columns))&#xa;            self.notes_list.pack(fill=tk.X, expand=1)&#xa;&#xa;            note_frame = tk.Frame(paned_window)&#xa;&#xa;        paned_window.add(note_frame)&#xa;&#xa;        note_pinned_frame = tk.Frame(note_frame)&#xa;        note_pinned_frame.pack(side=tk.BOTTOM, fill=tk.X)&#xa;&#xa;        pinned_label = tk.Label(note_pinned_frame, text=""Pinned"")&#xa;        pinned_label.pack(side=tk.LEFT)&#xa;        self.pinned_checkbutton_var = tk.IntVar()&#xa;        self.pinned_checkbutton = tk.Checkbutton(note_pinned_frame, variable=self.pinned_checkbutton_var)&#xa;        self.pinned_checkbutton.pack(side=tk.LEFT)&#xa;&#xa;        note_tags_frame = tk.Frame(note_pinned_frame)&#xa;        note_tags_frame.pack(side=tk.LEFT)&#xa;&#xa;        tags_label = tk.Label(note_tags_frame, text=""Add Tags"")&#xa;        tags_label.pack(side=tk.LEFT)&#xa;&#xa;        self.tags_entry_var = tk.StringVar()&#xa;        self.tags_entry = tk.Entry(note_tags_frame, textvariable=self.tags_entry_var)&#xa;        self.tags_entry.pack(side=tk.LEFT, fill=tk.X, expand=1, pady=3, padx=3)&#xa;&#xa;        self.note_existing_tags_frame = tk.Frame(note_tags_frame)&#xa;        self.note_existing_tags_frame.pack(side=tk.LEFT)&#xa;&#xa;        # we'll use this method to create the different edit boxes&#xa;        def create_scrolled_text(master):&#xa;            yscrollbar = tk.Scrollbar(master)&#xa;            yscrollbar.pack(side=tk.RIGHT, fill=tk.Y)&#xa;&#xa;            #f = tkFont.nametofont('TkFixedFont')&#xa;            f = tkFont.Font(family=self.config.font_family,&#xa;                            size=self.config.font_size)&#xa;            # tkFont.families(root) returns list of available font family names&#xa;            # this determines the width of the complete interface (yes)&#xa;            text = RedirectedText(master, height=self.config.text_height, width=self.config.text_width,&#xa;                                  wrap=tk.WORD,&#xa;                                  font=f, tabs=(4 * f.measure(0), 'left'), tabstyle='wordprocessor',&#xa;                                  yscrollcommand=yscrollbar.set,&#xa;                                  undo=True,&#xa;                                  background=self.config.background_color)&#xa;            # change default font at runtime with:&#xa;            text.config(font=f)&#xa;&#xa;            # need expand=1 so that when user resizes window, text widget gets the extra space&#xa;            text.pack(fill=tk.BOTH, expand=1)&#xa;&#xa;            #xscrollbar.config(command=text.xview)&#xa;            yscrollbar.config(command=text.yview)&#xa;&#xa;            return text&#xa;&#xa;        # setup user_text ###############################################&#xa;        self.text_note = create_scrolled_text(note_frame)&#xa;        self.fonts = self.notes_list.fonts + self.text_note.fonts&#xa;&#xa;        # setup generic tags for markdown highlighting&#xa;        bold_font = tkFont.Font(self.text_note, self.text_note.cget(""font""))&#xa;        bold_font.configure(weight=""bold"")&#xa;        self.text_note.tag_config('md-bold', font=bold_font)&#xa;&#xa;        # finish UI creation ###########################################&#xa;&#xa;        # now set the minsize so that things can not disappear&#xa;        self.root.minsize(self.root.winfo_width(), self.root.winfo_height())&#xa;&#xa;        # call update so we know that sizes are up to date&#xa;        self.root.update_idletasks()&#xa;&#xa;    def get_number_of_notes(self):&#xa;        return self.notes_list.get_number_of_notes()&#xa;&#xa;    def handler_close(self, evt=None):&#xa;        """"""Handler for exit menu command and close window event.&#xa;        """"""&#xa;        self.notify_observers('close', None)&#xa;&#xa;    def clear_note_ui(self, silent=True):&#xa;        """"""Called when no note has been selected.&#xa;&#xa;        Should give the user clear indication that no note has been selected,&#xa;        hence no note editing actions can be taken.&#xa;&#xa;        @param silent: The default is not to fire any event handlers when&#xa;        clearing the note.&#xa;        @return:&#xa;        """"""&#xa;&#xa;        # ascii art created with: http://patorjk.com/software/taag/&#xa;&#xa;        msg = """"""&#xa;        No note currently selected.&#xa;&#xa;        Either select a note, or press Ctrl-N to create&#xa;        a new note titled with the current search string,&#xa;        or modify the search string.&#xa;&#xa;        .__   __. ____    ____ .______   ____    ____&#xa;        |  \ |  | \   \  /   / |   _  \  \   \  /   /&#xa;        |   \|  |  \   \/   /  |  |_)  |  \   \/   /&#xa;        |  . `  |   \      /   |   ___/    \_    _/&#xa;        |  |\   |    \    /    |  |          |  |&#xa;        |__| \__|     \__/     | _|          |__|&#xa;&#xa;&#xa;        """"""&#xa;&#xa;        if silent:&#xa;            self.mute_note_data_changes()&#xa;&#xa;        self.text_note.delete(1.0, tk.END)  # clear all&#xa;        self.text_note.insert(1.0, msg)&#xa;        self.tags_entry_var.set('')&#xa;&#xa;        self.statusbar.set_note_status('No note selected.')&#xa;&#xa;        if silent:&#xa;            self.unmute_note_data_changes()&#xa;&#xa;    def close(self):&#xa;        """"""Programmatically close application windows.&#xa;&#xa;        Called by controller.&#xa;        """"""&#xa;        self.root.destroy()&#xa;&#xa;    def cmd_cut(self):&#xa;        self.text_note.event_generate('<<Cut>>')&#xa;&#xa;    def cmd_copy(self):&#xa;        self.text_note.event_generate('<<Copy>>')&#xa;&#xa;    def cmd_markdown(self, event=None):&#xa;        self.notify_observers('command:markdown', None)&#xa;&#xa;    def cmd_paste(self):&#xa;        self.text_note.event_generate('<<Paste>>')&#xa;&#xa;    def cmd_help_about(self):&#xa;&#xa;        tkMessageBox.showinfo(&#xa;            'Help | About',&#xa;            'nvPY %s is copyright 2012-2016 by Charl P. Botha '&#xa;            '<http://charlbotha.com/>\n\n'&#xa;            'A rather ugly but cross-platform simplenote client.' % (self.config.app_version,),&#xa;            parent=self.root)&#xa;&#xa;    def cmd_help_bindings(self):&#xa;        h = HelpBindings()&#xa;        self.root.wait_window(h)&#xa;&#xa;    def cmd_rest(self, event=None):&#xa;        self.notify_observers('command:rest', None)&#xa;&#xa;    def cmd_sync_current_note(self, event=None):&#xa;        self.notify_observers('command:sync_current_note', None)&#xa;&#xa;    def cmd_sync_local(self, event=None):&#xa;        self.handler_housekeeper()&#xa;&#xa;    def cmd_sync_full(self, event=None):&#xa;        self.notify_observers('command:sync_full', None)&#xa;&#xa;    def cmd_font_size(self, inc_size):&#xa;        for f in self.fonts:&#xa;            f.configure(size=f['size'] + inc_size)&#xa;&#xa;    def handler_cs_checkbutton(self, *args):&#xa;        self.notify_observers('change:cs',&#xa;            utils.KeyValueObject(value=self.cs_checkbutton_var.get()))&#xa;&#xa;    def handler_housekeeper(self):&#xa;        # nvPY will do saving and syncing!&#xa;        self.notify_observers('keep:house', None)&#xa;&#xa;        # check if titles need refreshing&#xa;        refresh_notes_list = False&#xa;        prev_title = None&#xa;        prev_createdate = None&#xa;        prev_modifydate = None&#xa;        prev_pinned = 0&#xa;        for i, o in enumerate(self.notes_list_model.list):&#xa;            # order should be the same as our listbox&#xa;            nt = utils.get_note_title(o.note)&#xa;            ot = self.notes_list.get_title(i)&#xa;            # if we strike a note with an out-of-date title, redo.&#xa;            if nt != ot:&#xa;                logging.debug('title ""%s"" resync' % (nt,))&#xa;                refresh_notes_list = True&#xa;                break&#xa;&#xa;            # compare modifydate timestamp in our notes_list_model to what's displayed&#xa;            # if these are more than 60 seconds apart, we want to update our&#xa;            # mod-date display.&#xa;            cd = float(o.note.get('createdate', 0))&#xa;            ocd = self.notes_list.get_createdate(i)&#xa;            if cd != ocd:&#xa;                # we log the title&#xa;                logging.debug('createdate ""%s"" resync, %d - %d' % (nt,cd,ocd))&#xa;                refresh_notes_list = True&#xa;                break&#xa;&#xa;            md = float(o.note.get('modifydate', 0))&#xa;            omd = self.notes_list.get_modifydate(i)&#xa;            if abs(md - omd) > 60:&#xa;                # we log the title&#xa;                logging.debug('modifydate ""%s"" resync' % (nt,))&#xa;                refresh_notes_list = True&#xa;                break&#xa;&#xa;            pinned = utils.note_pinned(o.note)&#xa;            old_pinned = self.notes_list.get_pinned(i)&#xa;            if pinned != old_pinned:&#xa;                # we log the title&#xa;                logging.debug('pinned ""%s"" resync' % (nt,))&#xa;                refresh_notes_list = True&#xa;                break&#xa;&#xa;            tags = o.note.get('tags', 0)&#xa;            old_tags = self.notes_list.get_tags(i)&#xa;            if tags != old_tags:&#xa;                # we log the title&#xa;                logging.debug('tags ""%s"" resync' % (nt,))&#xa;                refresh_notes_list = True&#xa;                break&#xa;&#xa;            if self.config.sort_mode == 0:&#xa;                # alpha&#xa;                if prev_title is not None and prev_title > nt:&#xa;                    logging.debug(""alpha resort triggered"")&#xa;                    refresh_notes_list = True&#xa;                    break&#xa;&#xa;                prev_title = nt&#xa;&#xa;            elif self.config.sort_mode == 2:&#xa;                if prev_createdate is not None and prev_createdate < cd and \&#xa;                   not prev_pinned:&#xa;                    logging.debug(""createdate resort triggered %d > %d"" % (cd, prev_createdate))&#xa;                    refresh_notes_list = True&#xa;                    break&#xa;&#xa;                prev_createdate = cd&#xa;                if self.config.pinned_ontop:&#xa;                    prev_pinned = utils.note_pinned(o.note)&#xa;&#xa;            else:&#xa;&#xa;                # we go from top to bottom, newest to oldest&#xa;                # this means that prev_modifydate (above) needs to be larger&#xa;                # than md (below). if it's not, re-sort.&#xa;                if prev_modifydate is not None and prev_modifydate < md and \&#xa;                   not prev_pinned:&#xa;                    logging.debug(""modifydate resort triggered"")&#xa;                    refresh_notes_list = True&#xa;                    break&#xa;&#xa;                prev_modifydate = md&#xa;                if self.config.pinned_ontop:&#xa;                    prev_pinned = utils.note_pinned(o.note)&#xa;&#xa;        if refresh_notes_list:&#xa;            self.refresh_notes_list()&#xa;&#xa;        self.root.after(self.config.housekeeping_interval_ms, self.handler_housekeeper)&#xa;&#xa;    def handler_pinned_checkbutton(self, *args):&#xa;        self.notify_observers('change:pinned',&#xa;            utils.KeyValueObject(value=self.pinned_checkbutton_var.get()))&#xa;&#xa;    def handler_search_enter(self, evt):&#xa;        # user has pressed enter whilst searching&#xa;        # 1. if a note is selected, focus that&#xa;        # 2. if nothing is selected, create a new note with this title&#xa;&#xa;        if self.notes_list.selected_idx >= 0:&#xa;            self.text_note.focus()&#xa;            self.text_note.see(tk.INSERT)&#xa;&#xa;        else:&#xa;            # nothing selected&#xa;            self.notify_observers('create:note', utils.KeyValueObject(title=self.get_search_entry_text()))&#xa;            # the note will be created synchronously, so we can focus the text area already&#xa;            self.text_note.focus()&#xa;&#xa;    def handler_search_entry(self, *args):&#xa;        self.notify_observers('change:entry',&#xa;                              utils.KeyValueObject(value=self.search_entry_var.get()))&#xa;&#xa;    def handler_search_mode(self, *args):&#xa;        """"""&#xa;        Called when the user changes the search mode via the OptionMenu.&#xa;&#xa;        This will also be called even if the user reselects the same option.&#xa;&#xa;        @param args:&#xa;        @return:&#xa;        """"""&#xa;&#xa;        self.notify_observers('change:search_mode',&#xa;            utils.KeyValueObject(value=self.search_mode_var.get()))&#xa;&#xa;    def handler_add_tags_to_selected_note(self, evt=None):&#xa;        self.notify_observers('add:tag', utils.KeyValueObject(tags=self.tags_entry_var.get()))&#xa;&#xa;    def handler_click_link(self, link):&#xa;        if link.startswith('[['):&#xa;            link = link[2:-2]&#xa;            self.notify_observers('click:notelink', link)&#xa;&#xa;        else:&#xa;            webbrowser.open(link)&#xa;&#xa;    def activate_search_string_highlights(self):&#xa;        # no note selected, so no highlights.&#xa;        if self.notes_list.selected_idx < 0:&#xa;            return&#xa;&#xa;        t = self.text_note&#xa;&#xa;        # remove all existing tags&#xa;        for tag in self.text_tags_search:&#xa;            t.tag_remove(tag, '1.0', 'end')&#xa;&#xa;        del self.text_tags_search[:]&#xa;&#xa;        st = self.notes_list_model.match_regexp&#xa;        if not st:&#xa;            return&#xa;&#xa;        # take care of invalid regular expressions...&#xa;        try:&#xa;            if self.config.case_sensitive == 0:&#xa;                pat = re.compile(st, re.I)&#xa;            else:&#xa;                pat = re.compile(st)&#xa;&#xa;        except re.error:&#xa;            return&#xa;&#xa;        for mo in pat.finditer(t.get('1.0', 'end')):&#xa;&#xa;            # start creating a new tkinter text tag&#xa;            tag = 'search-%d' % (len(self.text_tags_search),)&#xa;            t.tag_config(tag, background=""yellow"")&#xa;&#xa;            # mo.start(), mo.end() or mo.span() in one go&#xa;            t.tag_add(tag, '1.0+%dc' % (mo.start(),), '1.0+%dc' %&#xa;                    (mo.end(),))&#xa;&#xa;            # record the tag name so we can delete it later&#xa;            self.text_tags_search.append(tag)&#xa;&#xa;    def activate_links(self):&#xa;        """"""&#xa;        Also see this post on URL detection regular expressions:&#xa;        http://www.regexguru.com/2008/11/detecting-urls-in-a-block-of-text/&#xa;        (mine is slightly modified)&#xa;        """"""&#xa;&#xa;        t = self.text_note&#xa;        # the last group matches [[bla bla]] inter-note links&#xa;        pat = \&#xa;        r""\b((https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;\(\)]*[A-Za-z0-9+&@#/%=~_|])|(\[\[[^][]*\]\])""&#xa;&#xa;        # remove all existing tags&#xa;        for tag in self.text_tags_links:&#xa;            t.tag_remove(tag, '1.0', 'end')&#xa;&#xa;        del self.text_tags_links[:]&#xa;&#xa;        for mo in re.finditer(pat, t.get('1.0', 'end')):&#xa;            # extract the link from the match object&#xa;            if mo.groups()[2] is not None:&#xa;                link = mo.groups()[2]&#xa;                ul = 0&#xa;            else:&#xa;                link = mo.groups()[0]&#xa;                ul = 1&#xa;&#xa;            # start creating a new tkinter text tag&#xa;            tag = 'web-%d' % (len(self.text_tags_links),)&#xa;            t.tag_config(tag, foreground=""blue"", underline=ul)&#xa;            # hovering should give us the finger (cursor) hehe&#xa;            t.tag_bind(tag, '<Enter>',&#xa;                    lambda e: t.config(cursor=""hand2""))&#xa;            t.tag_bind(tag, '<Leave>',&#xa;                    lambda e: t.config(cursor=""""))&#xa;            # and clicking on it should do something sensible&#xa;            t.tag_bind(tag, '<Button-1>', lambda e, link=link:&#xa;                    self.handler_click_link(link))&#xa;&#xa;            # mo.start(), mo.end() or mo.span() in one go&#xa;            t.tag_add(tag, '1.0+%dc' % (mo.start(),), '1.0+%dc' %&#xa;                    (mo.end(),))&#xa;&#xa;            # record the tag name so we can delete it later&#xa;            self.text_tags_links.append(tag)&#xa;&#xa;    def activate_markdown_highlighting(self):&#xa;        t = self.text_note&#xa;        content = t.get('1.0', 'end')&#xa;&#xa;        # we have multiple tags with the same name, e.g. md-bold&#xa;        # this will remove all of them.&#xa;        t.tag_remove('md-bold', '1.0', 'end')&#xa;&#xa;        # first just use our standard regular expression for finding the first&#xa;        # non whitespace line, wherever it is:&#xa;        mo = utils.note_title_re.match(content)&#xa;        if mo:&#xa;            t.tag_add('md-bold',&#xa;                      '1.0+{0}c'.format(mo.start()),&#xa;                      '1.0+{0}c'.format(mo.end()))&#xa;&#xa;        # then do headings&#xa;        pat = re.compile(r""^#.*$"", re.MULTILINE)&#xa;&#xa;        for mo in pat.finditer(content):&#xa;            # mo.start(), mo.end() or mo.span() in one go&#xa;            t.tag_add('md-bold',&#xa;                      '1.0+{0}c'.format(mo.start()),&#xa;                      '1.0+{0}c'.format(mo.end()))&#xa;&#xa;    def handler_text_change(self, evt):&#xa;        self.notify_observers('change:text', None)&#xa;        # FIXME: consider having this called from the housekeeping&#xa;        # handler, so that the poor regexp doesn't have to do every&#xa;        # single keystroke.&#xa;        self.activate_links()&#xa;        self.activate_search_string_highlights()&#xa;        self.activate_markdown_highlighting()&#xa;&#xa;    def is_note_different(self, note):&#xa;        """"""&#xa;        Determine if note would cause a UI update.&#xa;        """"""&#xa;&#xa;        if self.get_text() != note.get('content'):&#xa;            return True&#xa;&#xa;        tags = note.get('tags', [])&#xa;&#xa;        # get list of string tags from ui&#xa;        tag_elements = self.note_existing_tags_frame.children.values()&#xa;        ui_tags = [element['text'].replace(' x', '') for element in tag_elements]&#xa;&#xa;        if sorted(ui_tags) != sorted(tags):&#xa;            return True&#xa;&#xa;        if bool(self.pinned_checkbutton_var.get()) != bool(utils.note_pinned(note)):&#xa;            return True&#xa;&#xa;    def observer_notes_list(self, notes_list_model, evt_type, evt):&#xa;        if evt_type == 'set:list':&#xa;            # re-render!&#xa;            self.set_notes(notes_list_model.list)&#xa;&#xa;    def main_loop(self):&#xa;        self.root.mainloop()&#xa;&#xa;    def mute_note_data_changes(self):&#xa;        self.mute('change:text')&#xa;        self.mute('add:tag')&#xa;        self.mute('delete:tag')&#xa;        self.mute('change:pinned')&#xa;&#xa;    def search(self, e):&#xa;        self.search_entry.focus()&#xa;        self.search_entry.selection_range(0, tk.END)&#xa;&#xa;    def set_cs(self, cs, silent=False):&#xa;        if silent:&#xa;            self.mute('change:cs')&#xa;&#xa;        self.cs_checkbutton_var.set(cs)&#xa;&#xa;        self.unmute('change:cs')&#xa;&#xa;    def set_search_mode(self, search_mode, silent=False):&#xa;        """"""&#xa;&#xa;        @param search_mode: the search mode, ""gstyle"" or ""regexp""&#xa;        @param silent: Specify True if you don't want the view to trigger any events.&#xa;        @return:&#xa;        """"""&#xa;&#xa;        if silent:&#xa;            self.mute('change:search_mode')&#xa;&#xa;        self.search_mode_var.set(search_mode)&#xa;&#xa;        self.unmute('change:search_mode')&#xa;&#xa;    def set_status_text(self, txt):&#xa;        self.statusbar.set_status(txt)&#xa;&#xa;    def handler_delete_tag_from_selected_note(self,tag_name):&#xa;        self.notify_observers('delete:tag', utils.KeyValueObject(tag=tag_name))&#xa;&#xa;    def set_note_data(self, note, reset_undo=True, content_unchanged=False):&#xa;        """"""Replace text in editor with content.&#xa;&#xa;        This is usually called when a new note is selected (case 1), or&#xa;        when a modified note comes back from the server (case 2).&#xa;&#xa;        @param reset_undo: Set to False if you don't want to have the undo&#xa;        buffer to reset.&#xa;        @param content_unchanged: Set to True if you know that the content&#xa;        has not changed, only the tags and pinned status.&#xa;        """"""&#xa;&#xa;        if not content_unchanged:&#xa;            self.text_note.delete(1.0, tk.END)  # clear all&#xa;&#xa;        if note is not None:&#xa;            if not content_unchanged:&#xa;                self.text_note.insert(tk.END, note['content'])&#xa;&#xa;            # default to an empty array for tags&#xa;            tags = note.get('tags', [])&#xa;&#xa;        else:&#xa;            # note is None - for tags machinery further down, we have empty list&#xa;            tags = []&#xa;&#xa;        for tag_button in self.note_existing_tags_frame.children.values():&#xa;            tag_button.destroy()&#xa;&#xa;        for tag in tags:&#xa;            tag_button = tk.Button(&#xa;                    self.note_existing_tags_frame, width=0, text=tag + "" x"",&#xa;                    command=lambda tag=tag:&#xa;                    self.handler_delete_tag_from_selected_note(tag))&#xa;            tag_button.pack(side=tk.LEFT)&#xa;&#xa;            #self.tags_entry_var.set(','.join(tags))&#xa;            self.pinned_checkbutton_var.set(utils.note_pinned(note))&#xa;&#xa;        if reset_undo:&#xa;            # usually when a new note is selected, we want to reset the&#xa;            # undo buffer, so that a user can't undo right into the previously&#xa;            # selected note.&#xa;            self.text_note.edit_reset()&#xa;&#xa;    def set_notes(self, notes):&#xa;        # this method is called by View.observer_notes_list()&#xa;&#xa;        # clear the notes list&#xa;        self.notes_list.clear()&#xa;        taglist = []&#xa;&#xa;        for o in notes:&#xa;            tags = o.note.get('tags')&#xa;            if tags:&#xa;                taglist += tags&#xa;&#xa;            self.notes_list.append(o.note, utils.KeyValueObject(tagfound=o.tagfound))&#xa;&#xa;        if self.taglist is None:&#xa;            # first time we get called, so we need to initialise&#xa;            self.taglist = taglist&#xa;            self.search_entry.set_completion_list(self.taglist)&#xa;&#xa;        else:&#xa;            # only set completion list if the new combined taglist is larger.&#xa;            taglist = list(set(self.taglist + taglist))&#xa;            if len(taglist) > len(self.taglist):&#xa;                self.taglist = taglist&#xa;                self.search_entry.set_completion_list(self.taglist)&#xa;&#xa;    def show_error(self, title, msg):&#xa;        tkMessageBox.showerror(title, msg)&#xa;&#xa;    def show_info(self, title, msg):&#xa;        tkMessageBox.showinfo(title, msg, parent=self.root)&#xa;&#xa;    def show_warning(self, title, msg):&#xa;        tkMessageBox.showwarning(title, msg)&#xa;&#xa;    def unmute_note_data_changes(self):&#xa;        self.unmute('change:text')&#xa;        self.unmute('add:tag')&#xa;        self.unmute('delete:tag')&#xa;        self.unmute('change:pinned')&#xa;&#xa;    def update_selected_note_data(self, note):&#xa;        """"""&#xa;        Update currently selected note's data.&#xa;&#xa;        This is called when the user triggers a per-note sync and a newer&#xa;        note comes back, but also when the search string changes, and the&#xa;        currently selected note gets a newer version due to background or&#xa;        foreground syncing.&#xa;&#xa;        We take care only to update the note content if it has actually&#xa;        changed, to minimise visual glitches.&#xa;        """"""&#xa;&#xa;        # the user is not changing anything, so we don't want the event to fire&#xa;        self.mute_note_data_changes()&#xa;&#xa;        current_content = self.get_text()&#xa;        new_content = note.get('content', '')&#xa;&#xa;        if new_content != current_content:&#xa;            # store cursor position&#xa;            cursor_pos = self.text_note.index(tk.INSERT)&#xa;            # also store visible window&#xa;            first, last = self.text_note.yview()&#xa;&#xa;            # set new note contents, pinned status and tags&#xa;            # but keep user's undo buffer&#xa;            self.set_note_data(note, reset_undo=False)&#xa;&#xa;            # restore visible window&#xa;            self.text_note.yview('moveto', first)&#xa;            self.text_note.mark_set(tk.INSERT, cursor_pos)&#xa;            self.activate_links()&#xa;            self.activate_search_string_highlights()&#xa;&#xa;        else:&#xa;            # we know the content is the same, so we only set the rest&#xa;            # obviously keep user's undo buffer.&#xa;            self.set_note_data(note, reset_undo=False, content_unchanged=True)&#xa;&#xa;        # reactivate event handlers&#xa;        self.unmute_note_data_changes()&#xa;&#xa;    def word_count(self):&#xa;        """"""&#xa;        Display count of total words and selected words in a dialog box.&#xa;        """"""&#xa;&#xa;        sel = self.get_selected_text()&#xa;        slen = len(sel.split())&#xa;&#xa;        txt = self.get_text()&#xa;        tlen = len(txt.split())&#xa;&#xa;        self.show_info('Word Count', '%d words in total\n%d words in selection' % (tlen, slen))&#xa;"
14620633|"# Natural Language Toolkit: Framenet Corpus Reader&#xa;#&#xa;# Copyright (C) 2001-2015 NLTK Project&#xa;# Authors: Chuck Wooters <wooters@icsi.berkeley.edu>,&#xa;#          Nathan Schneider <nschneid@cs.cmu.edu>&#xa;# URL: <http://nltk.org/>&#xa;# For license information, see LICENSE.TXT&#xa;from __future__ import print_function, unicode_literals&#xa;&#xa;""""""&#xa;Corpus reader for the Framenet 1.5 Corpus.&#xa;""""""&#xa;&#xa;__docformat__ = 'epytext en'&#xa;&#xa;import os, sys&#xa;import re&#xa;import textwrap&#xa;from collections import defaultdict&#xa;from pprint import pprint, pformat&#xa;from nltk.internals import ElementWrapper&#xa;from nltk.corpus.reader import XMLCorpusReader, XMLCorpusView&#xa;from nltk.compat import text_type, string_types, python_2_unicode_compatible&#xa;from nltk.util import AbstractLazySequence, LazyMap&#xa;&#xa;&#xa;def _pretty_longstring(defstr, prefix='', wrap_at=65):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a long string.&#xa;&#xa;    :param defstr: The string to be printed.&#xa;    :type defstr: str&#xa;    :return: A nicely formated string representation of the long string.&#xa;    :rtype: str&#xa;    """"""&#xa;&#xa;    outstr = """"&#xa;    for line in textwrap.fill(defstr, wrap_at).split('\n'):&#xa;        outstr += prefix + line + '\n'&#xa;    return outstr&#xa;&#xa;def _pretty_any(obj):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing any AttrDict object.&#xa;&#xa;    :param obj: The obj to be printed.&#xa;    :type obj: AttrDict&#xa;    :return: A nicely formated string representation of the AttrDict object.&#xa;    :rtype: str&#xa;    """"""&#xa;&#xa;    outstr = """"&#xa;    for k in obj:&#xa;        if isinstance(obj[k], string_types) and len(obj[k]) > 65:&#xa;            outstr += ""[{0}]\n"".format(k)&#xa;            outstr += ""{0}"".format(_pretty_longstring(obj[k], prefix='  '))&#xa;            outstr += '\n'&#xa;        else:&#xa;            outstr += ""[{0}] {1}\n"".format(k, obj[k])&#xa;&#xa;    return outstr&#xa;&#xa;def _pretty_semtype(st):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a semantic type.&#xa;&#xa;    :param st: The semantic type to be printed.&#xa;    :type st: AttrDict&#xa;    :return: A nicely formated string representation of the semantic type.&#xa;    :rtype: str&#xa;    """"""&#xa;&#xa;    semkeys = st.keys()&#xa;    if len(semkeys) == 1: return ""<None>""&#xa;&#xa;    outstr = """"&#xa;    outstr += ""semantic type ({0.ID}): {0.name}\n"".format(st)&#xa;    if 'abbrev' in semkeys:&#xa;        outstr += ""[abbrev] {0}\n"".format(st.abbrev)&#xa;    if 'definition' in semkeys:&#xa;        outstr += ""[definition]\n""&#xa;        outstr += _pretty_longstring(st.definition,'  ')&#xa;    outstr += ""[rootType] {0}({1})\n"".format(st.rootType.name, st.rootType.ID)&#xa;    if st.superType is None:&#xa;        outstr += ""[superType] <None>\n""&#xa;    else:&#xa;        outstr += ""[superType] {0}({1})\n"".format(st.superType.name, st.superType.ID)&#xa;    outstr += ""[subTypes] {0} subtypes\n"".format(len(st.subTypes))&#xa;    outstr += ""  "" + "", "".join('{0}({1})'.format(x.name, x.ID) for x in st.subTypes) + '\n'*(len(st.subTypes)>0)&#xa;    return outstr&#xa;&#xa;def _pretty_frame_relation_type(freltyp):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a frame relation type.&#xa;&#xa;    :param freltyp: The frame relation type to be printed.&#xa;    :type freltyp: AttrDict&#xa;    :return: A nicely formated string representation of the frame relation type.&#xa;    :rtype: str&#xa;    """"""&#xa;    outstr = ""<frame relation type ({0.ID}): {0.superFrameName} -- {0.name} -> {0.subFrameName}>"".format(freltyp)&#xa;    return outstr&#xa;&#xa;def _pretty_frame_relation(frel):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a frame relation.&#xa;&#xa;    :param frel: The frame relation to be printed.&#xa;    :type frel: AttrDict&#xa;    :return: A nicely formated string representation of the frame relation.&#xa;    :rtype: str&#xa;    """"""&#xa;    outstr = ""<{0.type.superFrameName}={0.superFrameName} -- {0.type.name} -> {0.type.subFrameName}={0.subFrameName}>"".format(frel)&#xa;    return outstr&#xa;&#xa;def _pretty_fe_relation(ferel):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing an FE relation.&#xa;&#xa;    :param ferel: The FE relation to be printed.&#xa;    :type ferel: AttrDict&#xa;    :return: A nicely formated string representation of the FE relation.&#xa;    :rtype: str&#xa;    """"""&#xa;    outstr = ""<{0.type.superFrameName}={0.frameRelation.superFrameName}.{0.superFEName} -- {0.type.name} ->{0.type.subFrameName}={0.frameRelation.subFrameName}.{0.subFEName}>"".format(ferel)&#xa;    return outstr&#xa;&#xa;def _pretty_lu(lu):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a lexical unit.&#xa;&#xa;    :param lu: The lu to be printed.&#xa;    :type lu: AttrDict&#xa;    :return: A nicely formated string representation of the lexical unit.&#xa;    :rtype: str&#xa;    """"""&#xa;&#xa;    lukeys = lu.keys()&#xa;    outstr = """"&#xa;    outstr += ""lexical unit ({0.ID}): {0.name}\n\n"".format(lu)&#xa;    if 'definition' in lukeys:&#xa;        outstr += ""[definition]\n""&#xa;        outstr += _pretty_longstring(lu.definition,'  ')&#xa;    if 'frame' in lukeys:&#xa;        outstr += ""\n[frame] {0}({1})\n"".format(lu.frame.name,lu.frame.ID)&#xa;    if 'incorporatedFE' in lukeys:&#xa;        outstr += ""\n[incorporatedFE] {0}\n"".format(lu.incorporatedFE)&#xa;    if 'POS' in lukeys:&#xa;        outstr += ""\n[POS] {0}\n"".format(lu.POS)&#xa;    if 'status' in lukeys:&#xa;        outstr += ""\n[status] {0}\n"".format(lu.status)&#xa;    if 'totalAnnotated' in lukeys:&#xa;        outstr += ""\n[totalAnnotated] {0} annotated examples\n"".format(lu.totalAnnotated)&#xa;    if 'lexemes' in lukeys:&#xa;        outstr += ""\n[lexemes] {0}\n"".format(' '.join('{0}/{1}'.format(lex.name,lex.POS) for lex in lu.lexemes))&#xa;    if 'semTypes' in lukeys:&#xa;        outstr += ""\n[semTypes] {0} semantic types\n"".format(len(lu.semTypes))&#xa;        outstr += ""  ""*(len(lu.semTypes)>0) + "", "".join('{0}({1})'.format(x.name, x.ID) for x in lu.semTypes) + '\n'*(len(lu.semTypes)>0)&#xa;    if 'subCorpus' in lukeys:&#xa;        subc = [x.name for x in lu.subCorpus]&#xa;        outstr += ""\n[subCorpus] {0} subcorpora\n"".format(len(lu.subCorpus))&#xa;        for line in textwrap.fill("", "".join(sorted(subc)), 60).split('\n'):&#xa;            outstr += ""  {0}\n"".format(line)&#xa;&#xa;    return outstr&#xa;&#xa;def _pretty_fe(fe):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a frame element.&#xa;&#xa;    :param fe: The frame element to be printed.&#xa;    :type fe: AttrDict&#xa;    :return: A nicely formated string representation of the frame element.&#xa;    :rtype: str&#xa;    """"""&#xa;    fekeys = fe.keys()&#xa;    outstr = """"&#xa;    outstr += ""frame element ({0.ID}): {0.name}\n    of {1.name}({1.ID})\n"".format(fe, fe.frame)&#xa;    if 'definition' in fekeys:&#xa;        outstr += ""[definition]\n""&#xa;        outstr += _pretty_longstring(fe.definition,'  ')&#xa;    if 'abbrev' in fekeys:&#xa;        outstr += ""[abbrev] {0}\n"".format(fe.abbrev)&#xa;    if 'coreType' in fekeys:&#xa;        outstr += ""[coreType] {0}\n"".format(fe.coreType)&#xa;    if 'requiresFE' in fekeys:&#xa;        outstr += ""[requiresFE] ""&#xa;        if fe.requiresFE is None:&#xa;            outstr += ""<None>\n""&#xa;        else:&#xa;            outstr += ""{0}({1})\n"".format(fe.requiresFE.name, fe.requiresFE.ID)&#xa;    if 'excludesFE' in fekeys:&#xa;        outstr += ""[excludesFE] ""&#xa;        if fe.excludesFE is None:&#xa;            outstr += ""<None>\n""&#xa;        else:&#xa;            outstr += ""{0}({1})\n"".format(fe.excludesFE.name, fe.excludesFE.ID)&#xa;    if 'semType' in fekeys:&#xa;        outstr += ""[semType] ""&#xa;        if fe.semType is None:&#xa;            outstr += ""<None>\n""&#xa;        else:&#xa;            outstr += ""\n  "" + ""{0}({1})"".format(fe.semType.name, fe.semType.ID) + '\n'&#xa;&#xa;    return outstr&#xa;&#xa;def _pretty_frame(frame):&#xa;&#xa;    """"""&#xa;    Helper function for pretty-printing a frame.&#xa;&#xa;    :param frame: The frame to be printed.&#xa;    :type frame: AttrDict&#xa;    :return: A nicely formated string representation of the frame.&#xa;    :rtype: str&#xa;    """"""&#xa;&#xa;    outstr = """"&#xa;    outstr += ""frame ({0.ID}): {0.name}\n\n"".format(frame)&#xa;    outstr += ""[definition]\n""&#xa;    outstr += _pretty_longstring(frame.definition, '  ') + '\n'&#xa;&#xa;    outstr += ""[semTypes] {0} semantic types\n"".format(len(frame.semTypes))&#xa;    outstr += ""  ""*(len(frame.semTypes)>0) + "", "".join(""{0}({1})"".format(x.name, x.ID) for x in frame.semTypes) + '\n'*(len(frame.semTypes)>0)&#xa;&#xa;    outstr += ""\n[frameRelations] {0} frame relations\n"".format(len(frame.frameRelations))&#xa;    outstr += '  ' + '\n  '.join(repr(frel) for frel in frame.frameRelations) + '\n'&#xa;&#xa;    outstr += ""\n[lexUnit] {0} lexical units\n"".format(len(frame.lexUnit))&#xa;    lustrs = []&#xa;    for luName,lu in sorted(frame.lexUnit.items()):&#xa;        tmpstr = '{0} ({1})'.format(luName, lu.ID)&#xa;        lustrs.append(tmpstr)&#xa;    outstr += ""{0}\n"".format(_pretty_longstring(', '.join(lustrs),prefix='  '))&#xa;&#xa;    outstr += ""\n[FE] {0} frame elements\n"".format(len(frame.FE))&#xa;    fes = {}&#xa;    for feName,fe in sorted(frame.FE.items()):&#xa;        try:&#xa;            fes[fe.coreType].append(""{0} ({1})"".format(feName, fe.ID))&#xa;        except KeyError:&#xa;            fes[fe.coreType] = []&#xa;            fes[fe.coreType].append(""{0} ({1})"".format(feName, fe.ID))&#xa;    for ct in sorted(fes.keys(), key=lambda ct2: ['Core','Core-Unexpressed','Peripheral','Extra-Thematic'].index(ct2)):&#xa;        outstr += ""{0:>16}: {1}\n"".format(ct, ', '.join(sorted(fes[ct])))&#xa;&#xa;    outstr += ""\n[FEcoreSets] {0} frame element core sets\n"".format(len(frame.FEcoreSets))&#xa;    outstr += ""  "" + '\n  '.join("", "".join([x.name for x in coreSet]) for coreSet in frame.FEcoreSets) + '\n'&#xa;&#xa;    return outstr&#xa;&#xa;class FramenetError(Exception):&#xa;&#xa;    """"""An exception class for framenet-related errors.""""""&#xa;&#xa;@python_2_unicode_compatible&#xa;class AttrDict(dict):&#xa;&#xa;    """"""A class that wraps a dict and allows accessing the keys of the&#xa;    dict as if they were attributes. Taken from here:&#xa;       http://stackoverflow.com/a/14620633/8879&#xa;&#xa;    >>> foo = {'a':1, 'b':2, 'c':3}&#xa;    >>> bar = AttrDict(foo)&#xa;    >>> pprint(dict(bar))&#xa;    {'a': 1, 'b': 2, 'c': 3}&#xa;    >>> bar.b&#xa;    2&#xa;    >>> bar.d = 4&#xa;    >>> pprint(dict(bar))&#xa;    {'a': 1, 'b': 2, 'c': 3, 'd': 4}&#xa;    """"""&#xa;&#xa;    def __init__(self, *args, **kwargs):&#xa;        super(AttrDict, self).__init__(*args, **kwargs)&#xa;        #self.__dict__ = self&#xa;&#xa;    def __setattr__(self, name, value):&#xa;        self[name] = value&#xa;    def __getattr__(self, name):&#xa;        if name=='_short_repr':&#xa;            return self._short_repr&#xa;        return self[name]&#xa;    def __getitem__(self, name):&#xa;        v = super(AttrDict,self).__getitem__(name)&#xa;        if isinstance(v,Future):&#xa;            return v._data()&#xa;        return v&#xa;&#xa;    def _short_repr(self):&#xa;        if '_type' in self:&#xa;            if self['_type'].endswith('relation'):&#xa;                return self.__repr__()&#xa;            try:&#xa;                return ""<{0} ID={1} name={2}>"".format(self['_type'], self['ID'], self['name'])&#xa;            except KeyError:    # no ID--e.g., for _type=lusubcorpus&#xa;                return ""<{0} name={1}>"".format(self['_type'], self['name'])&#xa;        else:&#xa;            return self.__repr__()&#xa;&#xa;    def _str(self):&#xa;        outstr = """"&#xa;&#xa;        if not '_type' in self:&#xa;            outstr = _pretty_any(self)&#xa;        elif self['_type'] == 'frame':&#xa;            outstr = _pretty_frame(self)&#xa;        elif self['_type'] == 'fe':&#xa;            outstr = _pretty_fe(self)&#xa;        elif self['_type'] == 'lu':&#xa;            outstr = _pretty_lu(self)&#xa;        elif self['_type'] == 'semtype':&#xa;            outstr = _pretty_semtype(self)&#xa;        elif self['_type'] == 'framerelationtype':&#xa;            outstr = _pretty_frame_relation_type(self)&#xa;        elif self['_type'] == 'framerelation':&#xa;            outstr = _pretty_frame_relation(self)&#xa;        elif self['_type'] == 'ferelation':&#xa;            outstr = _pretty_fe_relation(self)&#xa;        else:&#xa;            outstr = _pretty_any(self)&#xa;&#xa;        # ensure result is unicode string prior to applying the&#xa;        # @python_2_unicode_compatible decorator (because non-ASCII characters&#xa;        # could in principle occur in the data and would trigger an encoding error when&#xa;        # passed as arguments to str.format()).&#xa;        # assert isinstance(outstr, unicode) # not in Python 3.2&#xa;        return outstr&#xa;&#xa;    def __str__(self):&#xa;        return self._str()&#xa;    def __repr__(self):&#xa;        return self.__str__()&#xa;&#xa;class Future(object):&#xa;    """"""&#xa;    Wraps and acts as a proxy for a value to be loaded lazily (on demand).&#xa;    Adapted from https://gist.github.com/sergey-miryanov/2935416&#xa;    """"""&#xa;    def __init__(self, loader, *args, **kwargs):&#xa;        """"""&#xa;        :param loader: when called with no arguments, returns the value to be stored&#xa;        :type loader: callable&#xa;        """"""&#xa;        super (Future, self).__init__(*args, **kwargs)&#xa;        self._loader = loader&#xa;        self._d = None&#xa;    def _data(self):&#xa;        if callable(self._loader):&#xa;            self._d = self._loader()&#xa;            self._loader = None # the data is now cached&#xa;        return self._d&#xa;&#xa;    def __nonzero__(self):&#xa;        return bool(self._data())&#xa;    def __len__(self):&#xa;        return len(self._data())&#xa;&#xa;    def __setitem__(self, key, value):&#xa;        return self._data ().__setitem__(key, value)&#xa;    def __getitem__(self, key):&#xa;        return self._data ().__getitem__(key)&#xa;    def __getattr__(self, key):&#xa;        return self._data().__getattr__(key)&#xa;&#xa;    def __str__(self):&#xa;        return self._data().__str__()&#xa;    def __repr__(self):&#xa;        return self._data().__repr__()&#xa;&#xa;&#xa;@python_2_unicode_compatible&#xa;class PrettyDict(AttrDict):&#xa;    """"""&#xa;    Displays an abbreviated repr of values where possible.&#xa;    Inherits from AttrDict, so a callable value will&#xa;    be lazily converted to an actual value.&#xa;    """"""&#xa;    def __init__(self, *args, **kwargs):&#xa;        _BREAK_LINES = kwargs.pop('breakLines', False)&#xa;        super(PrettyDict, self).__init__(*args, **kwargs)&#xa;        dict.__setattr__(self, '_BREAK_LINES', _BREAK_LINES)&#xa;    def __repr__(self):&#xa;        parts = []&#xa;        for k,v in sorted(self.items()):&#xa;            kv = repr(k)+': '&#xa;            try:&#xa;                kv += v._short_repr()&#xa;            except AttributeError:&#xa;                kv += repr(v)&#xa;            parts.append(kv)&#xa;        return '{'+(',\n ' if self._BREAK_LINES else ', ').join(parts)+'}'&#xa;&#xa;@python_2_unicode_compatible&#xa;class PrettyList(list):&#xa;    """"""&#xa;    Displays an abbreviated repr of only the first several elements, not the whole list.&#xa;    """"""&#xa;    # from nltk.util&#xa;    def __init__(self, *args, **kwargs):&#xa;        self._MAX_REPR_SIZE = kwargs.pop('maxReprSize', 60)&#xa;        self._BREAK_LINES = kwargs.pop('breakLines', False)&#xa;        super(PrettyList, self).__init__(*args, **kwargs)&#xa;    def __repr__(self):&#xa;        """"""&#xa;        Return a string representation for this corpus view that is&#xa;        similar to a list's representation; but if it would be more&#xa;        than 60 characters long, it is truncated.&#xa;        """"""&#xa;        pieces = []&#xa;        length = 5&#xa;&#xa;        for elt in self:&#xa;            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()&#xa;            length += len(pieces[-1]) + 2&#xa;            if self._MAX_REPR_SIZE and length > self._MAX_REPR_SIZE and len(pieces) > 2:&#xa;                return ""[%s, ...]"" % text_type(',\n ' if self._BREAK_LINES else ', ').join(pieces[:-1])&#xa;        return ""[%s]"" % text_type(',\n ' if self._BREAK_LINES else ', ').join(pieces)&#xa;&#xa;@python_2_unicode_compatible&#xa;class PrettyLazyMap(LazyMap):&#xa;    """"""&#xa;    Displays an abbreviated repr of only the first several elements, not the whole list.&#xa;    """"""&#xa;    # from nltk.util&#xa;    _MAX_REPR_SIZE = 60&#xa;    def __repr__(self):&#xa;        """"""&#xa;        Return a string representation for this corpus view that is&#xa;        similar to a list's representation; but if it would be more&#xa;        than 60 characters long, it is truncated.&#xa;        """"""&#xa;        pieces = []&#xa;        length = 5&#xa;        for elt in self:&#xa;            pieces.append(elt._short_repr()) # key difference from inherited version: call to _short_repr()&#xa;            length += len(pieces[-1]) + 2&#xa;            if length > self._MAX_REPR_SIZE and len(pieces) > 2:&#xa;                return ""[%s, ...]"" % text_type(', ').join(pieces[:-1])&#xa;        else:&#xa;            return ""[%s]"" % text_type(', ').join(pieces)&#xa;&#xa;class FramenetCorpusReader(XMLCorpusReader):&#xa;    """"""A corpus reader for the Framenet Corpus.&#xa;&#xa;    >>> from nltk.corpus import framenet as fn&#xa;    >>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)&#xa;    True&#xa;    >>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame&#xa;    True&#xa;    >>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')&#xa;    True&#xa;    """"""&#xa;&#xa;    _bad_statuses = ['Problem']&#xa;    """"""&#xa;    When loading LUs for a frame, those whose status is in this list will be ignored.&#xa;    Due to caching, if user code modifies this, it should do so before loading any data.&#xa;    'Problem' should always be listed for FrameNet 1.5, as these LUs are not included&#xa;    in the XML index.&#xa;    """"""&#xa;&#xa;    def __init__(self, root, fileids):&#xa;        XMLCorpusReader.__init__(self, root, fileids)&#xa;&#xa;        # framenet corpus sub dirs&#xa;        # sub dir containing the xml files for frames&#xa;        self._frame_dir = ""frame""&#xa;        # sub dir containing the xml files for lexical units&#xa;        self._lu_dir = ""lu""&#xa;        # sub dir containing the xml files for fulltext annotation files&#xa;        self._fulltext_dir = ""fulltext""&#xa;&#xa;        # Indexes used for faster look-ups&#xa;        self._frame_idx = None&#xa;        self._cached_frames = {}    # name -> ID&#xa;        self._lu_idx = None&#xa;        self._fulltext_idx = None&#xa;        self._semtypes = None&#xa;        self._freltyp_idx = None    # frame relation types (Inheritance, Using, etc.)&#xa;        self._frel_idx = None   # frame-to-frame relation instances&#xa;        self._ferel_idx = None  # FE-to-FE relation instances&#xa;        self._frel_f_idx = None # frame-to-frame relations associated with each frame&#xa;&#xa;    def _buildframeindex(self):&#xa;        # The total number of Frames in Framenet is fairly small (~1200) so&#xa;        # this index should not be very large&#xa;        if not self._frel_idx:&#xa;            self._buildrelationindex()  # always load frame relations before frames,&#xa;            # otherwise weird ordering effects might result in incomplete information&#xa;        self._frame_idx = {}&#xa;        for f in XMLCorpusView(self.abspath(""frameIndex.xml""),&#xa;                               'frameIndex/frame', self._handle_elt):&#xa;            self._frame_idx[f['ID']] = f&#xa;&#xa;    def _buildcorpusindex(self):&#xa;        # The total number of fulltext annotated documents in Framenet&#xa;        # is fairly small (~90) so this index should not be very large&#xa;        self._fulltext_idx = {}&#xa;        for doclist in XMLCorpusView(self.abspath(""fulltextIndex.xml""),&#xa;                                     'fulltextIndex/corpus',&#xa;                                     self._handle_fulltextindex_elt):&#xa;            for doc in doclist:&#xa;                self._fulltext_idx[doc.ID] = doc&#xa;&#xa;    def _buildluindex(self):&#xa;        # The number of LUs in Framenet is about 13,000 so this index&#xa;        # should not be very large&#xa;        self._lu_idx = {}&#xa;        for lu in XMLCorpusView(self.abspath(""luIndex.xml""),&#xa;                                'luIndex/lu', self._handle_elt):&#xa;            self._lu_idx[lu['ID']] = lu # populate with LU index entries. if any of these&#xa;            # are looked up they will be replaced by full LU objects.&#xa;&#xa;    def _buildrelationindex(self):&#xa;        #print('building relation index...', file=sys.stderr)&#xa;        freltypes = PrettyList(x for x in XMLCorpusView(self.abspath(""frRelation.xml""),&#xa;                                            'frameRelations/frameRelationType',&#xa;                                            self._handle_framerelationtype_elt))&#xa;        self._freltyp_idx = {}&#xa;        self._frel_idx = {}&#xa;        self._frel_f_idx = defaultdict(set)&#xa;        self._ferel_idx = {}&#xa;&#xa;        for freltyp in freltypes:&#xa;            self._freltyp_idx[freltyp.ID] = freltyp&#xa;            for frel in freltyp.frameRelations:&#xa;                supF = frel.superFrame = frel[freltyp.superFrameName] = Future((lambda fID: lambda: self.frame_by_id(fID))(frel.supID))&#xa;                subF = frel.subFrame = frel[freltyp.subFrameName] = Future((lambda fID: lambda: self.frame_by_id(fID))(frel.subID))&#xa;                self._frel_idx[frel.ID] = frel&#xa;                self._frel_f_idx[frel.supID].add(frel.ID)&#xa;                self._frel_f_idx[frel.subID].add(frel.ID)&#xa;                for ferel in frel.feRelations:&#xa;                    ferel.superFrame = supF&#xa;                    ferel.subFrame = subF&#xa;                    ferel.superFE = Future((lambda fer: lambda: fer.superFrame.FE[fer.superFEName])(ferel))&#xa;                    ferel.subFE = Future((lambda fer: lambda: fer.subFrame.FE[fer.subFEName])(ferel))&#xa;                    self._ferel_idx[ferel.ID] = ferel&#xa;        #print('...done building relation index', file=sys.stderr)&#xa;&#xa;    def readme(self):&#xa;        """"""&#xa;        Return the contents of the corpus README.txt (or README) file.&#xa;        """"""&#xa;        try:&#xa;            return self.open(""README.txt"").read()&#xa;        except IOError:&#xa;            return self.open(""README"").read()&#xa;&#xa;    def buildindexes(self):&#xa;        """"""&#xa;        Build the internal indexes to make look-ups faster.&#xa;        """"""&#xa;        # Frames&#xa;        self._buildframeindex()&#xa;        # LUs&#xa;        self._buildluindex()&#xa;        # Fulltext annotation corpora index&#xa;        self._buildcorpusindex()&#xa;        # frame and FE relations&#xa;        self._buildrelationindex()&#xa;&#xa;    def annotated_document(self, fn_docid):&#xa;        """"""&#xa;        Returns the annotated document whose id number is&#xa;        ``fn_docid``. This id number can be obtained by calling the&#xa;        Documents() function.&#xa;&#xa;        The dict that is returned from this function will contain the&#xa;        following keys:&#xa;&#xa;        - '_type'      : 'fulltextannotation'&#xa;        - 'sentence'   : a list of sentences in the document&#xa;           - Each item in the list is a dict containing the following keys:&#xa;              - 'ID'    : the ID number of the sentence&#xa;              - '_type' : 'sentence'&#xa;              - 'text'  : the text of the sentence&#xa;              - 'paragNo' : the paragraph number&#xa;              - 'sentNo'  : the sentence number&#xa;              - 'docID'   : the document ID number&#xa;              - 'corpID'  : the corpus ID number&#xa;              - 'aPos'    : the annotation position&#xa;              - 'annotationSet' : a list of annotation layers for the sentence&#xa;                 - Each item in the list is a dict containing the following keys:&#xa;                    - 'ID'       : the ID number of the annotation set&#xa;                    - '_type'    : 'annotationset'&#xa;                    - 'status'   : either 'MANUAL' or 'UNANN'&#xa;                    - 'luName'   : (only if status is 'MANUAL')&#xa;                    - 'luID'     : (only if status is 'MANUAL')&#xa;                    - 'frameID'  : (only if status is 'MANUAL')&#xa;                    - 'frameName': (only if status is 'MANUAL')&#xa;                    - 'layer' : a list of labels for the layer&#xa;                       - Each item in the layer is a dict containing the&#xa;                         following keys:&#xa;                          - '_type': 'layer'&#xa;                          - 'rank'&#xa;                          - 'name'&#xa;                          - 'label' : a list of labels in the layer&#xa;                             - Each item is a dict containing the following keys:&#xa;                                - 'start'&#xa;                                - 'end'&#xa;                                - 'name'&#xa;                                - 'feID' (optional)&#xa;&#xa;        :param fn_docid: The Framenet id number of the document&#xa;        :type fn_docid: int&#xa;        :return: Information about the annotated document&#xa;        :rtype: dict&#xa;        """"""&#xa;        try:&#xa;            xmlfname = self._fulltext_idx[fn_docid].filename&#xa;        except TypeError:  # happens when self._fulltext_idx == None&#xa;            # build the index&#xa;            self._buildcorpusindex()&#xa;            xmlfname = self._fulltext_idx[fn_docid].filename&#xa;        except KeyError:  # probably means that fn_docid was not in the index&#xa;            raise FramenetError(""Unknown document id: {0}"".format(fn_docid))&#xa;&#xa;        # construct the path name for the xml file containing the document info&#xa;        locpath = os.path.join(&#xa;            ""{0}"".format(self._root), self._fulltext_dir, xmlfname)&#xa;&#xa;        # Grab the top-level xml element containing the fulltext annotation&#xa;        elt = XMLCorpusView(locpath, 'fullTextAnnotation')[0]&#xa;        return self._handle_fulltextannotation_elt(elt)&#xa;&#xa;    def frame_by_id(self, fn_fid, ignorekeys=[]):&#xa;        """"""&#xa;        Get the details for the specified Frame using the frame's id&#xa;        number.&#xa;&#xa;        Usage examples:&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> f = fn.frame_by_id(256)&#xa;        >>> f.ID&#xa;        256&#xa;        >>> f.name&#xa;        'Medical_specialties'&#xa;        >>> f.definition&#xa;        ""This frame includes words that name ...""&#xa;&#xa;        :param fn_fid: The Framenet id number of the frame&#xa;        :type fn_fid: int&#xa;        :param ignorekeys: The keys to ignore. These keys will not be&#xa;            included in the output. (optional)&#xa;        :type ignorekeys: list(str)&#xa;        :return: Information about a frame&#xa;        :rtype: dict&#xa;&#xa;        Also see the ``frame()`` function for details about what is&#xa;        contained in the dict that is returned.&#xa;        """"""&#xa;&#xa;        # get the name of the frame with this id number&#xa;        try:&#xa;            fentry = self._frame_idx[fn_fid]&#xa;            if '_type' in fentry:&#xa;                return fentry   # full frame object is cached&#xa;            name = fentry['name']&#xa;        except TypeError:&#xa;            self._buildframeindex()&#xa;            name = self._frame_idx[fn_fid]['name']&#xa;        except KeyError:&#xa;            raise FramenetError('Unknown frame id: {0}'.format(fn_fid))&#xa;&#xa;        return self.frame_by_name(name, ignorekeys, check_cache=False)&#xa;&#xa;    def frame_by_name(self, fn_fname, ignorekeys=[], check_cache=True):&#xa;        """"""&#xa;        Get the details for the specified Frame using the frame's name.&#xa;&#xa;        Usage examples:&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> f = fn.frame_by_name('Medical_specialties')&#xa;        >>> f.ID&#xa;        256&#xa;        >>> f.name&#xa;        'Medical_specialties'&#xa;        >>> f.definition&#xa;        ""This frame includes words that name ...""&#xa;&#xa;        :param fn_fname: The name of the frame&#xa;        :type fn_fname: str&#xa;        :param ignorekeys: The keys to ignore. These keys will not be&#xa;            included in the output. (optional)&#xa;        :type ignorekeys: list(str)&#xa;        :return: Information about a frame&#xa;        :rtype: dict&#xa;&#xa;        Also see the ``frame()`` function for details about what is&#xa;        contained in the dict that is returned.&#xa;        """"""&#xa;&#xa;        if check_cache and fn_fname in self._cached_frames:&#xa;            return self._frame_idx[self._cached_frames[fn_fname]]&#xa;        elif not self._frame_idx:&#xa;            self._buildframeindex()&#xa;&#xa;        # construct the path name for the xml file containing the Frame info&#xa;        locpath = os.path.join(&#xa;            ""{0}"".format(self._root), self._frame_dir, fn_fname + "".xml"")&#xa;        #print(locpath, file=sys.stderr)&#xa;        # Grab the xml for the frame&#xa;        try:&#xa;            elt = XMLCorpusView(locpath, 'frame')[0]&#xa;        except IOError:&#xa;            raise FramenetError('Unknown frame: {0}'.format(fn_fname))&#xa;&#xa;        fentry = self._handle_frame_elt(elt, ignorekeys)&#xa;        assert fentry&#xa;&#xa;        # INFERENCE RULE: propagate lexical semtypes from the frame to all its LUs&#xa;        for st in fentry.semTypes:&#xa;            if st.rootType.name=='Lexical_type':&#xa;                for lu in fentry.lexUnit.values():&#xa;                    if st not in lu.semTypes:&#xa;                        lu.semTypes.append(st)&#xa;&#xa;&#xa;        self._frame_idx[fentry.ID] = fentry&#xa;        self._cached_frames[fentry.name] = fentry.ID&#xa;        '''&#xa;        # now set up callables to resolve the LU pointers lazily.&#xa;        # (could also do this here--caching avoids infinite recursion.)&#xa;        for luName,luinfo in fentry.lexUnit.items():&#xa;            fentry.lexUnit[luName] = (lambda luID: Future(lambda: self.lu(luID)))(luinfo.ID)&#xa;        '''&#xa;        return fentry&#xa;&#xa;    def frame(self, fn_fid_or_fname, ignorekeys=[]):&#xa;        """"""&#xa;        Get the details for the specified Frame using the frame's name&#xa;        or id number.&#xa;&#xa;        Usage examples:&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> f = fn.frame(256)&#xa;        >>> f.name&#xa;        'Medical_specialties'&#xa;        >>> f = fn.frame('Medical_specialties')&#xa;        >>> f.ID&#xa;        256&#xa;        >>> # ensure non-ASCII character in definition doesn't trigger an encoding error:&#xa;        >>> fn.frame('Imposing_obligation')&#xa;        frame (1494): Imposing_obligation...&#xa;&#xa;        The dict that is returned from this function will contain the&#xa;        following information about the Frame:&#xa;&#xa;        - 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)&#xa;        - 'definition' : textual definition of the Frame&#xa;        - 'ID'         : the internal ID number of the Frame&#xa;        - 'semTypes'   : a list of semantic types for this frame&#xa;           - Each item in the list is a dict containing the following keys:&#xa;              - 'name' : can be used with the semtype() function&#xa;              - 'ID'   : can be used with the semtype() function&#xa;&#xa;        - 'lexUnit'    : a dict containing all of the LUs for this frame.&#xa;                         The keys in this dict are the names of the LUs and&#xa;                         the value for each key is itself a dict containing&#xa;                         info about the LU (see the lu() function for more info.)&#xa;&#xa;        - 'FE' : a dict containing the Frame Elements that are part of this frame&#xa;                 The keys in this dict are the names of the FEs (e.g. 'Body_system')&#xa;                 and the values are dicts containing the following keys&#xa;              - 'definition' : The definition of the FE&#xa;              - 'name'       : The name of the FE e.g. 'Body_system'&#xa;              - 'ID'         : The id number&#xa;              - '_type'      : 'fe'&#xa;              - 'abbrev'     : Abbreviation e.g. 'bod'&#xa;              - 'coreType'   : one of ""Core"", ""Peripheral"", or ""Extra-Thematic""&#xa;              - 'semType'    : if not None, a dict with the following two keys:&#xa;                 - 'name' : name of the semantic type. can be used with&#xa;                            the semtype() function&#xa;                 - 'ID'   : id number of the semantic type. can be used with&#xa;                            the semtype() function&#xa;              - 'requiresFE' : if not None, a dict with the following two keys:&#xa;                 - 'name' : the name of another FE in this frame&#xa;                 - 'ID'   : the id of the other FE in this frame&#xa;              - 'excludesFE' : if not None, a dict with the following two keys:&#xa;                 - 'name' : the name of another FE in this frame&#xa;                 - 'ID'   : the id of the other FE in this frame&#xa;&#xa;        - 'frameRelation'      : a list of objects describing frame relations&#xa;        - 'FEcoreSets'  : a list of Frame Element core sets for this frame&#xa;           - Each item in the list is a list of FE objects&#xa;&#xa;        :param fn_fid_or_fname: The Framenet name or id number of the frame&#xa;        :type fn_fid_or_fname: int or str&#xa;        :param ignorekeys: The keys to ignore. These keys will not be&#xa;            included in the output. (optional)&#xa;        :type ignorekeys: list(str)&#xa;        :return: Information about a frame&#xa;        :rtype: dict&#xa;        """"""&#xa;&#xa;        # get the frame info by name or id number&#xa;        if isinstance(fn_fid_or_fname, string_types):&#xa;            f = self.frame_by_name(fn_fid_or_fname, ignorekeys)&#xa;        else:&#xa;            f = self.frame_by_id(fn_fid_or_fname, ignorekeys)&#xa;&#xa;        return f&#xa;&#xa;    def frames_by_lemma(self, pat):&#xa;        """"""&#xa;        Returns a list of all frames that contain LUs in which the&#xa;        ``name`` attribute of the LU matchs the given regular expression&#xa;        ``pat``. Note that LU names are composed of ""lemma.POS"", where&#xa;        the ""lemma"" part can be made up of either a single lexeme&#xa;        (e.g. 'run') or multiple lexemes (e.g. 'a little').&#xa;&#xa;        Note: if you are going to be doing a lot of this type of&#xa;        searching, you'd want to build an index that maps from lemmas to&#xa;        frames because each time frames_by_lemma() is called, it has to&#xa;        search through ALL of the frame XML files in the db.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> fn.frames_by_lemma(r'(?i)a little')&#xa;        [<frame ID=189 name=Quantity>, <frame ID=2001 name=Degree>]&#xa;&#xa;        :return: A list of frame objects.&#xa;        :rtype: list(AttrDict)&#xa;        """"""&#xa;        return PrettyList(f for f in self.frames() if any(re.search(pat, luName) for luName in f.lexUnit))&#xa;&#xa;    def lu_basic(self, fn_luid):&#xa;        """"""&#xa;        Returns basic information about the LU whose id is&#xa;        ``fn_luid``. This is basically just a wrapper around the&#xa;        ``lu()`` function with ""subCorpus"" info excluded.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> PrettyDict(fn.lu_basic(256), breakLines=True)&#xa;        {'ID': 256,&#xa;         'POS': 'V',&#xa;         '_type': 'lu',&#xa;         'definition': 'COD: be aware of beforehand; predict.',&#xa;         'frame': <frame ID=26 name=Expectation>,&#xa;         'lemmaID': 15082,&#xa;         'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],&#xa;         'name': 'foresee.v',&#xa;         'semTypes': [],&#xa;         'sentenceCount': {'annotated': 44, 'total': 227},&#xa;         'status': 'FN1_Sent'}&#xa;&#xa;        :param fn_luid: The id number of the desired LU&#xa;        :type fn_luid: int&#xa;        :return: Basic information about the lexical unit&#xa;        :rtype: dict&#xa;        """"""&#xa;        return self.lu(fn_luid, ignorekeys=['subCorpus'])&#xa;&#xa;    def lu(self, fn_luid, ignorekeys=[]):&#xa;        """"""&#xa;        Get information about a specific Lexical Unit using the id number&#xa;        ``fn_luid``. This function reads the LU information from the xml&#xa;        file on disk each time it is called. You may want to cache this&#xa;        info if you plan to call this function with the same id number&#xa;        multiple times.&#xa;&#xa;        Usage examples:&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> fn.lu(256).name&#xa;        'foresee.v'&#xa;        >>> fn.lu(256).definition&#xa;        'COD: be aware of beforehand; predict.'&#xa;        >>> fn.lu(256).frame.name&#xa;        'Expectation'&#xa;        >>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))&#xa;        [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]&#xa;&#xa;        The dict that is returned from this function will contain most of the&#xa;        following information about the LU. Note that some LUs do not contain&#xa;        all of these pieces of information - particularly 'totalAnnotated' and&#xa;        'incorporatedFE' may be missing in some LUs:&#xa;&#xa;        - 'name'       : the name of the LU (e.g. 'merger.n')&#xa;        - 'definition' : textual definition of the LU&#xa;        - 'ID'         : the internal ID number of the LU&#xa;        - '_type'      : 'lu'&#xa;        - 'status'     : e.g. 'Created'&#xa;        - 'frame'      : Frame that this LU belongs to&#xa;        - 'POS'        : the part of speech of this LU (e.g. 'N')&#xa;        - 'totalAnnotated' : total number of examples annotated with this LU&#xa;        - 'incorporatedFE' : FE that incorporates this LU (e.g. 'Ailment')&#xa;        - 'sentenceCount'  : a dict with the following two keys:&#xa;                 - 'annotated': number of sentences annotated with this LU&#xa;                 - 'total'    : total number of sentences with this LU&#xa;&#xa;        - 'lexemes'  : a list of dicts describing the lemma of this LU.&#xa;           Each dict in the list contains these keys:&#xa;           - 'POS'     : part of speech e.g. 'N'&#xa;           - 'name'    : either single-lexeme e.g. 'merger' or&#xa;                         multi-lexeme e.g. 'a little'&#xa;           - 'order': the order of the lexeme in the lemma (starting from 1)&#xa;           - 'headword': a boolean ('true' or 'false')&#xa;           - 'breakBefore': Can this lexeme be separated from the previous lexeme?&#xa;                Consider: ""take over.v"" as in:&#xa;                         Germany took over the Netherlands in 2 days.&#xa;                         Germany took the Netherlands over in 2 days.&#xa;                In this case, 'breakBefore' would be ""true"" for the lexeme&#xa;                ""over"". Contrast this with ""take after.v"" as in:&#xa;                         Mary takes after her grandmother.&#xa;                        *Mary takes her grandmother after.&#xa;                In this case, 'breakBefore' would be ""false"" for the lexeme ""after""&#xa;&#xa;        - 'lemmaID'    : Can be used to connect lemmas in different LUs&#xa;        - 'semTypes'   : a list of semantic type objects for this LU&#xa;        - 'subCorpus'  : a list of subcorpora&#xa;           - Each item in the list is a dict containing the following keys:&#xa;              - 'name' :&#xa;              - 'sentence' : a list of sentences in the subcorpus&#xa;                 - each item in the list is a dict with the following keys:&#xa;                    - 'ID':&#xa;                    - 'sentNo':&#xa;                    - 'text': the text of the sentence&#xa;                    - 'aPos':&#xa;                    - 'annotationSet': a list of annotation sets&#xa;                       - each item in the list is a dict with the following keys:&#xa;                          - 'ID':&#xa;                          - 'status':&#xa;                          - 'layer': a list of layers&#xa;                             - each layer is a dict containing the following keys:&#xa;                                - 'name': layer name (e.g. 'BNC')&#xa;                                - 'rank':&#xa;                                - 'label': a list of labels for the layer&#xa;                                   - each label is a dict containing the following keys:&#xa;                                      - 'start': start pos of label in sentence 'text' (0-based)&#xa;                                      - 'end': end pos of label in sentence 'text' (0-based)&#xa;                                      - 'name': name of label (e.g. 'NN1')&#xa;&#xa;        Under the hood, this implementation looks up the lexical unit information&#xa;        in the *frame* definition file. That file does not contain&#xa;        corpus annotations, so the LU files will be accessed on demand if those are&#xa;        needed. In principle, valence patterns could be loaded here too,&#xa;        though these are not currently supported.&#xa;&#xa;        :param fn_luid: The id number of the lexical unit&#xa;        :type fn_luid: int&#xa;        :param ignorekeys: The keys to ignore. These keys will not be&#xa;            included in the output. (optional)&#xa;        :type ignorekeys: list(str)&#xa;        :return: All information about the lexical unit&#xa;        :rtype: dict&#xa;        """"""&#xa;        # look for this LU in cache&#xa;        if not self._lu_idx:&#xa;            self._buildluindex()&#xa;        luinfo = self._lu_idx[fn_luid]&#xa;        if '_type' not in luinfo:&#xa;            # we only have an index entry for the LU. loading the frame will replace this.&#xa;            f = self.frame_by_id(luinfo.frameID)&#xa;            luinfo = self._lu_idx[fn_luid]&#xa;        if ignorekeys:&#xa;            return AttrDict(dict((k, v) for k, v in luinfo.items() if k not in ignorekeys))&#xa;&#xa;        return luinfo&#xa;&#xa;    def _lu_file(self, lu, ignorekeys=[]):&#xa;        """"""&#xa;        Augment the LU information that was loaded from the frame file&#xa;        with additional information from the LU file.&#xa;        """"""&#xa;        fn_luid = lu.ID&#xa;&#xa;        fname = ""lu{0}.xml"".format(fn_luid)&#xa;        locpath = os.path.join(""{0}"".format(self._root), self._lu_dir, fname)&#xa;        #print(locpath, file=sys.stderr)&#xa;        if not self._lu_idx:&#xa;            self._buildluindex()&#xa;&#xa;        try:&#xa;            elt = XMLCorpusView(locpath, 'lexUnit')[0]&#xa;        except IOError:&#xa;            raise FramenetError('Unknown LU id: {0}'.format(fn_luid))&#xa;&#xa;        lu2 = self._handle_lexunit_elt(elt, ignorekeys)&#xa;        lu.subCorpus = lu2.subCorpus&#xa;&#xa;        return lu.subCorpus&#xa;&#xa;    def _loadsemtypes(self):&#xa;        """"""Create the semantic types index.""""""&#xa;        self._semtypes = AttrDict()&#xa;        semtypeXML = [x for x in XMLCorpusView(self.abspath(""semTypes.xml""),&#xa;                                             'semTypes/semType',&#xa;                                             self._handle_semtype_elt)]&#xa;        for st in semtypeXML:&#xa;            n = st['name']&#xa;            a = st['abbrev']&#xa;            i = st['ID']&#xa;            # Both name and abbrev should be able to retrieve the&#xa;            # ID. The ID will retrieve the semantic type dict itself.&#xa;            self._semtypes[n] = i&#xa;            self._semtypes[a] = i&#xa;            self._semtypes[i] = st&#xa;        # now that all individual semtype XML is loaded, we can link them together&#xa;        roots = []&#xa;        for st in self.semtypes():&#xa;            if st.superType:&#xa;                st.superType = self.semtype(st.superType.supID)&#xa;                st.superType.subTypes.append(st)&#xa;            else:&#xa;                if st not in roots: roots.append(st)&#xa;                st.rootType = st&#xa;        queue = list(roots)&#xa;        assert queue&#xa;        while queue:&#xa;            st = queue.pop(0)&#xa;            for child in st.subTypes:&#xa;                child.rootType = st.rootType&#xa;                queue.append(child)&#xa;        #self.propagate_semtypes()  # apply inferencing over FE relations&#xa;&#xa;    def propagate_semtypes(self):&#xa;        """"""&#xa;        Apply inference rules to distribute semtypes over relations between FEs.&#xa;        For FrameNet 1.5, this results in 1011 semtypes being propagated.&#xa;        (Not done by default because it requires loading all frame files,&#xa;        which takes several seconds. If this needed to be fast, it could be rewritten&#xa;        to traverse the neighboring relations on demand for each FE semtype.)&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)&#xa;        4241&#xa;        >>> fn.propagate_semtypes()&#xa;        >>> sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)&#xa;        5252&#xa;        """"""&#xa;        if not self._semtypes:&#xa;            self._loadsemtypes()&#xa;        if not self._ferel_idx:&#xa;            self._buildrelationindex()&#xa;        changed = True&#xa;        i = 0&#xa;        nPropagations = 0&#xa;        while changed:&#xa;            # make a pass and see if anything needs to be propagated&#xa;            i += 1&#xa;            changed = False&#xa;            for ferel in self.fe_relations():&#xa;                superST = ferel.superFE.semType&#xa;                subST = ferel.subFE.semType&#xa;                try:&#xa;                    if superST and superST is not subST:&#xa;                        # propagate downward&#xa;                        assert subST is None or self.semtype_inherits(subST, superST),(superST.name,ferel,subST.name)&#xa;                        if subST is None:&#xa;                            ferel.subFE.semType = subST = superST&#xa;                            changed = True&#xa;                            nPropagations += 1&#xa;                    if ferel.type.name in ['Perspective_on', 'Subframe', 'Precedes'] and subST \&#xa;                        and subST is not superST:&#xa;                        # propagate upward&#xa;                        assert superST is None,(superST.name,ferel,subST.name)&#xa;                        ferel.superFE.semType = superST = subST&#xa;                        changed = True&#xa;                        nPropagations += 1&#xa;                except AssertionError as ex:&#xa;                    # bug in the data! ignore&#xa;                    #print(ex, file=sys.stderr)&#xa;                    continue&#xa;            #print(i, nPropagations, file=sys.stderr)&#xa;&#xa;    def semtype(self, key):&#xa;        """"""&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> fn.semtype(233).name&#xa;        'Temperature'&#xa;        >>> fn.semtype(233).abbrev&#xa;        'Temp'&#xa;        >>> fn.semtype('Temperature').ID&#xa;        233&#xa;&#xa;        :param key: The name, abbreviation, or id number of the semantic type&#xa;        :type key: string or int&#xa;        :return: Information about a semantic type&#xa;        :rtype: dict&#xa;        """"""&#xa;        if isinstance(key, int):&#xa;            stid = key&#xa;        else:&#xa;            try:&#xa;                stid = self._semtypes[key]&#xa;            except TypeError:&#xa;                self._loadsemtypes()&#xa;                stid = self._semtypes[key]&#xa;&#xa;        try:&#xa;            st = self._semtypes[stid]&#xa;        except TypeError:&#xa;            self._loadsemtypes()&#xa;            st = self._semtypes[stid]&#xa;&#xa;        return st&#xa;&#xa;    def semtype_inherits(self, st, superST):&#xa;        if not isinstance(st, dict):&#xa;            st = self.semtype(st)&#xa;        if not isinstance(superST, dict):&#xa;            superST = self.semtype(superST)&#xa;        par = st.superType&#xa;        while par:&#xa;            if par is superST:&#xa;                return True&#xa;            par = par.superType&#xa;        return False&#xa;&#xa;    def frames(self, name=None):&#xa;        """"""&#xa;        Obtain details for a specific frame.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> len(fn.frames())&#xa;        1019&#xa;        >>> PrettyList(fn.frames(r'(?i)medical'), maxReprSize=0, breakLines=True)&#xa;        [<frame ID=256 name=Medical_specialties>,&#xa;         <frame ID=257 name=Medical_instruments>,&#xa;         <frame ID=255 name=Medical_professionals>,&#xa;         <frame ID=239 name=Medical_conditions>]&#xa;&#xa;        A brief intro to Frames (excerpted from ""FrameNet II: Extended&#xa;        Theory and Practice"" by Ruppenhofer et. al., 2010):&#xa;&#xa;        A Frame is a script-like conceptual structure that describes a&#xa;        particular type of situation, object, or event along with the&#xa;        participants and props that are needed for that Frame. For&#xa;        example, the ""Apply_heat"" frame describes a common situation&#xa;        involving a Cook, some Food, and a Heating_Instrument, and is&#xa;        evoked by words such as bake, blanch, boil, broil, brown,&#xa;        simmer, steam, etc.&#xa;&#xa;        We call the roles of a Frame ""frame elements"" (FEs) and the&#xa;        frame-evoking words are called ""lexical units"" (LUs).&#xa;&#xa;        FrameNet includes relations between Frames. Several types of&#xa;        relations are defined, of which the most important are:&#xa;&#xa;           - Inheritance: An IS-A relation. The child frame is a subtype&#xa;             of the parent frame, and each FE in the parent is bound to&#xa;             a corresponding FE in the child. An example is the&#xa;             ""Revenge"" frame which inherits from the&#xa;             ""Rewards_and_punishments"" frame.&#xa;&#xa;           - Using: The child frame presupposes the parent frame as&#xa;             background, e.g the ""Speed"" frame ""uses"" (or presupposes)&#xa;             the ""Motion"" frame; however, not all parent FEs need to be&#xa;             bound to child FEs.&#xa;&#xa;           - Subframe: The child frame is a subevent of a complex event&#xa;             represented by the parent, e.g. the ""Criminal_process"" frame&#xa;             has subframes of ""Arrest"", ""Arraignment"", ""Trial"", and&#xa;             ""Sentencing"".&#xa;&#xa;           - Perspective_on: The child frame provides a particular&#xa;             perspective on an un-perspectivized parent frame. A pair of&#xa;             examples consists of the ""Hiring"" and ""Get_a_job"" frames,&#xa;             which perspectivize the ""Employment_start"" frame from the&#xa;             Employer's and the Employee's point of view, respectively.&#xa;&#xa;        :param name: A regular expression pattern used to match against&#xa;            Frame names. If 'name' is None, then a list of all&#xa;            Framenet Frames will be returned.&#xa;        :type name: str&#xa;        :return: A list of matching Frames (or all Frames).&#xa;        :rtype: list(AttrDict)&#xa;        """"""&#xa;        try:&#xa;            fIDs = list(self._frame_idx.keys())&#xa;        except AttributeError:&#xa;            self._buildframeindex()&#xa;            fIDs = list(self._frame_idx.keys())&#xa;&#xa;        if name is not None:&#xa;            return PrettyList(self.frame(fID) for fID,finfo in self.frame_ids_and_names(name).items())&#xa;        else:&#xa;            return PrettyLazyMap(self.frame, fIDs)&#xa;&#xa;    def frame_ids_and_names(self, name=None):&#xa;        """"""&#xa;        Uses the frame index, which is much faster than looking up each frame definition&#xa;        if only the names and IDs are needed.&#xa;        """"""&#xa;        if not self._frame_idx:&#xa;            self._buildframeindex()&#xa;        return dict((fID, finfo.name) for fID,finfo in self._frame_idx.items() if name is None or re.search(name, finfo.name) is not None)&#xa;&#xa;    def fes(self, name=None):&#xa;        '''&#xa;        Lists frame element objects. If 'name' is provided, this is treated as &#xa;        a case-insensitive regular expression to filter by frame name. &#xa;        (Case-insensitivity is because casing of frame element names is not always &#xa;        consistent across frames.)&#xa;        &#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> fn.fes('Noise_maker')&#xa;        [<fe ID=6043 name=Noise_maker>]&#xa;        >>> sorted([(fe.frame.name,fe.name) for fe in fn.fes('sound')])&#xa;        [('Cause_to_make_noise', 'Sound_maker'), ('Make_noise', 'Sound'), &#xa;         ('Make_noise', 'Sound_source'), ('Sound_movement', 'Location_of_sound_source'), &#xa;         ('Sound_movement', 'Sound'), ('Sound_movement', 'Sound_source'), &#xa;         ('Sounds', 'Component_sound'), ('Sounds', 'Location_of_sound_source'), &#xa;         ('Sounds', 'Sound_source'), ('Vocalizations', 'Location_of_sound_source'), &#xa;         ('Vocalizations', 'Sound_source')]&#xa;        >>> sorted(set(fe.name for fe in fn.fes('^sound')))&#xa;        ['Sound', 'Sound_maker', 'Sound_source']&#xa;        >>> len(fn.fes('^sound$'))&#xa;        2&#xa;        &#xa;        :param name: A regular expression pattern used to match against&#xa;            frame element names. If 'name' is None, then a list of all&#xa;            frame elements will be returned.&#xa;        :type name: str&#xa;        :return: A list of matching frame elements&#xa;        :rtype: list(AttrDict)&#xa;        '''&#xa;        return PrettyList(fe for f in self.frames() for fename,fe in f.FE.items() if name is None or re.search(name, fename, re.I))&#xa;&#xa;    def lus(self, name=None):&#xa;        """"""&#xa;        Obtain details for a specific lexical unit.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> len(fn.lus())&#xa;        11829&#xa;        >>> PrettyList(fn.lus(r'(?i)a little'), maxReprSize=0, breakLines=True)&#xa;        [<lu ID=14744 name=a little bit.adv>,&#xa;         <lu ID=14733 name=a little.n>,&#xa;         <lu ID=14743 name=a little.adv>]&#xa;&#xa;        A brief intro to Lexical Units (excerpted from ""FrameNet II:&#xa;        Extended Theory and Practice"" by Ruppenhofer et. al., 2010):&#xa;&#xa;        A lexical unit (LU) is a pairing of a word with a meaning. For&#xa;        example, the ""Apply_heat"" Frame describes a common situation&#xa;        involving a Cook, some Food, and a Heating Instrument, and is&#xa;        _evoked_ by words such as bake, blanch, boil, broil, brown,&#xa;        simmer, steam, etc. These frame-evoking words are the LUs in the&#xa;        Apply_heat frame. Each sense of a polysemous word is a different&#xa;        LU.&#xa;&#xa;        We have used the word ""word"" in talking about LUs. The reality&#xa;        is actually rather complex. When we say that the word ""bake"" is&#xa;        polysemous, we mean that the lemma ""bake.v"" (which has the&#xa;        word-forms ""bake"", ""bakes"", ""baked"", and ""baking"") is linked to&#xa;        three different frames:&#xa;&#xa;           - Apply_heat: ""Michelle baked the potatoes for 45 minutes.""&#xa;&#xa;           - Cooking_creation: ""Michelle baked her mother a cake for her birthday.""&#xa;&#xa;           - Absorb_heat: ""The potatoes have to bake for more than 30 minutes.""&#xa;&#xa;        These constitute three different LUs, with different&#xa;        definitions.&#xa;&#xa;        Multiword expressions such as ""given name"" and hyphenated words&#xa;        like ""shut-eye"" can also be LUs. Idiomatic phrases such as&#xa;        ""middle of nowhere"" and ""give the slip (to)"" are also defined as&#xa;        LUs in the appropriate frames (""Isolated_places"" and ""Evading"",&#xa;        respectively), and their internal structure is not analyzed.&#xa;&#xa;        Framenet provides multiple annotated examples of each sense of a&#xa;        word (i.e. each LU).  Moreover, the set of examples&#xa;        (approximately 20 per LU) illustrates all of the combinatorial&#xa;        possibilities of the lexical unit.&#xa;&#xa;        Each LU is linked to a Frame, and hence to the other words which&#xa;        evoke that Frame. This makes the FrameNet database similar to a&#xa;        thesaurus, grouping together semantically similar words.&#xa;&#xa;        In the simplest case, frame-evoking words are verbs such as&#xa;        ""fried"" in:&#xa;&#xa;           ""Matilde fried the catfish in a heavy iron skillet.""&#xa;&#xa;        Sometimes event nouns may evoke a Frame. For example,&#xa;        ""reduction"" evokes ""Cause_change_of_scalar_position"" in:&#xa;&#xa;           ""...the reduction of debt levels to $665 million from $2.6 billion.""&#xa;&#xa;        Adjectives may also evoke a Frame. For example, ""asleep"" may&#xa;        evoke the ""Sleep"" frame as in:&#xa;&#xa;           ""They were asleep for hours.""&#xa;&#xa;        Many common nouns, such as artifacts like ""hat"" or ""tower"",&#xa;        typically serve as dependents rather than clearly evoking their&#xa;        own frames.&#xa;&#xa;        :param name: A regular expression pattern used to search the LU&#xa;            names. Note that LU names take the form of a dotted&#xa;            string (e.g. ""run.v"" or ""a little.adv"") in which a&#xa;            lemma preceeds the ""."" and a POS follows the&#xa;            dot. The lemma may be composed of a single lexeme&#xa;            (e.g. ""run"") or of multiple lexemes (e.g. ""a&#xa;            little""). If 'name' is not given, then all LUs will&#xa;            be returned.&#xa;&#xa;            The valid POSes are:&#xa;&#xa;                   v    - verb&#xa;                   n    - noun&#xa;                   a    - adjective&#xa;                   adv  - adverb&#xa;                   prep - preposition&#xa;                   num  - numbers&#xa;                   intj - interjection&#xa;                   art  - article&#xa;                   c    - conjunction&#xa;                   scon - subordinating conjunction&#xa;&#xa;        :type name: str&#xa;        :return: A list of selected (or all) lexical units&#xa;        :rtype: list of LU objects (dicts). See the lu() function for info&#xa;          about the specifics of LU objects.&#xa;&#xa;        """"""&#xa;        try:&#xa;            luIDs = list(self._lu_idx.keys())&#xa;        except AttributeError:&#xa;            self._buildluindex()&#xa;            luIDs = list(self._lu_idx.keys())&#xa;&#xa;        if name is not None:&#xa;            return PrettyList(self.lu(luID) for luID,luName in self.lu_ids_and_names(name).items())&#xa;        else:&#xa;            return PrettyLazyMap(self.lu, luIDs)&#xa;&#xa;    def lu_ids_and_names(self, name=None):&#xa;        """"""&#xa;        Uses the LU index, which is much faster than looking up each LU definition&#xa;        if only the names and IDs are needed.&#xa;        """"""&#xa;        if not self._lu_idx:&#xa;            self._buildluindex()&#xa;        return dict((luID, luinfo.name) for luID,luinfo in self._lu_idx.items() if name is None or re.search(name, luinfo.name) is not None)&#xa;&#xa;    def documents(self, name=None):&#xa;        """"""&#xa;        Return a list of the annotated documents in Framenet.&#xa;&#xa;        Details for a specific annotated document can be obtained using this&#xa;        class's annotated_document() function and pass it the value of the 'ID' field.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> len(fn.documents())&#xa;        78&#xa;        >>> set([x.corpname for x in fn.documents()])==set(['ANC', 'C-4', 'KBEval', \&#xa;                    'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank', 'QA', 'SemAnno'])&#xa;        True&#xa;&#xa;        :param name: A regular expression pattern used to search the&#xa;            file name of each annotated document. The document's&#xa;            file name contains the name of the corpus that the&#xa;            document is from, followed by two underscores ""__""&#xa;            followed by the document name. So, for example, the&#xa;            file name ""LUCorpus-v0.3__20000410_nyt-NEW.xml"" is&#xa;            from the corpus named ""LUCorpus-v0.3"" and the&#xa;            document name is ""20000410_nyt-NEW.xml"".&#xa;        :type name: str&#xa;        :return: A list of selected (or all) annotated documents&#xa;        :rtype: list of dicts, where each dict object contains the following&#xa;                keys:&#xa;&#xa;                - 'name'&#xa;                - 'ID'&#xa;                - 'corpid'&#xa;                - 'corpname'&#xa;                - 'description'&#xa;                - 'filename'&#xa;        """"""&#xa;        try:&#xa;            ftlist = PrettyList(self._fulltext_idx.values())&#xa;        except AttributeError:&#xa;            self._buildcorpusindex()&#xa;            ftlist = PrettyList(self._fulltext_idx.values())&#xa;&#xa;        if name is None:&#xa;            return ftlist&#xa;        else:&#xa;            return PrettyList(x for x in ftlist if re.search(name, x['filename']) is not None)&#xa;&#xa;    def frame_relation_types(self):&#xa;        """"""&#xa;        Obtain a list of frame relation types.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> frts = list(fn.frame_relation_types())&#xa;        >>> isinstance(frts, list)&#xa;        True&#xa;        >>> len(frts)&#xa;        9&#xa;        >>> PrettyDict(frts[0], breakLines=True)&#xa;        {'ID': 1,&#xa;         '_type': 'framerelationtype',&#xa;         'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],&#xa;         'name': 'Inheritance',&#xa;         'subFrameName': 'Child',&#xa;         'superFrameName': 'Parent'}&#xa;&#xa;        :return: A list of all of the frame relation types in framenet&#xa;        :rtype: list(dict)&#xa;        """"""&#xa;        if not self._freltyp_idx:&#xa;            self._buildrelationindex()&#xa;        return self._freltyp_idx.values()&#xa;&#xa;    def frame_relations(self, frame=None, frame2=None, type=None):&#xa;        """"""&#xa;        :param frame: (optional) frame object, name, or ID; only relations involving&#xa;        this frame will be returned&#xa;        :param frame2: (optional; 'frame' must be a different frame) only show relations&#xa;        between the two specified frames, in either direction&#xa;        :param type: (optional) frame relation type (name or object); show only relations&#xa;        of this type&#xa;        :type frame: int or str or AttrDict&#xa;        :return: A list of all of the frame relations in framenet&#xa;        :rtype: list(dict)&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> frels = fn.frame_relations()&#xa;        >>> isinstance(frels, list)&#xa;        True&#xa;        >>> len(frels)&#xa;        1676&#xa;        >>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)&#xa;        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,&#xa;         <Parent=Apply_heat -- Using -> Child=Cooking_creation>,&#xa;         <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]&#xa;        >>> PrettyList(fn.frame_relations(373), breakLines=True)&#xa;        [<Parent=Topic -- Using -> Child=Communication>,&#xa;         <Source=Discussion -- ReFraming_Mapping -> Target=Topic>, ...]&#xa;        >>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)&#xa;        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,&#xa;         <Parent=Apply_heat -- Using -> Child=Cooking_creation>, ...]&#xa;        >>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))&#xa;        [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>]&#xa;        >>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)&#xa;        [<Parent=Apply_heat -- Using -> Child=Cooking_creation>,&#xa;        <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]&#xa;        """"""&#xa;        relation_type = type&#xa;&#xa;        if not self._frel_idx:&#xa;            self._buildrelationindex()&#xa;&#xa;        rels = None&#xa;&#xa;        if relation_type is not None:&#xa;            if not isinstance(relation_type, dict):&#xa;                type = [rt for rt in self.frame_relation_types() if rt.name==type][0]&#xa;                assert isinstance(type,dict)&#xa;&#xa;        # lookup by 'frame'&#xa;        if frame is not None:&#xa;            if isinstance(frame,dict) and 'frameRelations' in frame:&#xa;                rels = PrettyList(frame.frameRelations)&#xa;            else:&#xa;                if not isinstance(frame, int):&#xa;                    if isinstance(frame, dict):&#xa;                        frame = frame.ID&#xa;                    else:&#xa;                        frame = self.frame_by_name(frame).ID&#xa;                rels = [self._frel_idx[frelID] for frelID in self._frel_f_idx[frame]]&#xa;&#xa;            # filter by 'type'&#xa;            if type is not None:&#xa;                rels = [rel for rel in rels if rel.type is type]&#xa;        elif type is not None:&#xa;            # lookup by 'type'&#xa;            rels = type.frameRelations&#xa;        else:&#xa;            rels = self._frel_idx.values()&#xa;&#xa;        # filter by 'frame2'&#xa;        if frame2 is not None:&#xa;            if frame is None:&#xa;                raise FramenetError(""frame_relations(frame=None, frame2=<value>) is not allowed"")&#xa;            if not isinstance(frame2, int):&#xa;                if isinstance(frame2, dict):&#xa;                    frame2 = frame2.ID&#xa;                else:&#xa;                    frame2 = self.frame_by_name(frame2).ID&#xa;            if frame==frame2:&#xa;                raise FramenetError(""The two frame arguments to frame_relations() must be different frames"")&#xa;            rels = [rel for rel in rels if rel.superFrame.ID==frame2 or rel.subFrame.ID==frame2]&#xa;&#xa;        return PrettyList(sorted(rels,&#xa;                key=lambda frel: (frel.type.ID, frel.superFrameName, frel.subFrameName)))&#xa;&#xa;    def fe_relations(self):&#xa;        """"""&#xa;        Obtain a list of frame element relations.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> ferels = fn.fe_relations()&#xa;        >>> isinstance(ferels, list)&#xa;        True&#xa;        >>> len(ferels)&#xa;        10020&#xa;        >>> PrettyDict(ferels[0], breakLines=True)&#xa;        {'ID': 14642,&#xa;        '_type': 'ferelation',&#xa;        'frameRelation': <Parent=Abounding_with -- Inheritance -> Child=Lively_place>,&#xa;        'subFE': <fe ID=11370 name=Degree>,&#xa;        'subFEName': 'Degree',&#xa;        'subFrame': <frame ID=1904 name=Lively_place>,&#xa;        'subID': 11370,&#xa;        'supID': 2271,&#xa;        'superFE': <fe ID=2271 name=Degree>,&#xa;        'superFEName': 'Degree',&#xa;        'superFrame': <frame ID=262 name=Abounding_with>,&#xa;        'type': <framerelationtype ID=1 name=Inheritance>}&#xa;&#xa;        :return: A list of all of the frame element relations in framenet&#xa;        :rtype: list(dict)&#xa;        """"""&#xa;        if not self._ferel_idx:&#xa;            self._buildrelationindex()&#xa;        return PrettyList(sorted(self._ferel_idx.values(),&#xa;                key=lambda ferel: (ferel.type.ID, ferel.frameRelation.superFrameName,&#xa;                    ferel.superFEName, ferel.frameRelation.subFrameName, ferel.subFEName)))&#xa;&#xa;    def semtypes(self):&#xa;        """"""&#xa;        Obtain a list of semantic types.&#xa;&#xa;        >>> from nltk.corpus import framenet as fn&#xa;        >>> stypes = fn.semtypes()&#xa;        >>> len(stypes)&#xa;        73&#xa;        >>> sorted(stypes[0].keys())&#xa;        ['ID', '_type', 'abbrev', 'definition', 'name', 'rootType', 'subTypes', 'superType']&#xa;&#xa;        :return: A list of all of the semantic types in framenet&#xa;        :rtype: list(dict)&#xa;        """"""&#xa;        if not self._semtypes:&#xa;            self._loadsemtypes()&#xa;        return PrettyList(self._semtypes[i] for i in self._semtypes if isinstance(i, int))&#xa;&#xa;    def _load_xml_attributes(self, d, elt):&#xa;        """"""&#xa;        Extracts a subset of the attributes from the given element and&#xa;        returns them in a dictionary.&#xa;&#xa;        :param d: A dictionary in which to store the attributes.&#xa;        :type d: dict&#xa;        :param elt: An ElementTree Element&#xa;        :type elt: Element&#xa;        :return: Returns the input dict ``d`` possibly including attributes from ``elt``&#xa;        :rtype: dict&#xa;        """"""&#xa;&#xa;        d = type(d)(d)&#xa;&#xa;        try:&#xa;            attr_dict = elt.attrib&#xa;        except AttributeError:&#xa;            return d&#xa;&#xa;        if attr_dict is None:&#xa;            return d&#xa;&#xa;        # Ignore these attributes when loading attributes from an xml node&#xa;        ignore_attrs = ['cBy', 'cDate', 'mDate', 'xsi',&#xa;                        'schemaLocation', 'xmlns', 'bgColor', 'fgColor']&#xa;&#xa;        for attr in attr_dict:&#xa;&#xa;            if any(attr.endswith(x) for x in ignore_attrs):&#xa;                continue&#xa;&#xa;            val = attr_dict[attr]&#xa;            if val.isdigit():&#xa;                d[attr] = int(val)&#xa;            else:&#xa;                d[attr] = val&#xa;&#xa;        return d&#xa;&#xa;    def _strip_tags(self, data):&#xa;        """"""&#xa;        Gets rid of all tags and newline characters from the given input&#xa;&#xa;        :return: A cleaned-up version of the input string&#xa;        :rtype: str&#xa;        """"""&#xa;&#xa;        try:&#xa;            data = data.replace('<t>', '')&#xa;            data = data.replace('</t>', '')&#xa;            data = re.sub('<fex name=""[^""]+"">', '', data)&#xa;            data = data.replace('</fex>', '')&#xa;            data = data.replace('<fen>', '')&#xa;            data = data.replace('</fen>', '')&#xa;            data = data.replace('<m>', '')&#xa;            data = data.replace('</m>', '')&#xa;            data = data.replace('<ment>', '')&#xa;            data = data.replace('</ment>', '')&#xa;            data = data.replace('<ex>', ""'"")&#xa;            data = data.replace('</ex>', ""'"")&#xa;            data = data.replace('<gov>', '')&#xa;            data = data.replace('</gov>', '')&#xa;            data = data.replace('<x>', '')&#xa;            data = data.replace('</x>', '')&#xa;&#xa;            # Get rid of <def-root> and </def-root> tags&#xa;            data = data.replace('<def-root>', '')&#xa;            data = data.replace('</def-root>', '')&#xa;&#xa;            data = data.replace('\n', ' ')&#xa;        except AttributeError:&#xa;            pass&#xa;&#xa;        return data&#xa;&#xa;    def _handle_elt(self, elt, tagspec=None):&#xa;        """"""Extracts and returns the attributes of the given element""""""&#xa;        return self._load_xml_attributes(AttrDict(), elt)&#xa;&#xa;    def _handle_fulltextindex_elt(self, elt, tagspec=None):&#xa;        """"""&#xa;        Extracts corpus/document info from the fulltextIndex.xml file.&#xa;&#xa;        Note that this function ""flattens"" the information contained&#xa;        in each of the ""corpus"" elements, so that each ""document""&#xa;        element will contain attributes for the corpus and&#xa;        corpusid. Also, each of the ""document"" items will contain a&#xa;        new attribute called ""filename"" that is the base file name of&#xa;        the xml file for the document in the ""fulltext"" subdir of the&#xa;        Framenet corpus.&#xa;        """"""&#xa;        ftinfo = self._load_xml_attributes(AttrDict(), elt)&#xa;        corpname = ftinfo.name&#xa;        corpid = ftinfo.ID&#xa;        retlist = []&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('document'):&#xa;                doc = self._load_xml_attributes(AttrDict(), sub)&#xa;                if 'name' in doc:&#xa;                    docname = doc.name&#xa;                else:&#xa;                    docname = doc.description&#xa;                doc.filename = ""{0}__{1}.xml"".format(corpname, docname)&#xa;                doc.corpname = corpname&#xa;                doc.corpid = corpid&#xa;                retlist.append(doc)&#xa;&#xa;        return retlist&#xa;&#xa;    def _handle_frame_elt(self, elt, ignorekeys=[]):&#xa;        """"""Load the info for a Frame from an frame xml file""""""&#xa;        frinfo = self._load_xml_attributes(AttrDict(), elt)&#xa;&#xa;        frinfo['_type'] = 'frame'&#xa;        frinfo['definition'] = """"&#xa;        frinfo['FE'] = PrettyDict()&#xa;        frinfo['FEcoreSets'] = []&#xa;        frinfo['lexUnit'] = PrettyDict()&#xa;        frinfo['semTypes'] = []&#xa;        for k in ignorekeys:&#xa;            if k in frinfo:&#xa;                del frinfo[k]&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('definition') and 'definition' not in ignorekeys:&#xa;                frinfo['definition'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('FE') and 'FE' not in ignorekeys:&#xa;                feinfo = self._handle_fe_elt(sub)&#xa;                frinfo['FE'][feinfo.name] = feinfo&#xa;                feinfo['frame'] = frinfo    # backpointer&#xa;            elif sub.tag.endswith('FEcoreSet') and 'FEcoreSet' not in ignorekeys:&#xa;                coreset = self._handle_fecoreset_elt(sub)&#xa;                # assumes all FEs have been loaded before coresets&#xa;                frinfo['FEcoreSets'].append(PrettyList(frinfo['FE'][fe.name] for fe in coreset))&#xa;            elif sub.tag.endswith('lexUnit') and 'lexUnit' not in ignorekeys:&#xa;                luentry = self._handle_framelexunit_elt(sub)&#xa;                if luentry['status'] in self._bad_statuses:&#xa;                    # problematic LU entry; ignore it&#xa;                    continue&#xa;                luentry['frame'] = frinfo&#xa;                luentry['subCorpus'] = Future((lambda lu: lambda: self._lu_file(lu))(luentry))&#xa;                frinfo['lexUnit'][luentry.name] = luentry&#xa;                if not self._lu_idx:&#xa;                    self._buildluindex()&#xa;                self._lu_idx[luentry.ID] = luentry&#xa;            elif sub.tag.endswith('semType') and 'semTypes' not in ignorekeys:&#xa;                semtypeinfo = self._load_xml_attributes(AttrDict(), sub)&#xa;                frinfo['semTypes'].append(self.semtype(semtypeinfo.ID))&#xa;&#xa;        frinfo['frameRelations'] = self.frame_relations(frame=frinfo)&#xa;&#xa;        # resolve 'requires' and 'excludes' links between FEs of this frame&#xa;        for fe in frinfo.FE.values():&#xa;            if fe.requiresFE:&#xa;                name, ID = fe.requiresFE.name, fe.requiresFE.ID&#xa;                fe.requiresFE = frinfo.FE[name]&#xa;                assert fe.requiresFE.ID==ID&#xa;            if fe.excludesFE:&#xa;                name, ID = fe.excludesFE.name, fe.excludesFE.ID&#xa;                fe.excludesFE = frinfo.FE[name]&#xa;                assert fe.excludesFE.ID==ID&#xa;&#xa;        return frinfo&#xa;&#xa;    def _handle_fecoreset_elt(self, elt):&#xa;        """"""Load fe coreset info from xml.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        tmp = []&#xa;        for sub in elt:&#xa;            tmp.append(self._load_xml_attributes(AttrDict(), sub))&#xa;&#xa;        return tmp&#xa;&#xa;    def _handle_framerelationtype_elt(self, elt, *args):&#xa;        """"""Load frame-relation element and its child fe-relation elements from frRelation.xml.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = 'framerelationtype'&#xa;        info['frameRelations'] = PrettyList()&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('frameRelation'):&#xa;                frel = self._handle_framerelation_elt(sub)&#xa;                frel['type'] = info   # backpointer&#xa;                for ferel in frel.feRelations:&#xa;                    ferel['type'] = info&#xa;                info['frameRelations'].append(frel)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_framerelation_elt(self, elt):&#xa;        """"""Load frame-relation element and its child fe-relation elements from frRelation.xml.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        assert info['superFrameName']!=info['subFrameName'],(elt,info)&#xa;        info['_type'] = 'framerelation'&#xa;        info['feRelations'] = PrettyList()&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('FERelation'):&#xa;                ferel = self._handle_elt(sub)&#xa;                ferel['_type'] = 'ferelation'&#xa;                ferel['frameRelation'] = info   # backpointer&#xa;                info['feRelations'].append(ferel)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_fulltextannotation_elt(self, elt):&#xa;        """"""Load full annotation info for a document from its xml&#xa;        file. The main element (fullTextAnnotation) contains a 'header'&#xa;        element (which we ignore here) and a bunch of 'sentence'&#xa;        elements.""""""&#xa;        info = AttrDict()&#xa;        info['_type'] = 'fulltextannotation'&#xa;        info['sentence'] = []&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('header'):&#xa;                continue  # not used&#xa;            elif sub.tag.endswith('sentence'):&#xa;                s = self._handle_fulltext_sentence_elt(sub)&#xa;                info['sentence'].append(s)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_fulltext_sentence_elt(self, elt):&#xa;        """"""Load information from the given 'sentence' element. Each&#xa;        'sentence' element contains a ""text"" and an ""annotationSet"" sub&#xa;        element.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = ""sentence""&#xa;        info['annotationSet'] = []&#xa;        info['text'] = """"&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('text'):&#xa;                info['text'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('annotationSet'):&#xa;                a = self._handle_fulltextannotationset_elt(sub)&#xa;                info['annotationSet'].append(a)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_fulltextannotationset_elt(self, elt):&#xa;        """"""Load information from the given 'annotationSet' element. Each&#xa;        'annotationSet' contains several ""layer"" elements.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = ""annotationset""&#xa;        info['layer'] = []&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('layer'):&#xa;                l = self._handle_fulltextlayer_elt(sub)&#xa;                info['layer'].append(l)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_fulltextlayer_elt(self, elt):&#xa;        """"""Load information from the given 'layer' element. Each&#xa;        'layer' contains several ""label"" elements.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = 'layer'&#xa;        info['label'] = []&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('label'):&#xa;                l = self._load_xml_attributes(AttrDict(), sub)&#xa;                info['label'].append(l)&#xa;&#xa;        return info&#xa;&#xa;    def _handle_framelexunit_elt(self, elt):&#xa;        """"""Load the lexical unit info from an xml element in a frame's xml file.""""""&#xa;        luinfo = AttrDict()&#xa;        luinfo['_type'] = 'lu'&#xa;        luinfo = self._load_xml_attributes(luinfo, elt)&#xa;        luinfo[""definition""] = """"&#xa;        luinfo[""sentenceCount""] = PrettyDict()&#xa;        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes&#xa;        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('definition'):&#xa;                luinfo['definition'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('sentenceCount'):&#xa;                luinfo['sentenceCount'] = self._load_xml_attributes(&#xa;                    PrettyDict(), sub)&#xa;            elif sub.tag.endswith('lexeme'):&#xa;                luinfo['lexemes'].append(self._load_xml_attributes(PrettyDict(), sub))&#xa;            elif sub.tag.endswith('semType'):&#xa;                semtypeinfo = self._load_xml_attributes(PrettyDict(), sub)&#xa;                luinfo['semTypes'].append(self.semtype(semtypeinfo.ID))&#xa;&#xa;        return luinfo&#xa;&#xa;    def _handle_lexunit_elt(self, elt, ignorekeys):&#xa;        """"""&#xa;        Load full info for a lexical unit from its xml file.&#xa;        This should only be called when accessing corpus annotations&#xa;        (which are not included in frame files).&#xa;        """"""&#xa;        luinfo = self._load_xml_attributes(AttrDict(), elt)&#xa;        luinfo['_type'] = 'lu'&#xa;        luinfo['definition'] = """"&#xa;        luinfo['subCorpus'] = PrettyList()&#xa;        luinfo['lexemes'] = PrettyList()   # multiword LUs have multiple lexemes&#xa;        luinfo['semTypes'] = PrettyList()  # an LU can have multiple semtypes&#xa;        for k in ignorekeys:&#xa;            if k in luinfo:&#xa;                del luinfo[k]&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('header'):&#xa;                continue  # not used&#xa;            elif sub.tag.endswith('valences'):&#xa;                continue  # not used&#xa;            elif sub.tag.endswith('definition') and 'definition' not in ignorekeys:&#xa;                luinfo['definition'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('subCorpus') and 'subCorpus' not in ignorekeys:&#xa;                sc = self._handle_lusubcorpus_elt(sub)&#xa;                if sc is not None:&#xa;                    luinfo['subCorpus'].append(sc)&#xa;            elif sub.tag.endswith('lexeme') and 'lexeme' not in ignorekeys:&#xa;                luinfo['lexemes'].append(self._load_xml_attributes(PrettyDict(), sub))&#xa;            elif sub.tag.endswith('semType') and 'semType' not in ignorekeys:&#xa;                semtypeinfo = self._load_xml_attributes(AttrDict(), sub)&#xa;                luinfo['semTypes'].append(self.semtype(semtypeinfo.ID))&#xa;&#xa;        return luinfo&#xa;&#xa;    def _handle_lusubcorpus_elt(self, elt):&#xa;        """"""Load a subcorpus of a lexical unit from the given xml.""""""&#xa;        sc = AttrDict()&#xa;        try:&#xa;            sc['name'] = str(elt.get('name'))&#xa;        except AttributeError:&#xa;            return None&#xa;        sc['_type'] = ""lusubcorpus""&#xa;        sc['sentence'] = []&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('sentence'):&#xa;                s = self._handle_lusentence_elt(sub)&#xa;                if s is not None:&#xa;                    sc['sentence'].append(s)&#xa;&#xa;        return sc&#xa;&#xa;    def _handle_lusentence_elt(self, elt):&#xa;        """"""Load a sentence from a subcorpus of an LU from xml.""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = 'lusentence'&#xa;        info['annotationSet'] = []&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('text'):&#xa;                info['text'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('annotationSet'):&#xa;                annset = self._handle_luannotationset_elt(sub)&#xa;                if annset is not None:&#xa;                    info['annotationSet'].append(annset)&#xa;        return info&#xa;&#xa;    def _handle_luannotationset_elt(self, elt):&#xa;        """"""Load an annotation set from a sentence in an subcorpus of an LU""""""&#xa;        info = self._load_xml_attributes(AttrDict(), elt)&#xa;        info['_type'] = 'luannotationset'&#xa;        info['layer'] = []&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('layer'):&#xa;                l = self._handle_lulayer_elt(sub)&#xa;                if l is not None:&#xa;                    info['layer'].append(l)&#xa;        return info&#xa;&#xa;    def _handle_lulayer_elt(self, elt):&#xa;        """"""Load a layer from an annotation set""""""&#xa;        layer = self._load_xml_attributes(AttrDict(), elt)&#xa;        layer['_type'] = 'lulayer'&#xa;        layer['label'] = []&#xa;&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('label'):&#xa;                l = self._load_xml_attributes(AttrDict(), sub)&#xa;                if l is not None:&#xa;                    layer['label'].append(l)&#xa;        return layer&#xa;&#xa;    def _handle_fe_elt(self, elt):&#xa;        feinfo = self._load_xml_attributes(AttrDict(), elt)&#xa;        feinfo['_type'] = 'fe'&#xa;        feinfo['definition'] = """"&#xa;        feinfo['semType'] = None&#xa;        feinfo['requiresFE'] = None&#xa;        feinfo['excludesFE'] = None&#xa;        for sub in elt:&#xa;            if sub.tag.endswith('definition'):&#xa;                feinfo['definition'] = self._strip_tags(sub.text)&#xa;            elif sub.tag.endswith('semType'):&#xa;                stinfo = self._load_xml_attributes(AttrDict(), sub)&#xa;                feinfo['semType'] = self.semtype(stinfo.ID)&#xa;            elif sub.tag.endswith('requiresFE'):&#xa;                feinfo['requiresFE'] = self._load_xml_attributes(AttrDict(), sub)&#xa;            elif sub.tag.endswith('excludesFE'):&#xa;                feinfo['excludesFE'] = self._load_xml_attributes(AttrDict(), sub)&#xa;&#xa;        return feinfo&#xa;&#xa;    def _handle_semtype_elt(self, elt, tagspec=None):&#xa;        semt = self._load_xml_attributes(AttrDict(), elt)&#xa;        semt['_type'] = 'semtype'&#xa;        semt['superType'] = None&#xa;        semt['subTypes'] = PrettyList()&#xa;        for sub in elt:&#xa;            if sub.text is not None:&#xa;                semt['definition'] = self._strip_tags(sub.text)&#xa;            else:&#xa;                supertypeinfo = self._load_xml_attributes(AttrDict(), sub)&#xa;                semt['superType'] = supertypeinfo&#xa;                # the supertype may not have been loaded yet&#xa;&#xa;        return semt&#xa;&#xa;&#xa;#&#xa;# Demo&#xa;#&#xa;def demo():&#xa;    from nltk.corpus import framenet as fn&#xa;&#xa;    #&#xa;    # It is not necessary to explicitly build the indexes by calling&#xa;    # buildindexes(). We do this here just for demo purposes. If the&#xa;    # indexes are not built explicitely, they will be built as needed.&#xa;    #&#xa;    print('Building the indexes...')&#xa;    fn.buildindexes()&#xa;&#xa;    #&#xa;    # Get some statistics about the corpus&#xa;    #&#xa;    print('Number of Frames:', len(fn.frames()))&#xa;    print('Number of Lexical Units:', len(fn.lus()))&#xa;    print('Number of annotated documents:', len(fn.documents()))&#xa;    print()&#xa;&#xa;    #&#xa;    # Frames&#xa;    #&#xa;    print('getting frames whose name matches the (case insensitive) regex: ""(?i)medical""')&#xa;    medframes = fn.frames(r'(?i)medical')&#xa;    print(&#xa;        'Found {0} Frames whose name matches ""(?i)medical"":'.format(len(medframes)))&#xa;    print([(f.name, f.ID) for f in medframes])&#xa;&#xa;    #&#xa;    # store the first frame in the list of frames&#xa;    #&#xa;    tmp_id = medframes[0].ID&#xa;    m_frame = fn.frame(tmp_id)  # reads all info for the frame&#xa;&#xa;    #&#xa;    # get the frame relations&#xa;    #&#xa;    print(&#xa;        '\nNumber of frame relations for the ""{0}"" ({1}) frame:'.format(m_frame.name,&#xa;                                                                        m_frame.ID),&#xa;        len(m_frame.frameRelations))&#xa;    for fr in m_frame.frameRelations:&#xa;        print('   ', fr)&#xa;&#xa;    #&#xa;    # get the names of the Frame Elements&#xa;    #&#xa;    print(&#xa;        '\nNumber of Frame Elements in the ""{0}"" frame:'.format(m_frame.name),&#xa;        len(m_frame.FE))&#xa;    print('   ', [x for x in m_frame.FE])&#xa;&#xa;    #&#xa;    # get the names of the ""Core"" Frame Elements&#xa;    #&#xa;    print(&#xa;        '\nThe ""core"" Frame Elements in the ""{0}"" frame:'.format(m_frame.name))&#xa;    print('   ', [x.name for x in m_frame.FE.values() if x.coreType == ""Core""])&#xa;&#xa;    #&#xa;    # get all of the Lexical Units that are incorporated in the&#xa;    # 'Ailment' FE of the 'Medical_conditions' frame (id=239)&#xa;    #&#xa;    print('\nAll Lexical Units that are incorporated in the ""Ailment"" FE:')&#xa;    m_frame = fn.frame(239)&#xa;    ailment_lus = [x for x in m_frame.lexUnit.values() if 'incorporatedFE' in x and x.incorporatedFE == 'Ailment']&#xa;    print('   ', [x.name for x in ailment_lus])&#xa;&#xa;    #&#xa;    # get all of the Lexical Units for the frame&#xa;    #&#xa;    print('\nNumber of Lexical Units in the ""{0}"" frame:'.format(m_frame.name),&#xa;          len(m_frame.lexUnit))&#xa;    print('  ', [x.name for x in m_frame.lexUnit.values()][:5], '...')&#xa;&#xa;    #&#xa;    # get basic info on the second LU in the frame&#xa;    #&#xa;    tmp_id = m_frame.lexUnit['ailment.n'].ID  # grab the id of the specified LU&#xa;    luinfo = fn.lu_basic(tmp_id)  # get basic info on the LU&#xa;    print('\nInformation on the LU: {0}'.format(luinfo.name))&#xa;    pprint(luinfo)&#xa;&#xa;    #&#xa;    # Get a list of all of the corpora used for fulltext annotation&#xa;    #&#xa;    print('\nNames of all of the corpora used for fulltext annotation:')&#xa;    allcorpora = set([x.corpname for x in fn.documents()])&#xa;    pprint(list(allcorpora))&#xa;&#xa;    #&#xa;    # Get the names of the annotated documents in the first corpus&#xa;    #&#xa;    firstcorp = list(allcorpora)[0]&#xa;    firstcorp_docs = fn.documents(firstcorp)&#xa;    print(&#xa;        '\nNames of the annotated documents in the ""{0}"" corpus:'.format(firstcorp))&#xa;    pprint([x.filename for x in firstcorp_docs])&#xa;&#xa;    #&#xa;    # Search for frames containing LUs whose name attribute matches a&#xa;    # regexp pattern.&#xa;    #&#xa;    # Note: if you were going to be doing a lot of this type of&#xa;    #       searching, you'd want to build an index that maps from&#xa;    #       lemmas to frames because each time frames_by_lemma() is&#xa;    #       called, it has to search through ALL of the frame XML files&#xa;    #       in the db.&#xa;    print('\nSearching for all Frames that have a lemma that matches the regexp: ""^run.v$"":')&#xa;    pprint(fn.frames_by_lemma(r'^run.v$'))&#xa;&#xa;if __name__ == '__main__':&#xa;    demo()&#xa;"
19195935|"# coding: utf8&#xa;# app.py&#xa;# 10/8/2012 jichi&#xa;&#xa;__all__ = 'Application',&#xa;&#xa;from PySide.QtCore import QTranslator, QObject&#xa;from Qt5.QtWidgets import QApplication&#xa;from sakurakit import skos&#xa;from sakurakit.skdebug import dprint, dwarn, derror&#xa;from mytr import mytr_&#xa;import config, rc, settings&#xa;&#xa;#if skos.WIN:&#xa;#  from win32con import WM_QUIT, WM_ENDSESSION, WM_QUERYENDSESSION&#xa;&#xa;def ignoreWindowsExceptions():&#xa;  from sakurakit import skwinapi&#xa;  @skwinapi.LPTOP_LEVEL_EXCEPTION_FILTER&#xa;  def exceptionFilter(e):&#xa;    """"""&#xa;    LONG WINAPI exceptionFilter(PEXCEPTION_POINTERS e)&#xa;    """"""&#xa;    derror(""Application exception"")&#xa;&#xa;  skwinapi.SetUnhandledExceptionFilter(exceptionFilter)&#xa;  dprint(""pass"")&#xa;&#xa;#class _Application:&#xa;#&#xa;#  def __init__(self, q):&#xa;#    q.aboutToQuit.connect(self._onAboutToQuit)&#xa;#&#xa;#  def _onAboutToQuit(self):&#xa;#    dprint(""pass"")&#xa;&#xa;class Application(QApplication):&#xa;  def __init__(self, argv):&#xa;    super(Application, self).__init__(argv)&#xa;&#xa;    self.setApplicationName(mytr_(""Visual Novel Reader""))&#xa;    self.setApplicationVersion(str(config.VERSION_TIMESTAMP))&#xa;    self.setOrganizationName(config.VERSION_ORGANIZATION)&#xa;    self.setOrganizationDomain(config.VERSION_DOMAIN)&#xa;    self.setWindowIcon(rc.icon('logo-reader'))&#xa;&#xa;    #if skos.WIN:&#xa;    #  ignoreWindowsExceptions()&#xa;&#xa;    dprint(""pass"")&#xa;&#xa;  def setFontFamily(self, family):&#xa;    dprint(family)&#xa;    font = self.font()&#xa;    font.setFamily(family)&#xa;    self.setFont(font)&#xa;&#xa;  # FIXME: wParam does not exist in PySide MSG&#xa;  # Bug: https://bugreports.qt-project.org/browse/PYSIDE-84&#xa;  #def winEventFilter(self, msg):&#xa;  #  """"""&#xa;  #  @param  msg  PySide.QtCore.MSG&#xa;  #  @return  bool&#xa;  #  """"""&#xa;  #  # See: http://stackoverflow.com/questions/19195935/how-to-detect-windows-shutdown-or-logoff-in-qt&#xa;  #  if msg.wParam in (WM_QUIT, WM_ENDSESSION, WM_QUERYENDSESSION):&#xa;  #    dprint(""quit"")&#xa;  #  return super(Application, self).winEventFilter(msg)&#xa;&#xa;  def notify(self, receiver, event):&#xa;    """"""@reimp @protected&#xa;    virtual bool notify(QObject *receiver, QEvent *e)&#xa;&#xa;    See: http://www.02.246.ne.jp/~torutk/cxx/qt/QtMemo.html&#xa;    """"""&#xa;    try:&#xa;      if isinstance(receiver, QObject): # receiver could be QCursor. Only notify QObject&#xa;        return super(Application, self).notify(receiver, event)&#xa;    except KeyboardInterrupt, e:&#xa;      derror(e)&#xa;    except Exception, e:&#xa;      derror(e)&#xa;      if skos.WIN:&#xa;        import win32api, win32con&#xa;        win32api.MessageBox(None, """"""\&#xa;I am sorry that VNR got an unexpected error m(_ _)m&#xa;I am not sure what is happening, and whether you have to restart VNR T_T&#xa;Feel free to complain to me (annotcloud@gmail.com) if this error keeps bothering you.&#xa;&#xa;ERROR MESSAGE BEGIN&#xa;%s&#xa;ERROR MESSAGE END"""""" % e,&#xa;            ""VNR Error"",&#xa;            win32con.MB_OK|win32con.MB_ICONERROR)&#xa;    return True&#xa;&#xa;  @staticmethod&#xa;  def applicationLocale():&#xa;    #lang = settings.global_().userLanguage()&#xa;    lang = settings.global_().uiLanguage()&#xa;    locale = config.language2locale(lang)&#xa;    return locale&#xa;&#xa;  def loadTranslations(self):&#xa;    locale = self.applicationLocale()&#xa;    #if locale in config.TR_LOCALES:&#xa;    dprint(""loading translation for %s"" % locale)&#xa;&#xa;    t = QTranslator(self)&#xa;    ok = t.load('qt_%s' % locale, config.QT_TRANSLATIONS_LOCATION)&#xa;    #assert ok, ""failed to load qt translation""&#xa;    if ok:&#xa;      self.installTranslator(t)&#xa;    else:&#xa;      location = config.QT_TRANSLATIONS_LOCATION&#xa;      dprint(""cannot find translation at %s"" % location)&#xa;&#xa;    for location in config.TR_LOCATIONS:&#xa;      t = QTranslator(self)&#xa;      ok = t.load(locale, location)&#xa;      #assert ok, ""failed to load translation""&#xa;      if ok:&#xa;        self.installTranslator(t)&#xa;      else:&#xa;        dprint(""cannot find translation at %s"" % location)&#xa;&#xa;# EOF&#xa;"
16105485|"'''&#xa;chart_utils.py: Charting utilities for RL experiments.&#xa;&#xa;Functions:&#xa;    load_data: Loads data from csv files into lists.&#xa;    average_data: Averages data across instances.&#xa;    compute_conf_intervals: Confidence interval computation.&#xa;    compute_single_conf_interval: Helper function for above.&#xa;    _format_title()&#xa;    plot: Creates (and opens) a single plot using matplotlib.pyplot&#xa;    make_plots: Puts everything in order to create the plot.&#xa;    _get_agent_names: Grabs the agent names the experiment parameter file, named @Experiment.EXP_PARAM_FILE_NAME&#xa;    _get_agent_colors: Determines the relevant colors/markers for the plot.&#xa;    _is_episodic: Determines if the experiment was episodic from the experiment parameter file, named @Experiment.EXP_PARAM_FILE_NAME&#xa;    _is_disc_reward()&#xa;    parse_args: Parse command line arguments.&#xa;    main: Loads data from a given path and creates plot.&#xa;&#xa;Author: David Abel (cs.brown.edu/~dabel)&#xa;'''&#xa;&#xa;# Python imports.&#xa;from __future__ import print_function&#xa;import math&#xa;import decimal&#xa;import sys&#xa;import os&#xa;import matplotlib&#xa;matplotlib.use('TkAgg')&#xa;import matplotlib.pyplot as pyplot&#xa;import numpy as np&#xa;import subprocess&#xa;import argparse&#xa;&#xa;color_ls = [[118, 167, 125], [102, 120, 173],\&#xa;            [198, 113, 113], [94, 94, 94],\&#xa;            [169, 193, 213], [230, 169, 132],\&#xa;            [192, 197, 182], [210, 180, 226]]&#xa;&#xa;# Set font.&#xa;font = {'size':14}&#xa;matplotlib.rc('font', **font)&#xa;matplotlib.rcParams['pdf.fonttype'] = 42&#xa;# matplotlib.rcParams['text.usetex'] = True&#xa;&#xa;CUSTOM_TITLE = None&#xa;X_AXIS_LABEL = None&#xa;Y_AXIS_LABEL = None&#xa;X_AXIS_START_VAL = 0&#xa;X_AXIS_INCREMENT = 1&#xa;Y_AXIS_END_VAL = None&#xa;&#xa;def load_data(experiment_dir, experiment_agents):&#xa;    '''&#xa;    Args:&#xa;        experiment_dir (str): Points to the file containing all the data.&#xa;        experiment_agents (list): Points to which results files will be plotted.&#xa;&#xa;    Returns:&#xa;        result (list): A 3d matrix containing rewards, where the dimensions are [algorithm][instance][episode].&#xa;    '''&#xa;&#xa;    result = []&#xa;    for alg in experiment_agents:&#xa;&#xa;        # Load the reward for all instances of each agent&#xa;        all_reward = open(os.path.join(experiment_dir, str(alg)) + "".csv"", ""r"")&#xa;        all_instances = []&#xa;&#xa;        # Put the reward instances into a list of floats.&#xa;        for instance in all_reward.readlines():&#xa;            all_episodes_for_instance = [float(r) for r in instance.split("","")[:-1] if len(r) > 0]&#xa;            if len(all_episodes_for_instance) > 0:&#xa;                all_instances.append(all_episodes_for_instance)&#xa;&#xa;        result.append(all_instances)&#xa;&#xa;    return result&#xa;&#xa;&#xa;def average_data(data, cumulative=False):&#xa;    '''&#xa;    Args:&#xa;        data (list): a 3D matrix, [algorithm][instance][episode]&#xa;        cumulative (bool) *opt: determines if we should compute the average cumulative reward/cost or just regular.&#xa;&#xa;    Returns:&#xa;        (list): a 2D matrix, [algorithm][episode], where the instance rewards have been averaged.&#xa;    '''&#xa;    num_algorithms = len(data)&#xa;&#xa;    result = [None for i in range(num_algorithms)] # [Alg][avgRewardEpisode], where avg is summed up to episode i if @cumulative=True&#xa;&#xa;    for i, all_instances in enumerate(data):&#xa;&#xa;        # Take the average.&#xa;        num_instances = float(len(data[i]))&#xa;        all_instances_sum = np.array(np.array(all_instances).sum(axis=0))&#xa;        try:&#xa;            avged = all_instances_sum / num_instances&#xa;        except TypeError:&#xa;            raise ValueError(""(simple_rl) Plotting Error: an algorithm was run with inconsistent parameters (likely inconsistent number of EpisodesInstances. Try clearing old data)."")&#xa;        &#xa;        if cumulative:&#xa;            # If we're summing over episodes.&#xa;            temp = []&#xa;            total_so_far = 0&#xa;            for rew in avged:&#xa;                total_so_far += rew&#xa;                temp.append(total_so_far)&#xa;&#xa;            avged = temp&#xa;&#xa;        result[i] = avged&#xa;&#xa;    return result&#xa;&#xa;def compute_conf_intervals(data, cumulative=False):&#xa;    '''&#xa;    Args:&#xa;        data (list): A 3D matrix, [algorithm][instance][episode]&#xa;        cumulative (bool) *opt&#xa;    '''&#xa;&#xa;    confidence_intervals_each_alg = [] # [alg][conf_inv_for_episode]&#xa;&#xa;    for i, all_instances in enumerate(data):&#xa;&#xa;        num_instances = len(data[i])&#xa;        num_episodes = len(data[i][0])&#xa;&#xa;        all_instances_np_arr = np.array(all_instances)&#xa;        alg_i_ci = []&#xa;        total_so_far = np.zeros(num_instances)&#xa;        for j in range(num_episodes):&#xa;            # Compute datum for confidence interval.&#xa;            episode_j_all_instances = all_instances_np_arr[:, j]&#xa;&#xa;            if cumulative:&#xa;                # Cumulative.&#xa;                summed_vector = np.add(episode_j_all_instances, total_so_far)&#xa;                total_so_far = np.add(episode_j_all_instances, total_so_far)&#xa;                episode_j_all_instances = summed_vector&#xa;&#xa;            # Compute the interval and add it to list.&#xa;            conf_interv = compute_single_conf_interval(episode_j_all_instances)&#xa;            alg_i_ci.append(conf_interv)&#xa;&#xa;        confidence_intervals_each_alg.append(alg_i_ci)&#xa;&#xa;    return confidence_intervals_each_alg&#xa;&#xa;&#xa;def compute_single_conf_interval(datum):&#xa;    '''&#xa;    Args:&#xa;        datum (list): A vector of data points to compute the confidence interval of.&#xa;&#xa;    Returns:&#xa;        (float): Margin of error.&#xa;    '''&#xa;    std_deviation = np.std(datum)&#xa;    std_error = 1.96*(std_deviation / math.sqrt(len(datum)))&#xa;&#xa;    return std_error&#xa;&#xa;&#xa;def _format_title(plot_title):&#xa;    plot_title = plot_title.replace(""_"", "" "")&#xa;    plot_title = plot_title.replace(""-"", "" "")&#xa;    if len(plot_title.split("" "")) > 1:&#xa;        plot_title_final = "" "".join([w[0].upper() + w[1:] for w in plot_title.strip().split("" "")])&#xa;&#xa;    return plot_title_final&#xa;&#xa;def plot(results, experiment_dir, agents, plot_file_name="""", conf_intervals=[], use_cost=False, cumulative=False, episodic=True, open_plot=True,track_disc_reward=False):&#xa;    '''&#xa;    Args:&#xa;        results (list of lists): each element is itself the reward from an episode for an algorithm.&#xa;        experiment_dir (str): path to results.&#xa;        agents (list): each element is an agent that was run in the experiment.&#xa;        plot_file_name (str)&#xa;        conf_intervals (list of floats) [optional]: confidence intervals to display with the chart.&#xa;        use_cost (bool) [optional]: If true, plots are in terms of cost. Otherwise, plots are in terms of reward.&#xa;        cumulative (bool) [optional]: If true, plots are cumulative cost/reward.&#xa;        episodic (bool): If true, labels the x-axis ""Episode Number"". Otherwise, ""Step Number"". &#xa;        open_plot (bool)&#xa;        track_disc_reward (bool): If true, plots discounted reward.&#xa;&#xa;    Summary:&#xa;        Makes (and opens) a single reward chart plotting all of the data in @data.&#xa;    '''&#xa;&#xa;    # Set x-axis labels to be integers.&#xa;    from matplotlib.ticker import MaxNLocator&#xa;    ax = pyplot.figure().gca()&#xa;    ax.xaxis.set_major_locator(MaxNLocator(integer=True))&#xa;&#xa;    # Some nice markers and colors for plotting.&#xa;    markers = ['o', 's', 'D', '^', '*', 'x', 'p', '+', 'v','|']&#xa;&#xa;    x_axis_unit = ""episode"" if episodic else ""step""&#xa;&#xa;    # Map them to floats in [0:1].&#xa;    colors = [[shade / 255.0 for shade in rgb] for rgb in color_ls]&#xa;&#xa;    # Puts the legend into the best location in the plot and use a tight layout.&#xa;    pyplot.rcParams['legend.loc'] = 'best'&#xa;&#xa;    # Negate everything if we're plotting cost.&#xa;    if use_cost:&#xa;        results = [[-x for x in alg] for alg in results]&#xa;&#xa;    agent_colors = _get_agent_colors(experiment_dir, agents)&#xa;&#xa;    # Make the plot.&#xa;    print_prefix = ""\nAvg. cumulative reward"" if cumulative else ""Avg. reward""&#xa;    # For each agent.&#xa;    for i, agent_name in enumerate(agents):&#xa;&#xa;        # Add figure for this algorithm.&#xa;        agent_color_index = i if agent_name not in agent_colors else agent_colors[agent_name]&#xa;        agent_marker_index = agent_color_index&#xa;        &#xa;        # Grab new color/marker if we've gone over.&#xa;        if agent_color_index >= len(colors):&#xa;            agent_color_index = agent_color_index % len(colors)&#xa;        if agent_marker_index >= len(markers):&#xa;            agent_marker_index = agent_marker_index % len(markers)&#xa;        &#xa;        series_color = colors[agent_color_index]&#xa;        series_marker = markers[agent_marker_index]&#xa;        y_axis = results[i]&#xa;        x_axis = list(drange(X_AXIS_START_VAL, X_AXIS_START_VAL + len(y_axis) * X_AXIS_INCREMENT, X_AXIS_INCREMENT))&#xa;&#xa;        # Plot Confidence Intervals.&#xa;        if conf_intervals != []:&#xa;            alg_conf_interv = conf_intervals[i]&#xa;            top = np.add(y_axis, alg_conf_interv)&#xa;            bot = np.subtract(y_axis, alg_conf_interv)&#xa;            pyplot.fill_between(x_axis, top, bot, facecolor=series_color, edgecolor=series_color, alpha=0.25)&#xa;        print(""\t"" + str(agents[i]) + "":"", round(y_axis[-1], 5) , ""(conf_interv:"", round(alg_conf_interv[-1], 2), "")"")&#xa;&#xa;        marker_every = max(len(y_axis) / 30,1)&#xa;        pyplot.plot(x_axis, y_axis, color=series_color, marker=series_marker, markevery=marker_every, label=agent_name)&#xa;        pyplot.legend()&#xa;    print()&#xa;    &#xa;    # Configure plot naming information.&#xa;    unit = ""Cost"" if use_cost else ""Reward""&#xa;    plot_label = ""Cumulative"" if cumulative else ""Average""&#xa;    if ""times"" in experiment_dir:&#xa;        # If it's a time plot.&#xa;        unit = ""Time""&#xa;&#xa;    disc_ext = ""Discounted "" if track_disc_reward else """"&#xa;&#xa;    # Set names.&#xa;    exp_dir_split_list = experiment_dir.split(""/"")&#xa;    if 'results' in exp_dir_split_list:&#xa;        exp_name = exp_dir_split_list[exp_dir_split_list.index('results') + 1]&#xa;    else:&#xa;        exp_name = exp_dir_split_list[0]&#xa;    experiment_dir = os.path.join(experiment_dir, """")&#xa;    plot_file_name = os.path.join(experiment_dir, plot_file_name + "".pdf"") if plot_file_name != """" else experiment_dir + plot_label.lower() + ""_"" +unit.lower() + "".pdf""&#xa;    plot_title = CUSTOM_TITLE if CUSTOM_TITLE is not None else plot_label + "" "" + disc_ext + unit + "": "" + exp_name&#xa;    if CUSTOM_TITLE is None:&#xa;        plot_title = _format_title(plot_title)&#xa;&#xa;    # Axis labels.&#xa;    x_axis_label = X_AXIS_LABEL if X_AXIS_LABEL is not None else x_axis_unit[0].upper() + x_axis_unit[1:] + "" Number""&#xa;    y_axis_label = Y_AXIS_LABEL if Y_AXIS_LABEL is not None else plot_label + "" "" + unit&#xa;&#xa;    # Pyplot calls.&#xa;    pyplot.xlabel(x_axis_label)&#xa;    pyplot.ylabel(y_axis_label)&#xa;    pyplot.title(plot_title)&#xa;    pyplot.grid(True)&#xa;    pyplot.tight_layout() # Keeps the spacing nice.&#xa;&#xa;    # Save the plot.&#xa;    pyplot.savefig(plot_file_name, format=""pdf"")&#xa;    &#xa;    if open_plot:&#xa;        # Open it.&#xa;        open_prefix = ""gnome-"" if sys.platform == ""linux"" or sys.platform == ""linux2"" else """"&#xa;        os.system(open_prefix + ""open "" + plot_file_name)&#xa;&#xa;    # Clear and close.&#xa;    pyplot.cla()&#xa;    pyplot.close()&#xa;&#xa;def make_plots(experiment_dir, experiment_agents, plot_file_name="""", cumulative=True, use_cost=False, episodic=True, open_plot=True,track_disc_reward=False):&#xa;    '''&#xa;    Args:&#xa;        experiment_dir (str): path to results.&#xa;        experiment_agents (list): agent names (looks for ""<agent-name>.csv"").&#xa;        plot_file_name (str)&#xa;        cumulative (bool): If true, plots show cumulative trr&#xa;        use_cost (bool): If true, plots are in terms of cost. Otherwise, plots are in terms of reward.&#xa;        episodic (bool): If true, labels the x-axis ""Episode Number"". Otherwise, ""Step Number"". &#xa;        track_disc_reward (bool): If true, plots discounted reward (changes plot title, too).&#xa;&#xa;    Summary:&#xa;        Creates plots for all agents run under the experiment.&#xa;        Stores the plot in results/<experiment_name>/<plot_name>.pdf&#xa;    '''&#xa;&#xa;    # Load the data.&#xa;    data = load_data(experiment_dir, experiment_agents) # [alg][instance][episode]&#xa;&#xa;    # Average the data.&#xa;    avg_data = average_data(data, cumulative=cumulative)&#xa;&#xa;    # Compute confidence intervals.&#xa;    conf_intervals = compute_conf_intervals(data, cumulative=cumulative)&#xa;&#xa;    # Create plot.&#xa;    plot(avg_data, experiment_dir, experiment_agents,&#xa;                plot_file_name=plot_file_name,&#xa;                conf_intervals=conf_intervals,&#xa;                use_cost=use_cost,&#xa;                cumulative=cumulative,&#xa;                episodic=episodic,&#xa;                open_plot=open_plot,&#xa;                track_disc_reward=track_disc_reward)&#xa;&#xa;def drange(x_min, x_max, x_increment):&#xa;    '''&#xa;    Args:&#xa;        x_min (float)&#xa;        x_max (float)&#xa;        x_increment (float)&#xa;&#xa;    Returns:&#xa;        (generator): Makes a list.&#xa;&#xa;    Notes:&#xa;        A range function for generating lists of floats. Based on code from stack overflow user Sam Bruns:&#xa;            https://stackoverflow.com/questions/16105485/unsupported-operand-types-for-float-and-decimal&#xa;    '''&#xa;    x_min = decimal.Decimal(x_min)&#xa;    while x_min < x_max:&#xa;        yield float(x_min)&#xa;        x_min += decimal.Decimal(str(x_increment))&#xa;&#xa;def _get_agent_names(data_dir):&#xa;    '''&#xa;    Args:&#xa;        data_dir (str)&#xa;&#xa;    Returns:&#xa;        (list)&#xa;    '''&#xa;    from simple_rl.experiments import Experiment&#xa;&#xa;    try:&#xa;        params_file = open(os.path.join(data_dir, Experiment.EXP_PARAM_FILE_NAME), ""r"")&#xa;    except IOError:&#xa;        # No param file.&#xa;        return [agent_file.replace("".csv"", """") for agent_file in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, agent_file)) and"".csv"" in agent_file]&#xa;&#xa;    agent_names = []&#xa;    agent_flag = False&#xa;&#xa;    for line in params_file.readlines():&#xa;        if ""Agents"" in line:&#xa;            agent_flag = True&#xa;            continue&#xa;        if ""Params"" in line:&#xa;            agent_flag = False&#xa;        if agent_flag:&#xa;            agent_names.append(line.split("","")[0].strip())&#xa;&#xa;    return agent_names&#xa;&#xa;def _get_agent_colors(data_dir, agents):&#xa;    '''&#xa;    Args:&#xa;        data_dir (str)&#xa;        agents (list)&#xa;&#xa;    Returns:&#xa;        (list)&#xa;    '''&#xa;    from simple_rl.experiments import Experiment&#xa;&#xa;    try:&#xa;        params_file = open(os.path.join(data_dir, Experiment.EXP_PARAM_FILE_NAME), ""r"")&#xa;    except IOError:&#xa;        # No param file.&#xa;        return {agent : i for i, agent in enumerate(agents)}&#xa;&#xa;    colors = {}&#xa;&#xa;    # Check if episodes > 1.&#xa;    for line in params_file.readlines():&#xa;        for agent_name in agents:&#xa;            if agent_name == line.strip().split("","")[0]:&#xa;                colors[agent_name] = int(line[-2])&#xa;&#xa;    return colors&#xa;&#xa;def _is_episodic(data_dir):&#xa;    '''&#xa;    Returns:&#xa;        (bool) True iff the experiment was episodic.&#xa;    '''&#xa;    from simple_rl.experiments import Experiment&#xa;&#xa;    # Open param file for the experiment.&#xa;    if not os.path.exists(os.join(data_dir, Experiment.EXP_PARAM_FILE_NAME)):&#xa;        print(""Warning: no experiment parameters file found for experiment. Assuming non-episodic."")&#xa;        return False&#xa;&#xa;    params_file = open(os.join(data_dir, Experiment.EXP_PARAM_FILE_NAME), ""r"")&#xa;&#xa;    # Check if episodes > 1.&#xa;    for line in params_file.readlines():&#xa;        if ""episodes"" in line:&#xa;            vals = line.strip().split("":"")&#xa;            return int(vals[1]) > 1&#xa;&#xa;def _is_disc_reward(data_dir):&#xa;    '''&#xa;    Returns:&#xa;        (bool) True iff the experiment recorded discounted reward.&#xa;    '''&#xa;    from simple_rl.experiments import Experiment&#xa;&#xa;    # Open param file for the experiment.&#xa;    if not os.path.exists(os.join(data_dir, Experiment.EXP_PARAM_FILE_NAME)):&#xa;        print(""Warning: no experiment parameters file found for experiment. Assuming non-episodic."")&#xa;        return False&#xa;&#xa;    params_file = open(os.join(data_dir, Experiment.EXP_PARAM_FILE_NAME), ""r"")&#xa;&#xa;    # Check if episodes > 1.&#xa;    for line in params_file.readlines():&#xa;        if ""track_disc_reward"" in line:&#xa;            vals = line.strip().split("":"")&#xa;            if ""True"" == vals[1].strip():&#xa;                return True&#xa;&#xa;    return False&#xa;&#xa;def parse_args():&#xa;    '''&#xa;    Summary:&#xa;        Parses two arguments, 'dir' (directory pointer) and 'a' (bool to indicate avg. plot).&#xa;    '''&#xa;    parser = argparse.ArgumentParser()&#xa;    parser.add_argument(""-dir"", type = str, help = ""Path to relevant csv files of data."")&#xa;    parser.add_argument(""-a"", type = bool, default=False, help = ""If true, plots average reward (default is cumulative)."")&#xa;    return parser.parse_args()&#xa;&#xa;&#xa;def main():&#xa;    '''&#xa;    Summary:&#xa;        For manual plotting.&#xa;    '''&#xa;    &#xa;    # Parse args.&#xa;    args = parse_args()&#xa;&#xa;    # Grab agents.&#xa;    data_dir = args.dir&#xa;    agent_names = _get_agent_names(data_dir)&#xa;    if len(agent_names) == 0:&#xa;        raise ValueError(""Error: no csv files found."")&#xa;&#xa;    if data_dir[-1] != ""/"":&#xa;        data_dir = data_dir + ""/""&#xa;&#xa;    cumulative = not(args.a)&#xa;    episodic = _is_episodic(data_dir)&#xa;    track_disc_reward = _is_disc_reward(data_dir)&#xa;&#xa;    # Plot.&#xa;    make_plots(data_dir, agent_names, cumulative=cumulative, episodic=episodic, track_disc_reward=track_disc_reward)&#xa;&#xa;if __name__ == ""__main__"":&#xa;    main()&#xa;"
4770993|"""""""&#xa;Copyright (C) 2011  N.D. Price Lab&#xa;&#xa;This program is free software: you can redistribute it and/or modify&#xa;it under the terms of the GNU Affero General Public License as published by&#xa;the Free Software Foundation, either version 3 of the License, or&#xa;(at your option) any later version.&#xa;This program is distributed in the hope that it will be useful,&#xa;but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;GNU Affero General Public License for more details.&#xa;&#xa;You should have received a copy of the GNU Affero General Public License&#xa;along with this program.  If not, see <http://www.gnu.org/licenses/>.&#xa;""""""&#xa;&#xa;from Tkinter import *&#xa;import traceback, tkMessageBox&#xa;import Tkconstants, tkFileDialog&#xa;import AUREA&#xa;import AUREA.GUI.Page as Page&#xa;from AUREA.parser.SettingsParser import *&#xa;import os.path&#xa;import shutil&#xa;import sys&#xa;import platform&#xa;import Queue&#xa;import tkMessageBox&#xa;&#xa;from AUREA.GUI.Results import *&#xa;class App(Frame):&#xa;    def __init__(self, root, controller):&#xa;        &#xa;        root.report_callback_exception = self.report_callback_exception&#xa;        Frame.__init__(self, root)&#xa;        self.thread_message_queue = Queue.Queue()&#xa;        self.thread_classify_queue = Queue.Queue()&#xa;        root.rowconfigure( 0, weight = 1 )&#xa;        root.columnconfigure( 0, weight = 1 )&#xa;        self.grid(sticky=W+E+N+S )&#xa;        self.err_disp = False#only display one error&#xa;        self.root = root&#xa;        self.root.title( ""AUREA - Adaptive Unified Relative Expression Analyser"")&#xa;        self.controller = controller&#xa;        if os.path.exists(self.controller.config.getSetting(""datatable"", ""Data Folder"")[0]):&#xa;            self.data_dir = self.controller.config.getSetting(""datatable"", ""Data Folder"")[0]&#xa;        else:&#xa;            self.data_dir = os.path.join(controller.workspace, 'data')&#xa;        self.curr_page = None&#xa;        self.pages = []&#xa;        self._initApp()&#xa;        self.rowconfigure( 1, weight = 1 )&#xa;        self.columnconfigure( 1, weight = 1 )&#xa;        self.checkTMQ()&#xa;        &#xa;&#xa;    def _initApp(self):&#xa;        """"""&#xa;        Set up all of the App Frame components&#xa;        """"""&#xa;        self.menu = AUREAMenu(self)&#xa;        self.remote = AUREARemote(self)&#xa;        self.buttonList = self.remote.buttonList&#xa;        self.root.config(menu = self.menu)&#xa;        self.status = StatusBar(self)&#xa;        self.AppTitle = StringVar()&#xa;        &#xa;        self.AppTitleLabel = Label(self, textvariable=self.AppTitle)&#xa;        import tkFont&#xa;        font = tkFont.Font(font=self.AppTitleLabel['font'])&#xa;        font.config(size=font.cget('size')*2, weight='bold', underline=1)&#xa;        self.AppTitleLabel['font'] = font&#xa;        self.AppTitle.set(""Data Summary"")&#xa;        self.AppTitleLabel.grid(row=0, column=0, columnspan=2, sticky=N+W+E+S)&#xa;        self.grid_columnconfigure(1,minsize=205)&#xa;        self.remote.grid(row=1, column=0, sticky=N+E+W)&#xa;        self.initPages()&#xa;        self.displayPage('Home')&#xa;        numcolumns, numrows = self.grid_size()&#xa;        self.status.grid(row=numrows,column=0, columnspan=numcolumns, sticky=W+E+S)&#xa;        self.update_idletasks()&#xa;    def checkTMQ(self):&#xa;        """"""&#xa;        Handles thread-based message passing&#xa;        """"""&#xa;        #I should really rewrite this to handle threads from the start&#xa;        #this is a hacky thing added because windows gets snarky about&#xa;        #not returning control back to the GUI when performing&#xa;        #meh&#xa;        while not self.thread_message_queue.empty():&#xa;            type, msg = self.thread_message_queue.get()&#xa;            if type == 'error':#pass stacktrace to error handler&#xa;                self.report_callback_exception(*msg)&#xa;            elif type == 'tspResult':#training completed&#xa;                TSPResults(self.curr_page)                &#xa;            elif type == 'tstResult':&#xa;                TSTResults(self.curr_page)&#xa;            elif type == 'diracResult':&#xa;                DiracResults(self.curr_page)&#xa;            elif type == 'ktspResult':&#xa;                KTSPResults(self.curr_page)&#xa;            elif type == 'adaptiveResult':&#xa;                AdaptiveResults(self.curr_page)&#xa;            elif type == 'adaptiveMessage':#didnt put in the time or something&#xa;                tkMessageBox.showerror(message=msg)&#xa;            elif type == 'statusbarset':&#xa;                self.status.set(msg)&#xa;            elif type == 'statusbarclear':&#xa;                self.status.clear()&#xa;            elif type == 'releaseButtons':&#xa;                self.curr_page.enableButtons()&#xa;            elif type == 'classifyComplete':&#xa;                self.curr_page.handleResults()#clear out messages&#xa;                with self.curr_page.results_lock:&#xa;                    self.curr_page.hRon = False&#xa;                self.curr_page.displayResults()&#xa;                self.status.clear()&#xa;        self.after(500, self.checkTMQ)     &#xa;&#xa;    def setAppTitle(self, title):&#xa;        self.AppTitle.set(title)&#xa;       &#xa;    def initPages(self):&#xa;     &#xa;        self.pages.append(Page.HomePage(self))&#xa;        self.pages.append(Page.ImportDataPage(self))&#xa;        self.pages.append(Page.ClassDefinitionPage(self))&#xa;        self.pages.append(Page.LearnerSettingsPage(self))&#xa;        self.pages.append(Page.TrainClassifiers(self))&#xa;        self.pages.append(Page.TestClassifiers(self))&#xa;        self.pages.append(Page.EvaluateClassifiers(self)) &#xa;&#xa;    def displayPage(self, page_id):&#xa;        self.status.clear()&#xa;        if self.curr_page:&#xa;            self.curr_page.clearPage()&#xa;            self.curr_page.grid_forget()&#xa;            self.curr_page = None&#xa;        for page in self.pages:&#xa;            if page.id == page_id:&#xa;                page.setUpPage()&#xa;                page.drawPage()&#xa;                self.curr_page = page&#xa;                break&#xa;        self.curr_page.grid(column=1, row=1,sticky=N+S+E+W)&#xa;        self.curr_page.rowconfigure( 1, weight = 1 )&#xa;        self.curr_page.columnconfigure( 1, weight = 1 )&#xa;        self.root.update_idletasks()&#xa;&#xa;    def next(self):&#xa;        try:&#xa;            next = self.curr_page.next()&#xa;        except Page.ImplementationError as e:&#xa;            print e.msg&#xa;&#xa;        if next:&#xa;            self.displayPage(next)&#xa;&#xa;    def prev(self):&#xa;        try:&#xa;            prev = self.curr_page.prev()&#xa;        except Page.ImplementationError as e:&#xa;            print e.msg&#xa;&#xa;        if prev:&#xa;            self.displayPage(prev)&#xa;&#xa;&#xa;    def report_callback_exception(self, *args):&#xa;        """"""&#xa;        displays exceptions&#xa;        TYVM : http://stackoverflow.com/questions/4770993/silent-exceptions-in-python-tkinter-should-i-make-them-louder-how&#xa;        """"""&#xa;        #several local functions&#xa;        #a lot of try/except blocks so errors in error catching&#xa;        #does not stop display of errors &#xa;        #(i.e. cant be sure of the state of a crashed system)&#xa;        def getSysInfo():&#xa;            """"""&#xa;            Returns a string containing as much system info as we can get&#xa;            """"""&#xa;            import sys, os&#xa;            sysString = """"&#xa;            nl = os.linesep&#xa;            u = ""Unavailable"" + nl&#xa;            sysString +=""Platform:""            &#xa;            try:&#xa;                sysString += sys.platform + nl&#xa;            except:&#xa;                sysString += u&#xa;            sysString += ""Python version: ""&#xa;            try:&#xa;                sysString += '.'.join([str(x) for x in sys.version_info]) + nl&#xa;            except:&#xa;                sysString += u&#xa;            sysString += ""Path: ""&#xa;            try:&#xa;                sysString += ' ,'.join(sys.path) + nl&#xa;            except:&#xa;                sysString += u&#xa;            return sysString&#xa;&#xa;        def getInstanceInfo(self):&#xa;            """"""&#xa;            Returns a string with information about this AUREA instance&#xa;            """"""&#xa;            insStr = """"&#xa;            nl = os.linesep&#xa;            u = ""unavailable"" + nl&#xa;            insStr += ""AUREA version: ""&#xa;            try:&#xa;                import AUREA&#xa;                insStr += AUREA.__version__ + nl&#xa;            except:&#xa;                insStr += u&#xa;            insStr += ""Current Page: ""&#xa;            try:&#xa;                insStr += self.curr_page.id + nl&#xa;            except:&#xa;                insStr += u&#xa;            insStr += ""GUI State: ""&#xa;            try:&#xa;                insStr +=  "","".join(map(str,self.controller.dependency_state))+nl&#xa;            except:&#xa;                insStr += u&#xa;            insStr += ""Data Files: ""&#xa;            try:&#xa;                insStr += nl.join(self.controller.softFile)&#xa;            except:&#xa;                insStr += u&#xa;&#xa;            return insStr&#xa;&#xa;            &#xa;                &#xa;        def saveError(self,errString):&#xa;            """"""&#xa;            Saves errString(a string) to a text file&#xa;            """"""&#xa;            import tkFileDialog &#xa;            from datetime import datetime&#xa;            dt = datetime(2000,1,1)&#xa;            today = '-'.join(dt.today().isoformat('-').split(':')[:2])&#xa;            options = {}&#xa;            options['defaultextension'] = '' # couldn't figure out how this works&#xa;            options['initialdir'] = self.data_dir&#xa;            options['initialfile'] = 'AHREA-Error-' + today + '.txt'&#xa;            options['parent'] = self&#xa;            options['title'] = 'Save Results'&#xa;            filename = tkFileDialog.asksaveasfilename(**options)&#xa;            if filename:&#xa;                o = open(filename, 'w')&#xa;                o.write(errString)&#xa;                o.close()&#xa;&#xa;        if self.err_disp:&#xa;            #if we are already displaying one error&#xa;            #lets not display more, no avalanche of popups&#xa;            return&#xa;        try:#clear buttons&#xa;            self.remote.disableAllButtons()&#xa;        except Exception, e:&#xa;            print e&#xa;            &#xa;        err = traceback.format_exception(*args)&#xa;        msg = ""An Error has occurred.""&#xa;        t = Toplevel(self)&#xa;        t.title(""AUREA Error"")&#xa;        Label(t,text=msg).pack()&#xa;        import os&#xa;        errmsg = 'Please save this error to a text file and go to https://github.com/JohnCEarls/AUREA/issues to report it.'+ os.linesep&#xa;        errmsg += 'If you do not want to create a github account, you may email the error file to john.c.earls+aurea@gmail.com.' + os.linesep&#xa;        errmsg +=os.linesep.join(err)&#xa;        errmsg += getSysInfo()&#xa;        errmsg += getInstanceInfo(self)&#xa;        scroll = Scrollbar(t)&#xa;        scroll.pack(side=RIGHT, fill=Y)&#xa;        &#xa;        errBox = Text(t,wrap=WORD)&#xa;        scroll.config(command=errBox.yview)&#xa;        errBox.config(yscrollcommand=scroll.set)&#xa;        errBox.pack()&#xa;        errBox.insert(END, errmsg) &#xa;        errSave = Button(t, text=""Save..."", command=lambda:saveError(self,errmsg))&#xa;        errSave.pack()&#xa;        self.err_disp = True&#xa;&#xa;&#xa;class StatusBar(Frame):&#xa;    """"""&#xa;    Note the statusbar is a child of the root&#xa;    """"""&#xa;    def __init__(self, master):&#xa;        Frame.__init__(self, master)&#xa;        self.label = Label(self, bd=1, relief=SUNKEN, anchor=W)&#xa;        self.label.pack(fill=X)&#xa;&#xa;    def set(self, format, *args):&#xa;        self.label.config(text=format % args)&#xa;        self.label.update_idletasks()&#xa;&#xa;    def clear(self):&#xa;        self.label.config(text="""")&#xa;        self.label.update_idletasks()               &#xa;        &#xa;&#xa;class AUREARemote(Frame):&#xa;    """"""&#xa;    This frame acts as the remote control for the gui.&#xa;    """"""&#xa;    #Dependency enum, these should be static&#xa;    DataImport = 0&#xa;    NetworkImport = 1&#xa;    ClassCreation = 2&#xa;    TrainDirac = 3&#xa;    TrainTSP = 4&#xa;    TrainTST = 5&#xa;    TrainKTSP = 6&#xa;    TrainAdaptive = 7&#xa;    TrainAny = 8#specialCase&#xa;    #end enum&#xa;    NumStates = 9 #number of states in dependencies&#xa;    def __init__(self, master):&#xa;        """"""&#xa;        Builds the left side controller of AUREA&#xa;        master - the App Frame&#xa;        """"""&#xa;        Frame.__init__(self, master, width=200)&#xa;        self.m = master&#xa;        aa=AUREARemote&#xa;        self.depGraph = [[0 for x in range(aa.NumStates)] for y in range(aa.NumStates)]&#xa;        self.setNavDependencies()&#xa;        self.buildNav()&#xa;        self.buildDependencyGraph()&#xa;        self.layoutNav()&#xa;        self.stateChange()&#xa;&#xa;    def buildDependencyGraph(self):&#xa;        """"""&#xa;        The dependency graph stores direct dependencies&#xa;        """"""&#xa;        def depends(A,B):&#xa;            for x in A:&#xa;                for y in B:&#xa;                    self.depGraph[x][y] = 1&#xa;        aa= AUREARemote&#xa;&#xa;        depends([aa.TrainAdaptive,aa.TrainDirac,aa.TrainKTSP,aa.TrainTSP,aa.TrainTST], [aa.ClassCreation])&#xa;        depends([aa.TrainAdaptive, aa.TrainDirac],[aa.NetworkImport])&#xa;        depends([aa.ClassCreation], [aa.DataImport])&#xa;&#xa;    def getDependents(self, dependency):&#xa;        return [g[dependency] for g in self.depGraph]&#xa;&#xa;&#xa;    def getDepVector(self, dependencies):&#xa;        """"""&#xa;        Given a list of dependencies(A), return a list of 1s and 0s&#xa;        showing associated dependencies(B)&#xa;        Where A depends on B&#xa;        """"""&#xa;        def setDep(dvector, dependency):&#xa;            dvector[dependency] = 1&#xa;        def traverse(node):&#xa;            traversal = [node]&#xa;            for i,x in enumerate(self.depGraph[node]):&#xa;                if x == 1:&#xa;                    traversal.append(i)&#xa;                    for j in traverse(i):&#xa;                        traversal.append(j)&#xa;            return traversal&#xa;        aa=AUREARemote&#xa;        dvec = [0 for x in range(aa.NumStates)]&#xa;        #basically unioning all dependency paths&#xa;        for dep in dependencies:&#xa;            t = traverse(dep)&#xa;            for node in t:&#xa;                setDep(dvec, node)&#xa;        &#xa;        return dvec&#xa;&#xa;    def dependenciesSatisfied(self,currState, depVector):&#xa;        """"""&#xa;        Given the current state and the dependency vector&#xa;        Return True if all dependencies have been satisfied&#xa;        """"""&#xa;        aa = AUREARemote&#xa;&#xa;        if depVector[aa.TrainAny] == 1:&#xa;            #special case for when any LA has run&#xa;            for d in [self.getDepVector([x]) for x in [aa.TrainDirac, aa.TrainTSP, aa.TrainTST, aa.TrainKTSP, aa.TrainAdaptive]]:&#xa;                if self.dependenciesSatisfied(currState, d):&#xa;                    return True&#xa;            return False&#xa;&#xa;        for i, on in enumerate(currState):&#xa;            if depVector[i] == 1 and on != 1:&#xa;                return False&#xa;        return True&#xa;&#xa;    def stateChange(self):&#xa;        """"""&#xa;        Enables or disables buttons based on their dependencies&#xa;        and the current state of controller&#xa;        """"""&#xa;        for i,button in enumerate(self.buttonList):&#xa;            dvec = self.getDepVector(self.navDep[i])&#xa;            if self.dependenciesSatisfied(self.m.controller.dependency_state,dvec ):&#xa;               button.configure(state=NORMAL)&#xa;            else:&#xa;               button.configure(state=DISABLED)&#xa;&#xa;    def disableAllButtons(self):&#xa;        """"""&#xa;        Disables all buttons&#xa;        """"""&#xa;        for button in self.buttonList:&#xa;            button.configure(state=DISABLED)&#xa;&#xa;&#xa;&#xa;    def setNavDependencies(self):&#xa;        """"""&#xa;        Maps between buttons and dependencies&#xa;        """"""&#xa;        aa = AUREARemote&#xa;        self.navDep = []&#xa;        #home&#xa;        self.navDep.append([])&#xa;        #import data&#xa;        self.navDep.append([])&#xa;        #class definition&#xa;        self.navDep.append([aa.DataImport])&#xa;        #Learner Settings&#xa;        self.navDep.append([])&#xa;        #Train Classifiers&#xa;        self.navDep.append([aa.ClassCreation])&#xa;        #Test Classifiers&#xa;        self.navDep.append([aa.TrainAny])&#xa;        #Evaluate Performance&#xa;        self.navDep.append([aa.ClassCreation])&#xa;&#xa;    def getMessage(self, msg_key):&#xa;        """"""&#xa;        Returns the string associated with a message to be displayed&#xa;        in the window that holds the AUREA picture&#xa;        """"""&#xa;        messages = {}&#xa;        nl = os.linesep&#xa;        #nav mouseover messages&#xa;        #home page&#xa;        messages['home'] = ""Home:""+ 2*nl&#xa;        messages['home'] += ""View a summary of the current state of AUREA."" +nl&#xa;        messages['home'] += ""Review imported files, statistics, classification results, etc.""&#xa;        #import page&#xa;        messages['import'] = ""Import Data:""+2*nl&#xa;        messages['import'] += ""Import the data on which to base the models."" + nl&#xa;        messages['import'] += ""Browse for or download the data files for training and classification."" + nl&#xa;        messages['import'] += ""Select the network and synonym files for DiRaC and Adaptive algorithms."" &#xa;&#xa;        #class definition &#xa;        messages['classd'] = 'Class Definition:' + 2*nl&#xa;        messages['classd'] += ""Label and partition the imported data into classes for training.""&#xa;        &#xa;        #settings&#xa;        messages['settings'] = ""Learner Settings:"" + 2*nl&#xa;        messages['settings'] += ""Configure the parameters of the learning algorithms.""&#xa;        &#xa;        #train&#xa;        messages['train'] = ""Train Classifiers:"" +2*nl&#xa;        messages['train'] += ""Train the learning algorithms.""&#xa;&#xa;        #test&#xa;        messages['test'] = ""Test Classifiers:"" + 2*nl&#xa;        messages['test'] += ""Use your trained classifiers to classify samples not in the training set""&#xa;        #evaluate&#xa;        messages['evaluate'] = ""Evaluate Performance:"" + 2*nl&#xa;        messages['evaluate'] += ""Perform k-fold cross validation on the learning algorithms.""&#xa;        return messages[msg_key]&#xa;&#xa;&#xa;    def buildNav(self):&#xa;        """"""&#xa;        Create button objects&#xa;        """"""&#xa;        import os&#xa;        &#xa;        dp = self.m.displayPage&#xa;&#xa;        a=self.home_button = Button(self, text=""Home"", command=lambda:dp('Home'))&#xa;        &#xa;        b=self.import_button = Button(self, text=""Import Data"", command=lambda:dp('Import'))&#xa;        c=self.class_button = Button(self, text=""Class Definition"", command=lambda:dp('Class'))&#xa;        d=self.settings_button = Button(self, text=""Learner Settings"", command=lambda:dp('Settings'))&#xa;        e=self.train_button = Button(self, text=""Train Classifiers"", command=lambda:dp('Train'))&#xa;        f=self.test_button = Button(self, text=""Test Classifiers"", command=lambda:dp('Test'))&#xa;        g=self.evaluate_button = Button(self, text=""Evaluate Performance"", command=lambda:dp('Evaluate'))&#xa;        &#xa;        bm = self.bindMessage&#xa;        bm(a,self.getMessage('home'))&#xa;        bm(b,self.getMessage('import'))&#xa;        bm(c,self.getMessage('classd'))&#xa;        bm(d,self.getMessage('settings'))&#xa;        bm(e,self.getMessage('train'))&#xa;        bm(f,self.getMessage('test'))&#xa;        bm(g,self.getMessage('evaluate'))&#xa;        self.buttonList = [a,b,c,d,e,f,g]&#xa;        welcome_img = os.path.join(self.m.controller.workspace, 'data', 'AUREA-logo-200.pgm')&#xa;        self.photo = photo = PhotoImage(file=welcome_img)&#xa;        self.plabel = Label(self,  image=photo)&#xa;&#xa;    def bindMessage(self, widget, msg):&#xa;        import random&#xa;        i = random.randint(0,100)&#xa;        widget.bind(""<Enter>"", lambda e: self.messageOn(msg,i))&#xa;        widget.bind(""<Leave>"", lambda e: self.messageOff(i))&#xa;&#xa;&#xa;    def layoutNav(self):&#xa;        """"""&#xa;        Layout Button objects&#xa;        """"""&#xa;        for i,button in enumerate(self.buttonList):&#xa;            button.grid(row=i, column=0, sticky=E+W,padx=3, pady=4 )&#xa;        self.plabel.config(width=200)&#xa;        self.plabel.grid(row=len(self.buttonList), column=0)&#xa;&#xa;&#xa;    def messageOn(self, msg, rand):&#xa;        self.tag = rand&#xa;        self.plabel = Message(self, text=msg,relief=SUNKEN, width=200 )&#xa;        self.plabel.grid(row=len(self.buttonList),column=0, sticky =N+S+E+W)&#xa;    &#xa;    def messageOff(self,rand):&#xa;        from time import sleep&#xa;        sleep(.1)&#xa;        if self.tag == rand:&#xa;            self.plabel = Label(self,  image=self.photo)&#xa;            self.plabel.grid(row=len(self.buttonList), column=0, sticky=N+S+E+W)&#xa;           &#xa;    def nullaction(self):&#xa;        print self.m.pack_info()&#xa;&#xa;class AUREAMenu(Menu):&#xa;    def __init__(self, daRoot):&#xa;        Menu.__init__(self, daRoot)&#xa;        self.root = daRoot&#xa;        self.data_dir = self.root.data_dir&#xa;        self.buildFile()&#xa;        #self.buildSettings()&#xa;        self.buildHelp()&#xa;        self.dialog = None&#xa;&#xa;&#xa;    def buildFile(self):&#xa;        filemenu = Menu( self )&#xa;        self.add_cascade(label=""File"", menu=filemenu)&#xa;        filemenu.add_command(label=""Load Settings..."", command=self.load_settings)&#xa;        filemenu.add_command(label=""Save Settings..."", command=self.save_settings)&#xa;&#xa;    def buildSettings(self):&#xa;        """"""&#xa;        Deprecated, they have their own Page now.&#xa;        """"""&#xa;        raise Exception(""menu.buildSettings is deprecated. Use the Page.learnerSettings"")&#xa;        settingsmenu = Menu( self )&#xa;        self.add_cascade(label=""Settings"", menu=settingsmenu)&#xa;        settingsmenu.add_command(label=""Data..."", command=self.data_settings)&#xa;        settingsmenu.add_command(label=""Dirac..."", command=self.dirac_settings)&#xa;        settingsmenu.add_command(label=""TSP..."", command=self.tsp_settings)&#xa;        settingsmenu.add_command(label=""k-TSP..."", command=self.ktsp_settings)&#xa;        settingsmenu.add_command(label=""TST..."", command=self.tst_settings)&#xa;        settingsmenu.add_command(label=""Adaptive..."", command=self.adaptive_settings)&#xa;         &#xa;    def buildHelp(self):&#xa;        helpmenu = Menu(self)&#xa;        self.add_cascade(label=""Help"", menu=helpmenu)&#xa;        helpmenu.add_command(label=""Content"", command=self.getHelp)&#xa;        helpmenu.add_command(label=""Report a Problem"", command=self.reportProblem)&#xa;        helpmenu.add_command(label=""About"", command=self.about)&#xa;&#xa;    def reportProblem(self):&#xa;        msg = ""Please go to https://github.com/JohnCEarls/AUREA/issues to report any bugs or make any feature requests. Thank you for helping make AUREA better.""&#xa;        tkMessageBox.showinfo(""AUREA: report error"", msg)&#xa;&#xa;    def getHelp(self):&#xa;        msg = ""Please go to [http://price.systemsbiology.net/AUREA/] for documentation.""&#xa;        tkMessageBox.showinfo(""AUREA: get help"", msg)&#xa; &#xa;    def about(self):&#xa;        msg = ""AUREA v. "" + AUREA.__version__&#xa;        msg += "" Copyright (c) 2010-2013 The N.D. Price Lab @ ISB""&#xa;        tkMessageBox.showinfo(""AUREA"", msg)&#xa;        &#xa;&#xa;    def dirac_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""dirac"", self.dirac_settings_write)&#xa;&#xa;    def data_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""datatable"", self.data_settings_write)&#xa;&#xa;    def data_settings_write(self):&#xa;        self.settings_write(""datatable"")&#xa;        self.dialog.destroy()&#xa;        self.root.data_dir = self.root.controller.config.getSetting(""datatable"", ""Data Folder"")[0]&#xa;        self.data_dir = self.root.data_dir&#xa;        self.dialog = None&#xa;&#xa;    &#xa;    def settings_display(self, learner, write_function):&#xa;        dialog = self.dialog = Toplevel(self.root)&#xa;        self.dialog.protocol('WM_DELETE_WINDOW', self.close_window)&#xa;        self.dialog.title(learner + "" settings"")&#xa;        self.dialog.transient(self.root)&#xa;        self.dialog.geometry(""+%d+%d"" % (self.root.winfo_rootx()+50,&#xa;                                  self.root.winfo_rooty()+50))&#xa;        settings = self.root.controller.config.getSettings(learner)&#xa;        self.entries = []&#xa;        for i, setting in enumerate(settings):&#xa;            Label(dialog, text=setting[0]).grid(row=i, column=0)&#xa;            c_off = 1&#xa;            for j,val in enumerate(setting[1]):&#xa;                e = Entry(dialog)&#xa;                e.insert(0,val)&#xa;                self.entries.append(e)&#xa;                e.grid(row=i, column=c_off+j)&#xa;        b = Button(dialog, text=""OK"", command=write_function)&#xa;        b.grid(row=len(settings), column=0)&#xa;       &#xa;    def close_window(self):&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;    def settings_write(self, learner):&#xa;        settings = self.root.controller.config.getSettings(learner)&#xa;        curr_entry = 0&#xa;        for setting in settings:&#xa;            config_list = []&#xa;            for x in setting[1]:&#xa;                vals = self.entries[curr_entry].get()&#xa;                config_list.append(vals)&#xa;                curr_entry += 1&#xa;            self.root.controller.config.setSetting(learner, setting[0], config_list)&#xa;&#xa;        self.dialog.destroy()&#xa;    def dirac_settings_write(self):&#xa;        self.settings_write(""dirac"")&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;    def tsp_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""tsp"", self.tsp_settings_write)&#xa;    def tsp_settings_write(self):&#xa;        self.settings_write(""tsp"")&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;    def ktsp_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""ktsp"", self.ktsp_settings_write)&#xa;    def ktsp_settings_write(self):&#xa;        self.settings_write(""ktsp"")&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;    def tst_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""tst"", self.tst_settings_write)&#xa;&#xa;    def tst_settings_write(self):&#xa;        self.settings_write(""tst"")&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;    def adaptive_settings(self):&#xa;        if self.dialog is None:&#xa;            self.settings_display(""adaptive"", self.adaptive_settings_write)&#xa;&#xa;    def adaptive_settings_write(self):&#xa;        self.settings_write(""adaptive"")&#xa;        self.dialog.destroy()&#xa;        self.dialog = None&#xa;&#xa;&#xa;    def save_settings(self):        &#xa;        options = {}&#xa;        options['defaultextension'] = '' # couldn't figure out how this works&#xa;        if platform.platform()[0:3] != 'Dar':#mac does not like this&#xa;            options['filetypes'] = [('xml config', '.xml')]&#xa;        options['initialdir'] = self.data_dir&#xa;        options['initialfile'] = ''&#xa;        options['parent'] = self&#xa;        options['title'] = 'Save config'&#xa;        filename = tkFileDialog.asksaveasfilename(**options)&#xa;        if filename:&#xa;            self.root.controller.config.writeSettings(filename)&#xa;&#xa;    def load_settings(self):&#xa;        options = {}&#xa;        options['defaultextension'] = '' # couldn't figure out how this works&#xa;        if platform.platform()[0:3] != 'Dar':#mac does not like this&#xa;            options['filetypes'] = [('xml config', '.xml')]&#xa;        options['initialdir'] = self.data_dir&#xa;        options['initialfile'] = 'config.xml'&#xa;        options['parent'] = self&#xa;        options['title'] = 'Load config'&#xa;        filename = tkFileDialog.askopenfilename(**options)&#xa;        if filename:&#xa;            self.root.controller.config = SettingsParser(filename)&#xa;&#xa;            self.root.data_dir = self.root.controller.config.getSetting(""datatable"", ""Data Folder"")[0]&#xa;&#xa;    def unImplemented(self):&#xa;        print ""unimplemented""&#xa;&#xa;"
862173|"#!/usr/bin/env python&#xa;'''This is a quick and dirty way to provide developers a way to download&#xa;resources needed to build and test components of the open tree software.&#xa;&#xa;environmental variables:&#xa;    OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_LEVEL=debug for more info&#xa;    &#xa;    OPEN_TREE_DOWNLOAD_DEV_RESOURCE_CFG or OPEN_TREE_USER_SETTINGS_DIR to change the &#xa;        default location for config file that stores local filepaths&#xa;&#xa;'''&#xa;import os&#xa;import sys&#xa;import logging&#xa;from os.path import basename&#xa;from urlparse import urlsplit&#xa;import urllib2&#xa;import urllib&#xa;import shutil&#xa;import subprocess&#xa;import zipfile&#xa;from open_tree_env import get_otol_build_env, put_otol_build_env_into_env, abbreviate_path&#xa;&#xa;&#xa;&#xa;# Global dictionary of resources grouped by 'category' Note that dict is filled&#xa;#   as a side effect of creating an OpenTreeResource&#xa;ALL_RESOURCES = {}&#xa;RESOURCE_CATEGORIES = {'taxonomy' : 'OPEN_TREE_TAXONOMY_DIR',&#xa;                       'dependency' : 'OPEN_TREE_DEPENDENCY_DIR'&#xa;                       }&#xa;for key in RESOURCE_CATEGORIES.keys():&#xa;    ALL_RESOURCES[key] = []&#xa;SUPPORTED_PROTOCOLS = ['http', 'svn']&#xa;&#xa;# global config file parser&#xa;_CFG_PARSER = None&#xa;_ACTION_LOG_FILE = None&#xa;_LAST_LOGGED_DIR = None&#xa;def _get_action_log_file():&#xa;    global _ACTION_LOG_FILE&#xa;    if _ACTION_LOG_FILE is None:&#xa;        fp = os.path.join(get_otol_build_env('OPEN_TREE_DEPENDENCY_DIR'), 'log-bootstrap.txt')&#xa;        _LOG.debug('Opening ""%s"" as action log' % fp)&#xa;        fo = open(fp, 'a')&#xa;        _ACTION_LOG_FILE = fo&#xa;    return _ACTION_LOG_FILE&#xa;        &#xa;def log_action(message):&#xa;    global _LAST_LOGGED_DIR&#xa;    op = os.path.abspath(os.curdir)&#xa;    action_log_file = _get_action_log_file()&#xa;    if (_LAST_LOGGED_DIR is None) or (_LAST_LOGGED_DIR != op):&#xa;        _LAST_LOGGED_DIR = op&#xa;        abbrev = abbreviate_path(op)&#xa;        action_log_file.write('cd ""%s""\n' % abbrev)        &#xa;    action_log_file.write(message + '\n')&#xa;    action_log_file.flush()&#xa;    _LOG.debug(""ACTION: "" + message)&#xa;    &#xa;&#xa;################################################################################&#xa;# The following is from http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way&#xa;############################&#xa;&#xa;def url2name(url):&#xa;    return basename(urllib.unquote(urlsplit(url)[2]))&#xa;&#xa;def download_http(url, localFileName=None):&#xa;    localName = url2name(url)&#xa;    req = urllib2.Request(url)&#xa;    r = urllib2.urlopen(req)&#xa;    if r.info().has_key('Content-Disposition'):&#xa;        # If the response has Content-Disposition, we take file name from it&#xa;        localName = r.info()['Content-Disposition'].split('filename=')[1]&#xa;        dq = (localName[0] == '""' and localName[-1] == '""')&#xa;        sq = (localName[0] == ""'"" and localName[-1] == ""'"")&#xa;        if sq or dq:&#xa;            localName = localName[1:-1]&#xa;    elif r.url != url: &#xa;        # if we were redirected, the real file name we take from the final URL&#xa;        localName = url2name(r.url)&#xa;    if localFileName: &#xa;        # we can force to save the file as specified name&#xa;        localName = localFileName&#xa;    f = open(localName, 'wb')&#xa;    d = os.path.abspath(os.curdir)&#xa;    _LOG.info('Downloading ""%s"" to ""%s""...\n' % (localName, d))&#xa;    shutil.copyfileobj(r, f)&#xa;    f.close()&#xa;    r.close()&#xa;    log_action('wget ""%s""' % (url))&#xa;    return localName&#xa;&#xa;def _my_makedirs(dir):&#xa;    '''Calls os.makedirs (if needed), and logs the action.'''&#xa;    if not os.path.exists(dir):&#xa;        os.makedirs(dir)&#xa;        log_action('mkdir -p ""%s""' % os.path.abspath(dir))&#xa;def _my_chdir(dir):&#xa;    '''Calls os.chdir, and logs the action.'''&#xa;    if not os.path.exists(dir):&#xa;        _my_makedirs(dir)&#xa;    os.chdir(dir)&#xa;&#xa;&#xa;############################&#xa;# End code snippet from stackoverflow&#xa;################################################################################&#xa;## The following is modified from: http://code.activestate.com/recipes/252508/ (r2)&#xa;# unzip.py&#xa;#     Version: 1.1&#xa;# &#xa;#     Extract a zipfile to the directory provided&#xa;#     It first creates the directory structure to house the files&#xa;#     then it extracts the files to it.&#xa;#     &#xa;# &#xa;#     By Doug Tolton&#xa;&#xa;&#xa;class unzip:&#xa;        &#xa;    def extract(self, file, dir):&#xa;        if not dir.endswith(':'):&#xa;            _my_makedirs(dir)&#xa;    &#xa;        zf = zipfile.ZipFile(file)&#xa;&#xa;        # create directory structure to house files&#xa;        self._createstructure(file, dir)&#xa;&#xa;        # extract files to directory structure&#xa;        for i, name in enumerate(zf.namelist()):&#xa;            if not name.endswith('/'):&#xa;                outfile = open(os.path.join(dir, name), 'wb')&#xa;                outfile.write(zf.read(name))&#xa;                outfile.flush()&#xa;                outfile.close()&#xa;        log_action('unzip ""%s""' % (file))&#xa;&#xa;    def _createstructure(self, file, dir):&#xa;        self._makedirs(self._listdirs(file), dir)&#xa;&#xa;&#xa;    def _makedirs(self, directories, basedir):&#xa;        """""" Create any directories that don't currently exist """"""&#xa;        for dir in directories:&#xa;            curdir = os.path.join(basedir, dir)&#xa;            _my_makedirs(curdir)&#xa;&#xa;    def _listdirs(self, file):&#xa;        """""" Grabs all the directories in the zip structure&#xa;        This is necessary to create the structure before trying&#xa;        to extract the file to it. """"""&#xa;        zf = zipfile.ZipFile(file)&#xa;&#xa;        dirs = []&#xa;&#xa;        for name in zf.namelist():&#xa;            if name.endswith('/'):&#xa;                dirs.append(name)&#xa;&#xa;        dirs.sort()&#xa;        return dirs&#xa;&#xa;def unzip_file(file, dest_parent):&#xa;    u = unzip()&#xa;    u.extract(file, dest_parent)&#xa;## end of http://code.activestate.com/recipes/252508/ }}}&#xa;&#xa;def untar_gz(file, dest_parent):&#xa;    system_call(['tar', 'xfvz', os.path.abspath(file)], wd=dest_parent)&#xa;&#xa;class RESOURCE_STATUS_CODE:&#xa;    SKIPPED, MISSING, DOWNLOADED, INSTALLED = range(4)&#xa;&#xa;&#xa;def system_call(invoc, wd=None):&#xa;    prev_dir = None&#xa;    if wd is not None:&#xa;        prev_dir = os.path.abspath(os.curdir)&#xa;        wd = os.path.expandvars(wd)&#xa;        _my_makedirs(wd)&#xa;    else:&#xa;        wd = os.curdir&#xa;    try:&#xa;        wl = []&#xa;        for word in invoc:&#xa;            if (len(word.split()) > 1) or (len(word.split('$')) > 1):&#xa;                wl.append('""%s""' % word)&#xa;            else:&#xa;                wl.append(word)&#xa;        m = ' '.join(wl)&#xa;        d = os.path.abspath(wd)&#xa;        _my_chdir(d)&#xa;        log_action(m)&#xa;        rc = subprocess.call(invoc, shell=True)&#xa;        if rc != 0:&#xa;            message = 'The command:\n""%s""\nexecuted from %s failed with returncode %d\n' % (m, d, rc)&#xa;            raise RuntimeError(message)&#xa;    finally:&#xa;        if prev_dir is not None:&#xa;            _my_chdir(prev_dir)&#xa;        &#xa;class OpenTreeResource(object):&#xa;    '''A bundle of information about a resource. Attributes:&#xa;        `name` - name resource when downloaded and unpacked&#xa;        `url` - location of the downloadable resource&#xa;        `protocol` = http, git, svn&#xa;        `compression` - compression protocol (empty or zip)&#xa;        `min_version` - tuple of 3 integers (major, minor, revision)&#xa;        `category` - name of the type of resource (taxonomy, dependency...)&#xa;        `compressed_name` - if different from the default for the compression type.&#xa;                should be identical to the file name created&#xa;        `description` - string describing the resource&#xa;        `install_steps` is a list of dicts. each one has a wd key (working directory)&#xa;                and a list of commands to be run from that directory&#xa;        `install_parent` the parent directory of the installed products&#xa;        `install_sub` a path to be joined to `install_parent` to verify that &#xa;                the install step worked.&#xa;    '''&#xa;    UNPACKING_PROTOCOLS = ['', 'zip', 'tar.gz']&#xa;    def __init__(self,&#xa;                 name,&#xa;                 url,&#xa;                 protocol='http',&#xa;                 min_version=(0,0,0),&#xa;                 category=None,&#xa;                 compression='',&#xa;                 compressed_name=None,&#xa;                 contact='',&#xa;                 description='',&#xa;                 install_steps=None,&#xa;                 install_parent='',&#xa;                 install_sub='',&#xa;                 requires=None):&#xa;        self.name = name&#xa;        self.url = url&#xa;        self.protocol = protocol.lower()&#xa;        if self.protocol not in SUPPORTED_PROTOCOLS:&#xa;            raise ValueError('Protocol ' + protocol + ' unrecognized')&#xa;        self.category = category.lower()&#xa;        if self.category not in RESOURCE_CATEGORIES.keys():&#xa;            raise ValueError('Category ' + category + ' unrecognized')&#xa;        self.compression = compression.lower()&#xa;        if self.compression not in OpenTreeResource.UNPACKING_PROTOCOLS:&#xa;            raise ValueError('Compression method ' + compression + ' unrecognized')&#xa;        if compressed_name is None:&#xa;            if self.compression == '':&#xa;                self.compressed_name = self.name&#xa;            elif self.compression == 'zip':&#xa;                self.compressed_name = self.name + '.zip'&#xa;            elif self.compression == 'tar.gz':&#xa;                self.compressed_name = self.name + '.tar.gz'&#xa;            else:&#xa;                assert False&#xa;        else:&#xa;            self.compressed_name = compressed_name&#xa;        self.contact = contact&#xa;        self.description = description&#xa;        self.path = None&#xa;        self.installed_path = None&#xa;        self.status = None&#xa;        self.install_steps = install_steps&#xa;        self.install_parent = install_parent&#xa;        self.install_sub = install_sub&#xa;        self.requires = requires&#xa;        # here is the hack in which we add resource to the global list&#xa;        ALL_RESOURCES[self.category].append(self)&#xa;&#xa;    def listing(self, opts):&#xa;        return '%s = %s' % (self.name, self.description)&#xa;&#xa;    def do_install(self, cfg_path, opts):&#xa;        if self.status < RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;            self.do_download()&#xa;        if self.install_steps:&#xa;            final_path = os.path.join(os.path.expandvars(self.install_parent), &#xa;                                      os.path.expandvars(self.install_sub))&#xa;        else:&#xa;            final_path = self.path&#xa;        if os.path.exists(final_path):&#xa;            self.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;            self.installed_path = final_path&#xa;            return&#xa;        if self.requires:&#xa;            for requirement in self.requires:&#xa;                install_command(requirement, cfg_path, opts)&#xa;        cwd = os.path.abspath(os.getcwd())&#xa;        try:&#xa;            &#xa;            parent_var = os.path.expandvars(self.install_parent)&#xa;            _my_makedirs(parent_var)&#xa;            download_parent = os.path.dirname(os.path.abspath(self.path))&#xa;            _my_chdir(self.path)&#xa;            for step_dict in self.install_steps:&#xa;                wd = step_dict.get('wd')&#xa;                cmd_list = step_dict.get('commands')&#xa;                for cmd in cmd_list:&#xa;                    system_call(cmd, wd)&#xa;        finally:&#xa;            _my_chdir(cwd)&#xa;        if not os.path.exists(final_path):&#xa;            raise RuntimeError('Installation steps completed, but installation products were not found at ""%s""' % final_path)&#xa;        self.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;        self.installed_path = final_path&#xa;            &#xa;    def do_download(self):&#xa;        parent_var = RESOURCE_CATEGORIES[self.category]&#xa;        parent_dir = get_otol_build_env(parent_var)&#xa;        assert parent_dir&#xa;        _my_makedirs(parent_dir)&#xa;        cwd = os.path.abspath(os.getcwd())&#xa;        try:&#xa;            parent_dir = os.path.abspath(parent_dir)&#xa;            expected_path = os.path.join(parent_dir, self.compressed_name)&#xa;            _my_chdir(parent_dir)&#xa;            if self.protocol == 'http':&#xa;                if os.path.exists(expected_path):&#xa;                    _LOG.warn('Path ""%s"" already exists. It will not be downloaded again...\n' % expected_path)&#xa;                    fp = expected_path&#xa;                else:&#xa;                    p = download_http(self.url)&#xa;                    fp = os.path.join(parent_dir, p)&#xa;                self.compressed_path = fp&#xa;&#xa;                expected_path = os.path.join(parent_dir, self.name)&#xa;                if os.path.exists(expected_path):&#xa;                    if expected_path != fp:&#xa;                        _LOG.warn('Path ""%s"" already exists. It will not be unpacked again...\n' % expected_path)&#xa;                    fp = expected_path&#xa;                else:&#xa;                    if self.compression == 'zip':&#xa;                        _LOG.info('Unzipping ""%s""...\n""' % fp)&#xa;                        unzip_file(fp, parent_dir)&#xa;                        fp = os.path.join(parent_dir, self.name)&#xa;                    elif self.compression == 'tar.gz':&#xa;                        _LOG.info('Unpacking ""%s""...\n""' % fp)&#xa;                        untar_gz(fp, parent_dir)&#xa;                        fp = os.path.join(parent_dir, self.name)&#xa;                    _LOG.info('Obtained ""%s""...\n""' % fp)&#xa;                self.path = fp&#xa;                return fp&#xa;            elif self.protocol == 'svn':&#xa;                if os.path.exists(expected_path):&#xa;                    _LOG.warn('Path ""%s"" already exists. It will not be downloaded again...\n' % fp)&#xa;                else:&#xa;                    system_call(['svn', 'checkout', self.url, self.name])&#xa;                self.path = expected_path&#xa;                self.compressed_path = self.path&#xa;            else:&#xa;                assert False&#xa;            self.status = RESOURCE_STATUS_CODE.DOWNLOADED&#xa;&#xa;        finally:&#xa;            _my_chdir(cwd)&#xa;&#xa;def get_resource_obj(name=''):&#xa;    nl = name.lower()&#xa;    for row in ALL_RESOURCES.itervalues():&#xa;        for resource in row:&#xa;            if resource.name.lower() == nl:&#xa;                return resource&#xa;    return None&#xa;&#xa;def get_flattened_resource_list():&#xa;    r = []&#xa;    for key in RESOURCE_CATEGORIES.keys():&#xa;        r.extend(ALL_RESOURCES[key])&#xa;    return r&#xa;&#xa;def list_command(opts):&#xa;    '''Lists all the known resources to stdout.'''&#xa;    out = sys.stdout&#xa;    for resource in get_flattened_resource_list():&#xa;        out.write(resource.listing(opts))&#xa;        out.write('\n')&#xa;&#xa;&#xa;def get_cfg_interface(cfg_path):&#xa;    global _CFG_PARSER&#xa;    if _CFG_PARSER is None:&#xa;        from ConfigParser import RawConfigParser&#xa;        c = RawConfigParser()&#xa;        for resource in get_flattened_resource_list():&#xa;            c.add_section(resource.name.lower())&#xa;        if cfg_path and os.path.exists(cfg_path):&#xa;            co = open(cfg_path)&#xa;            c.readfp(co)&#xa;            co.close()&#xa;        _CFG_PARSER = c&#xa;    return _CFG_PARSER&#xa;        &#xa;&#xa;def get_resource_status_code(res_id, cfg_path, opts):&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if resource is None:&#xa;        raise ValueError('Resource ""%s"" not recognized' % res_id)&#xa;    resource_path = None&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    try:&#xa;        p = cfg_interface.get(res_id.lower(), 'path')&#xa;    except:&#xa;        p = None&#xa;    resource.status = RESOURCE_STATUS_CODE.MISSING&#xa;    if p and os.path.exists(p):&#xa;        resource.path = p&#xa;        resource.status = RESOURCE_STATUS_CODE.DOWNLOADED&#xa;    try:&#xa;        p = cfg_interface.get(res_id.lower(), 'installed_path')&#xa;    except:&#xa;        p = None&#xa;    if p and os.path.exists(p):&#xa;        resource.installed_path = p&#xa;        resource.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;    return resource.status&#xa;        &#xa;    &#xa;    &#xa;def status_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('""%s"" is INSTALLED at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        _LOG.info('""%s"" is DOWNLOADED at ""%s""' % (res_id, resource.path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.MISSING:&#xa;        _LOG.info('""%s"" is MISSING' % (res_id))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.SKIPPED:&#xa;        _LOG.info('""%s"" is SKIPPED' % (res_id))&#xa;        return&#xa;    assert False&#xa;    &#xa;    &#xa;def get_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('Resource ""%s"" already installed at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        _LOG.info('Resource ""%s"" already obtained at ""%s""' % (res_id, resource.path))&#xa;        return&#xa;    p = resource.do_download()&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    cfg_interface.set(res_id.lower(), 'path', resource.path)&#xa;    cfg_path_par = os.path.dirname(cfg_path)&#xa;    _my_makedirs(cfg_path_par)&#xa;    with open(cfg_path, 'wb') as cfg_fileobj:&#xa;        cfg_interface.write(cfg_fileobj)&#xa;&#xa;def install_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('Resource ""%s"" already installed at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s != RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        get_command(res_id, cfg_path, opts)&#xa;        &#xa;    p = resource.do_install(cfg_path, opts)&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    cfg_interface.set(res_id.lower(), 'installed_path', resource.installed_path)&#xa;    cfg_path_par = os.path.dirname(cfg_path)&#xa;    _my_makedirs(cfg_path_par)&#xa;    with open(cfg_path, 'wb') as cfg_fileobj:&#xa;        cfg_interface.write(cfg_fileobj)&#xa;    &#xa;_LOGGING_LEVEL_ENVAR=""OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_LEVEL""&#xa;_LOGGING_FORMAT_ENVAR=""OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_FORMAT""&#xa;&#xa;def get_logging_level():&#xa;    if _LOGGING_LEVEL_ENVAR in os.environ:&#xa;        if os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""NOTSET"":&#xa;            level = logging.NOTSET&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""DEBUG"":&#xa;            level = logging.DEBUG&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""INFO"":&#xa;            level = logging.INFO&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""WARNING"":&#xa;            level = logging.WARNING&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""ERROR"":&#xa;            level = logging.ERROR&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""CRITICAL"":&#xa;            level = logging.CRITICAL&#xa;        else:&#xa;            level = logging.NOTSET&#xa;    else:&#xa;        level = logging.INFO&#xa;    return level&#xa;&#xa;def get_logger(name=""download_dev_resource""):&#xa;    """"""&#xa;    Returns a logger with name set as given, and configured&#xa;    to the level given by the environment variable _LOGGING_LEVEL_ENVAR.&#xa;    """"""&#xa;&#xa;#     package_dir = os.path.dirname(module_path)&#xa;#     config_filepath = os.path.join(package_dir, _LOGGING_CONFIG_FILE)&#xa;#     if os.path.exists(config_filepath):&#xa;#         try:&#xa;#             logging.config.fileConfig(config_filepath)&#xa;#             logger_set = True&#xa;#         except:&#xa;#             logger_set = False&#xa;    logger = logging.getLogger(name)&#xa;    if not hasattr(logger, 'is_configured'):&#xa;        logger.is_configured = False&#xa;    if not logger.is_configured:&#xa;        level = get_logging_level()&#xa;        rich_formatter = logging.Formatter(""[%(asctime)s] %(filename)s (%(lineno)d): %(levelname) 8s: %(message)s"")&#xa;        simple_formatter = logging.Formatter(""%(levelname) 8s: %(message)s"")&#xa;        raw_formatter = logging.Formatter(""%(message)s"")&#xa;        default_formatter = None&#xa;        logging_formatter = default_formatter&#xa;        if _LOGGING_FORMAT_ENVAR in os.environ:&#xa;            if os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""RICH"":&#xa;                logging_formatter = rich_formatter&#xa;            elif os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""SIMPLE"":&#xa;                logging_formatter = simple_formatter&#xa;            elif os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""NONE"":&#xa;                logging_formatter = None&#xa;            else:&#xa;                logging_formatter = default_formatter&#xa;        else:&#xa;            logging_formatter = default_formatter&#xa;        if logging_formatter is not None:&#xa;            logging_formatter.datefmt='%H:%M:%S'&#xa;        logger.setLevel(level)&#xa;        ch = logging.StreamHandler()&#xa;        ch.setLevel(level)&#xa;        ch.setFormatter(logging_formatter)&#xa;        logger.addHandler(ch)&#xa;        logger.is_configured = True&#xa;    return logger&#xa;_LOG = get_logger()&#xa;        &#xa;            &#xa;&#xa;################################################################################&#xa;# Script by Mark T. Holder available for use under the terms of either the &#xa;# BSD or GPL (v3) license. (see BSDLicense.txt GPL.txt)&#xa;#&#xa;################################################################################&#xa;"
17199950|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# https://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	# Skip non-scalar values.&#xa;	if codePoint >= 0xD800 and codePoint <= 0xDFFF:&#xa;		continue&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
1315595|"# http://mathworld.wolfram.com/DecimalExpansion.html&#xa;# http://stackoverflow.com/questions/1315595/algorithm-for-detecting-repeating-decimals&#xa;# http://stackoverflow.com/questions/12098461/how-can-i-detect-if-a-float-has-a-repeating-decimal-expansion-in-c&#xa;&#xa;# Adapted from http://www.nerdparadise.com/tech/python/repeatingdecimaldivision/&#xa;def find_repeating(d):&#xa;	v = 1 // d&#xa;	n = 10 * (1 - v * d)&#xa;	a = str(v)&#xa;	a += '.'&#xa;	&#xa;	# Maintain a list of all the intermediate ns &#xa;	# and the length of the output at the point where that &#xa;	# n was encountered. If you encounter the same &#xa;	# n again, then the decimal repeats itself from &#xa;	# the last index that n was encountered at. &#xa;	seen_ns = {}&#xa;	&#xa;	while n > 0:&#xa;		&#xa;		existing_n = seen_ns.get(n, None)&#xa;		if existing_n != None:&#xa;			start_repeat_index = existing_n&#xa;			return a[start_repeat_index:]&#xa;		seen_ns[n] = len(a)&#xa;		&#xa;		v = n // d&#xa;		a += str(v)&#xa;		n -= v * d&#xa;		n *= 10&#xa;	&#xa;	return ''&#xa;&#xa;max = [0,'', 0]&#xa;for i in range(2,1001):&#xa;	n = find_repeating(i)&#xa;	if len(n) > max[2]:&#xa;		max = [i,n, len(n)]&#xa;&#xa;print max&#xa;&#xa;#######&#xa;# Wow #&#xa;#######&#xa;&#xa;# I took an approach to this based on the following observations (which I discovered on Wikipedia):&#xa;# &#xa;# Observation 1: Any denominator divisible by 2 or 5 does not produce a repeating decimal.  All others do.&#xa;# Observation 2: There are the following cases for repeating decimals...&#xa;# &#xa;# A) Prime denominators p (other than 2 or 5)&#xa;# &#xa;#    i.  If 10 is a primitive root modulo p, the period is equal to p−1.&#xa;# &#xa;#    ii. If not, the period is equal to a prime factor of p−1. &#xa;# &#xa;# B) Reciprocals of composite integers coprime to 10 &#xa;# &#xa;#    i.  The period of 1p2 is equal to λ(p2) where p is a prime other than 2 or 5 and λ(n) is the Carmichael function.&#xa;# &#xa;#    ii. The period of 1pq equals LCM(λ(p),λ(q)) where p and q are primes other than 2 and 5.&#xa;# &#xa;#    iii. If p, q, r, etc. are primes other than 2 or 5, and k, ℓ, m, etc. are positive integers then 1pkqℓrm… is a repeating decimal with a period of LCM(Tpk,Tqℓ,Trm,…) where Tpk,Tqℓ,Trm, etc. are respectively the periods of the repeating decimals 1pk, 1qℓ, 1rm, etc. as defined above.&#xa;# &#xa;# C) Reciprocals of integers not coprime to 10&#xa;# &#xa;#    i. This is of the form 12a5bpkqℓ… which has an initial transient of max(a,b) digits and then a subsequent repeatend which is the same as 1pkqℓ…&#xa;# &#xa;# Of the cases in Obs. 2, only Ai was relevant, since C reduces to cases A or B, case Biii reduces the cases above it, and the output of λ(n) cannot exceed n−1 nor can the output for all non-prime n between primes pi and pi+1 exceed pi.  Hence, you only have to find the greatest prime p such that 10 is a primitive root modulo p.&#xa;# &#xa;# If you wanted to make the implementation more efficient you could try to find the closest prime to 1000 by counting down instead.&#xa;# &#xa;# import math&#xa;# &#xa;# def is_prime(num):&#xa;#   """"""Return True if `num' is prime and False otherwise.""""""&#xa;#   for n in range(2,int(math.sqrt(num))+1):&#xa;#     if num % n == 0:&#xa;#       return False&#xa;#   return True&#xa;# &#xa;# def primitive_root(b,p):&#xa;#   """"""Return True if b is a primitive root (modulo p), False otherwise.""""""&#xa;#   return set([b**r % p for r in range(1,p)]) == set(range(1,p))&#xa;# &#xa;# print max([p for p in range(2,1001) if is_prime(p) and primitive_root(10,p)])"
14061195|"from __future__ import unicode_literals&#xa;from ftfy import fix_text&#xa;&#xa;import readability&#xa;from goose import Goose&#xa;from pyteaser import SummarizeUrl&#xa;import requests&#xa;&#xa;from learningobjects.utils.pdf import extract_pdf&#xa;from learningobjects.utils import youtube&#xa;import unfurl&#xa;from urlunshort import resolve&#xa;&#xa;###Readability parser&#xa;&#xa;class dotdict(dict):&#xa;    """"""dot.notation access to dictionary attributes""""""&#xa;    def __getattr__(self, attr):&#xa;        return self.get(attr)&#xa;    __setattr__= dict.__setitem__&#xa;    __delattr__= dict.__delitem__&#xa;&#xa;&#xa;class URLObject(object):&#xa;&#xa;    def clean2(self, url):&#xa;        furl=url&#xa;        i=0&#xa;        while resolve(url)!=None and i<5:&#xa;            furl=url&#xa;            url=resolve(url)&#xa;            i+=1&#xa;            print i&#xa;        return furl&#xa;&#xa;    def clean(self, url):&#xa;&#xa;        try:&#xa;            furl = expand_url(url)&#xa;        except:&#xa;            furl = url&#xa;&#xa;        return furl&#xa;        &#xa;    @property&#xa;    def available_fields():&#xa;        return self.descriptor.viewkeys()&#xa;&#xa;    @property&#xa;    def headers(self):&#xa;        r = requests.head(self.url)&#xa;        return r.headers&#xa;&#xa;    @property&#xa;    def content_type(self):&#xa;        return self.headers['content-type']&#xa;&#xa;    def __init__(self, url=None):&#xa;        if url != None:&#xa;            self.url = self.clean(url)&#xa;            self.descriptor = dotdict({'url':self.url})&#xa;&#xa;class ReadibilityParser(URLObject):&#xa;&#xa;    parser = readability.ParserClient('03b5d5676456982e868cf57e5b6757f198ef479d')&#xa;&#xa;    def describe(self, url=None):&#xa;&#xa;        if url == None:&#xa;            url = self.url&#xa;&#xa;        if url == None:    &#xa;            print ""No URL provided""&#xa;            return&#xa;&#xa;        # Readability&#xa;        """"""&#xa;        [u'content',&#xa;         u'domain',&#xa;         u'author',&#xa;         u'url',&#xa;         u'short_url',&#xa;         u'title',&#xa;         u'excerpt',&#xa;         u'direction',&#xa;         u'word_count',&#xa;         u'total_pages',&#xa;         u'next_page_id',&#xa;         u'dek',&#xa;         u'lead_image_url',&#xa;         u'rendered_pages',&#xa;         u'date_published']&#xa;        """"""&#xa;&#xa;        try:&#xa;            response=self.parser.get_article_content(url.encode('utf-8'))&#xa;            a = dotdict(response.content)&#xa;            self.descriptor.update(a)&#xa;        except:&#xa;            pass&#xa;&#xa;        return self.descriptor&#xa;&#xa;class GooseParser(URLObject):&#xa;&#xa;    def describe(self, url=None):&#xa;&#xa;        if url == None:&#xa;            url = self.url&#xa;&#xa;        if url == None:    &#xa;            print ""No URL provided""&#xa;            return&#xa;&#xa;        #Goose&#xa;        """"""&#xa;        goose.article.Article()&#xa;&#xa;         'additional_data',&#xa;         'canonical_link',&#xa;         'cleaned_text',&#xa;         'doc',&#xa;         'domain',&#xa;         'final_url',&#xa;         'link_hash',&#xa;         'meta_description',&#xa;         'meta_favicon',&#xa;         'meta_keywords',&#xa;         'meta_lang',&#xa;         'movies',&#xa;         'publish_date',&#xa;         'raw_doc',&#xa;         'raw_html',&#xa;         'tags',&#xa;         'title',&#xa;         'top_image',&#xa;         'top_node']&#xa;&#xa;        """"""&#xa;&#xa;        g = Goose()&#xa;        try:&#xa;            a = g.extract(url=url.encode('utf-8'))  &#xa;            self.descriptor.url = a.final_url&#xa;            self.descriptor.text = a.cleaned_text&#xa;            self.descriptor.tags = set(a.meta_keywords.split(','))&#xa;            self.descriptor.html = a.raw_html &#xa;            &#xa;        except:&#xa;            pass&#xa;&#xa;        return self.descriptor&#xa;&#xa;&#xa;class SummaryParser(URLObject):&#xa;&#xa;    def describe(self, url=None):        &#xa;&#xa;        if url == None:&#xa;            url = self.url&#xa;&#xa;        if url == None:    &#xa;            print ""No URL provided""&#xa;            return&#xa;&#xa;        summaries=SummarizeUrl(url.encode('utf-8'))&#xa;&#xa;        resumen = u''&#xa;        if not(summaries is None):&#xa;            for s in summaries:&#xa;                print s&#xa;                resumen += fix_text(s.decode('utf-8'))&#xa;&#xa;        self.descriptor.update({'summary':resumen})&#xa;&#xa;        return self.descriptor&#xa;&#xa;class PDFParser(URLObject):&#xa;&#xa;    def describe(self, url=None):        &#xa;&#xa;        if url == None:&#xa;            url = self.url&#xa;&#xa;        if url == None:    &#xa;            print ""No URL provided""&#xa;            return&#xa;&#xa;        texto = extract_pdf(url)&#xa;        &#xa;        self.descriptor.update({'fulltext':texto})&#xa;&#xa;        return self.descriptor&#xa;&#xa;#https://gdata.youtube.com/feeds/api/videos/MLK-_4Vb8Iw/captions&access_token=AIzaSyDQjDxSwnFVVvUmaZPJMhOJd0bbTRVIU48&#xa;#http://video.google.com/timedtext?lang=en&v=MLK-_4Vb8Iw&#xa;#http://www.serpsite.com/transcript.php?videoid=https://www.youtube.com/watch?v=MLK-_4Vb8Iw&#xa;#http://stackoverflow.com/questions/14061195/how-to-get-transcript-in-youtube-api-v3&#xa;#https://gdata.youtube.com/feeds/api/videos/MLK-_4Vb8Iw/captions&access_token=AIzaSyDQjDxSwnFVVvUmaZPJMhOJd0bbTRVIU48&#xa;#http://video.google.com/timedtext?lang=en&v=MLK-_4Vb8Iw&#xa;#http://www.serpsite.com/transcript.php?videoid=https://www.youtube.com/watch?v=MLK-_4Vb8Iw&#xa;#http://stackoverflow.com/questions/14013431/extract-automatic-captions-from-youtube-video&#xa;#http://video.google.com/timedtext?hl=en&v=bgvrYAXjQHI&lang=en&type=text&#xa;#https://www.youtube.com/api/timedtext?v=MLK-_4Vb8Iw&type=track&lang=en&name&kind=asr&fmt=1&#xa;#v=MLK-_4Vb8Iw&#xa;#&hl=es-ES&#xa;#&signature=F08A79C29F199C9EF7E17BDA9DAC1D1000D7EE81.B0F3417880C31FFC7B4F8DDA50FE83CC79D8B9EA&#xa;#&caps=asr&#xa;#&sparams=asr_langs%2Ccaps%2Cv%2Cexpire&key=yttt1&#xa;#&expire=1417039177&#xa;#&asr_langs=en%2Cde%2Cko%2Cja%2Cru%2Cfr%2Cnl%2Cpt%2Cit%2Ces&#xa;#http://www.serpsite.com/transcript.php?videoid=https://www.youtube.com/watch?v=MLK-_4Vb8Iw&#xa;&#xa;class YoutubeParser(URLObject):&#xa;&#xa;    def describe(self, url=None):        &#xa;&#xa;        if url == None:&#xa;            url = self.url&#xa;&#xa;        if url == None:    &#xa;            print ""No URL provided""&#xa;            return&#xa;&#xa;        video = youtube.new(url)&#xa;        video.url = url&#xa;        self.descriptor = video&#xa;&#xa;        return self.descriptor&#xa;&#xa;#response=getAtrib(response)&#xa;#data=[]&#xa;#['tags',a.tags],&#xa;#keys=['domain','author','url_hash','url','meta_keywords','meta_lang','meta_description','short_url','title','excerpt','date_published','publish_date']&#xa;#for key in keys:&#xa;#    if response!=None and key in response and response[key]!=None and len(response[key])>0:&#xa;#        rowData=[key,response[key]]&#xa;#    elif a!=None and key in dir(a) and getattr(a,key)!=None and len(getattr(a,key))>0:&#xa;#        rowData=[key,getattr(a,key)]&#xa;#    else:&#xa;#        rowData=[key,""""]&#xa;#    data.append(rowData)&#xa;#return data&#xa;&#xa;&#xa;def getAtrib(res):&#xa;    keys=['domain','author','url','short_url','title','excerpt','date_published']&#xa;    for key in keys:&#xa;        if key not in res.viewkeys():&#xa;            res[key]=""""&#xa;    return res&#xa;"
263890|# -*- coding: utf-8 -*-&#xa;# vim: sw=4 ts=4 fenc=utf-8&#xa;'''&#xa;getTerminalSize()&#xa; - get width and height of console&#xa; - works on linux,os x,windows,cygwin(windows)&#xa; - taken from http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python&#xa;'''&#xa;&#xa;# Import python libs&#xa;from __future__ import absolute_import, print_function&#xa;import os&#xa;import platform&#xa;import struct&#xa;import ctypes&#xa;import subprocess&#xa;import fcntl&#xa;import termios&#xa;&#xa;__all__ = ['getTerminalSize']&#xa;&#xa;&#xa;def getTerminalSize():&#xa;    current_os = platform.system()&#xa;    tuple_xy = None&#xa;    if current_os == 'Windows':&#xa;        tuple_xy = _getTerminalSize_windows()&#xa;        if tuple_xy is None:&#xa;            tuple_xy = _getTerminalSize_tput()&#xa;            # needed for window's python in cygwin's xterm!&#xa;    if current_os == 'Linux' or current_os == 'Darwin' or \&#xa;            current_os.startswith('CYGWIN'):&#xa;        tuple_xy = _getTerminalSize_linux()&#xa;    if tuple_xy is None:&#xa;        tuple_xy = (80, 25)      # default value&#xa;    return tuple_xy&#xa;&#xa;&#xa;def _getTerminalSize_windows():&#xa;    res = None&#xa;    try:&#xa;        # stdin handle is -10&#xa;        # stdout handle is -11&#xa;        # stderr handle is -12&#xa;&#xa;        h = ctypes.windll.kernel32.GetStdHandle(-12)&#xa;        csbi = ctypes.create_string_buffer(22)&#xa;        res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)&#xa;    except Exception:&#xa;        return None&#xa;    if res:&#xa;        (bufx, bufy, curx, cury, wattr,&#xa;         left, top, right, bottom, maxx, maxy) = struct.unpack(&#xa;             b'hhhhHhhhhhh', csbi.raw)&#xa;        sizex = right - left + 1&#xa;        sizey = bottom - top + 1&#xa;        return sizex, sizey&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;def _getTerminalSize_tput():&#xa;    # get terminal width&#xa;    # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window&#xa;    try:&#xa;        proc = subprocess.Popen(&#xa;            ['tput', 'cols'], stdin=subprocess.PIPE, stdout=subprocess.PIPE&#xa;        )&#xa;        output = proc.communicate(input=None)&#xa;        cols = int(output[0])&#xa;        proc = subprocess.Popen(&#xa;            ['tput', 'lines'], stdin=subprocess.PIPE, stdout=subprocess.PIPE&#xa;        )&#xa;        output = proc.communicate(input=None)&#xa;        rows = int(output[0])&#xa;        return (cols, rows)&#xa;    except Exception:&#xa;        return None&#xa;&#xa;&#xa;def _getTerminalSize_linux():&#xa;    def ioctl_GWINSZ(fd):&#xa;        try:&#xa;            cr = struct.unpack(&#xa;                b'hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234')&#xa;            )&#xa;        except Exception:&#xa;            return None&#xa;        return cr&#xa;    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)&#xa;    if not cr:&#xa;        try:&#xa;            fd = os.open(os.ctermid(), os.O_RDONLY)&#xa;            cr = ioctl_GWINSZ(fd)&#xa;            os.close(fd)&#xa;        except Exception:&#xa;            pass&#xa;    if not cr:&#xa;        try:&#xa;            cr = (os.environ['LINES'], os.environ['COLUMNS'])&#xa;        except Exception:&#xa;            return None&#xa;    return int(cr[1]), int(cr[0])&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    sizex, sizey = getTerminalSize()&#xa;    print('width = {0}  height = {1}'.format(sizex, sizey))&#xa;
16856788|"import sys&#xa;import getopt&#xa;import numpy as np&#xa;import matplotlib.pyplot as plt&#xa;import math&#xa;import pywt&#xa;from scipy.fftpack import dct, idct&#xa;from PIL import Image&#xa;&#xa;'''&#xa;References:&#xa;    https://sukhbinder.wordpress.com/2014/09/11/karhunen-loeve-&#xa;    transform-in-python/&#xa;&#xa;    http://stackoverflow.com/questions/3680262/how-to-slice-a-2d-python-&#xa;    array-fails-with-typeerror-list-indices-must-be-int&#xa;&#xa;    http://stackoverflow.com/questions/16856788/slice-2d-array-into-smaller&#xa;    -2d-arrays&#xa;'''&#xa;&#xa;def KLT(array):&#xa;&#xa;    val, vec = np.linalg.eig(np.cov(array))&#xa;    klt = np.dot(vec, array)&#xa;    return klt, vec, val&#xa;&#xa;def load_image_as_array(imgFile):&#xa;	img = Image.open(imgFile)&#xa;	imgArray = np.asarray(img)&#xa;&#xa;	return imgArray&#xa;&#xa;def get_2d_list_slice(self, matrix, start_row, end_row, start_col, end_col):&#xa;    return [row[start_col:end_col] for row in matrix[start_row:end_row]]&#xa;&#xa;""""""&#xa;Return an array of shape (n, nrows, ncols) where&#xa;n * nrows * ncols = arr.size&#xa;&#xa;If arr is a 2D array, the returned array should look like n subblocks with&#xa;each subblock preserving the ""physical"" layout of arr.&#xa;""""""&#xa;def blockshaped(arr, nrows, ncols):&#xa;    h, w = arr.shape&#xa;    return (arr.reshape(h//nrows, nrows, -1, ncols)&#xa;               .swapaxes(1,2)&#xa;               .reshape(-1, nrows, ncols))&#xa;&#xa;""""""&#xa;Return an array of shape (h, w) where&#xa;h * w = arr.size&#xa;&#xa;If arr is of shape (n, nrows, ncols), n sublocks of shape (nrows, ncols),&#xa;then the returned array preserves the ""physical"" layout of the sublocks.&#xa;""""""&#xa;def unblockshaped(arr, h, w):&#xa;    n, nrows, ncols = arr.shape&#xa;    return (arr.reshape(h//nrows, -1, nrows, ncols)&#xa;               .swapaxes(1,2)&#xa;               .reshape(h, w))&#xa;&#xa;def dct_loop(img_array, loop_times):&#xa;    i = 0&#xa;&#xa;    for i in range(0, loop_times):&#xa;        # slice into 8x8 chunks&#xa;        blockArr = blockshaped(img_array, 8, 8)&#xa;&#xa;        # run DCT on each chunk&#xa;        j = 0&#xa;        for j in range (0, 4096): # for all the 8x8 blocks&#xa;            blockArr[j] = dct(np.array(blockArr[j]), 2, norm='ortho')&#xa;&#xa;        # reshape&#xa;        dct_arr = unblockshaped(blockArr, 512, 512)&#xa;        img_array = dct_arr&#xa;&#xa;    return dct_arr&#xa;&#xa;def idct_loop(img_array, loop_times):&#xa;    i = 0&#xa;&#xa;    for i in range(0, loop_times):&#xa;        # slice into 8x8 chunks&#xa;        blockArr = blockshaped(img_array, 8, 8)&#xa;&#xa;        # run DCT on each chunk&#xa;        j = 0&#xa;        for j in range (0, 4096):&#xa;            blockArr[j] = idct(np.array(blockArr[j]), 2, norm='ortho')&#xa;&#xa;&#xa;        # reshape&#xa;        dct_arr = unblockshaped(blockArr, 512, 512)&#xa;        img_array = dct_arr&#xa;&#xa;    return dct_arr&#xa;&#xa;def calculate_error(img1_arr, img2_arr):&#xa;    arr1 = img1_arr.ravel()&#xa;    arr2 = img2_arr.ravel()&#xa;&#xa;    i = 0&#xa;    error = 0.0&#xa;    for i in range (0, 262144):&#xa;        error += math.exp(int(arr1[i]) - int(arr2[i]))&#xa;        error = error / 262144&#xa;&#xa;    return error&#xa;&#xa;def main(argv):&#xa;    inputFile = """"&#xa;&#xa;    # load file with command line args&#xa;    try:&#xa;        opts, args = getopt.getopt(argv,""i:"")&#xa;    except getopt.GetoptError:&#xa;        print(""USAGE: python3 main.py -i <file>"")&#xa;        sys.exit(2)&#xa;&#xa;    for opt, arg in opts:&#xa;        if opt == ""-i"":&#xa;            inputFile = arg&#xa;        else:&#xa;            print(""USAGE: python3 main.py -i <file>"")&#xa;            sys.exit()&#xa;&#xa;    if (inputFile is """"):&#xa;        print(""USAGE: python3 main.py -i <file>"")&#xa;        sys.exit()&#xa;&#xa;    # load image as array&#xa;    print(""Loading image..."")&#xa;    imgArr = load_image_as_array(inputFile)&#xa;    print(""    Image loaded!"")&#xa;    print()&#xa;&#xa;    # do klt&#xa;    print(""Determining KLT values..."")&#xa;    ktm, vect, val = KLT(imgArr);&#xa;    print(""    Found KLT values!"")&#xa;    print()&#xa;&#xa;    # run it through DCT 8x8 at a time&#xa;    # 4096 8x8 blocks in 512 x 512 array&#xa;    # 64 blocks in row, col&#xa;&#xa;    print(""Printing image array..."")&#xa;    print(imgArr)&#xa;    print(""    Done!"")&#xa;    print()&#xa;&#xa;    print(""Applying one-level DWT..."")&#xa;    coeffs = pywt.dwt2(imgArr, 'haar')&#xa;    cA, (cH, cV, cD) = coeffs&#xa;    print(""    Done!"")&#xa;&#xa;    dwtCompList = [cA, cH, cV, cD]&#xa;    stitchArr = unblockshaped(np.array(dwtCompList), 512, 512)&#xa;&#xa;    print(""Displaying image..."")&#xa;    #pass1 = Image.fromarray(stitchArr, 'L')&#xa;    pass1 = Image.fromarray(stitchArr)&#xa;    pass1.show()&#xa;    #pass1.save('DWT2D_transform.spi', format='SPIDER')&#xa;    print(""    Done!"")&#xa;&#xa;    print(""Applying inverse transform and displaying image..."")&#xa;    finalArr = np.array((pywt.idwt2(coeffs, 'haar')), dtype=np.uint8)&#xa;    pass2 = Image.fromarray(finalArr)&#xa;    pass2.save('iDWT2D_reconstruction.bmp')&#xa;    print(""    Done!"")&#xa;    print()&#xa;&#xa;    '''&#xa;    Calculate the error between the two images&#xa;    '''&#xa;    print(""Calculating error between the original image and final..."")&#xa;    imgError = calculate_error(np.asarray(imgArr), np.asarray(finalArr))&#xa;    print (""Mean squared error: """"{:.12}%"".format(float(imgError * 100)))&#xa;&#xa;if __name__ == ""__main__"":&#xa;	main(sys.argv[1:])&#xa;"
566746|"#!/usr/bin/env python&#xa;&#xa;# ROOT command line tools module: cmdLineUtils&#xa;# Author: Julien Ripoche&#xa;# Mail: julien.ripoche@u-psud.fr&#xa;# Date: 20/08/15&#xa;&#xa;""""""Contain utils for ROOT command line tools""""""&#xa;&#xa;##########&#xa;# Stream redirect functions&#xa;# The original code of the these functions can be found here :&#xa;# http://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262&#xa;# Thanks J.F. Sebastian !!&#xa;&#xa;from contextlib import contextmanager&#xa;import os&#xa;import sys&#xa;&#xa;def fileno(file_or_fd):&#xa;    """"""&#xa;    Look for 'fileno' attribute.&#xa;    """"""&#xa;    fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()&#xa;    if not isinstance(fd, int):&#xa;        raise ValueError(""Expected a file (`.fileno()`) or a file descriptor"")&#xa;    return fd&#xa;&#xa;@contextmanager&#xa;def streamRedirected(source=sys.stdout, destination=os.devnull):&#xa;    """"""&#xa;    Redirect the output from source to destination.&#xa;    """"""&#xa;    stdout_fd = fileno(source)&#xa;    # copy stdout_fd before it is overwritten&#xa;    #NOTE: `copied` is inheritable on Windows when duplicating a standard stream&#xa;    with os.fdopen(os.dup(stdout_fd), 'wb') as copied:&#xa;        source.flush()  # flush library buffers that dup2 knows nothing about&#xa;        try:&#xa;            os.dup2(fileno(destination), stdout_fd)  # $ exec >&destination&#xa;        except ValueError:  # filename&#xa;            with open(destination, 'wb') as destination_file:&#xa;                os.dup2(destination_file.fileno(), stdout_fd)  # $ exec > destination&#xa;        try:&#xa;            yield source # allow code to be run with the redirected stream&#xa;        finally:&#xa;            # restore source to its previous value&#xa;            #NOTE: dup2 makes stdout_fd inheritable unconditionally&#xa;            source.flush()&#xa;            os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied&#xa;&#xa;def stdoutRedirected():&#xa;    """"""&#xa;    Redirect the output from sys.stdout to os.devnull.&#xa;    """"""&#xa;    return streamRedirected(sys.stdout, os.devnull)&#xa;&#xa;def stderrRedirected():&#xa;    """"""&#xa;    Redirect the output from sys.stderr to os.devnull.&#xa;    """"""&#xa;    return streamRedirected(sys.stderr, os.devnull)&#xa;&#xa;# The end of streamRedirected functions&#xa;##########&#xa;&#xa;##########&#xa;# Imports&#xa;&#xa;##&#xa;# redirect output (escape characters during ROOT importation...)&#xa;# The gymnastic with sys argv  is necessary to workaround for ROOT-7577&#xa;argvTmp = sys.argv[:]&#xa;sys.argv = []&#xa;with stdoutRedirected():&#xa;    import ROOT&#xa;ROOT.gROOT.GetVersion()&#xa;sys.argv = argvTmp&#xa;&#xa;import argparse&#xa;import glob&#xa;import fnmatch&#xa;import logging&#xa;&#xa;# The end of imports&#xa;##########&#xa;&#xa;##########&#xa;# Different functions to get a parser of arguments and options&#xa;&#xa;def _getParser(theHelp, theEpilog):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils.&#xa;   """"""&#xa;   return argparse.ArgumentParser(description=theHelp,&#xa;                                  formatter_class=argparse.RawDescriptionHelpFormatter,&#xa;                                  epilog = theEpilog)&#xa;&#xa;def getParserSingleFile(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils and a&#xa;   source file or not.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""FILE"", nargs='?', help=""Input file"")&#xa;   return parser&#xa;&#xa;def getParserFile(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils and a&#xa;   list of source files.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""FILE"", nargs='+', help=""Input file"")&#xa;   return parser&#xa;&#xa;def getParserSourceDest(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils,&#xa;   a list of source files and a destination file.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""SOURCE"", nargs='+', help=""Source file"")&#xa;   parser.add_argument(""DEST"", help=""Destination file"")&#xa;   return parser&#xa;&#xa;# The end of get parser functions&#xa;##########&#xa;&#xa;##########&#xa;# Several utils&#xa;&#xa;@contextmanager&#xa;def _setIgnoreLevel(level):&#xa;    originalLevel = ROOT.gErrorIgnoreLevel&#xa;    ROOT.gErrorIgnoreLevel = level&#xa;    yield&#xa;    ROOT.gErrorIgnoreLevel = originalLevel&#xa;&#xa;def changeDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Change the current directory (ROOT.gDirectory) by the corresponding (rootFile,pathSplit)&#xa;    """"""&#xa;    rootFile.cd()&#xa;    for directoryName in pathSplit:&#xa;        theDir = ROOT.gDirectory.Get(directoryName)&#xa;        if not theDir:&#xa;            logging.warning(""Directory %s does not exist."" %directoryName)&#xa;            return 1&#xa;        else:&#xa;            theDir.cd()&#xa;    return 0&#xa;&#xa;def createDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Add a directory named 'pathSplit[-1]' in (rootFile,pathSplit[:-1])&#xa;    """"""&#xa;    retcode = changeDirectory(rootFile,pathSplit[:-1])&#xa;    if retcode == 0: ROOT.gDirectory.mkdir(pathSplit[-1])&#xa;    return retcode&#xa;&#xa;def getFromDirectory(objName):&#xa;    """"""&#xa;    Get the object objName from the current directory&#xa;    """"""&#xa;    return ROOT.gDirectory.Get(objName)&#xa;&#xa;def isExisting(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), exits&#xa;    """"""&#xa;    changeDirectory(rootFile,pathSplit[:-1])&#xa;    return ROOT.gDirectory.GetListOfKeys().Contains(pathSplit[-1])&#xa;&#xa;def isDirectoryKey(key):&#xa;    """"""&#xa;    Return True if the object, corresponding to the key, inherits from TDirectory&#xa;    """"""&#xa;    classname = key.GetClassName()&#xa;    cl = ROOT.gROOT.GetClass(classname)&#xa;    return cl.InheritsFrom(ROOT.TDirectory.Class())&#xa;&#xa;def isTreeKey(key):&#xa;    """"""&#xa;    Return True if the object, corresponding to the key, inherits from TTree&#xa;    """"""&#xa;    classname = key.GetClassName()&#xa;    cl = ROOT.gROOT.GetClass(classname)&#xa;    return cl.InheritsFrom(ROOT.TTree.Class())&#xa;&#xa;def getKey(rootFile,pathSplit):&#xa;    """"""&#xa;    Get the key of the corresponding object (rootFile,pathSplit)&#xa;    """"""&#xa;    changeDirectory(rootFile,pathSplit[:-1])&#xa;    return ROOT.gDirectory.GetKey(pathSplit[-1])&#xa;&#xa;def isDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), inherits from TDirectory&#xa;    """"""&#xa;    if pathSplit == []: return True # the object is the rootFile itself&#xa;    else: return isDirectoryKey(getKey(rootFile,pathSplit))&#xa;&#xa;def isTree(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), inherits from TTree&#xa;    """"""&#xa;    if pathSplit == []: return False # the object is the rootFile itself&#xa;    else: return isTreeKey(getKey(rootFile,pathSplit))&#xa;&#xa;def getKeyList(rootFile,pathSplit):&#xa;    """"""&#xa;    Get the list of keys of the directory (rootFile,pathSplit),&#xa;    if (rootFile,pathSplit) is not a directory then get the key in a list&#xa;    """"""&#xa;    if isDirectory(rootFile,pathSplit):&#xa;        changeDirectory(rootFile,pathSplit)&#xa;        return ROOT.gDirectory.GetListOfKeys()&#xa;    else: return [getKey(rootFile,pathSplit)]&#xa;&#xa;def keyListSort(keyList):&#xa;    """"""&#xa;    Sort list of keys by their names ignoring the case&#xa;    """"""&#xa;    keyList.sort(key=lambda x: x.GetName().lower())&#xa;&#xa;def tupleListSort(tupleList):&#xa;    """"""&#xa;    Sort list of tuples by their first elements ignoring the case&#xa;    """"""&#xa;    tupleList.sort(key=lambda x: x[0].lower())&#xa;&#xa;def dirListSort(dirList):&#xa;    """"""&#xa;    Sort list of directories by their names ignoring the case&#xa;    """"""&#xa;    dirList.sort(key=lambda x: [n.lower() for n in x])&#xa;&#xa;def keyClassSpliter(rootFile,pathSplitList):&#xa;    """"""&#xa;    Return a list of directories and a list of keys corresponding&#xa;    to the other objects, for rootLs and rooprint use&#xa;    """"""&#xa;    keyList = []&#xa;    dirList = []&#xa;    for pathSplit in pathSplitList:&#xa;        if pathSplit == []: dirList.append(pathSplit)&#xa;        elif isDirectory(rootFile,pathSplit): dirList.append(pathSplit)&#xa;        else: keyList.append(getKey(rootFile,pathSplit))&#xa;    keyListSort(keyList)&#xa;    dirListSort(dirList)&#xa;    return keyList,dirList&#xa;&#xa;def openROOTFile(fileName, mode=""read""):&#xa;    """"""&#xa;    Open the ROOT file corresponding to fileName in the corresponding mode,&#xa;    redirecting the output not to see missing dictionnaries&#xa;    """"""&#xa;    #with stderrRedirected():&#xa;    with _setIgnoreLevel(ROOT.kError):&#xa;        theFile = ROOT.TFile.Open(fileName, mode)&#xa;    if not theFile:&#xa;        logging.warning(""File %s does not exist"", fileName)&#xa;    return theFile&#xa;&#xa;def openROOTFileCompress(fileName, compress, recreate):&#xa;    """"""&#xa;    Open a ROOT file (like openROOTFile) with the possibility&#xa;    to change compression settings&#xa;    """"""&#xa;    if compress != None and os.path.isfile(fileName):&#xa;        logging.warning(""can't change compression settings on existing file"")&#xa;        return None&#xa;    mode = ""recreate"" if recreate else ""update""&#xa;    theFile = openROOTFile(fileName, mode)&#xa;    if compress != None: theFile.SetCompressionSettings(compress)&#xa;    return theFile&#xa;&#xa;def joinPathSplit(pathSplit):&#xa;    """"""&#xa;    Join the pathSplit with '/'&#xa;    """"""&#xa;    return ""/"".join(pathSplit)&#xa;&#xa;MANY_OCCURENCE_WARNING = ""Same name objects aren't supported: '{0}' of '{1}' won't be processed""&#xa;&#xa;def manyOccurenceRemove(pathSplitList,fileName):&#xa;    """"""&#xa;    Search for double occurence of the same pathSplit and remove them&#xa;    """"""&#xa;    if len(pathSplitList) > 1:&#xa;        for n in pathSplitList:&#xa;            if pathSplitList.count(n) != 1:&#xa;                logging.warning(MANY_OCCURENCE_WARNING.format(joinPathSplit(n),fileName))&#xa;                while n in pathSplitList: pathSplitList.remove(n)&#xa;&#xa;def patternToPathSplitList(fileName,pattern):&#xa;    """"""&#xa;    Get the list of pathSplit of objects in the ROOT file&#xa;    corresponding to fileName that match with the pattern&#xa;    """"""&#xa;    # Open ROOT file&#xa;    rootFile = openROOTFile(fileName)&#xa;    if not rootFile: return []&#xa;&#xa;    # Split pattern avoiding multiple slash problem&#xa;    patternSplit = [n for n in pattern.split(""/"") if n != """"]&#xa;&#xa;    # Main loop&#xa;    pathSplitList = [[]]&#xa;    for patternPiece in patternSplit:&#xa;        newPathSplitList = []&#xa;        for pathSplit in pathSplitList:&#xa;            if isDirectory(rootFile,pathSplit):&#xa;                changeDirectory(rootFile,pathSplit)&#xa;                newPathSplitList.extend( \&#xa;                    [pathSplit + [key.GetName()] \&#xa;                    for key in ROOT.gDirectory.GetListOfKeys() \&#xa;                    if fnmatch.fnmatch(key.GetName(),patternPiece)])&#xa;        pathSplitList = newPathSplitList&#xa;&#xa;    # No match&#xa;    if pathSplitList == []:&#xa;        logging.warning(""can't find {0} in {1}"".format(pattern,fileName))&#xa;&#xa;    # Same match (remove double occurences from the list)&#xa;    manyOccurenceRemove(pathSplitList,fileName)&#xa;&#xa;    return pathSplitList&#xa;&#xa;def fileNameListMatch(filePattern,wildcards):&#xa;    """"""&#xa;    Get the list of fileName that match with objPattern&#xa;    """"""&#xa;    if wildcards: return [os.path.expandvars(os.path.expanduser(i)) for i in glob.iglob(filePattern)]&#xa;    else: return [os.path.expandvars(os.path.expanduser(filePattern))]&#xa;&#xa;def pathSplitListMatch(fileName,objPattern,wildcards):&#xa;    """"""&#xa;    Get the list of pathSplit that match with objPattern&#xa;    """"""&#xa;    if wildcards: return patternToPathSplitList(fileName,objPattern)&#xa;    else: return [[n for n in objPattern.split(""/"") if n != """"]]&#xa;&#xa;def patternToFileNameAndPathSplitList(pattern,wildcards = True):&#xa;    """"""&#xa;    Get the list of tuple containing both :&#xa;    - ROOT file name&#xa;    - list of splited path (in the corresponding file) of objects that matche&#xa;    Use unix wildcards by default&#xa;    """"""&#xa;    rootFilePattern = ""*.root""&#xa;    rootObjPattern = rootFilePattern+"":*""&#xa;    httpRootFilePattern = ""htt*://*.root""&#xa;    httpRootObjPattern = httpRootFilePattern+"":*""&#xa;    xrootdRootFilePattern = ""root://*.root""&#xa;    xrootdRootObjPattern = xrootdRootFilePattern+"":*""&#xa;    s3RootFilePattern = ""s3://*.root""&#xa;    s3RootObjPattern = s3RootFilePattern+"":*""&#xa;    gsRootFilePattern = ""gs://*.root""&#xa;    gsRootObjPattern = gsRootFilePattern+"":*""&#xa;    rfioRootFilePattern = ""rfio://*.root""&#xa;    rfioRootObjPattern = rfioRootFilePattern+"":*""&#xa;    pcmFilePattern = ""*.pcm""&#xa;    pcmObjPattern = pcmFilePattern+"":*""&#xa;&#xa;    if fnmatch.fnmatch(pattern,httpRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,xrootdRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,s3RootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,gsRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,rfioRootObjPattern):&#xa;        patternSplit = pattern.rsplit("":"", 1)&#xa;        fileName = patternSplit[0]&#xa;        objPattern = patternSplit[1]&#xa;        pathSplitList = pathSplitListMatch(fileName,objPattern,wildcards)&#xa;        return [(fileName,pathSplitList)]&#xa;&#xa;    if fnmatch.fnmatch(pattern,httpRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,xrootdRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,s3RootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,gsRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,rfioRootFilePattern):&#xa;        fileName = pattern&#xa;        pathSplitList = [[]]&#xa;        return [(fileName,pathSplitList)]&#xa;&#xa;    if fnmatch.fnmatch(pattern,rootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,pcmObjPattern):&#xa;        patternSplit = pattern.split("":"")&#xa;        filePattern = patternSplit[0]&#xa;        objPattern = patternSplit[1]&#xa;        fileNameList = fileNameListMatch(filePattern,wildcards)&#xa;        return [(fileName,pathSplitListMatch(fileName,objPattern,wildcards)) for fileName in fileNameList]&#xa;&#xa;    if fnmatch.fnmatch(pattern,rootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,pcmFilePattern):&#xa;        filePattern = pattern&#xa;        fileNameList = fileNameListMatch(filePattern,wildcards)&#xa;        pathSplitList = [[]]&#xa;        return [(fileName,pathSplitList) for fileName in fileNameList]&#xa;&#xa;    logging.warning(""{0}: No such file (or extension not supported)"".format(pattern))&#xa;    return []&#xa;&#xa;# End of utils&#xa;##########&#xa;&#xa;##########&#xa;# Set of functions to put the arguments in shape&#xa;&#xa;def getArgs(parser):&#xa;   """"""&#xa;   Get arguments corresponding to parser.&#xa;   """"""&#xa;   return parser.parse_args()&#xa;&#xa;def getSourceListArgs(parser, wildcards = True):&#xa;   """"""&#xa;   Create a list of tuples that contain source ROOT file names&#xa;   and lists of path in these files as well as the original arguments&#xa;   """"""&#xa;   args = getArgs(parser)&#xa;   inputFiles = []&#xa;   try:&#xa;      inputFiles = args.FILE&#xa;   except:&#xa;      inputFiles = args.SOURCE&#xa;   sourceList = \&#xa;      [tup for pattern in inputFiles \&#xa;      for tup in patternToFileNameAndPathSplitList(pattern,wildcards)]&#xa;   return sourceList, args&#xa;&#xa;def getSourceListOptDict(parser, wildcards = True):&#xa;    """"""&#xa;    Get the list of tuples and the dictionary with options&#xa;    """"""&#xa;    sourceList, args = getSourceListArgs(parser, wildcards)&#xa;    return sourceList, vars(args)&#xa;&#xa;def getSourceDestListOptDict(parser, wildcards = True):&#xa;    """"""&#xa;    Get the list of tuples of sources, create destination name, destination pathSplit&#xa;    and the dictionary with options&#xa;    """"""&#xa;    sourceList, args = getSourceListArgs(parser, wildcards)&#xa;    destList = \&#xa;        patternToFileNameAndPathSplitList( \&#xa;        args.DEST,wildcards=False)&#xa;    if destList != []:&#xa;        destFileName,destPathSplitList = destList[0]&#xa;        destPathSplit = destPathSplitList[0]&#xa;    else:&#xa;        destFileName = """"&#xa;        destPathSplit = []&#xa;    return sourceList, destFileName, destPathSplit, vars(args)&#xa;&#xa;# The end of the set of functions to put the arguments in shape&#xa;##########&#xa;&#xa;##########&#xa;# Several functions shared by roocp, roomv and roorm&#xa;&#xa;TARGET_ERROR = ""target '{0}' is not a directory""&#xa;OMITTING_FILE_ERROR = ""omitting file '{0}'""&#xa;OMITTING_DIRECTORY_ERROR = ""omitting directory '{0}'""&#xa;OVERWRITE_ERROR = ""cannot overwrite non-directory '{0}' with directory '{1}'""&#xa;&#xa;def copyRootObject(sourceFile,sourcePathSplit,destFile,destPathSplit,oneSource,recursive,replace):&#xa;    """"""&#xa;    Initialize the recursive function 'copyRootObjectRecursive', written to be as unix-like as possible&#xa;    """"""&#xa;    retcode = 0&#xa;    isMultipleInput = not (oneSource and sourcePathSplit != [])&#xa;    recursiveOption = recursive&#xa;    # Multiple input and unexisting or non-directory destination&#xa;    # TARGET_ERROR&#xa;    if isMultipleInput and destPathSplit != [] \&#xa;        and not (isExisting(destFile,destPathSplit) \&#xa;        and isDirectory(destFile,destPathSplit)):&#xa;        logging.warning(TARGET_ERROR.format(destPathSplit[-1]))&#xa;        retcode += 1&#xa;    # Entire ROOT file or directory in input omitting ""-r"" option&#xa;    # OMITTING_FILE_ERROR or OMITTING_DIRECTORY_ERROR&#xa;    if not recursiveOption:&#xa;        if sourcePathSplit == []:&#xa;            logging.warning(OMITTING_FILE_ERROR.format( \&#xa;                sourceFile.GetName()))&#xa;            retcode += 1&#xa;        elif isDirectory(sourceFile,sourcePathSplit):&#xa;            logging.warning(OMITTING_DIRECTORY_ERROR.format( \&#xa;                sourcePathSplit[-1]))&#xa;            retcode += 1&#xa;    # Run copyRootObjectRecursive function with the wish&#xa;    # to follow the unix copy behaviour&#xa;    if sourcePathSplit == []:&#xa;        retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;            destFile,destPathSplit,replace)&#xa;    else:&#xa;        setName = """"&#xa;        if not isMultipleInput and (destPathSplit != [] \&#xa;            and not isExisting(destFile,destPathSplit)):&#xa;            setName = destPathSplit[-1]&#xa;        objectName = sourcePathSplit[-1]&#xa;        if isDirectory(sourceFile,sourcePathSplit):&#xa;            if setName != """":&#xa;                createDirectory(destFile,destPathSplit[:-1]+[setName])&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1]+[setName],replace)&#xa;            elif isDirectory(destFile,destPathSplit):&#xa;                if not isExisting(destFile,destPathSplit+[objectName]):&#xa;                    createDirectory(destFile,destPathSplit+[objectName])&#xa;                if isDirectory(destFile,destPathSplit+[objectName]):&#xa;                    retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                        destFile,destPathSplit+[objectName],replace)&#xa;                else:&#xa;                    logging.warning(OVERWRITE_ERROR.format( \&#xa;                        objectName,objectName))&#xa;                    retcode += 1&#xa;            else:&#xa;                logging.warning(OVERWRITE_ERROR.format( \&#xa;                    destPathSplit[-1],objectName))&#xa;                retcode += 1&#xa;        else:&#xa;            if setName != """":&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1],replace,setName)&#xa;            elif isDirectory(destFile,destPathSplit):&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit,replace)&#xa;            else:&#xa;                setName = destPathSplit[-1]&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1],replace,setName)&#xa;    return retcode&#xa;&#xa;DELETE_ERROR = ""object {0} was not existing, so it is not deleted""&#xa;&#xa;def deleteObject(rootFile,pathSplit):&#xa;    """"""&#xa;    Delete the object 'pathSplit[-1]' from (rootFile,pathSplit[:-1])&#xa;    """"""&#xa;    retcode = changeDirectory(rootFile,pathSplit[:-1])&#xa;    if retcode == 0:&#xa;        fileName = pathSplit[-1]&#xa;        if isExisting(rootFile,pathSplit):&#xa;            ROOT.gDirectory.Delete(fileName+"";*"")&#xa;        else:&#xa;            logging.warning(DELETE_ERROR.format(fileName))&#xa;            retcode += 1&#xa;    return retcode&#xa;&#xa;def copyRootObjectRecursive(sourceFile,sourcePathSplit,destFile,destPathSplit,replace,setName=""""):&#xa;    """"""&#xa;    Copy objects from a file or directory (sourceFile,sourcePathSplit)&#xa;    to an other file or directory (destFile,destPathSplit)&#xa;    - Has the will to be unix-like&#xa;    - that's a recursive function&#xa;    - Python adaptation of a root input/output tutorial :&#xa;      $ROOTSYS/tutorials/io/copyFiles.C&#xa;    """"""&#xa;    retcode = 0&#xa;    replaceOption = replace&#xa;    for key in getKeyList(sourceFile,sourcePathSplit):&#xa;        objectName = key.GetName()&#xa;        if isDirectoryKey(key):&#xa;            if not isExisting(destFile,destPathSplit+[objectName]):&#xa;                createDirectory(destFile,destPathSplit+[objectName])&#xa;            if isDirectory(destFile,destPathSplit+[objectName]):&#xa;                retcode +=copyRootObjectRecursive(sourceFile, \&#xa;                    sourcePathSplit+[objectName], \&#xa;                    destFile,destPathSplit+[objectName],replace)&#xa;            else:&#xa;                logging.warning(OVERWRITE_ERROR.format( \&#xa;                    objectName,objectName))&#xa;                retcode += 1&#xa;        elif isTreeKey(key):&#xa;            T = key.GetMotherDir().Get(objectName+"";""+str(key.GetCycle()))&#xa;            if replaceOption and isExisting(destFile,destPathSplit+[T.GetName()]):&#xa;                retcodeTemp = deleteObject(destFile,destPathSplit+[T.GetName()])&#xa;                if retcodeTemp:&#xa;                    retcode += retcodeTemp&#xa;                    continue&#xa;            changeDirectory(destFile,destPathSplit)&#xa;            newT = T.CloneTree(-1,""fast"")&#xa;            if setName != """":&#xa;                newT.SetName(setName)&#xa;            newT.Write()&#xa;        else:&#xa;            obj = key.ReadObj()&#xa;            if replaceOption and isExisting(destFile,destPathSplit+[setName]):&#xa;                changeDirectory(destFile,destPathSplit)&#xa;                otherObj = getFromDirectory(setName)&#xa;                if not otherObj == obj:&#xa;                    retcodeTemp = deleteObject(destFile,destPathSplit+[setName])&#xa;                    if retcodeTemp:&#xa;                        retcode += retcodeTemp&#xa;                        continue&#xa;                    else:&#xa;                        obj.SetName(setName)&#xa;                        changeDirectory(destFile,destPathSplit)&#xa;                        obj.Write()&#xa;                else:&#xa;                    obj.SetName(setName)&#xa;                    changeDirectory(destFile,destPathSplit)&#xa;                    obj.Write()&#xa;            else:&#xa;                if setName != """":&#xa;                    obj.SetName(setName)&#xa;                changeDirectory(destFile,destPathSplit)&#xa;                obj.Write()&#xa;            obj.Delete()&#xa;    changeDirectory(destFile,destPathSplit)&#xa;    ROOT.gDirectory.SaveSelf(ROOT.kTRUE)&#xa;    return retcode&#xa;&#xa;FILE_REMOVE_ERROR = ""cannot remove '{0}': Is a ROOT file""&#xa;DIRECTORY_REMOVE_ERROR = ""cannot remove '{0}': Is a directory""&#xa;ASK_FILE_REMOVE = ""remove '{0}' ? (y/n) : ""&#xa;ASK_OBJECT_REMOVE = ""remove '{0}' from '{1}' ? (y/n) : ""&#xa;&#xa;def deleteRootObject(rootFile, pathSplit, interactive, recursive):&#xa;    """"""&#xa;    Remove the object (rootFile,pathSplit)&#xa;    -interactive : prompt before every removal&#xa;    -recursive : allow directory, and ROOT file, removal&#xa;    """"""&#xa;    retcode = 0&#xa;    if not recursive and isDirectory(rootFile,pathSplit):&#xa;        if pathSplit == []:&#xa;            logging.warning(FILE_REMOVE_ERROR.format(rootFile.GetName()))&#xa;            retcode += 1&#xa;        else:&#xa;            logging.warning(DIRECTORY_REMOVE_ERROR.format(pathSplit[-1]))&#xa;            retcode += 1&#xa;    else:&#xa;        if interactive:&#xa;            if pathSplit != []:&#xa;                answer = raw_input(ASK_OBJECT_REMOVE \&#xa;                    .format(""/"".join(pathSplit),rootFile.GetName()))&#xa;            else:&#xa;                answer = raw_input(ASK_FILE_REMOVE \&#xa;                    .format(rootFile.GetName()))&#xa;            remove = answer.lower() == 'y'&#xa;        else:&#xa;            remove = True&#xa;        if remove:&#xa;            if pathSplit != []:&#xa;                retcode += deleteObject(rootFile,pathSplit)&#xa;            else:&#xa;                rootFile.Close()&#xa;                os.remove(rootFile.GetName())&#xa;    return retcode&#xa;&#xa;# End of functions shared by roocp, roomv and roorm&#xa;##########&#xa;&#xa;##########&#xa;# Help strings for ROOT command line tools&#xa;&#xa;# Arguments&#xa;SOURCE_HELP = ""path of the source.""&#xa;SOURCES_HELP = ""path of the source(s).""&#xa;DEST_HELP = ""path of the destination.""&#xa;&#xa;# Options&#xa;COMPRESS_HELP = \&#xa;""""""change the compression settings of the&#xa;destination file (if not already existing).""""""&#xa;INTERACTIVE_HELP = ""prompt before every removal.""&#xa;RECREATE_HELP = ""recreate the destination file.""&#xa;RECURSIVE_HELP = ""recurse inside directories""&#xa;REPLACE_HELP = ""replace object if already existing""&#xa;&#xa;# End of help strings&#xa;##########&#xa;&#xa;##########&#xa;# ROOTBROWSE&#xa;&#xa;def _openBrowser(rootFile=None):&#xa;    browser = ROOT.TBrowser()&#xa;    if rootFile: rootFile.Browse(browser)&#xa;    ROOT.PyROOT.TPyROOTApplication.Run(ROOT.gApplication)&#xa;&#xa;def rootBrowse(fileName=None):&#xa;    if fileName:&#xa;        rootFile = openROOTFile(fileName)&#xa;        if not rootFile: return 1&#xa;        _openBrowser(rootFile)&#xa;        rootFile.Close()&#xa;    else:&#xa;        _openBrowser()&#xa;    return 0&#xa;&#xa;# End of ROOTBROWSE&#xa;##########&#xa;&#xa;##########&#xa;# ROOTCP&#xa;&#xa;def _copyObjects(fileName, pathSplitList, destFile, destPathSplit, oneFile, \&#xa;                 recursive, replace):&#xa;    retcode = 0&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName) \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(rootFile) # Fast copy necessity&#xa;    for pathSplit in pathSplitList:&#xa;        oneSource = oneFile and len(pathSplitList)==1&#xa;        retcode += copyRootObject(rootFile, pathSplit, destFile, destPathSplit, \&#xa;                                  oneSource, recursive, replace)&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootCp(sourceList, destFileName, destPathSplit, \&#xa;           compress=None, recreate=False, recursive=False, replace=False):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in [n[0] for n in sourceList]:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName, compress, recreate)&#xa;    if not destFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(destFile) # Fast copy necessity&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _copyObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                len(sourceList)==1, recursive, replace)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTCP&#xa;##########&#xa;&#xa;##########&#xa;# ROOTEVENTSELECTOR&#xa;&#xa;def _copyTreeSubset(sourceFile,sourcePathSplit,destFile,destPathSplit,firstEvent,lastEvent):&#xa;    """"""Copy a subset of the tree from (sourceFile,sourcePathSplit)&#xa;    to (destFile,destPathSplit) according to options in optDict""""""&#xa;    retcode = changeDirectory(sourceFile,sourcePathSplit[:-1])&#xa;    if retcode != 0: return retcode&#xa;    bigTree = getFromDirectory(sourcePathSplit[-1])&#xa;    nbrEntries = bigTree.GetEntries()&#xa;    # changeDirectory for the small tree not to be memory-resident&#xa;    retcode = changeDirectory(destFile,destPathSplit)&#xa;    if retcode != 0: return retcode&#xa;    smallTree = bigTree.CloneTree(0)&#xa;    if lastEvent == -1:&#xa;        lastEvent = nbrEntries-1&#xa;    isNtuple = bigTree.InheritsFrom(ROOT.TNtuple.Class())&#xa;    for i in xrange(firstEvent, lastEvent+1):&#xa;        bigTree.GetEntry(i)&#xa;        if isNtuple:&#xa;            super(ROOT.TNtuple,smallTree).Fill()&#xa;        else:&#xa;            smallTree.Fill()&#xa;    smallTree.Write()&#xa;    return retcode&#xa;&#xa;def _copyTreeSubsets(fileName, pathSplitList, destFile, destPathSplit, first, last):&#xa;    retcode = 0&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName) \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        if isTree(rootFile,pathSplit):&#xa;            retcode += _copyTreeSubset(rootFile,pathSplit, \&#xa;            destFile,destPathSplit,first,last)&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootEventselector(sourceList, destFileName, destPathSplit, \&#xa;                      compress=None, recreate=False, first=0, last=-1):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in sourceList:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName, compress, recreate)&#xa;    if not destFile: return 1&#xa;&#xa;    # Loop on the root file&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _copyTreeSubsets(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                    first, last)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTEVENTSELECTOR&#xa;##########&#xa;&#xa;##########&#xa;# ROOTLS&#xa;&#xa;# Ansi characters&#xa;ANSI_BOLD = ""\x1B[1m""&#xa;ANSI_BLUE = ""\x1B[34m""&#xa;ANSI_GREEN = ""\x1B[32m""&#xa;ANSI_END = ""\x1B[0m""&#xa;&#xa;# Needed for column width calculation&#xa;ANSI_BOLD_LENGTH = len(ANSI_BOLD+ANSI_END)&#xa;ANSI_BLUE_LENGTH = len(ANSI_BLUE+ANSI_END)&#xa;ANSI_GREEN_LENGTH = len(ANSI_GREEN+ANSI_END)&#xa;&#xa;# Terminal and platform booleans&#xa;IS_TERMINAL = sys.stdout.isatty()&#xa;IS_WIN32 = sys.platform == 'win32'&#xa;&#xa;def isSpecial(ansiCode,string):&#xa;    """"""Use ansi code on 'string' if the output is the&#xa;    terminal of a not Windows platform""""""&#xa;    if IS_TERMINAL and not IS_WIN32: return ansiCode+string+ANSI_END&#xa;    else: return string&#xa;&#xa;def write(string,indent=0,end=""""):&#xa;    """"""Use sys.stdout.write to write the string with an indentation&#xa;    equal to indent and specifying the end character""""""&#xa;    sys.stdout.write("" ""*indent+string+end)&#xa;&#xa;TREE_TEMPLATE = ""{0:{nameWidth}}""+""{1:{titleWidth}}{2:{memoryWidth}}""&#xa;&#xa;def _recursifTreePrinter(tree,indent):&#xa;    """"""Print recursively tree informations""""""&#xa;    listOfBranches = tree.GetListOfBranches()&#xa;    if len(listOfBranches) > 0: # Width informations&#xa;        maxCharName = max([len(branch.GetName()) \&#xa;            for branch in listOfBranches])&#xa;        maxCharTitle = max([len(branch.GetTitle()) \&#xa;            for branch in listOfBranches])&#xa;        dic = { \&#xa;            ""nameWidth"":maxCharName+2, \&#xa;            ""titleWidth"":maxCharTitle+4, \&#xa;            ""memoryWidth"":1}&#xa;    for branch in listOfBranches: # Print loop&#xa;        rec = \&#xa;            [branch.GetName(), \&#xa;            ""\""""+branch.GetTitle()+""\"""", \&#xa;            str(branch.GetTotBytes())]&#xa;        write(TREE_TEMPLATE.format(*rec,**dic),indent,end=""\n"")&#xa;        _recursifTreePrinter(branch,indent+2)&#xa;&#xa;def _prepareTime(time):&#xa;    """"""Get time in the proper shape&#xa;    ex : 174512 for 17h 45m 12s&#xa;    ex : 094023 for 09h 40m 23s""""""&#xa;    time = str(time)&#xa;    time = '000000'+time&#xa;    time = time[len(time)-6:]&#xa;    return time&#xa;&#xa;MONTH = {1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun', \&#xa;         7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}&#xa;LONG_TEMPLATE = \&#xa;    isSpecial(ANSI_BOLD,""{0:{classWidth}}"")+""{1:{timeWidth}}"" + \&#xa;    ""{2:{nameWidth}}{3:{titleWidth}}""&#xa;&#xa;def _rootLsPrintLongLs(keyList,indent,treeListing):&#xa;    """"""Print a list of Tkey in columns&#xa;    pattern : classname, datetime, name and title""""""&#xa;    if len(keyList) > 0: # Width informations&#xa;        maxCharClass = max([len(key.GetClassName()) for key in keyList])&#xa;        maxCharTime = 12&#xa;        maxCharName = max([len(key.GetName()) for key in keyList])&#xa;        dic = { \&#xa;            ""classWidth"":maxCharClass+2, \&#xa;            ""timeWidth"":maxCharTime+2, \&#xa;            ""nameWidth"":maxCharName+2, \&#xa;            ""titleWidth"":1}&#xa;    date = ROOT.Long(0)&#xa;    for key in keyList:&#xa;        time = ROOT.Long(0)&#xa;        datime = key.GetDatime()&#xa;        datime.GetDateTime(datime.Get(),date,time)&#xa;        time = _prepareTime(time)&#xa;        rec = \&#xa;            [key.GetClassName(), \&#xa;            MONTH[int(str(date)[4:6])]+"" "" +str(date)[6:]+ \&#xa;            "" ""+time[:2]+"":""+time[2:4], \&#xa;            key.GetName(), \&#xa;            ""\""""+key.GetTitle()+""\""""]&#xa;        write(LONG_TEMPLATE.format(*rec,**dic),indent,end=""\n"")&#xa;        if treeListing and isTreeKey(key):&#xa;            tree = key.ReadObj()&#xa;            _recursifTreePrinter(tree,indent+2)&#xa;&#xa;##&#xa;# The code of the getTerminalSize function can be found here :&#xa;# https://gist.github.com/jtriley/1108174&#xa;# Thanks jtriley !!&#xa;&#xa;import os&#xa;import shlex&#xa;import struct&#xa;import platform&#xa;import subprocess&#xa;&#xa;def getTerminalSize():&#xa;    """""" getTerminalSize()&#xa;     - get width and height of console&#xa;     - works on linux,os x,windows,cygwin(windows)&#xa;     originally retrieved from:&#xa;     http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python""""""&#xa;    current_os = platform.system()&#xa;    tuple_xy = None&#xa;    if current_os == 'Windows':&#xa;        tuple_xy = _get_terminal_size_windows()&#xa;        if tuple_xy is None:&#xa;            tuple_xy = _get_terminal_size_tput()&#xa;            # needed for window's python in cygwin's xterm!&#xa;    if current_os in ['Linux', 'Darwin'] or current_os.startswith('CYGWIN'):&#xa;        tuple_xy = _get_terminal_size_linux()&#xa;    if tuple_xy is None:&#xa;        #print ""default""&#xa;        #_get_terminal_size_windows() or _get_terminal_size_tput don't work&#xa;        tuple_xy = (80, 25)      # default value&#xa;    return tuple_xy&#xa;&#xa;def _get_terminal_size_windows():&#xa;    try:&#xa;        from ctypes import windll, create_string_buffer&#xa;        # stdin handle is -10&#xa;        # stdout handle is -11&#xa;        # stderr handle is -12&#xa;        h = windll.kernel32.GetStdHandle(-12)&#xa;        csbi = create_string_buffer(22)&#xa;        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)&#xa;        if res:&#xa;            (bufx, bufy, curx, cury, wattr,&#xa;             left, top, right, bottom,&#xa;             maxx, maxy) = struct.unpack(""hhhhHhhhhhh"", csbi.raw)&#xa;            sizex = right - left + 1&#xa;            sizey = bottom - top + 1&#xa;            return sizex, sizey&#xa;    except:&#xa;        pass&#xa;&#xa;def _get_terminal_size_tput():&#xa;    # get terminal width&#xa;    # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window&#xa;    try:&#xa;        cols = int(subprocess.check_call(shlex.split('tput cols')))&#xa;        rows = int(subprocess.check_call(shlex.split('tput lines')))&#xa;        return (cols, rows)&#xa;    except:&#xa;        pass&#xa;&#xa;def _get_terminal_size_linux():&#xa;    def ioctl_GWINSZ(fd):&#xa;        try:&#xa;            import fcntl&#xa;            import termios&#xa;            cr = struct.unpack('hh',&#xa;                               fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))&#xa;            return cr&#xa;        except:&#xa;            pass&#xa;    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)&#xa;    if not cr:&#xa;        try:&#xa;            fd = os.open(os.ctermid(), os.O_RDONLY)&#xa;            cr = ioctl_GWINSZ(fd)&#xa;            os.close(fd)&#xa;        except:&#xa;            pass&#xa;    if not cr:&#xa;        try:&#xa;            cr = (os.environ['LINES'], os.environ['COLUMNS'])&#xa;        except:&#xa;            return None&#xa;    return int(cr[1]), int(cr[0])&#xa;&#xa;# End of getTerminalSize code&#xa;##&#xa;&#xa;def _rootLsPrintSimpleLs(keyList,indent,oneColumn):&#xa;    """"""Print list of strings in columns&#xa;    - blue for directories&#xa;    - green for trees""""""&#xa;    # This code is adaptated from the pprint_list function here :&#xa;    # http://stackoverflow.com/questions/25026556/output-list-like-ls&#xa;    # Thanks hawkjo !!&#xa;    if len(keyList) == 0: return&#xa;    (term_width, term_height) = getTerminalSize()&#xa;    term_width = term_width - indent&#xa;    min_chars_between = 2&#xa;    min_element_width = min( len(key.GetName()) for key in keyList ) \&#xa;                        + min_chars_between&#xa;    max_element_width = max( len(key.GetName()) for key in keyList ) \&#xa;                        + min_chars_between&#xa;    if max_element_width >= term_width: ncol,col_widths = 1,[1]&#xa;    else:&#xa;        # Start with max possible number of columns and reduce until it fits&#xa;        ncol = 1 if oneColumn else min( len(keyList), term_width / min_element_width  )&#xa;        while True:&#xa;            col_widths = \&#xa;                [ max( len(key.GetName()) + min_chars_between \&#xa;                for j, key in enumerate(keyList) if j % ncol == i ) \&#xa;                for i in range(ncol) ]&#xa;            if sum( col_widths ) <= term_width: break&#xa;            else: ncol -= 1&#xa;&#xa;    for i, key in enumerate(keyList):&#xa;        if i%ncol == 0: write("""",indent) # indentation&#xa;        # Don't add spaces after the last element of the line or of the list&#xa;        if (i+1)%ncol != 0 and i != len(keyList)-1:&#xa;            if not IS_TERMINAL: write( \&#xa;                key.GetName().ljust(col_widths[i%ncol]))&#xa;            elif isDirectoryKey(keyList[i]): write( \&#xa;                isSpecial(ANSI_BLUE,key.GetName()).ljust( \&#xa;                    col_widths[i%ncol] + ANSI_BLUE_LENGTH))&#xa;            elif isTreeKey(keyList[i]): write( \&#xa;                isSpecial(ANSI_GREEN,key.GetName()).ljust( \&#xa;                    col_widths[i%ncol] + ANSI_GREEN_LENGTH))&#xa;            else: write(key.GetName().ljust(col_widths[i%ncol]))&#xa;        else: # No spaces after the last element of the line or of the list&#xa;            if not IS_TERMINAL: write(key.GetName())&#xa;            elif isDirectoryKey(keyList[i]):&#xa;                write(isSpecial(ANSI_BLUE, key.GetName()))&#xa;            elif isTreeKey(keyList[i]):&#xa;                write(isSpecial(ANSI_GREEN, key.GetName()))&#xa;            else: write(key.GetName())&#xa;            write('\n')&#xa;&#xa;def _rootLsPrint(keyList, indent, oneColumn, \&#xa;                 longListing, treeListing):&#xa;    """"""Print informations given by keyList with a rootLs&#xa;    style choosen with the options""""""&#xa;    if longListing or treeListing: \&#xa;       _rootLsPrintLongLs(keyList, indent, treeListing)&#xa;    else:&#xa;       _rootLsPrintSimpleLs(keyList, indent, oneColumn)&#xa;&#xa;def _rootLsProcessFile(fileName, pathSplitList, manySources, indent, \&#xa;                       oneColumn, longListing, treeListing):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName)&#xa;    if not rootFile: return 1&#xa;&#xa;    keyList,dirList = keyClassSpliter(rootFile,pathSplitList)&#xa;    if manySources: write(""{0} :"".format(fileName)+""\n"")&#xa;    _rootLsPrint(keyList, indent, oneColumn, longListing, treeListing)&#xa;&#xa;    # Loop on the directories&#xa;    manyPathSplits = len(pathSplitList) > 1&#xa;    indentDir = 2 if manyPathSplits else 0&#xa;    for pathSplit in dirList:&#xa;        keyList = getKeyList(rootFile,pathSplit)&#xa;        keyListSort(keyList)&#xa;        if manyPathSplits: write(""{0} :"".format(""/"".join(pathSplit)),indent,end=""\n"")&#xa;        _rootLsPrint(keyList, indent+indentDir, oneColumn, longListing, treeListing)&#xa;&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootLs(sourceList, oneColumn=False, longListing=False, treeListing=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;    tupleListSort(sourceList)&#xa;&#xa;    # Loop on the ROOT files&#xa;    retcode = 0&#xa;    manySources = len(sourceList) > 1&#xa;    indent = 2 if manySources else 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _rootLsProcessFile(fileName, pathSplitList, manySources, indent, \&#xa;                                      oneColumn, longListing, treeListing)&#xa;    return retcode&#xa;&#xa;# End of ROOTLS&#xa;##########&#xa;&#xa;##########&#xa;# ROOTMKDIR&#xa;&#xa;MKDIR_ERROR = ""cannot create directory '{0}'""&#xa;&#xa;def _createDirectories(rootFile,pathSplit,parents):&#xa;    """"""Same behaviour as createDirectory but allows the possibility&#xa;    to build an whole path recursively with the option \""parents\"" """"""&#xa;    retcode = 0&#xa;    lenPathSplit = len(pathSplit)&#xa;    if lenPathSplit == 0:&#xa;        pass&#xa;    elif parents:&#xa;        for i in xrange(lenPathSplit):&#xa;            currentPathSplit = pathSplit[:i+1]&#xa;            if not (isExisting(rootFile,currentPathSplit) \&#xa;                and isDirectory(rootFile,currentPathSplit)):&#xa;                retcode += createDirectory(rootFile,currentPathSplit)&#xa;    else:&#xa;        doMkdir = True&#xa;        for i in xrange(lenPathSplit-1):&#xa;            currentPathSplit = pathSplit[:i+1]&#xa;            if not (isExisting(rootFile,currentPathSplit) \&#xa;                and isDirectory(rootFile,currentPathSplit)):&#xa;                doMkdir = False&#xa;                break&#xa;        if doMkdir:&#xa;            retcode += createDirectory(rootFile,pathSplit)&#xa;        else:&#xa;            logging.warning(MKDIR_ERROR.format(""/"".join(pathSplit)))&#xa;            retcode += 1&#xa;    return retcode&#xa;&#xa;def _rootMkdirProcessFile(fileName, pathSplitList, parents):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName,""update"")&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        retcode+=_createDirectories(rootFile,pathSplit,parents)&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootMkdir(sourceList, parents=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;&#xa;    # Loop on the ROOT files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _rootMkdirProcessFile(fileName, pathSplitList, parents)&#xa;    return retcode&#xa;&#xa;# End of ROOTMKDIR&#xa;##########&#xa;&#xa;##########&#xa;# ROOTMV&#xa;&#xa;MOVE_ERROR = ""error during copy of {0}, it is not removed from {1}""&#xa;&#xa;def _moveObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                 oneFile, interactive):&#xa;    retcode = 0&#xa;    recursive = True&#xa;    replace = True&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName,""update"") \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(rootFile) # Fast copy necessity&#xa;    for pathSplit in pathSplitList:&#xa;        oneSource = oneFile and len(pathSplitList)==1&#xa;        retcodeTemp = copyRootObject(rootFile,pathSplit, \&#xa;            destFile,destPathSplit,oneSource,recursive,replace)&#xa;        if not retcodeTemp:&#xa;            retcode += deleteRootObject(rootFile, pathSplit, interactive, recursive)&#xa;        else:&#xa;            logging.warning(MOVE_ERROR.format(""/"".join(pathSplit),rootFile.GetName()))&#xa;            retcode += retcodeTemp&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootMv(sourceList, destFileName, destPathSplit, compress=None, \&#xa;           interactive=False, recreate=False):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in sourceList:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName,compress,recreate)&#xa;    if not destFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(destFile) # Fast copy necessity&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _moveObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                len(sourceList)==1, interactive)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTMV&#xa;##########&#xa;&#xa;##########&#xa;# ROOTPRINT&#xa;&#xa;def _keyListExtended(rootFile,pathSplitList):&#xa;    keyList,dirList = keyClassSpliter(rootFile,pathSplitList)&#xa;    for pathSplit in dirList: keyList.extend(getKeyList(rootFile,pathSplit))&#xa;    keyList = [key for key in keyList if not isDirectoryKey(key)]&#xa;    keyListSort(keyList)&#xa;    return keyList&#xa;&#xa;def rootPrint(sourceList, directoryOption = None, divideOption = None, drawOption = """", formatOption = None, \&#xa;              outputOption = None, sizeOption = None, styleOption = None, verboseOption = False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;    tupleListSort(sourceList)&#xa;&#xa;    # Don't open windows&#xa;    ROOT.gROOT.SetBatch()&#xa;&#xa;    # (Style option)&#xa;    if styleOption: ROOT.gInterpreter.ProcessLine("".x {0}"".format(styleOption))&#xa;&#xa;    # (Verbose option)&#xa;    if not verboseOption: ROOT.gErrorIgnoreLevel = 9999&#xa;&#xa;    # Initialize the canvas (Size option)&#xa;    if sizeOption:&#xa;        try:&#xa;            width,height = sizeOption.split(""x"")&#xa;            width = int(width)&#xa;            height = int(height)&#xa;        except ValueError:&#xa;            logging.warning(""canvas size is on a wrong format"")&#xa;            return 1&#xa;        canvas = ROOT.TCanvas(""canvas"",""canvas"",width,height)&#xa;    else:&#xa;        canvas = ROOT.TCanvas(""canvas"")&#xa;&#xa;    # Divide the canvas (Divide option)&#xa;    if divideOption:&#xa;        try:&#xa;            x,y = divideOption.split("","")&#xa;            x = int(x)&#xa;            y = int(y)&#xa;        except ValueError:&#xa;            logging.warning(""divide is on a wrong format"")&#xa;            return 1&#xa;        canvas.Divide(x,y)&#xa;        caseNumber = x*y&#xa;&#xa;    # Take the format of the output file (formatOutput option)&#xa;    if not formatOption and outputOption:&#xa;        fileName = outputOption&#xa;        fileFormat = fileName.split(""."")[-1]&#xa;        formatOption = fileFormat&#xa;&#xa;    # Use pdf as default format&#xa;    if not formatOption: formatOption = ""pdf""&#xa;&#xa;    # Create the output directory (directory option)&#xa;    if directoryOption:&#xa;        if not os.path.isdir(os.path.join(os.getcwd(),directoryOption)):&#xa;            os.mkdir(directoryOption)&#xa;&#xa;    # Make the output name, begin to print (output option)&#xa;    if outputOption:&#xa;        if formatOption in ['ps','pdf']:&#xa;            outputFileName = outputOption&#xa;            if directoryOption: outputFileName = \&#xa;                directoryOption + ""/"" + outputFileName&#xa;            canvas.Print(outputFileName+""["",formatOption)&#xa;        else:&#xa;            logging.warning(""can't merge pictures, only postscript or pdf files"")&#xa;            return 1&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    objDrawnNumber = 0&#xa;    openRootFiles = []&#xa;    for fileName, pathSplitList in sourceList:&#xa;        rootFile = openROOTFile(fileName)&#xa;        if not rootFile:&#xa;            retcode += 1&#xa;            continue&#xa;        openRootFiles.append(rootFile)&#xa;        # Fill the key list (almost the same as in rools)&#xa;        keyList = _keyListExtended(rootFile,pathSplitList)&#xa;        for key in keyList:&#xa;            if isTreeKey(key):&#xa;                pass&#xa;            else:&#xa;                if divideOption:&#xa;                    canvas.cd(objDrawnNumber%caseNumber + 1)&#xa;                    objDrawnNumber += 1&#xa;                obj = key.ReadObj()&#xa;                obj.Draw(drawOption)&#xa;                if divideOption:&#xa;                    if objDrawnNumber%caseNumber == 0:&#xa;                        if not outputOption:&#xa;                            outputFileName = str(objDrawnNumber//caseNumber)+"".""+formatOption&#xa;                            if directoryOption:&#xa;                                outputFileName = os.path.join( \&#xa;                                    directoryOption,outputFileName)&#xa;                        canvas.Print(outputFileName,formatOption)&#xa;                        canvas.Clear()&#xa;                        canvas.Divide(x,y)&#xa;                else:&#xa;                    if not outputOption:&#xa;                        outputFileName = key.GetName() + ""."" +formatOption&#xa;                        if directoryOption:&#xa;                            outputFileName = os.path.join( \&#xa;                                directoryOption,outputFileName)&#xa;                    if outputOption or formatOption == 'pdf':&#xa;                        objTitle = ""Title:""+key.GetClassName()+"" : ""+key.GetTitle()&#xa;                        canvas.Print(outputFileName,objTitle)&#xa;                    else:&#xa;                        canvas.Print(outputFileName,formatOption)&#xa;&#xa;    # Last page (divideOption)&#xa;    if divideOption:&#xa;        if objDrawnNumber%caseNumber != 0:&#xa;            if not outputOption:&#xa;                outputFileName = str(objDrawnNumber//caseNumber + 1)+"".""+formatOption&#xa;                if directoryOption:&#xa;                    outputFileName = os.path.join(directoryOption,outputFileName)&#xa;            canvas.Print(outputFileName,formatOption)&#xa;&#xa;    # End to print (output option)&#xa;    if outputOption:&#xa;        if not divideOption:&#xa;            canvas.Print(outputFileName+""]"",objTitle)&#xa;        else:&#xa;            canvas.Print(outputFileName+""]"")&#xa;&#xa;    # Close ROOT files&#xa;    map(lambda rootFile: rootFile.Close(),openRootFiles)&#xa;&#xa;    return retcode&#xa;&#xa;# End of ROOTPRINT&#xa;##########&#xa;&#xa;##########&#xa;# ROOTRM&#xa;&#xa;def _removeObjects(fileName, pathSplitList, interactive=False, recursive=False):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName,""update"")&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        retcode += deleteRootObject(rootFile, pathSplit, interactive, recursive)&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootRm(sourceList, interactive=False, recursive=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _removeObjects(fileName, pathSplitList, interactive, recursive)&#xa;    return retcode&#xa;&#xa;# End of ROOTRM&#xa;##########&#xa;"
473973|"# cribbage_objects.py&#xa;# hand, card, and other objects&#xa;# J. Hassler Thurston&#xa;# 23 December 2015&#xa;&#xa;import random&#xa;import os&#xa;import math&#xa;&#xa;class Hand(object):&#xa;    def __init__(self, cards):&#xa;        self.cards = cards&#xa;        self.sort()&#xa;&#xa;    # prints out a player's hand&#xa;    def __str__(self):&#xa;        n = 1&#xa;        print_str = """"&#xa;        for card in self.cards:&#xa;            print_str += ""(""+str(n)+"") ""&#xa;            print_str += str(card) + ""   ""&#xa;            n += 1&#xa;        return print_str&#xa;&#xa;    # from http://stackoverflow.com/questions/12933964/printing-a-list-of-objects-of-user-defined-class&#xa;    def __repr__(self):&#xa;        return str(self)&#xa;&#xa;    # sorts a player's hand&#xa;    def sort(self):&#xa;        self.cards.sort(key=lambda card: card.number)&#xa;&#xa;    # scores a player's hand&#xa;    def score(self, top_card):&#xa;        # temporarily add the top card to the hand&#xa;        if top_card:&#xa;            self.cards.append(top_card)&#xa;        self.sort()&#xa;        # fifteens&#xa;        sc = self.score_fifteens(0)&#xa;        # pairs etc&#xa;        sc += self.score_pairs(sc)&#xa;        # runs&#xa;        sc += self.score_runs(sc)&#xa;        # remove the top card from the hand&#xa;        if top_card:&#xa;            self.cards.remove(top_card)&#xa;        # flushes&#xa;        sc += self.score_flushes(top_card, sc)&#xa;        # his knobs&#xa;        sc += self.score_his_knobs(top_card, sc)&#xa;        # return the score&#xa;        return sc&#xa;&#xa;    def score_fifteens(self, current_score):&#xa;        # brute force way: for every combination of cards, see if the values add up to 15&#xa;        sc = 0&#xa;        for i in range(2**len(self.cards)):&#xa;            subset = []&#xa;            for j in range(len(self.cards)):&#xa;                if (i >> j) % 2 != 0:&#xa;                    subset.append(self.cards[j])&#xa;            # score the subset&#xa;            if get_sum(subset) == 15:&#xa;                for s in subset:&#xa;                    print s,&#xa;                print &#xa;                sc += 2&#xa;                say( ""fifteen for "" + str(current_score+sc))&#xa;        return sc&#xa;&#xa;    def score_pairs(self, current_score):&#xa;        sc = 0&#xa;        # cards are sorted, so check adjacent cards&#xa;        current_number = 0&#xa;        cards_at_number = 0&#xa;        actual_cards = []&#xa;        for c in self.cards:&#xa;            # TODO: doesn't detect king pairs&#xa;            newnum = c.number&#xa;            if newnum != current_number:&#xa;                if cards_at_number >= 2:&#xa;                    sc += math.factorial(cards_at_number)&#xa;                    num_pairs = math.factorial(cards_at_number)/2&#xa;                    for card in actual_cards:&#xa;                        print card,&#xa;                    print&#xa;                    if num_pairs == 1:&#xa;                        say(""pair for "" + str(current_score+sc))&#xa;                    else:&#xa;                        say(str(num_pairs) + "" pairs for "" + str(current_score+sc))&#xa;                current_number = newnum&#xa;                cards_at_number = 0&#xa;                actual_cards = []&#xa;&#xa;            cards_at_number += 1&#xa;            actual_cards.append(c)&#xa;        return sc&#xa;&#xa;    def score_runs(self, current_score):&#xa;        sc = 0&#xa;        # cards are sorted, so check adjacent runs&#xa;        # group the cards in buckets depending on their number&#xa;        d = {}&#xa;        card_dict = {}&#xa;        for i in range(1, 15):&#xa;            d[i] = 0&#xa;            card_dict[i] = []&#xa;        for c in self.cards:&#xa;            d[c.number] += 1&#xa;            card_dict[c.number].append(c)&#xa;        # loop through the buckets again and compute the number of runs&#xa;        num_in_row = 0&#xa;        num_runs_in_row = 1&#xa;        actual_cards = []&#xa;        for i in range(1, 15):&#xa;            if d[i] == 0:&#xa;                if num_in_row >= 3:&#xa;                    sc += num_in_row * num_runs_in_row&#xa;                    for card in actual_cards:&#xa;                        print card,&#xa;                    print&#xa;                    say(str(num_runs_in_row) + "" runs of "" + str(num_in_row) + "" for "" + str(current_score+sc))&#xa;                num_in_row = 0&#xa;                num_runs_in_row = 1&#xa;                actual_cards = []&#xa;            else:&#xa;                num_in_row += 1&#xa;                num_runs_in_row *= d[i]&#xa;                actual_cards.extend(card_dict[i])&#xa;        return sc&#xa;&#xa;    def score_flushes(self, top_card, current_score):&#xa;        if len(self.cards) == 0:&#xa;            return 0&#xa;        sc = 0&#xa;        # check if the cards in the hand are all of the same suit&#xa;        same_suit = True&#xa;        suit = self.cards[0].suit&#xa;        for c in self.cards:&#xa;            if c.suit != suit:&#xa;                same_suit = False&#xa;        if same_suit:&#xa;            sc += len(self.cards)&#xa;            if top_card and top_card.suit == suit:&#xa;                sc += 1&#xa;        if sc > 0:&#xa;            say(""flush for "" + str(current_score+sc))&#xa;        return sc&#xa;&#xa;    def score_his_knobs(self, top_card, current_score):&#xa;        # check if there is a Jack in the hand that is the same suit as the top card&#xa;        if not top_card:&#xa;            return 0&#xa;        suit = top_card.suit&#xa;        for c in self.cards:&#xa;            if c.number == 11 and c.suit == suit:&#xa;                print c, ""   "", top_card&#xa;                say(""his knobs for "" + str(current_score+1))&#xa;                return 1&#xa;        return 0&#xa;&#xa;&#xa;# sums up the values of cards&#xa;def get_sum(cards):&#xa;    s = 0&#xa;    for c in cards:&#xa;        s += c.value&#xa;    return s&#xa;&#xa;&#xa;class Table(object):&#xa;    def __init__(self, game, round, deck, crib_player):&#xa;        self.game = game&#xa;        self.round = round&#xa;        self.deck = deck&#xa;        self.crib_player = crib_player&#xa;        self.hands = deck.hands&#xa;        self.table_cards = []&#xa;        self.count = 0&#xa;        self.go_declared = False&#xa;        self.go_declared_player = -1&#xa;        # the person who doesn't have the crib starts&#xa;        self.player = (self.crib_player+1)%2&#xa;        self.num_cards_played = 0&#xa;&#xa;    def __str__(self):&#xa;        s = """"&#xa;        s += ""Table cards: "" + str(self.table_cards) + "". Count: "" + str(self.count)&#xa;        s += "". ""&#xa;        if self.go_declared:&#xa;            s += ""Go declared by player "" + str(self.go_declared_player) + "".""&#xa;        return s&#xa;&#xa;    def __repr__(self):&#xa;        return str(self)&#xa;&#xa;    # plays cards interactively&#xa;    def play_cards_interactive(self):&#xa;        # initialize the table&#xa;        self.reset_table()&#xa;        for hand in self.hands:&#xa;            for c in hand.cards:&#xa;                c.played = False&#xa;        # keep track of fifteens, pairs, three+four of a kinds, and runs&#xa;        while self.num_cards_played < 8:&#xa;            if self.player == 0:&#xa;                self.prompt_user_play_cards()&#xa;            else:&#xa;                self.computer_play_cards()&#xa;&#xa;    def reset_table(self):&#xa;        self.count = 0&#xa;        self.table_cards = []&#xa;        self.go_declared = False&#xa;        self.go_declared_player = -1&#xa;&#xa;    def prompt_user_play_cards(self):&#xa;        while True:&#xa;            try:&#xa;                print ""Count is "" + str(self.count) + "". Here is what's on the table: ""&#xa;                print self.table_cards, "".""&#xa;                print ""Here is what's in your hand: "",&#xa;                print self.hands[0]&#xa;                print ""Here are the cards you can play: "",&#xa;                playable_cards = self.get_playable_cards(self.hands[0])&#xa;                print playable_cards&#xa;                if self.go_declared:&#xa;                    print ""Computer said Go.""&#xa;                if len(playable_cards) == 0:&#xa;                    # Go&#xa;                    self.declare_go()&#xa;                else:&#xa;                    # prompt the user to enter a card&#xa;                    card = int(raw_input(""Please select a card (1-""+str(len(playable_cards))+"") to play.""))&#xa;                    if card <= 0 or card > len(playable_cards):&#xa;                        raise ValueError&#xa;                    print ""Selected card "" + str(card) +  "".""&#xa;                    self.play_card(playable_cards[card-1])&#xa;                break&#xa;            except ValueError:&#xa;                print ""Invalid card number.""&#xa;&#xa;    def computer_play_cards(self):&#xa;        # get the cards that can be played&#xa;        playable_cards = self.get_playable_cards(self.hands[1])&#xa;        # if we can't play any cards, Go.&#xa;        if len(playable_cards) == 0:&#xa;            self.declare_go()&#xa;        else:&#xa;            # otherwise, pick a random card and play it&#xa;            self.play_card(playable_cards[0])&#xa;&#xa;    def get_playable_cards(self, hand):&#xa;        # returns a subset of the cards that are allowed to be played, given the count&#xa;        playable_cards = []&#xa;        for c in hand.cards:&#xa;            if not c.played and self.count + c.value <= 31:&#xa;                playable_cards.append(c)&#xa;        return playable_cards&#xa;&#xa;    # plays a card.&#xa;    def play_card(self, card):&#xa;        # extract the card from the hand and put it on the table&#xa;        self.table_cards.append(card)&#xa;        print ""cards on table: "", str(self.table_cards)&#xa;        print ""card just played: "", card&#xa;        card.played = True&#xa;        self.count += card.value&#xa;        say(str(self.count))&#xa;        self.num_cards_played += 1&#xa;        # score points&#xa;        # Fifteen and thirty-one&#xa;        if self.count == 15 or self.count == 31:&#xa;            self.game.update_score(self.player, 2)&#xa;            say(""for two"")&#xa;        # Pairs etc.&#xa;        self.table_score_pairs()&#xa;        # Runs etc.&#xa;        self.table_score_runs()&#xa;        # update the player&#xa;        if not self.go_declared:&#xa;            self.player = (self.player+1)%2&#xa;&#xa;    # declares Go&#xa;    def declare_go(self):&#xa;        if self.go_declared:&#xa;            # go was already declared by the other player, so reset the table&#xa;            # and give yourself one point&#xa;            self.game.update_score(self.player, 1)&#xa;            self.reset_table()&#xa;        else:&#xa;            # we are the first ones to declare go, so declare it&#xa;            self.go_declared = True&#xa;            self.go_declared_player = self.player&#xa;        # update the player&#xa;        self.player = (self.player+1)%2&#xa;&#xa;&#xa;    def table_score_pairs(self):&#xa;        # look back and see how many pairs there were in a row&#xa;        num_pairs = 0&#xa;        paired_value = self.table_cards[-1].number&#xa;        for i in range(len(self.table_cards)-1,-1,-1):&#xa;            c = self.table_cards[i]&#xa;            if c.number == paired_value:&#xa;                num_pairs += 1&#xa;            else:&#xa;                break&#xa;        if num_pairs >= 2:&#xa;            self.game.update_score(self.player, math.factorial(num_pairs))&#xa;            say(str(num_pairs/2) + "" pairs for "" + str(math.factorial(num_pairs)))&#xa;&#xa;    def table_score_runs(self):&#xa;        # look back and see if the last n cards constitute a run. Get the largest such n&#xa;        # first, bucket sort all the cards on the table&#xa;        buckets = {}&#xa;        for i in range(1, 14):&#xa;            buckets[i] = 0&#xa;        for c in self.table_cards:&#xa;            buckets[c.number] += 1&#xa;        # start with all the cards in the table, and see if we can make a run with them.&#xa;        # If so, return. If not, incrementally remove a card from the table and repeat.&#xa;        num_cards = len(self.table_cards)&#xa;        for i in range(len(self.table_cards)):&#xa;            # identify whether the counts are all 0s and 1s&#xa;            all_01s = True&#xa;            min_val = 15&#xa;            max_val = -1&#xa;            for j in range(1, 14):&#xa;                all_01s = all_01s and (buckets[j] == 0 or buckets[j] == 1)&#xa;                if buckets[j] == 1:&#xa;                    min_val = min(min_val, j)&#xa;                    max_val = max(max_val, j)&#xa;            # if not, we can't make a run with these cards&#xa;            if not all_01s:&#xa;                buckets[self.table_cards[i].number] -= 1&#xa;                num_cards -= 1&#xa;                continue&#xa;            # otherwise, see if the minimum and maximum values differ by the right amount&#xa;            if max_val - min_val + 1 == num_cards and num_cards >= 3:&#xa;                # we found a match&#xa;                self.game.update_score(self.player, num_cards)&#xa;                say(""run of "" + str(num_cards) + "" for "" + str(num_cards))&#xa;                return&#xa;            else:&#xa;                buckets[self.table_cards[i].number] -= 1&#xa;                num_cards -= 1&#xa;                continue&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;class Game(object):&#xa;    def __init__(self, interactive):&#xa;        self.interactive = interactive&#xa;        self.score = [0, 0]&#xa;        self.crib_player = 0&#xa;&#xa;    def play(self):&#xa;        while not self.game_over():&#xa;            self.play_round()&#xa;            self.crib_player = (self.crib_player+1)%2&#xa;&#xa;    # one round of a game&#xa;    def play_round(self):&#xa;        Round(self)&#xa;&#xa;    # returns True if the game is over&#xa;    def game_over(self):&#xa;        return self.score[0] >= 121 or self.score[1] >= 121&#xa;&#xa;    # updates the score&#xa;    def update_score(self, player, points):&#xa;        self.score[player] += points&#xa;        self.print_score()&#xa;        if self.game_over():&#xa;            # the player just won&#xa;            self.winner = player&#xa;            self.destruct()&#xa;&#xa;    # wraps up the game&#xa;    def destruct(self):&#xa;        print ""Player "" + str(self.player+1) + "" has won. Final score: "",&#xa;        print_score(self)&#xa;&#xa;    # prints out the score&#xa;    def print_score(self):&#xa;        print ""YOU: "" + str(self.score[0]) + "" - "" + str(self.score[1]) + "" :COMPUTER""&#xa;&#xa;&#xa;class Round(object):&#xa;    def __init__(self, game):&#xa;        self.game = game&#xa;        self.crib_player = self.game.crib_player&#xa;        self.play()&#xa;&#xa;    def play(self):&#xa;        # print out the score to the player&#xa;        self.game.print_score()&#xa;        # shuffle the cards and deal them&#xa;        self.deck = Deck()&#xa;        # show the six cards to the player, and ask them to select cards to put in the crib.&#xa;        crib_cards = self.player_select_crib()&#xa;        crib_cards.extend(self.select_crib())&#xa;        self.deck.crib = Hand(crib_cards)&#xa;        # show the top card on the deck&#xa;        self.deck.show_top_card()&#xa;        # interactively play the cards&#xa;        Table(self.game, self, self.deck, self.crib_player).play_cards_interactive()&#xa;        # score the hands&#xa;        self.score_hands_interactive()&#xa;        # show the score at the end of the turn&#xa;        self.game.print_score()&#xa;&#xa;    # ask the player to select two cards for the crib&#xa;    def player_select_crib(self):&#xa;        crib_cards = []&#xa;        # show the hand to the player and ask them to pick two cards&#xa;        print ""Here are the cards in your hand. It is your"",&#xa;        if self.crib_player == 1:&#xa;            print ""opponent's"",&#xa;        print ""crib.""&#xa;        while len(crib_cards) < 2:&#xa;            # from https://docs.python.org/2/tutorial/errors.html&#xa;            try:&#xa;                print self.deck.hands[0]&#xa;                card = int(raw_input(""Please select a card (1-""+str(len(self.deck.hands[0].cards))+"") to put in your crib.""))&#xa;                if card <= 0 or card > 6:&#xa;                    raise ValueError&#xa;                if self.deck.hands[0].cards[card-1] in crib_cards:&#xa;                    raise ValueError&#xa;                crib_cards.append(self.deck.hands[0].cards.pop(card-1))&#xa;                print ""Selected card "" + str(card) +  "".""&#xa;            except ValueError:&#xa;                print ""Invalid card number.""&#xa;        return crib_cards&#xa;&#xa;    # computer program to select the crib&#xa;    def select_crib(self):&#xa;        # stub&#xa;        crib_cards = []&#xa;        crib_cards.append(self.deck.hands[1].cards.pop(0))&#xa;        crib_cards.append(self.deck.hands[1].cards.pop(1))&#xa;        return crib_cards&#xa;&#xa;    # scores the hands&#xa;    def score_hands_interactive(self):&#xa;        if self.crib_player == 1:&#xa;            print ""YOUR HAND...""&#xa;        else:&#xa;            print ""YOUR OPPONENT'S HAND...""&#xa;        self.game.update_score((self.crib_player+1)%2, self.deck.hands[(self.crib_player+1)%2].score(self.deck.top_card))&#xa;        if self.crib_player == 0:&#xa;            print ""YOUR HAND...""&#xa;        else:&#xa;            print ""YOUR OPPONENT'S HAND...""&#xa;        self.game.update_score(self.crib_player, self.deck.hands[self.crib_player].score(self.deck.top_card))&#xa;        print ""YOUR"",&#xa;        if self.crib_player == 1:&#xa;            print ""OPPONENT'S"",&#xa;        print ""CRIB...""&#xa;        self.game.update_score(self.crib_player, self.deck.crib.score(self.deck.top_card))&#xa;&#xa;class Deck(object):&#xa;    def __init__(self):&#xa;        self.deck = []&#xa;        # initialize the deck&#xa;        # TODO: make it so that these next few lines are only executed once when the program starts&#xa;        suits = ['Clubs', 'Spades', 'Diamonds', 'Hearts']&#xa;        for suit in suits:&#xa;            for num in range(1, 14):&#xa;                self.deck.append(Card(num, suit))&#xa;        # shuffle the deck&#xa;        self.shuffle()&#xa;        # assign hands to each player, and the top card&#xa;        self.hands = [Hand(self.deck[:6]), Hand(self.deck[6:12])]&#xa;        self.top_card = self.deck[12]&#xa;        self.crib = Hand([])&#xa;&#xa;    def shuffle(self):&#xa;        # from http://stackoverflow.com/questions/473973/shuffle-an-array-with-python&#xa;        random.shuffle(self.deck)&#xa;&#xa;&#xa;    def show_top_card(self):&#xa;        print ""top card is: "" + str(self.top_card)&#xa;&#xa;    # string representation of a deck&#xa;    def __str__(self):&#xa;        string = """"&#xa;        string += ""Player 1: "" + str(self.hands[0]) + ""\n""&#xa;        string += ""Player 2: "" + str(self.hands[1]) + ""\n""&#xa;        string += ""Top card: "" + str(self.topcard) + ""\n""&#xa;        string += ""Crib: "" + str(self.crib) + ""\n""&#xa;        return string&#xa;&#xa;    def __repr__(self):&#xa;        return str(self)&#xa;&#xa;&#xa;&#xa;class Card(object):&#xa;    def __init__(self, number, suit):&#xa;        self.number = number&#xa;        self.suit = suit&#xa;        self.name = str(number)&#xa;        self.played = False&#xa;        if number == 11:&#xa;            self.name = ""Jack""&#xa;        elif number == 12:&#xa;            self.name = ""Queen""&#xa;        elif number == 13:&#xa;            self.name = ""King""&#xa;        elif number == 1:&#xa;            self.name = ""Ace""&#xa;        self.value = min(number, 10)&#xa;&#xa;    def __str__(self):&#xa;        return self.name + "" of "" + self.suit&#xa;&#xa;    def __repr__(self):&#xa;        return str(self)&#xa;&#xa;def say(string):&#xa;    os.system(""say '"" + string + ""'"")&#xa;&#xa;&#xa;"
25433007|"# Support new str.format syntax in log messages&#xa;#&#xa;# Based on http://stackoverflow.com/a/25433007 and&#xa;# http://stackoverflow.com/a/26003573 and logging cookbook&#xa;# https://docs.python.org/3/howto/logging-cookbook.html#use-of-alternative-formatting-styles&#xa;#&#xa;# It's worth noting that this implementation has problems if key words&#xa;# used for brace substitution include level, msg, args, exc_info,&#xa;# extra or stack_info. These are argument names used by the log method&#xa;# of Logger. If you need to one of these names then modify process to&#xa;# exclude these names or just remove log_kwargs from the _log call. On&#xa;# a further note, this implementation also silently ignores misspelled&#xa;# keywords meant for the Logger (eg. ectra).&#xa;#&#xa;&#xa;&#xa;import logging&#xa;&#xa;&#xa;class NewStyleLogMessage(object):&#xa;    def __init__(self, message, *args, **kwargs):&#xa;        self.message = message&#xa;        self.args = args&#xa;        self.kwargs = kwargs&#xa;&#xa;    def __str__(self):&#xa;        args = (i() if callable(i) else i for i in self.args)&#xa;        kwargs = dict((k, v() if callable(v) else v)&#xa;                      for k, v in self.kwargs.items())&#xa;&#xa;        return self.message.format(*args, **kwargs)&#xa;&#xa;N = NewStyleLogMessage&#xa;&#xa;&#xa;class StyleAdapter(logging.LoggerAdapter):&#xa;    def __init__(self, logger, extra=None):&#xa;        super(StyleAdapter, self).__init__(logger, extra or {})&#xa;&#xa;    def log(self, level, msg, *args, **kwargs):&#xa;        if self.isEnabledFor(level):&#xa;            msg, log_kwargs = self.process(msg, kwargs)&#xa;            self.logger._log(level, N(msg, *args, **kwargs), (),&#xa;                             **log_kwargs)&#xa;&#xa;&#xa;logger = StyleAdapter(logging.getLogger(""project""))&#xa;#   Emits ""Lazily formatted log entry: 123 foo"" in log&#xa;# logger.debug('Lazily formatted entry: {0} {keyword}', 123, keyword='foo')&#xa;"
7218986|"# -*- coding: utf-8 -*-&#xa;from __future__ import unicode_literals&#xa;&#xa;from decimal import Decimal&#xa;from django.core.cache import cache&#xa;from django.db import models&#xa;from django.test import TestCase&#xa;from django.utils import unittest&#xa;from django.utils.translation import ugettext_lazy as _&#xa;from rest_framework import status, permissions&#xa;from rest_framework.compat import yaml, etree, patterns, url, include, six, StringIO&#xa;from rest_framework.response import Response&#xa;from rest_framework.views import APIView&#xa;from rest_framework.renderers import BaseRenderer, JSONRenderer, YAMLRenderer, \&#xa;    XMLRenderer, JSONPRenderer, BrowsableAPIRenderer, UnicodeJSONRenderer&#xa;from rest_framework.parsers import YAMLParser, XMLParser&#xa;from rest_framework.settings import api_settings&#xa;from rest_framework.test import APIRequestFactory&#xa;from collections import MutableMapping&#xa;import datetime&#xa;import json&#xa;import pickle&#xa;import re&#xa;&#xa;&#xa;DUMMYSTATUS = status.HTTP_200_OK&#xa;DUMMYCONTENT = 'dummycontent'&#xa;&#xa;RENDERER_A_SERIALIZER = lambda x: ('Renderer A: %s' % x).encode('ascii')&#xa;RENDERER_B_SERIALIZER = lambda x: ('Renderer B: %s' % x).encode('ascii')&#xa;&#xa;&#xa;expected_results = [&#xa;    ((elem for elem in [1, 2, 3]), JSONRenderer, b'[1, 2, 3]')  # Generator&#xa;]&#xa;&#xa;&#xa;class DummyTestModel(models.Model):&#xa;    name = models.CharField(max_length=42, default='')&#xa;&#xa;&#xa;class BasicRendererTests(TestCase):&#xa;    def test_expected_results(self):&#xa;        for value, renderer_cls, expected in expected_results:&#xa;            output = renderer_cls().render(value)&#xa;            self.assertEqual(output, expected)&#xa;&#xa;&#xa;class RendererA(BaseRenderer):&#xa;    media_type = 'mock/renderera'&#xa;    format = ""formata""&#xa;&#xa;    def render(self, data, media_type=None, renderer_context=None):&#xa;        return RENDERER_A_SERIALIZER(data)&#xa;&#xa;&#xa;class RendererB(BaseRenderer):&#xa;    media_type = 'mock/rendererb'&#xa;    format = ""formatb""&#xa;&#xa;    def render(self, data, media_type=None, renderer_context=None):&#xa;        return RENDERER_B_SERIALIZER(data)&#xa;&#xa;&#xa;class MockView(APIView):&#xa;    renderer_classes = (RendererA, RendererB)&#xa;&#xa;    def get(self, request, **kwargs):&#xa;        response = Response(DUMMYCONTENT, status=DUMMYSTATUS)&#xa;        return response&#xa;&#xa;&#xa;class MockGETView(APIView):&#xa;    def get(self, request, **kwargs):&#xa;        return Response({'foo': ['bar', 'baz']})&#xa;&#xa;&#xa;&#xa;class MockPOSTView(APIView):&#xa;    def post(self, request, **kwargs):&#xa;        return Response({'foo': request.DATA})&#xa;&#xa;&#xa;class EmptyGETView(APIView):&#xa;    renderer_classes = (JSONRenderer,)&#xa;&#xa;    def get(self, request, **kwargs):&#xa;        return Response(status=status.HTTP_204_NO_CONTENT)&#xa;&#xa;&#xa;class HTMLView(APIView):&#xa;    renderer_classes = (BrowsableAPIRenderer, )&#xa;&#xa;    def get(self, request, **kwargs):&#xa;        return Response('text')&#xa;&#xa;&#xa;class HTMLView1(APIView):&#xa;    renderer_classes = (BrowsableAPIRenderer, JSONRenderer)&#xa;&#xa;    def get(self, request, **kwargs):&#xa;        return Response('text')&#xa;&#xa;urlpatterns = patterns('',&#xa;    url(r'^.*\.(?P<format>.+)$', MockView.as_view(renderer_classes=[RendererA, RendererB])),&#xa;    url(r'^$', MockView.as_view(renderer_classes=[RendererA, RendererB])),&#xa;    url(r'^cache$', MockGETView.as_view()),&#xa;    url(r'^jsonp/jsonrenderer$', MockGETView.as_view(renderer_classes=[JSONRenderer, JSONPRenderer])),&#xa;    url(r'^jsonp/nojsonrenderer$', MockGETView.as_view(renderer_classes=[JSONPRenderer])),&#xa;    url(r'^parseerror$', MockPOSTView.as_view(renderer_classes=[JSONRenderer, BrowsableAPIRenderer])),&#xa;    url(r'^html$', HTMLView.as_view()),&#xa;    url(r'^html1$', HTMLView1.as_view()),&#xa;    url(r'^empty$', EmptyGETView.as_view()),&#xa;    url(r'^api', include('rest_framework.urls', namespace='rest_framework'))&#xa;)&#xa;&#xa;&#xa;class POSTDeniedPermission(permissions.BasePermission):&#xa;    def has_permission(self, request, view):&#xa;        return request.method != 'POST'&#xa;&#xa;&#xa;class POSTDeniedView(APIView):&#xa;    renderer_classes = (BrowsableAPIRenderer,)&#xa;    permission_classes = (POSTDeniedPermission,)&#xa;&#xa;    def get(self, request):&#xa;        return Response()&#xa;&#xa;    def post(self, request):&#xa;        return Response()&#xa;&#xa;    def put(self, request):&#xa;        return Response()&#xa;&#xa;    def patch(self, request):&#xa;        return Response()&#xa;&#xa;&#xa;class DocumentingRendererTests(TestCase):&#xa;    def test_only_permitted_forms_are_displayed(self):&#xa;        view = POSTDeniedView.as_view()&#xa;        request = APIRequestFactory().get('/')&#xa;        response = view(request).render()&#xa;        self.assertNotContains(response, '>POST<')&#xa;        self.assertContains(response, '>PUT<')&#xa;        self.assertContains(response, '>PATCH<')&#xa;&#xa;&#xa;class RendererEndToEndTests(TestCase):&#xa;    """"""&#xa;    End-to-end testing of renderers using an RendererMixin on a generic view.&#xa;    """"""&#xa;&#xa;    urls = 'rest_framework.tests.test_renderers'&#xa;&#xa;    def test_default_renderer_serializes_content(self):&#xa;        """"""If the Accept header is not set the default renderer should serialize the response.""""""&#xa;        resp = self.client.get('/')&#xa;        self.assertEqual(resp['Content-Type'], RendererA.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_A_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_head_method_serializes_no_content(self):&#xa;        """"""No response must be included in HEAD requests.""""""&#xa;        resp = self.client.head('/')&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;        self.assertEqual(resp['Content-Type'], RendererA.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, six.b(''))&#xa;&#xa;    def test_default_renderer_serializes_content_on_accept_any(self):&#xa;        """"""If the Accept header is set to */* the default renderer should serialize the response.""""""&#xa;        resp = self.client.get('/', HTTP_ACCEPT='*/*')&#xa;        self.assertEqual(resp['Content-Type'], RendererA.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_A_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_specified_renderer_serializes_content_default_case(self):&#xa;        """"""If the Accept header is set the specified renderer should serialize the response.&#xa;        (In this case we check that works for the default renderer)""""""&#xa;        resp = self.client.get('/', HTTP_ACCEPT=RendererA.media_type)&#xa;        self.assertEqual(resp['Content-Type'], RendererA.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_A_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_specified_renderer_serializes_content_non_default_case(self):&#xa;        """"""If the Accept header is set the specified renderer should serialize the response.&#xa;        (In this case we check that works for a non-default renderer)""""""&#xa;        resp = self.client.get('/', HTTP_ACCEPT=RendererB.media_type)&#xa;        self.assertEqual(resp['Content-Type'], RendererB.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_B_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_specified_renderer_serializes_content_on_accept_query(self):&#xa;        """"""The '_accept' query string should behave in the same way as the Accept header.""""""&#xa;        param = '?%s=%s' % (&#xa;            api_settings.URL_ACCEPT_OVERRIDE,&#xa;            RendererB.media_type&#xa;        )&#xa;        resp = self.client.get('/' + param)&#xa;        self.assertEqual(resp['Content-Type'], RendererB.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_B_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_unsatisfiable_accept_header_on_request_returns_406_status(self):&#xa;        """"""If the Accept header is unsatisfiable we should return a 406 Not Acceptable response.""""""&#xa;        resp = self.client.get('/', HTTP_ACCEPT='foo/bar')&#xa;        self.assertEqual(resp.status_code, status.HTTP_406_NOT_ACCEPTABLE)&#xa;&#xa;    def test_specified_renderer_serializes_content_on_format_query(self):&#xa;        """"""If a 'format' query is specified, the renderer with the matching&#xa;        format attribute should serialize the response.""""""&#xa;        param = '?%s=%s' % (&#xa;            api_settings.URL_FORMAT_OVERRIDE,&#xa;            RendererB.format&#xa;        )&#xa;        resp = self.client.get('/' + param)&#xa;        self.assertEqual(resp['Content-Type'], RendererB.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_B_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_specified_renderer_serializes_content_on_format_kwargs(self):&#xa;        """"""If a 'format' keyword arg is specified, the renderer with the matching&#xa;        format attribute should serialize the response.""""""&#xa;        resp = self.client.get('/something.formatb')&#xa;        self.assertEqual(resp['Content-Type'], RendererB.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_B_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_specified_renderer_is_used_on_format_query_with_matching_accept(self):&#xa;        """"""If both a 'format' query and a matching Accept header specified,&#xa;        the renderer with the matching format attribute should serialize the response.""""""&#xa;        param = '?%s=%s' % (&#xa;            api_settings.URL_FORMAT_OVERRIDE,&#xa;            RendererB.format&#xa;        )&#xa;        resp = self.client.get('/' + param,&#xa;                               HTTP_ACCEPT=RendererB.media_type)&#xa;        self.assertEqual(resp['Content-Type'], RendererB.media_type + '; charset=utf-8')&#xa;        self.assertEqual(resp.content, RENDERER_B_SERIALIZER(DUMMYCONTENT))&#xa;        self.assertEqual(resp.status_code, DUMMYSTATUS)&#xa;&#xa;    def test_parse_error_renderers_browsable_api(self):&#xa;        """"""Invalid data should still render the browsable API correctly.""""""&#xa;        resp = self.client.post('/parseerror', data='foobar', content_type='application/json', HTTP_ACCEPT='text/html')&#xa;        self.assertEqual(resp['Content-Type'], 'text/html; charset=utf-8')&#xa;        self.assertEqual(resp.status_code, status.HTTP_400_BAD_REQUEST)&#xa;&#xa;    def test_204_no_content_responses_have_no_content_type_set(self):&#xa;        """"""&#xa;        Regression test for #1196&#xa;&#xa;        https://github.com/tomchristie/django-rest-framework/issues/1196&#xa;        """"""&#xa;        resp = self.client.get('/empty')&#xa;        self.assertEqual(resp.get('Content-Type', None), None)&#xa;        self.assertEqual(resp.status_code, status.HTTP_204_NO_CONTENT)&#xa;&#xa;    def test_contains_headers_of_api_response(self):&#xa;        """"""&#xa;        Issue #1437&#xa;&#xa;        Test we display the headers of the API response and not those from the&#xa;        HTML response&#xa;        """"""&#xa;        resp = self.client.get('/html1')&#xa;        self.assertContains(resp, '>GET, HEAD, OPTIONS<')&#xa;        self.assertContains(resp, '>application/json<')&#xa;        self.assertNotContains(resp, '>text/html; charset=utf-8<')&#xa;&#xa;&#xa;_flat_repr = '{""foo"": [""bar"", ""baz""]}'&#xa;_indented_repr = '{\n  ""foo"": [\n    ""bar"",\n    ""baz""\n  ]\n}'&#xa;&#xa;&#xa;def strip_trailing_whitespace(content):&#xa;    """"""&#xa;    Seems to be some inconsistencies re. trailing whitespace with&#xa;    different versions of the json lib.&#xa;    """"""&#xa;    return re.sub(' +\n', '\n', content)&#xa;&#xa;&#xa;class JSONRendererTests(TestCase):&#xa;    """"""&#xa;    Tests specific to the JSON Renderer&#xa;    """"""&#xa;&#xa;    def test_render_lazy_strings(self):&#xa;        """"""&#xa;        JSONRenderer should deal with lazy translated strings.&#xa;        """"""&#xa;        ret = JSONRenderer().render(_('test'))&#xa;        self.assertEqual(ret, b'""test""')&#xa;&#xa;    def test_render_queryset_values(self):&#xa;        o = DummyTestModel.objects.create(name='dummy')&#xa;        qs = DummyTestModel.objects.values('id', 'name')&#xa;        ret = JSONRenderer().render(qs)&#xa;        data = json.loads(ret.decode('utf-8'))&#xa;        self.assertEquals(data, [{'id': o.id, 'name': o.name}])&#xa;&#xa;    def test_render_queryset_values_list(self):&#xa;        o = DummyTestModel.objects.create(name='dummy')&#xa;        qs = DummyTestModel.objects.values_list('id', 'name')&#xa;        ret = JSONRenderer().render(qs)&#xa;        data = json.loads(ret.decode('utf-8'))&#xa;        self.assertEquals(data, [[o.id, o.name]])&#xa;&#xa;    def test_render_dict_abc_obj(self):&#xa;        class Dict(MutableMapping):&#xa;            def __init__(self):&#xa;                self._dict = dict()&#xa;            def __getitem__(self, key):&#xa;                return self._dict.__getitem__(key)&#xa;            def __setitem__(self, key, value):&#xa;                return self._dict.__setitem__(key, value)&#xa;            def __delitem__(self, key):&#xa;                return self._dict.__delitem__(key)&#xa;            def __iter__(self):&#xa;                return self._dict.__iter__()&#xa;            def __len__(self):&#xa;                return self._dict.__len__()&#xa;            def keys(self):&#xa;                return self._dict.keys()&#xa;&#xa;        x = Dict()&#xa;        x['key'] = 'string value'&#xa;        x[2] = 3&#xa;        ret = JSONRenderer().render(x)&#xa;        data = json.loads(ret.decode('utf-8'))&#xa;        self.assertEquals(data, {'key': 'string value', '2': 3})    &#xa;&#xa;    def test_render_obj_with_getitem(self):&#xa;        class DictLike(object):&#xa;            def __init__(self):&#xa;                self._dict = {}&#xa;            def set(self, value):&#xa;                self._dict = dict(value)&#xa;            def __getitem__(self, key):&#xa;                return self._dict[key]&#xa;            &#xa;        x = DictLike()&#xa;        x.set({'a': 1, 'b': 'string'})&#xa;        with self.assertRaises(TypeError):&#xa;            JSONRenderer().render(x)&#xa;        &#xa;    def test_without_content_type_args(self):&#xa;        """"""&#xa;        Test basic JSON rendering.&#xa;        """"""&#xa;        obj = {'foo': ['bar', 'baz']}&#xa;        renderer = JSONRenderer()&#xa;        content = renderer.render(obj, 'application/json')&#xa;        # Fix failing test case which depends on version of JSON library.&#xa;        self.assertEqual(content.decode('utf-8'), _flat_repr)&#xa;&#xa;    def test_with_content_type_args(self):&#xa;        """"""&#xa;        Test JSON rendering with additional content type arguments supplied.&#xa;        """"""&#xa;        obj = {'foo': ['bar', 'baz']}&#xa;        renderer = JSONRenderer()&#xa;        content = renderer.render(obj, 'application/json; indent=2')&#xa;        self.assertEqual(strip_trailing_whitespace(content.decode('utf-8')), _indented_repr)&#xa;&#xa;    def test_check_ascii(self):&#xa;        obj = {'countries': ['United Kingdom', 'France', 'España']}&#xa;        renderer = JSONRenderer()&#xa;        content = renderer.render(obj, 'application/json')&#xa;        self.assertEqual(content, '{""countries"": [""United Kingdom"", ""France"", ""Espa\\u00f1a""]}'.encode('utf-8'))&#xa;&#xa;&#xa;class UnicodeJSONRendererTests(TestCase):&#xa;    """"""&#xa;    Tests specific for the Unicode JSON Renderer&#xa;    """"""&#xa;    def test_proper_encoding(self):&#xa;        obj = {'countries': ['United Kingdom', 'France', 'España']}&#xa;        renderer = UnicodeJSONRenderer()&#xa;        content = renderer.render(obj, 'application/json')&#xa;        self.assertEqual(content, '{""countries"": [""United Kingdom"", ""France"", ""España""]}'.encode('utf-8'))&#xa;&#xa;&#xa;class JSONPRendererTests(TestCase):&#xa;    """"""&#xa;    Tests specific to the JSONP Renderer&#xa;    """"""&#xa;&#xa;    urls = 'rest_framework.tests.test_renderers'&#xa;&#xa;    def test_without_callback_with_json_renderer(self):&#xa;        """"""&#xa;        Test JSONP rendering with View JSON Renderer.&#xa;        """"""&#xa;        resp = self.client.get('/jsonp/jsonrenderer',&#xa;                               HTTP_ACCEPT='application/javascript')&#xa;        self.assertEqual(resp.status_code, status.HTTP_200_OK)&#xa;        self.assertEqual(resp['Content-Type'], 'application/javascript; charset=utf-8')&#xa;        self.assertEqual(resp.content,&#xa;            ('callback(%s);' % _flat_repr).encode('ascii'))&#xa;&#xa;    def test_without_callback_without_json_renderer(self):&#xa;        """"""&#xa;        Test JSONP rendering without View JSON Renderer.&#xa;        """"""&#xa;        resp = self.client.get('/jsonp/nojsonrenderer',&#xa;                               HTTP_ACCEPT='application/javascript')&#xa;        self.assertEqual(resp.status_code, status.HTTP_200_OK)&#xa;        self.assertEqual(resp['Content-Type'], 'application/javascript; charset=utf-8')&#xa;        self.assertEqual(resp.content,&#xa;            ('callback(%s);' % _flat_repr).encode('ascii'))&#xa;&#xa;    def test_with_callback(self):&#xa;        """"""&#xa;        Test JSONP rendering with callback function name.&#xa;        """"""&#xa;        callback_func = 'myjsonpcallback'&#xa;        resp = self.client.get('/jsonp/nojsonrenderer?callback=' + callback_func,&#xa;                               HTTP_ACCEPT='application/javascript')&#xa;        self.assertEqual(resp.status_code, status.HTTP_200_OK)&#xa;        self.assertEqual(resp['Content-Type'], 'application/javascript; charset=utf-8')&#xa;        self.assertEqual(resp.content,&#xa;            ('%s(%s);' % (callback_func, _flat_repr)).encode('ascii'))&#xa;&#xa;&#xa;if yaml:&#xa;    _yaml_repr = 'foo: [bar, baz]\n'&#xa;&#xa;    class YAMLRendererTests(TestCase):&#xa;        """"""&#xa;        Tests specific to the YAML Renderer&#xa;        """"""&#xa;&#xa;        def test_render(self):&#xa;            """"""&#xa;            Test basic YAML rendering.&#xa;            """"""&#xa;            obj = {'foo': ['bar', 'baz']}&#xa;            renderer = YAMLRenderer()&#xa;            content = renderer.render(obj, 'application/yaml')&#xa;            self.assertEqual(content, _yaml_repr)&#xa;&#xa;        def test_render_and_parse(self):&#xa;            """"""&#xa;            Test rendering and then parsing returns the original object.&#xa;            IE obj -> render -> parse -> obj.&#xa;            """"""&#xa;            obj = {'foo': ['bar', 'baz']}&#xa;&#xa;            renderer = YAMLRenderer()&#xa;            parser = YAMLParser()&#xa;&#xa;            content = renderer.render(obj, 'application/yaml')&#xa;            data = parser.parse(StringIO(content))&#xa;            self.assertEqual(obj, data)&#xa;&#xa;        def test_render_decimal(self):&#xa;            """"""&#xa;            Test YAML decimal rendering.&#xa;            """"""&#xa;            renderer = YAMLRenderer()&#xa;            content = renderer.render({'field': Decimal('111.2')}, 'application/yaml')&#xa;            self.assertYAMLContains(content, ""field: '111.2'"")&#xa;&#xa;        def assertYAMLContains(self, content, string):&#xa;            self.assertTrue(string in content, '%r not in %r' % (string, content))&#xa;&#xa;&#xa;class XMLRendererTestCase(TestCase):&#xa;    """"""&#xa;    Tests specific to the XML Renderer&#xa;    """"""&#xa;&#xa;    _complex_data = {&#xa;        ""creation_date"": datetime.datetime(2011, 12, 25, 12, 45, 00),&#xa;        ""name"": ""name"",&#xa;        ""sub_data_list"": [&#xa;            {&#xa;                ""sub_id"": 1,&#xa;                ""sub_name"": ""first""&#xa;            },&#xa;            {&#xa;                ""sub_id"": 2,&#xa;                ""sub_name"": ""second""&#xa;            }&#xa;        ]&#xa;    }&#xa;&#xa;    def test_render_string(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({'field': 'astring'}, 'application/xml')&#xa;        self.assertXMLContains(content, '<field>astring</field>')&#xa;&#xa;    def test_render_integer(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({'field': 111}, 'application/xml')&#xa;        self.assertXMLContains(content, '<field>111</field>')&#xa;&#xa;    def test_render_datetime(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({&#xa;            'field': datetime.datetime(2011, 12, 25, 12, 45, 00)&#xa;        }, 'application/xml')&#xa;        self.assertXMLContains(content, '<field>2011-12-25 12:45:00</field>')&#xa;&#xa;    def test_render_float(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({'field': 123.4}, 'application/xml')&#xa;        self.assertXMLContains(content, '<field>123.4</field>')&#xa;&#xa;    def test_render_decimal(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({'field': Decimal('111.2')}, 'application/xml')&#xa;        self.assertXMLContains(content, '<field>111.2</field>')&#xa;&#xa;    def test_render_none(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render({'field': None}, 'application/xml')&#xa;        self.assertXMLContains(content, '<field></field>')&#xa;&#xa;    def test_render_complex_data(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = renderer.render(self._complex_data, 'application/xml')&#xa;        self.assertXMLContains(content, '<sub_name>first</sub_name>')&#xa;        self.assertXMLContains(content, '<sub_name>second</sub_name>')&#xa;&#xa;    @unittest.skipUnless(etree, 'defusedxml not installed')&#xa;    def test_render_and_parse_complex_data(self):&#xa;        """"""&#xa;        Test XML rendering.&#xa;        """"""&#xa;        renderer = XMLRenderer()&#xa;        content = StringIO(renderer.render(self._complex_data, 'application/xml'))&#xa;&#xa;        parser = XMLParser()&#xa;        complex_data_out = parser.parse(content)&#xa;        error_msg = ""complex data differs!IN:\n %s \n\n OUT:\n %s"" % (repr(self._complex_data), repr(complex_data_out))&#xa;        self.assertEqual(self._complex_data, complex_data_out, error_msg)&#xa;&#xa;    def assertXMLContains(self, xml, string):&#xa;        self.assertTrue(xml.startswith('<?xml version=""1.0"" encoding=""utf-8""?>\n<root>'))&#xa;        self.assertTrue(xml.endswith('</root>'))&#xa;        self.assertTrue(string in xml, '%r not in %r' % (string, xml))&#xa;&#xa;&#xa;# Tests for caching issue, #346&#xa;class CacheRenderTest(TestCase):&#xa;    """"""&#xa;    Tests specific to caching responses&#xa;    """"""&#xa;&#xa;    urls = 'rest_framework.tests.test_renderers'&#xa;&#xa;    cache_key = 'just_a_cache_key'&#xa;&#xa;    @classmethod&#xa;    def _get_pickling_errors(cls, obj, seen=None):&#xa;        """""" Return any errors that would be raised if `obj' is pickled&#xa;        Courtesy of koffie @ http://stackoverflow.com/a/7218986/109897&#xa;        """"""&#xa;        if seen == None:&#xa;            seen = []&#xa;        try:&#xa;            state = obj.__getstate__()&#xa;        except AttributeError:&#xa;            return&#xa;        if state == None:&#xa;            return&#xa;        if isinstance(state, tuple):&#xa;            if not isinstance(state[0], dict):&#xa;                state = state[1]&#xa;            else:&#xa;                state = state[0].update(state[1])&#xa;        result = {}&#xa;        for i in state:&#xa;            try:&#xa;                pickle.dumps(state[i], protocol=2)&#xa;            except pickle.PicklingError:&#xa;                if not state[i] in seen:&#xa;                    seen.append(state[i])&#xa;                    result[i] = cls._get_pickling_errors(state[i], seen)&#xa;        return result&#xa;&#xa;    def http_resp(self, http_method, url):&#xa;        """"""&#xa;        Simple wrapper for Client http requests&#xa;        Removes the `client' and `request' attributes from as they are&#xa;        added by django.test.client.Client and not part of caching&#xa;        responses outside of tests.&#xa;        """"""&#xa;        method = getattr(self.client, http_method)&#xa;        resp = method(url)&#xa;        del resp.client, resp.request&#xa;        try:&#xa;            del resp.wsgi_request&#xa;        except AttributeError:&#xa;            pass&#xa;        return resp&#xa;&#xa;    def test_obj_pickling(self):&#xa;        """"""&#xa;        Test that responses are properly pickled&#xa;        """"""&#xa;        resp = self.http_resp('get', '/cache')&#xa;&#xa;        # Make sure that no pickling errors occurred&#xa;        self.assertEqual(self._get_pickling_errors(resp), {})&#xa;&#xa;        # Unfortunately LocMem backend doesn't raise PickleErrors but returns&#xa;        # None instead.&#xa;        cache.set(self.cache_key, resp)&#xa;        self.assertTrue(cache.get(self.cache_key) is not None)&#xa;&#xa;    def test_head_caching(self):&#xa;        """"""&#xa;        Test caching of HEAD requests&#xa;        """"""&#xa;        resp = self.http_resp('head', '/cache')&#xa;        cache.set(self.cache_key, resp)&#xa;&#xa;        cached_resp = cache.get(self.cache_key)&#xa;        self.assertIsInstance(cached_resp, Response)&#xa;&#xa;    def test_get_caching(self):&#xa;        """"""&#xa;        Test caching of GET requests&#xa;        """"""&#xa;        resp = self.http_resp('get', '/cache')&#xa;        cache.set(self.cache_key, resp)&#xa;&#xa;        cached_resp = cache.get(self.cache_key)&#xa;        self.assertIsInstance(cached_resp, Response)&#xa;        self.assertEqual(cached_resp.content, resp.content)&#xa;"
3071415|"&#xa;import argparse&#xa;import commands&#xa;import getpass&#xa;import itertools&#xa;import numpy&#xa;import os&#xa;import os.path&#xa;import sys&#xa;import time&#xa;&#xa;from env import gidgetConfigVars&#xa;import miscIO&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;debugFlag = 0&#xa;&#xa;# order returns the order of each element in x as a list&#xa;&#xa;&#xa;def order(x):&#xa;    if (debugFlag):&#xa;        print "" in FUNC order() ... ""&#xa;    L = len(x)&#xa;    rangeL = range(L)&#xa;    z = itertools.izip(x, rangeL)&#xa;    z = itertools.izip(z, rangeL)    # avoid problems with duplicates&#xa;    D = sorted(z)&#xa;    return [d[1] for d in D]&#xa;&#xa;# rank returns the rankings of the elements in x as a list&#xa;&#xa;&#xa;def rank(x):&#xa;    if (debugFlag):&#xa;        print "" in FUNC rank() ... ""&#xa;    L = len(x)&#xa;    ordering = order(x)&#xa;    ranks = [0] * L&#xa;    for i in range(L):&#xa;        ranks[ordering[i]] = i&#xa;    return ranks&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# new 28-jan-2013 from&#xa;# http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python&#xa;&#xa;&#xa;def rank_simple(vector):&#xa;    return sorted(range(len(vector)), key=vector.__getitem__)&#xa;&#xa;&#xa;def rankdata(a):&#xa;    n = len(a)&#xa;    ivec = rank_simple(a)&#xa;    svec = [a[rank] for rank in ivec]&#xa;    sumranks = 0&#xa;    dupcount = 0&#xa;    newarray = [0] * n&#xa;    for i in xrange(n):&#xa;        sumranks += i&#xa;        dupcount += 1&#xa;        if i == n - 1 or svec[i] != svec[i + 1]:&#xa;            averank = sumranks / float(dupcount) + 1&#xa;            for j in xrange(i - dupcount + 1, i + 1):&#xa;                newarray[ivec[j]] = averank&#xa;            sumranks = 0&#xa;            dupcount = 0&#xa;&#xa;    print "" returning from rankdata ... "", min(newarray), max(newarray)&#xa;&#xa;    return newarray&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def countSamples(tokenList):&#xa;&#xa;    numSamp = 0&#xa;    for aTok in tokenList:&#xa;        if (aTok != ""NA""):&#xa;            numSamp += 1&#xa;&#xa;    return (numSamp)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getClinSampFeat(featureMatrixFile, min_samples):&#xa;&#xa;    fh = file(featureMatrixFile)&#xa;&#xa;    indexList = []&#xa;    featList = []&#xa;&#xa;    lineNo = 0&#xa;    for aLine in fh:&#xa;        aLine = aLine.strip()&#xa;        if (lineNo > 0):&#xa;            tokenList = aLine.split('\t')&#xa;            featName = tokenList[0]&#xa;            if (featName.find("":CLIN:"") > 0):&#xa;                numSamp = countSamples(tokenList)&#xa;                if (numSamp >= min_samples):&#xa;                    indexList += [lineNo - 1]&#xa;                    featList += [featName]&#xa;                else:&#xa;                    print "" skipping feature <%s> due to low counts (%d) "" % (featName, numSamp)&#xa;            elif (featName.find("":SAMP:"") > 0):&#xa;                numSamp = countSamples(tokenList)&#xa;                if (numSamp >= min_samples):&#xa;                    indexList += [lineNo - 1]&#xa;                    featList += [featName]&#xa;                else:&#xa;                    print "" skipping feature <%s> due to low counts (%d) "" % (featName, numSamp)&#xa;        lineNo += 1&#xa;&#xa;    fh.close()&#xa;&#xa;    return (indexList, featList)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getNumSamples(featureMatrixFile):&#xa;&#xa;    fh = file(featureMatrixFile)&#xa;    numCols = miscIO.num_cols(fh, '\t')&#xa;    numSamples = numCols - 1&#xa;    fh.close()&#xa;&#xa;    return (numSamples)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# input file is assumed to end in .tsv&#xa;# this function checks to see if the binFile exists and is up to date&#xa;# with respect to the tsvFile ... if necessary, it will call prep4pairwise&#xa;# to create the bin file&#xa;&#xa;&#xa;def preProcessTSV(tsvFile):&#xa;&#xa;    tsvTime = os.path.getmtime(tsvFile)&#xa;    # print tsvTime&#xa;&#xa;    binFile = tsvFile[:-4] + "".bin""&#xa;    catFile = tsvFile[:-4] + "".cat""&#xa;    try:&#xa;        binTime = os.path.getmtime(binFile)&#xa;        # print binTime&#xa;    except:&#xa;        binTime = 0&#xa;&#xa;    if (tsvTime > binTime):&#xa;&#xa;        # just to be sure, delete the *.bin and *.cat files ...&#xa;        cmdString = ""rm -fr %s"" % binFile&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        cmdString = ""rm -fr %s"" % catFile&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;&#xa;        print "" creating bin file ""&#xa;        cmdString = ""%s %s/prep4pairwise.py %s"" % (gidgetConfigVars['TCGAFMP_PYTHON3'], gidgetConfigVars['TCGAFMP_PAIRWISE_ROOT'], tsvFile)&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        if (status != 0):&#xa;            print "" ERROR ??? failed to execute command ??? ""&#xa;            print cmdString&#xa;            sys.exit(-1)&#xa;    else:&#xa;        print "" bin file already up to date ""&#xa;&#xa;    return (binFile)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getTypeRanks(aType, tmpDir, indexList, featList):&#xa;&#xa;    print "" ""&#xa;    print "" in getTypeRanks ... "", aType&#xa;&#xa;    outNames = []&#xa;    dTypeScores = []&#xa;    maxNum = 50000&#xa;&#xa;    pCounts = {}&#xa;&#xa;    for kk in range(len(indexList)):&#xa;&#xa;        index = indexList[kk]&#xa;        featName = featList[kk]&#xa;&#xa;        # first, open the previously generated pairwise file for this index&#xa;        # (feature)&#xa;        pwFile = tmpDir + ""/%d.pw"" % index&#xa;        try:&#xa;            fh = file(pwFile, 'r')&#xa;        except:&#xa;            continue&#xa;&#xa;        # print ""         --> reading input file <%s> "" % pwFile&#xa;&#xa;        # initialze number of p-values&#xa;        pCounts[featName] = 0&#xa;&#xa;        # allocate a vector of zeros&#xa;        dTypeVec = numpy.zeros(maxNum)&#xa;        iG = 0&#xa;&#xa;        # read through the input file, line by line, and keep only&#xa;        # those pairs that involve the current data type&#xa;&#xa;        # each line should look something like this:&#xa;        # <featA> <featB> <??>  +0.753   186   +9.236   0   -0.000   0   -0.000   <*>&#xa;&#xa;        for aLine in fh:&#xa;            if (aLine.startswith(""##"")):&#xa;                continue&#xa;            if (aLine.find(aType) >= 0):&#xa;                tokenList = aLine.split('\t')&#xa;                # check that the # of samples involved in the pairwise test was&#xa;                # at least 20 ...&#xa;                if (int(tokenList[4]) < 20):&#xa;                    continue&#xa;&#xa;                # and then grab the -log(p)&#xa;                pValue = float(tokenList[5])&#xa;                dTypeVec[iG] = pValue&#xa;                iG += 1&#xa;&#xa;        # close the file ...&#xa;        fh.close()&#xa;&#xa;        outNames += [featName]&#xa;&#xa;        if (iG > 0):&#xa;            # print ""         --> got %d p-values "" % iG&#xa;            pCounts[featName] += iG&#xa;            try:&#xa;                dTypeVec = dTypeVec[:iG]&#xa;                dTypeVec.sort()&#xa;                aScore = 0.&#xa;                for p in [0.80, 0.85, 0.90, 0.95]:&#xa;                    iG = int(p * len(dTypeVec))&#xa;                    aScore += dTypeVec[iG]&#xa;                # print index, featName, ""GEXP"", aScore, len(dTypeVec)&#xa;                dTypeScores += [aScore]&#xa;            except:&#xa;                dTypeScores += [0]&#xa;        else:&#xa;            # print ""         --> did NOT get any p-values ""&#xa;            dTypeScores += [0]&#xa;&#xa;    print "" ""&#xa;    print "" got this far ... returning from getTypeRanks ""&#xa;    print ""     names   : "", outNames[:5], outNames[-5:]&#xa;    print ""     scores  : "", dTypeScores[:5], dTypeScores[-5:]&#xa;    if (0):&#xa;        dTypeRanks = rank(dTypeScores)&#xa;    else:&#xa;        dTypeRanks = rankdata(dTypeScores)&#xa;    print ""     ranks   : "", dTypeRanks[:5], dTypeRanks[-5:]&#xa;&#xa;    print "" ""&#xa;    pKeys = pCounts.keys()&#xa;    for ii in range(min(10, len(pKeys))):&#xa;        print ""         pCounts[%s] = %d "" % (pKeys[ii], pCounts[pKeys[ii]])&#xa;    print "" ""&#xa;&#xa;    return (outNames, dTypeScores, dTypeRanks, pCounts)&#xa;&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;def makeOutFileName(tsvFile):&#xa;&#xa;    ii = len(tsvFile) - 1&#xa;    while (tsvFile[ii] != '/'):&#xa;        ii -= 1&#xa;&#xa;    # the output file name will look just like the input tsv file name,&#xa;    # but it will start with ""featScores_"" and end with "".txt"" rather&#xa;    # than "".tsv""&#xa;    outName = tsvFile[:ii] + '/' + ""featScores_"" + tsvFile[ii + 1:-3] + ""txt""&#xa;    return (outName)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;#&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    # ALL necessary inputs should be handled using this ArgumentParser ... there shouldn't&#xa;    # be any 'left-over' arguments ... any unrecognized command-line inputs will result&#xa;    # in an error like:&#xa;    # rkpw_list_gen.py: error: unrecognized arguments: abc def&#xa;&#xa;    parser = argparse.ArgumentParser(&#xa;        description='Create runlist for pairwise')&#xa;    parser.add_argument('--min-ct-cell', '-minct',&#xa;                        action='store', default=5, type=int)&#xa;    parser.add_argument('--min-mx-cell', '-minmx',&#xa;                        action='store', default=5, type=int)&#xa;    parser.add_argument('--min-samples', '-M',&#xa;                        action='store', default=30, type=int)&#xa;    parser.add_argument('--verbosity', '-v',&#xa;                        action='store', default=0, type=int)&#xa;    parser.add_argument('--tsvFile', '-f', action='store', required=True)&#xa;    ## parser.add_argument ( '--runFile', '-r', action='store', required=True )&#xa;&#xa;    args = parser.parse_args()&#xa;    print args&#xa;&#xa;    # force user to be using the 'cncrreg' group to run this ...&#xa;    if (0):&#xa;        cmdString = ""newgrp cncrreg""&#xa;        print "" trying to force group ... ""&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        print "" back from that ... ""&#xa;&#xa;    # at this point we should have a Namespace called 'args' that looks something like this:&#xa;    # Namespace ( tsvFile=['test.tsv'],&#xa;    # runFile=['test.run'],&#xa;    ##		   byname=False, input=None,&#xa;    # min_ct_cell=5,&#xa;    # tail=0, verbosity=0 )&#xa;&#xa;    # get the tsv feature matrix file and also the number of features it&#xa;    # contains&#xa;    tsvFile = args.tsvFile&#xa;    print "" input tsv file name <%s> "" % tsvFile&#xa;    if (not os.path.exists(tsvFile)):&#xa;        print "" <%s> is not a valid file, exiting ... "" % tsvFile&#xa;        sys.exit(-1)&#xa;    if (not tsvFile.endswith("".tsv"")):&#xa;        print "" <%s> input file should be a TSV file "" % tsvFile&#xa;        sys.exit(-1)&#xa;    if (tsvFile[0] != ""/""):&#xa;        print "" absolute path name for input file <%s> is required "" % tsvFile&#xa;        sys.exit(-1)&#xa;    (indexList, featList) = getClinSampFeat(tsvFile, args.min_samples)&#xa;&#xa;    if (len(indexList) < 5):&#xa;        print "" ERROR ... does not seem worth continuing ... ""&#xa;        print indexList&#xa;        print featList&#xa;        sys.exit(-1)&#xa;&#xa;    print "" --> number of features : "", len(indexList)&#xa;    numSamples = getNumSamples(tsvFile)&#xa;    print "" --> number of samples  : "", numSamples&#xa;&#xa;    # we need to pre-process the tsv file (unless it appears to have already&#xa;    # been done)&#xa;    binFile = preProcessTSV(tsvFile)&#xa;&#xa;    # create a random name for this particular run ...&#xa;    # and then make a subdirectory for the outputs ...&#xa;    curJobName = miscIO.make_random_fname()&#xa;    print "" ""&#xa;    print "" randomly generated job name : <%s> "" % curJobName&#xa;    print "" ""&#xa;&#xa;    tmpDir = ""%s/%s"" % (gidgetConfigVars['TCGAFMP_CLUSTER_SCRATCH'], curJobName)&#xa;    cmdString = ""mkdir %s"" % tmpDir&#xa;    (status, output) = commands.getstatusoutput(cmdString)&#xa;    if (not os.path.exists(tmpDir)):&#xa;        print "" mkdir command failed ??? ""&#xa;        print cmdString&#xa;        sys.exit(-1)&#xa;&#xa;    # write the jobInfo file ...&#xa;    jobFile = tmpDir + ""/jobInfo.txt""&#xa;    try:&#xa;        fh = file(jobFile, 'w')&#xa;    except:&#xa;        print "" failed to open output file <%s>, exiting ... "" % jobFile&#xa;        sys.exit(-1)&#xa;    fh.write(""tsvFile = %s\n"" % args.tsvFile)&#xa;    fh.close()&#xa;&#xa;    # open the runFile ...&#xa;    runFile = tmpDir + ""/runList.txt""&#xa;    try:&#xa;        fh = file(runFile, 'w')&#xa;    except:&#xa;        print "" failed to open output file <%s>, exiting ... "" % runFile&#xa;        sys.exit(-1)&#xa;&#xa;    pythonbin = sys.executable&#xa;&#xa;    golempwd = ""PASSWD_HERE""&#xa;    fhC = file ( gidgetConfigVars['TCGAFMP_CLUSTER_SCRATCH'] + ""/config"", 'r' )&#xa;    aLine = fhC.readline()&#xa;    fhC.close()&#xa;    aLine = aLine.strip()&#xa;    golempwd = aLine&#xa;    print "" got this ... <%s> "" % golempwd&#xa;&#xa;    numJobs = 0&#xa;    for index in indexList:&#xa;        outName = tmpDir + ""/"" + str(index) + "".pw""&#xa;        cmdString = ""1 "" + gidgetConfigVars['TCGAFMP_PAIRWISE_ROOT'] + ""/pairwise-1.1.2""&#xa;        cmdString += "" --pvalue 1. --min-ct-cell %d --min-mx-cell %d --min-samples %d"" \&#xa;            % (args.min_ct_cell, args.min_mx_cell, args.min_samples)&#xa;        cmdString += "" --outer %d:%d:1 --inner 0::1  %s  %s "" \&#xa;            % (index, index + 1, binFile, outName)&#xa;        fh.write(""%s\n"" % cmdString)&#xa;        numJobs += 1&#xa;&#xa;    fh.close()&#xa;&#xa;    # ok, now we want to actually launch the jobs ...&#xa;    cmdString = ""python "" + gidgetConfigVars['TCGAFMP_ROOT_DIR'] + ""/main/golem.py ""&#xa;    cmdString += ""http://glados.systemsbiology.net:7083 -p "" + golempwd + "" ""&#xa;    cmdString += ""-L scoreCatFeat -u ""&#xa;    cmdString += getpass.getuser() + "" ""&#xa;    cmdString += ""runlist "" + runFile&#xa;    print cmdString&#xa;    (status, output) = commands.getstatusoutput(cmdString)&#xa;    print status&#xa;    print output&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" --------------- ""&#xa;&#xa;    done = 0&#xa;    lastCheck = -1&#xa;    noChange = 0&#xa;    while not done:&#xa;&#xa;        ## count up the number of output files ...&#xa;        numOutFiles = 0&#xa;        for aName in os.listdir(tmpDir):&#xa;            if (aName.endswith("".pw"")):&#xa;                numOutFiles += 1&#xa;        print numOutFiles&#xa;&#xa;        ## if the number of output files matches the&#xa;        ## number of jobs, we're good to go&#xa;        if (numOutFiles == numJobs): done = 1&#xa;&#xa;        ## if this count has not changed in a while,&#xa;        ## they we probably want to bail ...&#xa;        if ( lastCheck == numOutFiles ):&#xa;            noChange += 1&#xa;        if ( noChange > 5 ): done = 1&#xa;        lastCheck = numOutFiles&#xa;&#xa;        time.sleep(10)&#xa;&#xa;    print "" should be done !!! "", numOutFiles, numJobs&#xa;&#xa;    # now we are ready for the post-processing ...&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    featScores = {}&#xa;    pCounts = {}&#xa;&#xa;    # we are looking for associations with 5 different molecular data types:&#xa;    typeList = [""N:GEXP:"", ""N:RPPA:"", ""N:METH:"", ""N:MIRN:"", ""N:CNVR:""]&#xa;    for aType in typeList:&#xa;        print "" aType : "", aType&#xa;&#xa;        outNames = []&#xa;&#xa;        (outNames, typeScores, typeRanks, pTmp) = getTypeRanks (aType, tmpDir, indexList, featList)&#xa;&#xa;        # at this point we have:&#xa;        # names : vector of feature names&#xa;        # typeScores : vector of scores (a higher score is better)&#xa;        # typeRanks  : vector of ranks (a higher rank is better)&#xa;        # pTmp : # of p-values considered for each feature name (this is a&#xa;        # dictionary with the feature name as the key)&#xa;&#xa;# for iR in range(len(typeRanks)):&#xa;####	    jR = len(typeRanks) - iR - 1&#xa;####	    kR = typeRanks.index(jR)&#xa;# print iR, jR, kR, outNames[kR], typeScores[kR]&#xa;# if ( outNames[kR] in featScores.keys() ):&#xa;####	        featScores[outNames[kR]] += [ jR ]&#xa;####		pCounts[outNames[kR]] += pTmp[outNames[kR]]&#xa;# else:&#xa;####		featScores[outNames[kR]] = [ jR ]&#xa;####		pCounts[outNames[kR]] = pTmp[outNames[kR]]&#xa;&#xa;        for iR in range(len(outNames)):&#xa;            if (outNames[iR] in featScores.keys()):&#xa;                featScores[outNames[iR]] += [typeRanks[iR]]&#xa;                pCounts[outNames[iR]] += pTmp[outNames[iR]]&#xa;            else:&#xa;                featScores[outNames[iR]] = [typeRanks[iR]]&#xa;                pCounts[outNames[iR]] = pTmp[outNames[iR]]&#xa;&#xa;        # now we have&#xa;        # featScores{} : this is now based on the *rank* (rather than the *score* which was based on p-values)&#xa;        # pCounts{}    : this keeps track of how many p-values contributed to&#xa;        # this rank/score&#xa;&#xa;        print "" ""&#xa;        keyList = featScores.keys()&#xa;        for ii in range(min(10, len(keyList))):&#xa;            print keyList[ii], featScores[keyList[ii]], pCounts[keyList[ii]]&#xa;        print "" ""&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" FINISHED looping over typeList ... ""&#xa;&#xa;    if (1):&#xa;        print "" ""&#xa;        print "" ""&#xa;        for aName in featScores.keys():&#xa;            print aName, featScores[aName], pCounts[aName]&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    # sum up the scores ...&#xa;    nameList = featScores.keys()&#xa;    sumScores = []&#xa;    for aName in nameList:&#xa;        aSum = 0&#xa;        for aScore in featScores[aName]:&#xa;            aSum += aScore&#xa;        sumScores += [aSum]&#xa;&#xa;    # also, find the median pCounts value, and reset the scores for those to 0&#xa;    # ...&#xa;    pList = []&#xa;    for aName in nameList:&#xa;        pList += [pCounts[aName]]&#xa;    pList.sort()&#xa;    nHalf = len(pList) / 2&#xa;    pMedian = pList[nHalf]&#xa;    pMin = pMedian / 2&#xa;    if (pMin == 0):&#xa;        pMin = 1&#xa;    print "" pMin = "", pMin&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    if (1):&#xa;        scoreRanks = rank(sumScores)&#xa;    else:&#xa;        scoreRanks = rankdata(sumScores)&#xa;    newScores = [0] * len(sumScores)&#xa;    maxScore = len(typeList) * len(nameList)&#xa;    for iR in range(len(sumScores)):&#xa;        jR = len(sumScores) - iR - 1&#xa;        kR = scoreRanks.index(jR)&#xa;        aName = nameList[kR]&#xa;        # print iR, jR, kR, nameList[kR], sumScores[kR], (&#xa;        # float(sumScores[kR])/float(maxScore) )&#xa;        if (pCounts[aName] >= pMin):&#xa;            # this line looks like this:&#xa;            # 57 48724 B_CLIN_person_neoplasm_cancer_status 338 0.140248962656&#xa;            print jR, pCounts[aName], aName, sumScores[kR], (float(sumScores[kR]) / float(maxScore))&#xa;            newScores[kR] = sumScores[kR]&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    ## outFile = tmpDir + ""/featScores.tsv""&#xa;    outFile = makeOutFileName(tsvFile)&#xa;    print "" --> opening output file <%s> "" % outFile&#xa;    fh = file(outFile, 'w')&#xa;&#xa;    # finally we want to pretty print the output ...&#xa;    numOut = 0&#xa;    for iR in range(len(sumScores)):&#xa;        jR = len(sumScores) - iR - 1&#xa;        kR = scoreRanks.index(jR)&#xa;        aName = nameList[kR]&#xa;        nameTokens = aName.split(':')&#xa;        shortName = nameTokens[2]&#xa;        featName = aName&#xa;        typeName = aName[2:6]&#xa;        if (newScores[kR] > 0):&#xa;            fh.write(""%s\t%s\t%s\t%.3f\n"" %&#xa;                     (shortName, featName, typeName, newScores[kR]))&#xa;            numOut += 1&#xa;&#xa;    if (numOut == 0):&#xa;        fh.write(&#xa;            ""## there were no significant associations between clinical/sample features and molecular features \n"")&#xa;    fh.close()&#xa;&#xa;    # and we can delete the individual *.pw files ...&#xa;    cmdString = ""rm -fr %s"" % tmpDir&#xa;    print cmdString&#xa;    ## ( status, output ) = commands.getstatusoutput ( cmdString )&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" FINISHED ""&#xa;    print "" ""&#xa;&#xa;&#xa;&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;"
19295725|"""""""Determine the layer plane angle of all the elements in a grid.&#xa;&#xa;Author: Perry Roth-Johnson&#xa;Last modified: March 17, 2014&#xa;&#xa;References:&#xa;http://stackoverflow.com/questions/3365171/calculating-the-angle-between-two-lines-without-having-to-calculate-the-slope/3366569#3366569&#xa;http://stackoverflow.com/questions/19295725/angle-less-than-180-between-two-segments-lines&#xa;&#xa;""""""&#xa;&#xa;import numpy as np&#xa;import matplotlib.pyplot as plt&#xa;import pandas as pd&#xa;import lib.grid as gr&#xa;reload(gr)&#xa;import lib.abaqus_utils2 as au&#xa;reload(au)&#xa;import lib.vabs_utils as vu&#xa;reload(vu)&#xa;from shapely.geometry import Polygon, LineString&#xa;from descartes import PolygonPatch&#xa;&#xa;&#xa;# -----------------------------------------------&#xa;# update these parameters!&#xa;station_num = 3&#xa;# -----------------------------------------------&#xa;&#xa;stn_str = 'stn{0:02d}'.format(station_num)&#xa;plt.close('all')&#xa;# create a figure&#xa;plt.figure(num='Station #{0:02d}'.format(station_num))&#xa;ax = plt.gcf().gca()&#xa;&#xa;# outer_edge_node_nums=[1,4], inner_edge_node_nums=[2,3]&#xa;list_of_unflipped_elementsets = [&#xa;    'rbtrile',&#xa;    'esgelle',&#xa;    'estrile',&#xa;    'isresle',&#xa;    'istrile'&#xa;    ]&#xa;&#xa;# outer_edge_node_nums=[3,2], inner_edge_node_nums=[4,1]&#xa;list_of_flipped_elementsets = [&#xa;    'teuniax',&#xa;    'rbtrite',&#xa;    'esgelte',&#xa;    'estrite',&#xa;    'isreste',&#xa;    'istrite'&#xa;    ]&#xa;&#xa;# outer_edge_node_nums=[2,1], inner_edge_node_nums=[3,4]&#xa;list_of_rotated_elementsets = [&#xa;    'esgellr',&#xa;    'estrilr',&#xa;    'isreslr',&#xa;    'istrilr',&#xa;    'rbtrilr',&#xa;    'rbtriscl',&#xa;    'esgelscl',&#xa;    'estriscl',&#xa;    'isresscl',&#xa;    'istriscl',&#xa;    'sclower'&#xa;]&#xa;&#xa;# outer_edge_node_nums=[4,3], inner_edge_node_nums=[1,2]&#xa;list_of_rotated_elementsets2 = [&#xa;    'esgelur',&#xa;    'estriur',&#xa;    'isresur',&#xa;    'istriur',&#xa;    'rbtriur',&#xa;    'rbtriscu',&#xa;    'esgelscu',&#xa;    'estriscu',&#xa;    'isresscu',&#xa;    'istriscu',&#xa;    'scupper'&#xa;]&#xa;&#xa;# import the initial grid object&#xa;fmt_grid = 'sandia_blade/' + stn_str + '/mesh_' + stn_str + '.abq'&#xa;g = au.AbaqusGrid(fmt_grid, debug_flag=True)&#xa;# update the grid object with all the layer plane angles&#xa;for elem in g.list_of_elements:&#xa;    if elem.element_set in list_of_unflipped_elementsets:&#xa;        elem.calculate_layer_plane_angle(outer_edge_node_nums=[1,4],&#xa;            inner_edge_node_nums=[2,3])&#xa;    elif elem.element_set in list_of_flipped_elementsets:&#xa;        elem.calculate_layer_plane_angle(outer_edge_node_nums=[3,2],&#xa;            inner_edge_node_nums=[4,1])&#xa;    elif elem.element_set in list_of_rotated_elementsets:&#xa;        elem.calculate_layer_plane_angle(outer_edge_node_nums=[2,1], &#xa;            inner_edge_node_nums=[3,4])&#xa;    elif elem.element_set in list_of_rotated_elementsets2:&#xa;        elem.calculate_layer_plane_angle(outer_edge_node_nums=[4,3], &#xa;            inner_edge_node_nums=[1,2])&#xa;    else:&#xa;        raise Warning(""Element #{0} has no element set!"".format(elem.elem_num))&#xa;# plot a small selection of elements to check the results&#xa;for elem in g.list_of_elements[::25]:&#xa;# for elem in g.list_of_elements[:150:5]:&#xa;    elem.plot(label_nodes=False)&#xa;    print elem.elem_num, elem.element_set, elem.theta1&#xa;# show the plot&#xa;plt.xlim([-3,3])&#xa;plt.ylim([-3,3])&#xa;ax.set_aspect('equal')&#xa;&#xa;plt.figure(num='Station #{0:02d}, theta1 vs. elem_num'.format(&#xa;    station_num))&#xa;enum=np.arange(g.number_of_elements)+1&#xa;theta=np.zeros(g.number_of_elements)&#xa;elemset=[]&#xa;for i,elem in enumerate(g.list_of_elements):&#xa;    theta[i] = elem.theta1&#xa;    elemset.append(elem.element_set)&#xa;plt.plot(enum,theta)&#xa;plt.xlabel('element number [#]')&#xa;plt.ylabel('theta1 [deg]')&#xa;plt.grid('on')&#xa;&#xa;plt.show()&#xa;# -----------------------------------------------------------------------------&#xa;# read layers.csv to determine the number of layers&#xa;layer_file = pd.read_csv('sandia_blade/layers.csv', index_col=0)&#xa;number_of_layers = len(layer_file)&#xa;# write the updated grid object to a VABS input file&#xa;fmt_vabs = 'sandia_blade/' + stn_str + '/mesh_' + stn_str + '.vabs'&#xa;f = vu.VabsInputFile(&#xa;    vabs_filename=fmt_vabs,&#xa;    grid=g,&#xa;    material_filename='sandia_blade/materials.csv',&#xa;    layer_filename='sandia_blade/layers.csv',&#xa;    debug_flag=True,&#xa;    flags={&#xa;        'format'           : 1,&#xa;        'Timoshenko'       : 1,&#xa;        'recover'          : 0,&#xa;        'thermal'          : 0,&#xa;        'curve'            : 0,&#xa;        'oblique'          : 0,&#xa;        'trapeze'          : 0,&#xa;        'Vlasov'           : 0&#xa;    })&#xa;"
1771314|"#!/usr/bin/env python&#xa;# coding: utf-8&#xa;#&#xa;# Copyright 2007 Google Inc.&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;&#xa;""""""Tool for uploading diffs from a version control system to the codereview app.&#xa;&#xa;Usage summary: upload.py [options] [-- diff_options] [path...]&#xa;&#xa;Diff options are passed to the diff command of the underlying system.&#xa;&#xa;Supported version control systems:&#xa;  Git&#xa;  Mercurial&#xa;  Subversion&#xa;  Perforce&#xa;  CVS&#xa;&#xa;It is important for Git/Mercurial users to specify a tree/node/branch to diff&#xa;against by using the '--rev' option.&#xa;""""""&#xa;# This code is derived from appcfg.py in the App Engine SDK (open source),&#xa;# and from ASPN recipe #146306.&#xa;&#xa;import ConfigParser&#xa;import cookielib&#xa;import errno&#xa;import fnmatch&#xa;import getpass&#xa;import logging&#xa;import marshal&#xa;import mimetypes&#xa;import optparse&#xa;import os&#xa;import re&#xa;import socket&#xa;import subprocess&#xa;import sys&#xa;import urllib&#xa;import urllib2&#xa;import urlparse&#xa;&#xa;# The md5 module was deprecated in Python 2.5.&#xa;try:&#xa;  from hashlib import md5&#xa;except ImportError:&#xa;  from md5 import md5&#xa;&#xa;try:&#xa;  import readline&#xa;except ImportError:&#xa;  pass&#xa;&#xa;try:&#xa;  import keyring&#xa;except ImportError:&#xa;  keyring = None&#xa;&#xa;# The logging verbosity:&#xa;#  0: Errors only.&#xa;#  1: Status messages.&#xa;#  2: Info logs.&#xa;#  3: Debug logs.&#xa;verbosity = 1&#xa;&#xa;# The account type used for authentication.&#xa;# This line could be changed by the review server (see handler for&#xa;# upload.py).&#xa;AUTH_ACCOUNT_TYPE = ""GOOGLE""&#xa;&#xa;# URL of the default review server. As for AUTH_ACCOUNT_TYPE, this line could be&#xa;# changed by the review server (see handler for upload.py).&#xa;DEFAULT_REVIEW_SERVER = ""codereview.appspot.com""&#xa;&#xa;# Max size of patch or base file.&#xa;MAX_UPLOAD_SIZE = 900 * 1024&#xa;&#xa;# Constants for version control names.  Used by GuessVCSName.&#xa;VCS_GIT = ""Git""&#xa;VCS_MERCURIAL = ""Mercurial""&#xa;VCS_SUBVERSION = ""Subversion""&#xa;VCS_PERFORCE = ""Perforce""&#xa;VCS_CVS = ""CVS""&#xa;VCS_UNKNOWN = ""Unknown""&#xa;&#xa;VCS_ABBREVIATIONS = {&#xa;  VCS_MERCURIAL.lower(): VCS_MERCURIAL,&#xa;  ""hg"": VCS_MERCURIAL,&#xa;  VCS_SUBVERSION.lower(): VCS_SUBVERSION,&#xa;  ""svn"": VCS_SUBVERSION,&#xa;  VCS_PERFORCE.lower(): VCS_PERFORCE,&#xa;  ""p4"": VCS_PERFORCE,&#xa;  VCS_GIT.lower(): VCS_GIT,&#xa;  VCS_CVS.lower(): VCS_CVS,&#xa;}&#xa;&#xa;# The result of parsing Subversion's [auto-props] setting.&#xa;svn_auto_props_map = None&#xa;&#xa;def GetEmail(prompt):&#xa;  """"""Prompts the user for their email address and returns it.&#xa;&#xa;  The last used email address is saved to a file and offered up as a suggestion&#xa;  to the user. If the user presses enter without typing in anything the last&#xa;  used email address is used. If the user enters a new address, it is saved&#xa;  for next time we prompt.&#xa;&#xa;  """"""&#xa;  last_email_file_name = os.path.expanduser(""~/.last_codereview_email_address"")&#xa;  last_email = """"&#xa;  if os.path.exists(last_email_file_name):&#xa;    try:&#xa;      last_email_file = open(last_email_file_name, ""r"")&#xa;      last_email = last_email_file.readline().strip(""\n"")&#xa;      last_email_file.close()&#xa;      prompt += "" [%s]"" % last_email&#xa;    except IOError, e:&#xa;      pass&#xa;  email = raw_input(prompt + "": "").strip()&#xa;  if email:&#xa;    try:&#xa;      last_email_file = open(last_email_file_name, ""w"")&#xa;      last_email_file.write(email)&#xa;      last_email_file.close()&#xa;    except IOError, e:&#xa;      pass&#xa;  else:&#xa;    email = last_email&#xa;  return email&#xa;&#xa;&#xa;def StatusUpdate(msg):&#xa;  """"""Print a status message to stdout.&#xa;&#xa;  If 'verbosity' is greater than 0, print the message.&#xa;&#xa;  Args:&#xa;    msg: The string to print.&#xa;  """"""&#xa;  if verbosity > 0:&#xa;    print msg&#xa;&#xa;&#xa;def ErrorExit(msg):&#xa;  """"""Print an error message to stderr and exit.""""""&#xa;  print >>sys.stderr, msg&#xa;  sys.exit(1)&#xa;&#xa;&#xa;class ClientLoginError(urllib2.HTTPError):&#xa;  """"""Raised to indicate there was an error authenticating with ClientLogin.""""""&#xa;&#xa;  def __init__(self, url, code, msg, headers, args):&#xa;    urllib2.HTTPError.__init__(self, url, code, msg, headers, None)&#xa;    self.args = args&#xa;    self._reason = args[""Error""]&#xa;    self.info = args.get(""Info"", None)&#xa;&#xa;  @property&#xa;  def reason(self):&#xa;    # reason is a property on python 2.7 but a member variable on <=2.6.&#xa;    # self.args is modified so it cannot be used as-is so save the value in&#xa;    # self._reason.&#xa;    return self._reason&#xa;&#xa;&#xa;class AbstractRpcServer(object):&#xa;  """"""Provides a common interface for a simple RPC server.""""""&#xa;&#xa;  def __init__(self, host, auth_function, host_override=None, extra_headers={},&#xa;               save_cookies=False, account_type=AUTH_ACCOUNT_TYPE):&#xa;    """"""Creates a new AbstractRpcServer.&#xa;&#xa;    Args:&#xa;      host: The host to send requests to.&#xa;      auth_function: A function that takes no arguments and returns an&#xa;        (email, password) tuple when called. Will be called if authentication&#xa;        is required.&#xa;      host_override: The host header to send to the server (defaults to host).&#xa;      extra_headers: A dict of extra headers to append to every request.&#xa;      save_cookies: If True, save the authentication cookies to local disk.&#xa;        If False, use an in-memory cookiejar instead.  Subclasses must&#xa;        implement this functionality.  Defaults to False.&#xa;      account_type: Account type used for authentication. Defaults to&#xa;        AUTH_ACCOUNT_TYPE.&#xa;    """"""&#xa;    self.host = host&#xa;    if (not self.host.startswith(""http://"") and&#xa;        not self.host.startswith(""https://"")):&#xa;      self.host = ""http://"" + self.host&#xa;    self.host_override = host_override&#xa;    self.auth_function = auth_function&#xa;    self.authenticated = False&#xa;    self.extra_headers = extra_headers&#xa;    self.save_cookies = save_cookies&#xa;    self.account_type = account_type&#xa;    self.opener = self._GetOpener()&#xa;    if self.host_override:&#xa;      logging.info(""Server: %s; Host: %s"", self.host, self.host_override)&#xa;    else:&#xa;      logging.info(""Server: %s"", self.host)&#xa;&#xa;  def _GetOpener(self):&#xa;    """"""Returns an OpenerDirector for making HTTP requests.&#xa;&#xa;    Returns:&#xa;      A urllib2.OpenerDirector object.&#xa;    """"""&#xa;    raise NotImplementedError()&#xa;&#xa;  def _CreateRequest(self, url, data=None):&#xa;    """"""Creates a new urllib request.""""""&#xa;    logging.debug(""Creating request for: '%s' with payload:\n%s"", url, data)&#xa;    req = urllib2.Request(url, data=data, headers={""Accept"": ""text/plain""})&#xa;    if self.host_override:&#xa;      req.add_header(""Host"", self.host_override)&#xa;    for key, value in self.extra_headers.iteritems():&#xa;      req.add_header(key, value)&#xa;    return req&#xa;&#xa;  def _GetAuthToken(self, email, password):&#xa;    """"""Uses ClientLogin to authenticate the user, returning an auth token.&#xa;&#xa;    Args:&#xa;      email:    The user's email address&#xa;      password: The user's password&#xa;&#xa;    Raises:&#xa;      ClientLoginError: If there was an error authenticating with ClientLogin.&#xa;      HTTPError: If there was some other form of HTTP error.&#xa;&#xa;    Returns:&#xa;      The authentication token returned by ClientLogin.&#xa;    """"""&#xa;    account_type = self.account_type&#xa;    if self.host.endswith("".google.com""):&#xa;      # Needed for use inside Google.&#xa;      account_type = ""HOSTED""&#xa;    req = self._CreateRequest(&#xa;        url=""https://www.google.com/accounts/ClientLogin"",&#xa;        data=urllib.urlencode({&#xa;            ""Email"": email,&#xa;            ""Passwd"": password,&#xa;            ""service"": ""ah"",&#xa;            ""source"": ""rietveld-codereview-upload"",&#xa;            ""accountType"": account_type,&#xa;        }),&#xa;    )&#xa;    try:&#xa;      response = self.opener.open(req)&#xa;      response_body = response.read()&#xa;      response_dict = dict(x.split(""="")&#xa;                           for x in response_body.split(""\n"") if x)&#xa;      return response_dict[""Auth""]&#xa;    except urllib2.HTTPError, e:&#xa;      if e.code == 403:&#xa;        body = e.read()&#xa;        response_dict = dict(x.split(""="", 1) for x in body.split(""\n"") if x)&#xa;        raise ClientLoginError(req.get_full_url(), e.code, e.msg,&#xa;                               e.headers, response_dict)&#xa;      else:&#xa;        raise&#xa;&#xa;  def _GetAuthCookie(self, auth_token):&#xa;    """"""Fetches authentication cookies for an authentication token.&#xa;&#xa;    Args:&#xa;      auth_token: The authentication token returned by ClientLogin.&#xa;&#xa;    Raises:&#xa;      HTTPError: If there was an error fetching the authentication cookies.&#xa;    """"""&#xa;    # This is a dummy value to allow us to identify when we're successful.&#xa;    continue_location = ""http://localhost/""&#xa;    args = {""continue"": continue_location, ""auth"": auth_token}&#xa;    req = self._CreateRequest(""%s/_ah/login?%s"" %&#xa;                              (self.host, urllib.urlencode(args)))&#xa;    try:&#xa;      response = self.opener.open(req)&#xa;    except urllib2.HTTPError, e:&#xa;      response = e&#xa;    if (response.code != 302 or&#xa;        response.info()[""location""] != continue_location):&#xa;      raise urllib2.HTTPError(req.get_full_url(), response.code, response.msg,&#xa;                              response.headers, response.fp)&#xa;    self.authenticated = True&#xa;&#xa;  def _Authenticate(self):&#xa;    """"""Authenticates the user.&#xa;&#xa;    The authentication process works as follows:&#xa;     1) We get a username and password from the user&#xa;     2) We use ClientLogin to obtain an AUTH token for the user&#xa;        (see http://code.google.com/apis/accounts/AuthForInstalledApps.html).&#xa;     3) We pass the auth token to /_ah/login on the server to obtain an&#xa;        authentication cookie. If login was successful, it tries to redirect&#xa;        us to the URL we provided.&#xa;&#xa;    If we attempt to access the upload API without first obtaining an&#xa;    authentication cookie, it returns a 401 response (or a 302) and&#xa;    directs us to authenticate ourselves with ClientLogin.&#xa;    """"""&#xa;    for i in range(3):&#xa;      credentials = self.auth_function()&#xa;      try:&#xa;        auth_token = self._GetAuthToken(credentials[0], credentials[1])&#xa;      except ClientLoginError, e:&#xa;        print >>sys.stderr, ''&#xa;        if e.reason == ""BadAuthentication"":&#xa;          if e.info == ""InvalidSecondFactor"":&#xa;            print >>sys.stderr, (&#xa;                ""Use an application-specific password instead ""&#xa;                ""of your regular account password.\n""&#xa;                ""See http://www.google.com/""&#xa;                ""support/accounts/bin/answer.py?answer=185833"")&#xa;          else:&#xa;            print >>sys.stderr, ""Invalid username or password.""&#xa;        elif e.reason == ""CaptchaRequired"":&#xa;          print >>sys.stderr, (&#xa;              ""Please go to\n""&#xa;              ""https://www.google.com/accounts/DisplayUnlockCaptcha\n""&#xa;              ""and verify you are a human.  Then try again.\n""&#xa;              ""If you are using a Google Apps account the URL is:\n""&#xa;              ""https://www.google.com/a/yourdomain.com/UnlockCaptcha"")&#xa;        elif e.reason == ""NotVerified"":&#xa;          print >>sys.stderr, ""Account not verified.""&#xa;        elif e.reason == ""TermsNotAgreed"":&#xa;          print >>sys.stderr, ""User has not agreed to TOS.""&#xa;        elif e.reason == ""AccountDeleted"":&#xa;          print >>sys.stderr, ""The user account has been deleted.""&#xa;        elif e.reason == ""AccountDisabled"":&#xa;          print >>sys.stderr, ""The user account has been disabled.""&#xa;          break&#xa;        elif e.reason == ""ServiceDisabled"":&#xa;          print >>sys.stderr, (""The user's access to the service has been ""&#xa;                               ""disabled."")&#xa;        elif e.reason == ""ServiceUnavailable"":&#xa;          print >>sys.stderr, ""The service is not available; try again later.""&#xa;        else:&#xa;          # Unknown error.&#xa;          raise&#xa;        print >>sys.stderr, ''&#xa;        continue&#xa;      self._GetAuthCookie(auth_token)&#xa;      return&#xa;&#xa;  def Send(self, request_path, payload=None,&#xa;           content_type=""application/octet-stream"",&#xa;           timeout=None,&#xa;           extra_headers=None,&#xa;           **kwargs):&#xa;    """"""Sends an RPC and returns the response.&#xa;&#xa;    Args:&#xa;      request_path: The path to send the request to, eg /api/appversion/create.&#xa;      payload: The body of the request, or None to send an empty request.&#xa;      content_type: The Content-Type header to use.&#xa;      timeout: timeout in seconds; default None i.e. no timeout.&#xa;        (Note: for large requests on OS X, the timeout doesn't work right.)&#xa;      extra_headers: Dict containing additional HTTP headers that should be&#xa;        included in the request (string header names mapped to their values),&#xa;        or None to not include any additional headers.&#xa;      kwargs: Any keyword arguments are converted into query string parameters.&#xa;&#xa;    Returns:&#xa;      The response body, as a string.&#xa;    """"""&#xa;    # TODO: Don't require authentication.  Let the server say&#xa;    # whether it is necessary.&#xa;    if not self.authenticated:&#xa;      self._Authenticate()&#xa;&#xa;    old_timeout = socket.getdefaulttimeout()&#xa;    socket.setdefaulttimeout(timeout)&#xa;    try:&#xa;      tries = 0&#xa;      while True:&#xa;        tries += 1&#xa;        args = dict(kwargs)&#xa;        url = ""%s%s"" % (self.host, request_path)&#xa;        if args:&#xa;          url += ""?"" + urllib.urlencode(args)&#xa;        req = self._CreateRequest(url=url, data=payload)&#xa;        req.add_header(""Content-Type"", content_type)&#xa;        if extra_headers:&#xa;          for header, value in extra_headers.items():&#xa;            req.add_header(header, value)&#xa;        try:&#xa;          f = self.opener.open(req)&#xa;          response = f.read()&#xa;          f.close()&#xa;          return response&#xa;        except urllib2.HTTPError, e:&#xa;          if tries > 3:&#xa;            raise&#xa;          elif e.code == 401 or e.code == 302:&#xa;            self._Authenticate()&#xa;          elif e.code == 301:&#xa;            # Handle permanent redirect manually.&#xa;            url = e.info()[""location""]&#xa;            url_loc = urlparse.urlparse(url)&#xa;            self.host = '%s://%s' % (url_loc[0], url_loc[1])&#xa;          elif e.code >= 500:&#xa;            ErrorExit(e.read())&#xa;          else:&#xa;            raise&#xa;    finally:&#xa;      socket.setdefaulttimeout(old_timeout)&#xa;&#xa;&#xa;class HttpRpcServer(AbstractRpcServer):&#xa;  """"""Provides a simplified RPC-style interface for HTTP requests.""""""&#xa;&#xa;  def _Authenticate(self):&#xa;    """"""Save the cookie jar after authentication.""""""&#xa;    super(HttpRpcServer, self)._Authenticate()&#xa;    if self.save_cookies:&#xa;      StatusUpdate(""Saving authentication cookies to %s"" % self.cookie_file)&#xa;      self.cookie_jar.save()&#xa;&#xa;  def _GetOpener(self):&#xa;    """"""Returns an OpenerDirector that supports cookies and ignores redirects.&#xa;&#xa;    Returns:&#xa;      A urllib2.OpenerDirector object.&#xa;    """"""&#xa;    opener = urllib2.OpenerDirector()&#xa;    opener.add_handler(urllib2.ProxyHandler())&#xa;    opener.add_handler(urllib2.UnknownHandler())&#xa;    opener.add_handler(urllib2.HTTPHandler())&#xa;    opener.add_handler(urllib2.HTTPDefaultErrorHandler())&#xa;    opener.add_handler(urllib2.HTTPSHandler())&#xa;    opener.add_handler(urllib2.HTTPErrorProcessor())&#xa;    if self.save_cookies:&#xa;      self.cookie_file = os.path.expanduser(""~/.codereview_upload_cookies"")&#xa;      self.cookie_jar = cookielib.MozillaCookieJar(self.cookie_file)&#xa;      if os.path.exists(self.cookie_file):&#xa;        try:&#xa;          self.cookie_jar.load()&#xa;          self.authenticated = True&#xa;          StatusUpdate(""Loaded authentication cookies from %s"" %&#xa;                       self.cookie_file)&#xa;        except (cookielib.LoadError, IOError):&#xa;          # Failed to load cookies - just ignore them.&#xa;          pass&#xa;      else:&#xa;        # Create an empty cookie file with mode 600&#xa;        fd = os.open(self.cookie_file, os.O_CREAT, 0600)&#xa;        os.close(fd)&#xa;      # Always chmod the cookie file&#xa;      os.chmod(self.cookie_file, 0600)&#xa;    else:&#xa;      # Don't save cookies across runs of update.py.&#xa;      self.cookie_jar = cookielib.CookieJar()&#xa;    opener.add_handler(urllib2.HTTPCookieProcessor(self.cookie_jar))&#xa;    return opener&#xa;&#xa;&#xa;class CondensedHelpFormatter(optparse.IndentedHelpFormatter):&#xa;   """"""Frees more horizontal space by removing indentation from group&#xa;      options and collapsing arguments between short and long, e.g.&#xa;      '-o ARG, --opt=ARG' to -o --opt ARG""""""&#xa;&#xa;   def format_heading(self, heading):&#xa;     return ""%s:\n"" % heading&#xa;&#xa;   def format_option(self, option):&#xa;     self.dedent()&#xa;     res = optparse.HelpFormatter.format_option(self, option)&#xa;     self.indent()&#xa;     return res&#xa;&#xa;   def format_option_strings(self, option):&#xa;     self.set_long_opt_delimiter("" "")&#xa;     optstr = optparse.HelpFormatter.format_option_strings(self, option)&#xa;     optlist = optstr.split("", "")&#xa;     if len(optlist) > 1:&#xa;       if option.takes_value():&#xa;         # strip METAVAR from all but the last option&#xa;         optlist = [x.split()[0] for x in optlist[:-1]] + optlist[-1:]&#xa;       optstr = "" "".join(optlist)&#xa;     return optstr&#xa;&#xa;&#xa;parser = optparse.OptionParser(&#xa;    usage=""%prog [options] [-- diff_options] [path...]"",&#xa;    add_help_option=False,&#xa;    formatter=CondensedHelpFormatter()&#xa;)&#xa;parser.add_option(""-h"", ""--help"", action=""store_true"",&#xa;                  help=""Show this help message and exit."")&#xa;parser.add_option(""-y"", ""--assume_yes"", action=""store_true"",&#xa;                  dest=""assume_yes"", default=False,&#xa;                  help=""Assume that the answer to yes/no questions is 'yes'."")&#xa;# Logging&#xa;group = parser.add_option_group(""Logging options"")&#xa;group.add_option(""-q"", ""--quiet"", action=""store_const"", const=0,&#xa;                 dest=""verbose"", help=""Print errors only."")&#xa;group.add_option(""-v"", ""--verbose"", action=""store_const"", const=2,&#xa;                 dest=""verbose"", default=1,&#xa;                 help=""Print info level logs."")&#xa;group.add_option(""--noisy"", action=""store_const"", const=3,&#xa;                 dest=""verbose"", help=""Print all logs."")&#xa;group.add_option(""--print_diffs"", dest=""print_diffs"", action=""store_true"",&#xa;                 help=""Print full diffs."")&#xa;# Review server&#xa;group = parser.add_option_group(""Review server options"")&#xa;group.add_option(""-s"", ""--server"", action=""store"", dest=""server"",&#xa;                 default=DEFAULT_REVIEW_SERVER,&#xa;                 metavar=""SERVER"",&#xa;                 help=(""The server to upload to. The format is host[:port]. ""&#xa;                       ""Defaults to '%default'.""))&#xa;group.add_option(""-e"", ""--email"", action=""store"", dest=""email"",&#xa;                 metavar=""EMAIL"", default=None,&#xa;                 help=""The username to use. Will prompt if omitted."")&#xa;group.add_option(""-H"", ""--host"", action=""store"", dest=""host"",&#xa;                 metavar=""HOST"", default=None,&#xa;                 help=""Overrides the Host header sent with all RPCs."")&#xa;group.add_option(""--no_cookies"", action=""store_false"",&#xa;                 dest=""save_cookies"", default=True,&#xa;                 help=""Do not save authentication cookies to local disk."")&#xa;group.add_option(""--account_type"", action=""store"", dest=""account_type"",&#xa;                 metavar=""TYPE"", default=AUTH_ACCOUNT_TYPE,&#xa;                 choices=[""GOOGLE"", ""HOSTED""],&#xa;                 help=(""Override the default account type ""&#xa;                       ""(defaults to '%default', ""&#xa;                       ""valid choices are 'GOOGLE' and 'HOSTED').""))&#xa;# Issue&#xa;group = parser.add_option_group(""Issue options"")&#xa;group.add_option(""-t"", ""--title"", action=""store"", dest=""title"",&#xa;                 help=""New issue subject or new patch set title"")&#xa;group.add_option(""-m"", ""--message"", action=""store"", dest=""message"",&#xa;                 default=None,&#xa;                 help=""New issue description or new patch set message"")&#xa;group.add_option(""-F"", ""--file"", action=""store"", dest=""file"",&#xa;                 default=None, help=""Read the message above from file."")&#xa;group.add_option(""-r"", ""--reviewers"", action=""store"", dest=""reviewers"",&#xa;                 metavar=""REVIEWERS"", default=None,&#xa;                 help=""Add reviewers (comma separated email addresses)."")&#xa;group.add_option(""--cc"", action=""store"", dest=""cc"",&#xa;                 metavar=""CC"", default=None,&#xa;                 help=""Add CC (comma separated email addresses)."")&#xa;group.add_option(""--private"", action=""store_true"", dest=""private"",&#xa;                 default=False,&#xa;                 help=""Make the issue restricted to reviewers and those CCed"")&#xa;# Upload options&#xa;group = parser.add_option_group(""Patch options"")&#xa;group.add_option(""-i"", ""--issue"", type=""int"", action=""store"",&#xa;                 metavar=""ISSUE"", default=None,&#xa;                 help=""Issue number to which to add. Defaults to new issue."")&#xa;group.add_option(""--base_url"", action=""store"", dest=""base_url"", default=None,&#xa;                 help=""Base URL path for files (listed as \""Base URL\"" when ""&#xa;                 ""viewing issue).  If omitted, will be guessed automatically ""&#xa;                 ""for SVN repos and left blank for others."")&#xa;group.add_option(""--download_base"", action=""store_true"",&#xa;                 dest=""download_base"", default=False,&#xa;                 help=""Base files will be downloaded by the server ""&#xa;                 ""(side-by-side diffs may not work on files with CRs)."")&#xa;group.add_option(""--rev"", action=""store"", dest=""revision"",&#xa;                 metavar=""REV"", default=None,&#xa;                 help=""Base revision/branch/tree to diff against. Use ""&#xa;                      ""rev1:rev2 range to review already committed changeset."")&#xa;group.add_option(""--send_mail"", action=""store_true"",&#xa;                 dest=""send_mail"", default=False,&#xa;                 help=""Send notification email to reviewers."")&#xa;group.add_option(""-p"", ""--send_patch"", action=""store_true"",&#xa;                 dest=""send_patch"", default=False,&#xa;                 help=""Same as --send_mail, but include diff as an ""&#xa;                      ""attachment, and prepend email subject with 'PATCH:'."")&#xa;group.add_option(""--vcs"", action=""store"", dest=""vcs"",&#xa;                 metavar=""VCS"", default=None,&#xa;                 help=(""Version control system (optional, usually upload.py ""&#xa;                       ""already guesses the right VCS).""))&#xa;group.add_option(""--emulate_svn_auto_props"", action=""store_true"",&#xa;                 dest=""emulate_svn_auto_props"", default=False,&#xa;                 help=(""Emulate Subversion's auto properties feature.""))&#xa;# Git-specific&#xa;group = parser.add_option_group(""Git-specific options"")&#xa;group.add_option(""--git_similarity"", action=""store"", dest=""git_similarity"",&#xa;                 metavar=""SIM"", type=""int"", default=50,&#xa;                 help=(""Set the minimum similarity index for detecting renames ""&#xa;                       ""and copies. See `git diff -C`. (default 50).""))&#xa;group.add_option(""--git_no_find_copies"", action=""store_false"", default=True,&#xa;                 dest=""git_find_copies"",&#xa;                 help=(""Prevents git from looking for copies (default off).""))&#xa;# Perforce-specific&#xa;group = parser.add_option_group(""Perforce-specific options ""&#xa;                                ""(overrides P4 environment variables)"")&#xa;group.add_option(""--p4_port"", action=""store"", dest=""p4_port"",&#xa;                 metavar=""P4_PORT"", default=None,&#xa;                 help=(""Perforce server and port (optional)""))&#xa;group.add_option(""--p4_changelist"", action=""store"", dest=""p4_changelist"",&#xa;                 metavar=""P4_CHANGELIST"", default=None,&#xa;                 help=(""Perforce changelist id""))&#xa;group.add_option(""--p4_client"", action=""store"", dest=""p4_client"",&#xa;                 metavar=""P4_CLIENT"", default=None,&#xa;                 help=(""Perforce client/workspace""))&#xa;group.add_option(""--p4_user"", action=""store"", dest=""p4_user"",&#xa;                 metavar=""P4_USER"", default=None,&#xa;                 help=(""Perforce user""))&#xa;&#xa;&#xa;class KeyringCreds(object):&#xa;  def __init__(self, server, host, email):&#xa;    self.server = server&#xa;    self.host = host&#xa;    self.email = email&#xa;    self.accounts_seen = set()&#xa;&#xa;  def GetUserCredentials(self):&#xa;    """"""Prompts the user for a username and password.&#xa;&#xa;    Only use keyring on the initial call. If the keyring contains the wrong&#xa;    password, we want to give the user a chance to enter another one.&#xa;    """"""&#xa;    # Create a local alias to the email variable to avoid Python's crazy&#xa;    # scoping rules.&#xa;    global keyring&#xa;    email = self.email&#xa;    if email is None:&#xa;      email = GetEmail(""Email (login for uploading to %s)"" % self.server)&#xa;    password = None&#xa;    if keyring and not email in self.accounts_seen:&#xa;      try:&#xa;        password = keyring.get_password(self.host, email)&#xa;      except:&#xa;        # Sadly, we have to trap all errors here as&#xa;        # gnomekeyring.IOError inherits from object. :/&#xa;        print ""Failed to get password from keyring""&#xa;        keyring = None&#xa;    if password is not None:&#xa;      print ""Using password from system keyring.""&#xa;      self.accounts_seen.add(email)&#xa;    else:&#xa;      password = getpass.getpass(""Password for %s: "" % email)&#xa;      if keyring:&#xa;        answer = raw_input(""Store password in system keyring?(y/N) "").strip()&#xa;        if answer == ""y"":&#xa;          keyring.set_password(self.host, email, password)&#xa;          self.accounts_seen.add(email)&#xa;    return (email, password)&#xa;&#xa;&#xa;def GetRpcServer(server, email=None, host_override=None, save_cookies=True,&#xa;                 account_type=AUTH_ACCOUNT_TYPE):&#xa;  """"""Returns an instance of an AbstractRpcServer.&#xa;&#xa;  Args:&#xa;    server: String containing the review server URL.&#xa;    email: String containing user's email address.&#xa;    host_override: If not None, string containing an alternate hostname to use&#xa;      in the host header.&#xa;    save_cookies: Whether authentication cookies should be saved to disk.&#xa;    account_type: Account type for authentication, either 'GOOGLE'&#xa;      or 'HOSTED'. Defaults to AUTH_ACCOUNT_TYPE.&#xa;&#xa;  Returns:&#xa;    A new HttpRpcServer, on which RPC calls can be made.&#xa;  """"""&#xa;&#xa;  # If this is the dev_appserver, use fake authentication.&#xa;  host = (host_override or server).lower()&#xa;  if re.match(r'(http://)?localhost([:/]|$)', host):&#xa;    if email is None:&#xa;      email = ""test@example.com""&#xa;      logging.info(""Using debug user %s.  Override with --email"" % email)&#xa;    server = HttpRpcServer(&#xa;        server,&#xa;        lambda: (email, ""password""),&#xa;        host_override=host_override,&#xa;        extra_headers={""Cookie"":&#xa;                       'dev_appserver_login=""%s:False""' % email},&#xa;        save_cookies=save_cookies,&#xa;        account_type=account_type)&#xa;    # Don't try to talk to ClientLogin.&#xa;    server.authenticated = True&#xa;    return server&#xa;&#xa;  return HttpRpcServer(server,&#xa;                       KeyringCreds(server, host, email).GetUserCredentials,&#xa;                       host_override=host_override,&#xa;                       save_cookies=save_cookies,&#xa;                       account_type=account_type)&#xa;&#xa;&#xa;def EncodeMultipartFormData(fields, files):&#xa;  """"""Encode form fields for multipart/form-data.&#xa;&#xa;  Args:&#xa;    fields: A sequence of (name, value) elements for regular form fields.&#xa;    files: A sequence of (name, filename, value) elements for data to be&#xa;           uploaded as files.&#xa;  Returns:&#xa;    (content_type, body) ready for httplib.HTTP instance.&#xa;&#xa;  Source:&#xa;    http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/146306&#xa;  """"""&#xa;  BOUNDARY = '-M-A-G-I-C---B-O-U-N-D-A-R-Y-'&#xa;  CRLF = '\r\n'&#xa;  lines = []&#xa;  for (key, value) in fields:&#xa;    lines.append('--' + BOUNDARY)&#xa;    lines.append('Content-Disposition: form-data; name=""%s""' % key)&#xa;    lines.append('')&#xa;    if isinstance(value, unicode):&#xa;      value = value.encode('utf-8')&#xa;    lines.append(value)&#xa;  for (key, filename, value) in files:&#xa;    lines.append('--' + BOUNDARY)&#xa;    lines.append('Content-Disposition: form-data; name=""%s""; filename=""%s""' %&#xa;             (key, filename))&#xa;    lines.append('Content-Type: %s' % GetContentType(filename))&#xa;    lines.append('')&#xa;    if isinstance(value, unicode):&#xa;      value = value.encode('utf-8')&#xa;    lines.append(value)&#xa;  lines.append('--' + BOUNDARY + '--')&#xa;  lines.append('')&#xa;  body = CRLF.join(lines)&#xa;  content_type = 'multipart/form-data; boundary=%s' % BOUNDARY&#xa;  return content_type, body&#xa;&#xa;&#xa;def GetContentType(filename):&#xa;  """"""Helper to guess the content-type from the filename.""""""&#xa;  return mimetypes.guess_type(filename)[0] or 'application/octet-stream'&#xa;&#xa;&#xa;# Use a shell for subcommands on Windows to get a PATH search.&#xa;use_shell = sys.platform.startswith(""win"")&#xa;&#xa;def RunShellWithReturnCodeAndStderr(command, print_output=False,&#xa;                           universal_newlines=True,&#xa;                           env=os.environ):&#xa;  """"""Executes a command and returns the output from stdout, stderr and the return code.&#xa;&#xa;  Args:&#xa;    command: Command to execute.&#xa;    print_output: If True, the output is printed to stdout.&#xa;                  If False, both stdout and stderr are ignored.&#xa;    universal_newlines: Use universal_newlines flag (default: True).&#xa;&#xa;  Returns:&#xa;    Tuple (stdout, stderr, return code)&#xa;  """"""&#xa;  logging.info(""Running %s"", command)&#xa;  env = env.copy()&#xa;  env['LC_MESSAGES'] = 'C'&#xa;  p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,&#xa;                       shell=use_shell, universal_newlines=universal_newlines,&#xa;                       env=env)&#xa;  if print_output:&#xa;    output_array = []&#xa;    while True:&#xa;      line = p.stdout.readline()&#xa;      if not line:&#xa;        break&#xa;      print line.strip(""\n"")&#xa;      output_array.append(line)&#xa;    output = """".join(output_array)&#xa;  else:&#xa;    output = p.stdout.read()&#xa;  p.wait()&#xa;  errout = p.stderr.read()&#xa;  if print_output and errout:&#xa;    print >>sys.stderr, errout&#xa;  p.stdout.close()&#xa;  p.stderr.close()&#xa;  return output, errout, p.returncode&#xa;&#xa;def RunShellWithReturnCode(command, print_output=False,&#xa;                           universal_newlines=True,&#xa;                           env=os.environ):&#xa;  """"""Executes a command and returns the output from stdout and the return code.""""""&#xa;  out, err, retcode = RunShellWithReturnCodeAndStderr(command, print_output,&#xa;                           universal_newlines, env)&#xa;  return out, retcode&#xa;&#xa;def RunShell(command, silent_ok=False, universal_newlines=True,&#xa;             print_output=False, env=os.environ):&#xa;  data, retcode = RunShellWithReturnCode(command, print_output,&#xa;                                         universal_newlines, env)&#xa;  if retcode:&#xa;    ErrorExit(""Got error status from %s:\n%s"" % (command, data))&#xa;  if not silent_ok and not data:&#xa;    ErrorExit(""No output from %s"" % command)&#xa;  return data&#xa;&#xa;&#xa;class VersionControlSystem(object):&#xa;  """"""Abstract base class providing an interface to the VCS.""""""&#xa;&#xa;  def __init__(self, options):&#xa;    """"""Constructor.&#xa;&#xa;    Args:&#xa;      options: Command line options.&#xa;    """"""&#xa;    self.options = options&#xa;&#xa;  def GetGUID(self):&#xa;    """"""Return string to distinguish the repository from others, for example to&#xa;    query all opened review issues for it""""""&#xa;    raise NotImplementedError(&#xa;        ""abstract method -- subclass %s must override"" % self.__class__)&#xa;&#xa;  def PostProcessDiff(self, diff):&#xa;    """"""Return the diff with any special post processing this VCS needs, e.g.&#xa;    to include an svn-style ""Index:"".""""""&#xa;    return diff&#xa;&#xa;  def GenerateDiff(self, args):&#xa;    """"""Return the current diff as a string.&#xa;&#xa;    Args:&#xa;      args: Extra arguments to pass to the diff command.&#xa;    """"""&#xa;    raise NotImplementedError(&#xa;        ""abstract method -- subclass %s must override"" % self.__class__)&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    """"""Return a list of files unknown to the VCS.""""""&#xa;    raise NotImplementedError(&#xa;        ""abstract method -- subclass %s must override"" % self.__class__)&#xa;&#xa;  def CheckForUnknownFiles(self):&#xa;    """"""Show an ""are you sure?"" prompt if there are unknown files.""""""&#xa;    unknown_files = self.GetUnknownFiles()&#xa;    if unknown_files:&#xa;      print ""The following files are not added to version control:""&#xa;      for line in unknown_files:&#xa;        print line&#xa;      prompt = ""Are you sure to continue?(y/N) ""&#xa;      answer = raw_input(prompt).strip()&#xa;      if answer != ""y"":&#xa;        ErrorExit(""User aborted"")&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    """"""Get the content of the upstream version of a file.&#xa;&#xa;    Returns:&#xa;      A tuple (base_content, new_content, is_binary, status)&#xa;        base_content: The contents of the base file.&#xa;        new_content: For text files, this is empty.  For binary files, this is&#xa;          the contents of the new file, since the diff output won't contain&#xa;          information to reconstruct the current file.&#xa;        is_binary: True iff the file is binary.&#xa;        status: The status of the file.&#xa;    """"""&#xa;&#xa;    raise NotImplementedError(&#xa;        ""abstract method -- subclass %s must override"" % self.__class__)&#xa;&#xa;&#xa;  def GetBaseFiles(self, diff):&#xa;    """"""Helper that calls GetBase file for each file in the patch.&#xa;&#xa;    Returns:&#xa;      A dictionary that maps from filename to GetBaseFile's tuple.  Filenames&#xa;      are retrieved based on lines that start with ""Index:"" or&#xa;      ""Property changes on:"".&#xa;    """"""&#xa;    files = {}&#xa;    for line in diff.splitlines(True):&#xa;      if line.startswith('Index:') or line.startswith('Property changes on:'):&#xa;        unused, filename = line.split(':', 1)&#xa;        # On Windows if a file has property changes its filename uses '\'&#xa;        # instead of '/'.&#xa;        filename = filename.strip().replace('\\', '/')&#xa;        files[filename] = self.GetBaseFile(filename)&#xa;    return files&#xa;&#xa;&#xa;  def UploadBaseFiles(self, issue, rpc_server, patch_list, patchset, options,&#xa;                      files):&#xa;    """"""Uploads the base files (and if necessary, the current ones as well).""""""&#xa;&#xa;    def UploadFile(filename, file_id, content, is_binary, status, is_base):&#xa;      """"""Uploads a file to the server.""""""&#xa;      file_too_large = False&#xa;      if is_base:&#xa;        type = ""base""&#xa;      else:&#xa;        type = ""current""&#xa;      if len(content) > MAX_UPLOAD_SIZE:&#xa;        print (""Not uploading the %s file for %s because it's too large."" %&#xa;               (type, filename))&#xa;        file_too_large = True&#xa;        content = """"&#xa;      checksum = md5(content).hexdigest()&#xa;      if options.verbose > 0 and not file_too_large:&#xa;        print ""Uploading %s file for %s"" % (type, filename)&#xa;      url = ""/%d/upload_content/%d/%d"" % (int(issue), int(patchset), file_id)&#xa;      form_fields = [(""filename"", filename),&#xa;                     (""status"", status),&#xa;                     (""checksum"", checksum),&#xa;                     (""is_binary"", str(is_binary)),&#xa;                     (""is_current"", str(not is_base)),&#xa;                    ]&#xa;      if file_too_large:&#xa;        form_fields.append((""file_too_large"", ""1""))&#xa;      if options.email:&#xa;        form_fields.append((""user"", options.email))&#xa;      ctype, body = EncodeMultipartFormData(form_fields,&#xa;                                            [(""data"", filename, content)])&#xa;      response_body = rpc_server.Send(url, body,&#xa;                                      content_type=ctype)&#xa;      if not response_body.startswith(""OK""):&#xa;        StatusUpdate(""  --> %s"" % response_body)&#xa;        sys.exit(1)&#xa;&#xa;    patches = dict()&#xa;    [patches.setdefault(v, k) for k, v in patch_list]&#xa;    for filename in patches.keys():&#xa;      base_content, new_content, is_binary, status = files[filename]&#xa;      file_id_str = patches.get(filename)&#xa;      if file_id_str.find(""nobase"") != -1:&#xa;        base_content = None&#xa;        file_id_str = file_id_str[file_id_str.rfind(""_"") + 1:]&#xa;      file_id = int(file_id_str)&#xa;      if base_content != None:&#xa;        UploadFile(filename, file_id, base_content, is_binary, status, True)&#xa;      if new_content != None:&#xa;        UploadFile(filename, file_id, new_content, is_binary, status, False)&#xa;&#xa;  def IsImage(self, filename):&#xa;    """"""Returns true if the filename has an image extension.""""""&#xa;    mimetype =  mimetypes.guess_type(filename)[0]&#xa;    if not mimetype:&#xa;      return False&#xa;    return mimetype.startswith(""image/"")&#xa;&#xa;  def IsBinaryData(self, data):&#xa;    """"""Returns true if data contains a null byte.""""""&#xa;    # Derived from how Mercurial's heuristic, see&#xa;    # http://selenic.com/hg/file/848a6658069e/mercurial/util.py#l229&#xa;    return bool(data and ""\0"" in data)&#xa;&#xa;&#xa;class SubversionVCS(VersionControlSystem):&#xa;  """"""Implementation of the VersionControlSystem interface for Subversion.""""""&#xa;&#xa;  def __init__(self, options):&#xa;    super(SubversionVCS, self).__init__(options)&#xa;    if self.options.revision:&#xa;      match = re.match(r""(\d+)(:(\d+))?"", self.options.revision)&#xa;      if not match:&#xa;        ErrorExit(""Invalid Subversion revision %s."" % self.options.revision)&#xa;      self.rev_start = match.group(1)&#xa;      self.rev_end = match.group(3)&#xa;    else:&#xa;      self.rev_start = self.rev_end = None&#xa;    # Cache output from ""svn list -r REVNO dirname"".&#xa;    # Keys: dirname, Values: 2-tuple (ouput for start rev and end rev).&#xa;    self.svnls_cache = {}&#xa;    # Base URL is required to fetch files deleted in an older revision.&#xa;    # Result is cached to not guess it over and over again in GetBaseFile().&#xa;    required = self.options.download_base or self.options.revision is not None&#xa;    self.svn_base = self._GuessBase(required)&#xa;&#xa;  def GetGUID(self):&#xa;    return self._GetInfo(""Repository UUID"")&#xa;&#xa;  def GuessBase(self, required):&#xa;    """"""Wrapper for _GuessBase.""""""&#xa;    return self.svn_base&#xa;&#xa;  def _GuessBase(self, required):&#xa;    """"""Returns base URL for current diff.&#xa;&#xa;    Args:&#xa;      required: If true, exits if the url can't be guessed, otherwise None is&#xa;        returned.&#xa;    """"""&#xa;    url = self._GetInfo(""URL"")&#xa;    if url:&#xa;        scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)&#xa;        guess = """"&#xa;        # TODO(anatoli) - repository specific hacks should be handled by server&#xa;        if netloc == ""svn.python.org"" and scheme == ""svn+ssh"":&#xa;          path = ""projects"" + path&#xa;          scheme = ""http""&#xa;          guess = ""Python ""&#xa;        elif netloc.endswith("".googlecode.com""):&#xa;          scheme = ""http""&#xa;          guess = ""Google Code ""&#xa;        path = path + ""/""&#xa;        base = urlparse.urlunparse((scheme, netloc, path, params,&#xa;                                    query, fragment))&#xa;        logging.info(""Guessed %sbase = %s"", guess, base)&#xa;        return base&#xa;    if required:&#xa;      ErrorExit(""Can't find URL in output from svn info"")&#xa;    return None&#xa;&#xa;  def _GetInfo(self, key):&#xa;    """"""Parses 'svn info' for current dir. Returns value for key or None""""""&#xa;    for line in RunShell([""svn"", ""info""]).splitlines():&#xa;      if line.startswith(key + "": ""):&#xa;        return line.split("":"", 1)[1].strip()&#xa;&#xa;  def _EscapeFilename(self, filename):&#xa;    """"""Escapes filename for SVN commands.""""""&#xa;    if ""@"" in filename and not filename.endswith(""@""):&#xa;      filename = ""%s@"" % filename&#xa;    return filename&#xa;&#xa;  def GenerateDiff(self, args):&#xa;    cmd = [""svn"", ""diff""]&#xa;    if self.options.revision:&#xa;      cmd += [""-r"", self.options.revision]&#xa;    cmd.extend(args)&#xa;    data = RunShell(cmd)&#xa;    count = 0&#xa;    for line in data.splitlines():&#xa;      if line.startswith(""Index:"") or line.startswith(""Property changes on:""):&#xa;        count += 1&#xa;        logging.info(line)&#xa;    if not count:&#xa;      ErrorExit(""No valid patches found in output from svn diff"")&#xa;    return data&#xa;&#xa;  def _CollapseKeywords(self, content, keyword_str):&#xa;    """"""Collapses SVN keywords.""""""&#xa;    # svn cat translates keywords but svn diff doesn't. As a result of this&#xa;    # behavior patching.PatchChunks() fails with a chunk mismatch error.&#xa;    # This part was originally written by the Review Board development team&#xa;    # who had the same problem (http://reviews.review-board.org/r/276/).&#xa;    # Mapping of keywords to known aliases&#xa;    svn_keywords = {&#xa;      # Standard keywords&#xa;      'Date':                ['Date', 'LastChangedDate'],&#xa;      'Revision':            ['Revision', 'LastChangedRevision', 'Rev'],&#xa;      'Author':              ['Author', 'LastChangedBy'],&#xa;      'HeadURL':             ['HeadURL', 'URL'],&#xa;      'Id':                  ['Id'],&#xa;&#xa;      # Aliases&#xa;      'LastChangedDate':     ['LastChangedDate', 'Date'],&#xa;      'LastChangedRevision': ['LastChangedRevision', 'Rev', 'Revision'],&#xa;      'LastChangedBy':       ['LastChangedBy', 'Author'],&#xa;      'URL':                 ['URL', 'HeadURL'],&#xa;    }&#xa;&#xa;    def repl(m):&#xa;       if m.group(2):&#xa;         return ""$%s::%s$"" % (m.group(1), "" "" * len(m.group(3)))&#xa;       return ""$%s$"" % m.group(1)&#xa;    keywords = [keyword&#xa;                for name in keyword_str.split("" "")&#xa;                for keyword in svn_keywords.get(name, [])]&#xa;    return re.sub(r""\$(%s):(:?)([^\$]+)\$"" % '|'.join(keywords), repl, content)&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    status = RunShell([""svn"", ""status"", ""--ignore-externals""], silent_ok=True)&#xa;    unknown_files = []&#xa;    for line in status.split(""\n""):&#xa;      if line and line[0] == ""?"":&#xa;        unknown_files.append(line)&#xa;    return unknown_files&#xa;&#xa;  def ReadFile(self, filename):&#xa;    """"""Returns the contents of a file.""""""&#xa;    file = open(filename, 'rb')&#xa;    result = """"&#xa;    try:&#xa;      result = file.read()&#xa;    finally:&#xa;      file.close()&#xa;    return result&#xa;&#xa;  def GetStatus(self, filename):&#xa;    """"""Returns the status of a file.""""""&#xa;    if not self.options.revision:&#xa;      status = RunShell([""svn"", ""status"", ""--ignore-externals"",&#xa;                         self._EscapeFilename(filename)])&#xa;      if not status:&#xa;        ErrorExit(""svn status returned no output for %s"" % filename)&#xa;      status_lines = status.splitlines()&#xa;      # If file is in a cl, the output will begin with&#xa;      # ""\n--- Changelist 'cl_name':\n"".  See&#xa;      # http://svn.collab.net/repos/svn/trunk/notes/changelist-design.txt&#xa;      if (len(status_lines) == 3 and&#xa;          not status_lines[0] and&#xa;          status_lines[1].startswith(""--- Changelist"")):&#xa;        status = status_lines[2]&#xa;      else:&#xa;        status = status_lines[0]&#xa;    # If we have a revision to diff against we need to run ""svn list""&#xa;    # for the old and the new revision and compare the results to get&#xa;    # the correct status for a file.&#xa;    else:&#xa;      dirname, relfilename = os.path.split(filename)&#xa;      if dirname not in self.svnls_cache:&#xa;        cmd = [""svn"", ""list"", ""-r"", self.rev_start,&#xa;               self._EscapeFilename(dirname) or "".""]&#xa;        out, err, returncode = RunShellWithReturnCodeAndStderr(cmd)&#xa;        if returncode:&#xa;          # Directory might not yet exist at start revison&#xa;          # svn: Unable to find repository location for 'abc' in revision nnn&#xa;          if re.match('^svn: Unable to find repository location for .+ in revision \d+', err):&#xa;            old_files = ()&#xa;          else:&#xa;            ErrorExit(""Failed to get status for %s:\n%s"" % (filename, err))&#xa;        else:&#xa;          old_files = out.splitlines()&#xa;        args = [""svn"", ""list""]&#xa;        if self.rev_end:&#xa;          args += [""-r"", self.rev_end]&#xa;        cmd = args + [self._EscapeFilename(dirname) or "".""]&#xa;        out, returncode = RunShellWithReturnCode(cmd)&#xa;        if returncode:&#xa;          ErrorExit(""Failed to run command %s"" % cmd)&#xa;        self.svnls_cache[dirname] = (old_files, out.splitlines())&#xa;      old_files, new_files = self.svnls_cache[dirname]&#xa;      if relfilename in old_files and relfilename not in new_files:&#xa;        status = ""D   ""&#xa;      elif relfilename in old_files and relfilename in new_files:&#xa;        status = ""M   ""&#xa;      else:&#xa;        status = ""A   ""&#xa;    return status&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    status = self.GetStatus(filename)&#xa;    base_content = None&#xa;    new_content = None&#xa;&#xa;    # If a file is copied its status will be ""A  +"", which signifies&#xa;    # ""addition-with-history"".  See ""svn st"" for more information.  We need to&#xa;    # upload the original file or else diff parsing will fail if the file was&#xa;    # edited.&#xa;    if status[0] == ""A"" and status[3] != ""+"":&#xa;      # We'll need to upload the new content if we're adding a binary file&#xa;      # since diff's output won't contain it.&#xa;      mimetype = RunShell([""svn"", ""propget"", ""svn:mime-type"",&#xa;                           self._EscapeFilename(filename)], silent_ok=True)&#xa;      base_content = """"&#xa;      is_binary = bool(mimetype) and not mimetype.startswith(""text/"")&#xa;      if is_binary:&#xa;        new_content = self.ReadFile(filename)&#xa;    elif (status[0] in (""M"", ""D"", ""R"") or&#xa;          (status[0] == ""A"" and status[3] == ""+"") or  # Copied file.&#xa;          (status[0] == "" "" and status[1] == ""M"")):  # Property change.&#xa;      args = []&#xa;      if self.options.revision:&#xa;        # filename must not be escaped. We already add an ampersand here.&#xa;        url = ""%s/%s@%s"" % (self.svn_base, filename, self.rev_start)&#xa;      else:&#xa;        # Don't change filename, it's needed later.&#xa;        url = filename&#xa;        args += [""-r"", ""BASE""]&#xa;      cmd = [""svn""] + args + [""propget"", ""svn:mime-type"", url]&#xa;      mimetype, returncode = RunShellWithReturnCode(cmd)&#xa;      if returncode:&#xa;        # File does not exist in the requested revision.&#xa;        # Reset mimetype, it contains an error message.&#xa;        mimetype = """"&#xa;      else:&#xa;        mimetype = mimetype.strip()&#xa;      get_base = False&#xa;      # this test for binary is exactly the test prescribed by the&#xa;      # official SVN docs at&#xa;      # http://subversion.apache.org/faq.html#binary-files&#xa;      is_binary = (bool(mimetype) and&#xa;        not mimetype.startswith(""text/"") and&#xa;        mimetype not in (""image/x-xbitmap"", ""image/x-xpixmap""))&#xa;      if status[0] == "" "":&#xa;        # Empty base content just to force an upload.&#xa;        base_content = """"&#xa;      elif is_binary:&#xa;        get_base = True&#xa;        if status[0] == ""M"":&#xa;          if not self.rev_end:&#xa;            new_content = self.ReadFile(filename)&#xa;          else:&#xa;            url = ""%s/%s@%s"" % (self.svn_base, filename, self.rev_end)&#xa;            new_content = RunShell([""svn"", ""cat"", url],&#xa;                                   universal_newlines=True, silent_ok=True)&#xa;      else:&#xa;        get_base = True&#xa;&#xa;      if get_base:&#xa;        if is_binary:&#xa;          universal_newlines = False&#xa;        else:&#xa;          universal_newlines = True&#xa;        if self.rev_start:&#xa;          # ""svn cat -r REV delete_file.txt"" doesn't work. cat requires&#xa;          # the full URL with ""@REV"" appended instead of using ""-r"" option.&#xa;          url = ""%s/%s@%s"" % (self.svn_base, filename, self.rev_start)&#xa;          base_content = RunShell([""svn"", ""cat"", url],&#xa;                                  universal_newlines=universal_newlines,&#xa;                                  silent_ok=True)&#xa;        else:&#xa;          base_content, ret_code = RunShellWithReturnCode(&#xa;            [""svn"", ""cat"", self._EscapeFilename(filename)],&#xa;            universal_newlines=universal_newlines)&#xa;          if ret_code and status[0] == ""R"":&#xa;            # It's a replaced file without local history (see issue208).&#xa;            # The base file needs to be fetched from the server.&#xa;            url = ""%s/%s"" % (self.svn_base, filename)&#xa;            base_content = RunShell([""svn"", ""cat"", url],&#xa;                                    universal_newlines=universal_newlines,&#xa;                                    silent_ok=True)&#xa;          elif ret_code:&#xa;            ErrorExit(""Got error status from 'svn cat %s'"" % filename)&#xa;        if not is_binary:&#xa;          args = []&#xa;          if self.rev_start:&#xa;            url = ""%s/%s@%s"" % (self.svn_base, filename, self.rev_start)&#xa;          else:&#xa;            url = filename&#xa;            args += [""-r"", ""BASE""]&#xa;          cmd = [""svn""] + args + [""propget"", ""svn:keywords"", url]&#xa;          keywords, returncode = RunShellWithReturnCode(cmd)&#xa;          if keywords and not returncode:&#xa;            base_content = self._CollapseKeywords(base_content, keywords)&#xa;    else:&#xa;      StatusUpdate(""svn status returned unexpected output: %s"" % status)&#xa;      sys.exit(1)&#xa;    return base_content, new_content, is_binary, status[0:5]&#xa;&#xa;&#xa;class GitVCS(VersionControlSystem):&#xa;  """"""Implementation of the VersionControlSystem interface for Git.""""""&#xa;&#xa;  def __init__(self, options):&#xa;    super(GitVCS, self).__init__(options)&#xa;    # Map of filename -> (hash before, hash after) of base file.&#xa;    # Hashes for ""no such file"" are represented as None.&#xa;    self.hashes = {}&#xa;    # Map of new filename -> old filename for renames.&#xa;    self.renames = {}&#xa;&#xa;  def GetGUID(self):&#xa;    revlist = RunShell(""git rev-list --parents HEAD"".split()).splitlines()&#xa;    # M-A: Return the 1st root hash, there could be multiple when a&#xa;    # subtree is merged. In that case, more analysis would need to&#xa;    # be done to figure out which HEAD is the 'most representative'.&#xa;    for r in revlist:&#xa;      if ' ' not in r:&#xa;        return r&#xa;&#xa;  def PostProcessDiff(self, gitdiff):&#xa;    """"""Converts the diff output to include an svn-style ""Index:"" line as well&#xa;    as record the hashes of the files, so we can upload them along with our&#xa;    diff.""""""&#xa;    # Special used by git to indicate ""no such content"".&#xa;    NULL_HASH = ""0""*40&#xa;&#xa;    def IsFileNew(filename):&#xa;      return filename in self.hashes and self.hashes[filename][0] is None&#xa;&#xa;    def AddSubversionPropertyChange(filename):&#xa;      """"""Add svn's property change information into the patch if given file is&#xa;      new file.&#xa;&#xa;      We use Subversion's auto-props setting to retrieve its property.&#xa;      See http://svnbook.red-bean.com/en/1.1/ch07.html#svn-ch-7-sect-1.3.2 for&#xa;      Subversion's [auto-props] setting.&#xa;      """"""&#xa;      if self.options.emulate_svn_auto_props and IsFileNew(filename):&#xa;        svnprops = GetSubversionPropertyChanges(filename)&#xa;        if svnprops:&#xa;          svndiff.append(""\n"" + svnprops + ""\n"")&#xa;&#xa;    svndiff = []&#xa;    filecount = 0&#xa;    filename = None&#xa;    for line in gitdiff.splitlines():&#xa;      match = re.match(r""diff --git a/(.*) b/(.*)$"", line)&#xa;      if match:&#xa;        # Add auto property here for previously seen file.&#xa;        if filename is not None:&#xa;          AddSubversionPropertyChange(filename)&#xa;        filecount += 1&#xa;        # Intentionally use the ""after"" filename so we can show renames.&#xa;        filename = match.group(2)&#xa;        svndiff.append(""Index: %s\n"" % filename)&#xa;        if match.group(1) != match.group(2):&#xa;          self.renames[match.group(2)] = match.group(1)&#xa;      else:&#xa;        # The ""index"" line in a git diff looks like this (long hashes elided):&#xa;        #   index 82c0d44..b2cee3f 100755&#xa;        # We want to save the left hash, as that identifies the base file.&#xa;        match = re.match(r""index (\w+)\.\.(\w+)"", line)&#xa;        if match:&#xa;          before, after = (match.group(1), match.group(2))&#xa;          if before == NULL_HASH:&#xa;            before = None&#xa;          if after == NULL_HASH:&#xa;            after = None&#xa;          self.hashes[filename] = (before, after)&#xa;      svndiff.append(line + ""\n"")&#xa;    if not filecount:&#xa;      ErrorExit(""No valid patches found in output from git diff"")&#xa;    # Add auto property for the last seen file.&#xa;    assert filename is not None&#xa;    AddSubversionPropertyChange(filename)&#xa;    return """".join(svndiff)&#xa;&#xa;  def GenerateDiff(self, extra_args):&#xa;    extra_args = extra_args[:]&#xa;    if self.options.revision:&#xa;      if "":"" in self.options.revision:&#xa;        extra_args = self.options.revision.split("":"", 1) + extra_args&#xa;      else:&#xa;        extra_args = [self.options.revision] + extra_args&#xa;&#xa;    # --no-ext-diff is broken in some versions of Git, so try to work around&#xa;    # this by overriding the environment (but there is still a problem if the&#xa;    # git config key ""diff.external"" is used).&#xa;    env = os.environ.copy()&#xa;    if ""GIT_EXTERNAL_DIFF"" in env:&#xa;      del env[""GIT_EXTERNAL_DIFF""]&#xa;    # -M/-C will not print the diff for the deleted file when a file is renamed.&#xa;    # This is confusing because the original file will not be shown on the&#xa;    # review when a file is renamed. So, get a diff with ONLY deletes, then&#xa;    # append a diff (with rename detection), without deletes.&#xa;    cmd = [&#xa;        ""git"", ""diff"", ""--no-color"", ""--no-ext-diff"", ""--full-index"",&#xa;        ""--ignore-submodules"",&#xa;    ]&#xa;    diff = RunShell(&#xa;        cmd + [""--no-renames"", ""--diff-filter=D""] + extra_args,&#xa;        env=env, silent_ok=True)&#xa;    if self.options.git_find_copies:&#xa;      similarity_options = [""--find-copies-harder"", ""-l100000"",&#xa;                            ""-C%s"" % self.options.git_similarity ]&#xa;    else:&#xa;      similarity_options = [""-M%s"" % self.options.git_similarity ]&#xa;    diff += RunShell(&#xa;        cmd + [""--diff-filter=AMCRT""] + similarity_options + extra_args,&#xa;        env=env, silent_ok=True)&#xa;&#xa;    # The CL could be only file deletion or not. So accept silent diff for both&#xa;    # commands then check for an empty diff manually.&#xa;    if not diff:&#xa;      ErrorExit(""No output from %s"" % (cmd + extra_args))&#xa;    return diff&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    status = RunShell([""git"", ""ls-files"", ""--exclude-standard"", ""--others""],&#xa;                      silent_ok=True)&#xa;    return status.splitlines()&#xa;&#xa;  def GetFileContent(self, file_hash, is_binary):&#xa;    """"""Returns the content of a file identified by its git hash.""""""&#xa;    data, retcode = RunShellWithReturnCode([""git"", ""show"", file_hash],&#xa;                                            universal_newlines=not is_binary)&#xa;    if retcode:&#xa;      ErrorExit(""Got error status from 'git show %s'"" % file_hash)&#xa;    return data&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    hash_before, hash_after = self.hashes.get(filename, (None,None))&#xa;    base_content = None&#xa;    new_content = None&#xa;    status = None&#xa;&#xa;    if filename in self.renames:&#xa;      status = ""A +""  # Match svn attribute name for renames.&#xa;      if filename not in self.hashes:&#xa;        # If a rename doesn't change the content, we never get a hash.&#xa;        base_content = RunShell(&#xa;            [""git"", ""show"", ""HEAD:"" + filename], silent_ok=True)&#xa;    elif not hash_before:&#xa;      status = ""A""&#xa;      base_content = """"&#xa;    elif not hash_after:&#xa;      status = ""D""&#xa;    else:&#xa;      status = ""M""&#xa;&#xa;    is_binary = self.IsBinaryData(base_content)&#xa;    is_image = self.IsImage(filename)&#xa;&#xa;    # Grab the before/after content if we need it.&#xa;    # Grab the base content if we don't have it already.&#xa;    if base_content is None and hash_before:&#xa;      base_content = self.GetFileContent(hash_before, is_binary)&#xa;    # Only include the ""after"" file if it's an image; otherwise it&#xa;    # it is reconstructed from the diff.&#xa;    if is_image and hash_after:&#xa;      new_content = self.GetFileContent(hash_after, is_binary)&#xa;&#xa;    return (base_content, new_content, is_binary, status)&#xa;&#xa;&#xa;class CVSVCS(VersionControlSystem):&#xa;  """"""Implementation of the VersionControlSystem interface for CVS.""""""&#xa;&#xa;  def __init__(self, options):&#xa;    super(CVSVCS, self).__init__(options)&#xa;&#xa;  def GetGUID(self):&#xa;    """"""For now we don't know how to get repository ID for CVS""""""&#xa;    return&#xa;&#xa;  def GetOriginalContent_(self, filename):&#xa;    RunShell([""cvs"", ""up"", filename], silent_ok=True)&#xa;    # TODO need detect file content encoding&#xa;    content = open(filename).read()&#xa;    return content.replace(""\r\n"", ""\n"")&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    base_content = None&#xa;    new_content = None&#xa;    status = ""A""&#xa;&#xa;    output, retcode = RunShellWithReturnCode([""cvs"", ""status"", filename])&#xa;    if retcode:&#xa;      ErrorExit(""Got error status from 'cvs status %s'"" % filename)&#xa;&#xa;    if output.find(""Status: Locally Modified"") != -1:&#xa;      status = ""M""&#xa;      temp_filename = ""%s.tmp123"" % filename&#xa;      os.rename(filename, temp_filename)&#xa;      base_content = self.GetOriginalContent_(filename)&#xa;      os.rename(temp_filename, filename)&#xa;    elif output.find(""Status: Locally Added""):&#xa;      status = ""A""&#xa;      base_content = """"&#xa;    elif output.find(""Status: Needs Checkout""):&#xa;      status = ""D""&#xa;      base_content = self.GetOriginalContent_(filename)&#xa;&#xa;    return (base_content, new_content, self.IsBinaryData(base_content), status)&#xa;&#xa;  def GenerateDiff(self, extra_args):&#xa;    cmd = [""cvs"", ""diff"", ""-u"", ""-N""]&#xa;    if self.options.revision:&#xa;      cmd += [""-r"", self.options.revision]&#xa;&#xa;    cmd.extend(extra_args)&#xa;    data, retcode = RunShellWithReturnCode(cmd)&#xa;    count = 0&#xa;    if retcode in [0, 1]:&#xa;      for line in data.splitlines():&#xa;        if line.startswith(""Index:""):&#xa;          count += 1&#xa;          logging.info(line)&#xa;&#xa;    if not count:&#xa;      ErrorExit(""No valid patches found in output from cvs diff"")&#xa;&#xa;    return data&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    data, retcode = RunShellWithReturnCode([""cvs"", ""diff""])&#xa;    if retcode not in [0, 1]:&#xa;      ErrorExit(""Got error status from 'cvs diff':\n%s"" % (data,))&#xa;    unknown_files = []&#xa;    for line in data.split(""\n""):&#xa;      if line and line[0] == ""?"":&#xa;        unknown_files.append(line)&#xa;    return unknown_files&#xa;&#xa;class MercurialVCS(VersionControlSystem):&#xa;  """"""Implementation of the VersionControlSystem interface for Mercurial.""""""&#xa;&#xa;  def __init__(self, options, repo_dir):&#xa;    super(MercurialVCS, self).__init__(options)&#xa;    # Absolute path to repository (we can be in a subdir)&#xa;    self.repo_dir = os.path.normpath(repo_dir)&#xa;    # Compute the subdir&#xa;    cwd = os.path.normpath(os.getcwd())&#xa;    assert cwd.startswith(self.repo_dir)&#xa;    self.subdir = cwd[len(self.repo_dir):].lstrip(r""\/"")&#xa;    if self.options.revision:&#xa;      self.base_rev = self.options.revision&#xa;    else:&#xa;      self.base_rev = RunShell([""hg"", ""parent"", ""-q""]).split(':')[1].strip()&#xa;&#xa;  def GetGUID(self):&#xa;    # See chapter ""Uniquely identifying a repository""&#xa;    # http://hgbook.red-bean.com/read/customizing-the-output-of-mercurial.html&#xa;    info = RunShell(""hg log -r0 --template {node}"".split())&#xa;    return info.strip()&#xa;&#xa;  def _GetRelPath(self, filename):&#xa;    """"""Get relative path of a file according to the current directory,&#xa;    given its logical path in the repo.""""""&#xa;    absname = os.path.join(self.repo_dir, filename)&#xa;    return os.path.relpath(absname)&#xa;&#xa;  def GenerateDiff(self, extra_args):&#xa;    cmd = [""hg"", ""diff"", ""--git"", ""-r"", self.base_rev] + extra_args&#xa;    data = RunShell(cmd, silent_ok=True)&#xa;    svndiff = []&#xa;    filecount = 0&#xa;    for line in data.splitlines():&#xa;      m = re.match(""diff --git a/(\S+) b/(\S+)"", line)&#xa;      if m:&#xa;        # Modify line to make it look like as it comes from svn diff.&#xa;        # With this modification no changes on the server side are required&#xa;        # to make upload.py work with Mercurial repos.&#xa;        # NOTE: for proper handling of moved/copied files, we have to use&#xa;        # the second filename.&#xa;        filename = m.group(2)&#xa;        svndiff.append(""Index: %s"" % filename)&#xa;        svndiff.append(""="" * 67)&#xa;        filecount += 1&#xa;        logging.info(line)&#xa;      else:&#xa;        svndiff.append(line)&#xa;    if not filecount:&#xa;      ErrorExit(""No valid patches found in output from hg diff"")&#xa;    return ""\n"".join(svndiff) + ""\n""&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    """"""Return a list of files unknown to the VCS.""""""&#xa;    args = []&#xa;    status = RunShell([""hg"", ""status"", ""--rev"", self.base_rev, ""-u"", "".""],&#xa;        silent_ok=True)&#xa;    unknown_files = []&#xa;    for line in status.splitlines():&#xa;      st, fn = line.split("" "", 1)&#xa;      if st == ""?"":&#xa;        unknown_files.append(fn)&#xa;    return unknown_files&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    # ""hg status"" and ""hg cat"" both take a path relative to the current subdir,&#xa;    # but ""hg diff"" has given us the path relative to the repo root.&#xa;    base_content = """"&#xa;    new_content = None&#xa;    is_binary = False&#xa;    oldrelpath = relpath = self._GetRelPath(filename)&#xa;    # ""hg status -C"" returns two lines for moved/copied files, one otherwise&#xa;    out = RunShell([""hg"", ""status"", ""-C"", ""--rev"", self.base_rev, relpath])&#xa;    out = out.splitlines()&#xa;    # HACK: strip error message about missing file/directory if it isn't in&#xa;    # the working copy&#xa;    if out[0].startswith('%s: ' % relpath):&#xa;      out = out[1:]&#xa;    status, _ = out[0].split(' ', 1)&#xa;    if len(out) > 1 and status == ""A"":&#xa;      # Moved/copied => considered as modified, use old filename to&#xa;      # retrieve base contents&#xa;      oldrelpath = out[1].strip()&#xa;      status = ""M""&#xa;    if "":"" in self.base_rev:&#xa;      base_rev = self.base_rev.split("":"", 1)[0]&#xa;    else:&#xa;      base_rev = self.base_rev&#xa;    if status != ""A"":&#xa;      base_content = RunShell([""hg"", ""cat"", ""-r"", base_rev, oldrelpath],&#xa;        silent_ok=True)&#xa;      is_binary = self.IsBinaryData(base_content)&#xa;    if status != ""R"":&#xa;      new_content = open(relpath, ""rb"").read()&#xa;      is_binary = is_binary or self.IsBinaryData(new_content)&#xa;    if is_binary and base_content:&#xa;      # Fetch again without converting newlines&#xa;      base_content = RunShell([""hg"", ""cat"", ""-r"", base_rev, oldrelpath],&#xa;        silent_ok=True, universal_newlines=False)&#xa;    if not is_binary:&#xa;      new_content = None&#xa;    return base_content, new_content, is_binary, status&#xa;&#xa;&#xa;class PerforceVCS(VersionControlSystem):&#xa;  """"""Implementation of the VersionControlSystem interface for Perforce.""""""&#xa;&#xa;  def __init__(self, options):&#xa;&#xa;    def ConfirmLogin():&#xa;      # Make sure we have a valid perforce session&#xa;      while True:&#xa;        data, retcode = self.RunPerforceCommandWithReturnCode(&#xa;            [""login"", ""-s""], marshal_output=True)&#xa;        if not data:&#xa;          ErrorExit(""Error checking perforce login"")&#xa;        if not retcode and (not ""code"" in data or data[""code""] != ""error""):&#xa;          break&#xa;        print ""Enter perforce password: ""&#xa;        self.RunPerforceCommandWithReturnCode([""login""])&#xa;&#xa;    super(PerforceVCS, self).__init__(options)&#xa;&#xa;    self.p4_changelist = options.p4_changelist&#xa;    if not self.p4_changelist:&#xa;      ErrorExit(""A changelist id is required"")&#xa;    if (options.revision):&#xa;      ErrorExit(""--rev is not supported for perforce"")&#xa;&#xa;    self.p4_port = options.p4_port&#xa;    self.p4_client = options.p4_client&#xa;    self.p4_user = options.p4_user&#xa;&#xa;    ConfirmLogin()&#xa;&#xa;    if not options.title:&#xa;      description = self.RunPerforceCommand([""describe"", self.p4_changelist],&#xa;                                            marshal_output=True)&#xa;      if description and ""desc"" in description:&#xa;        # Rietveld doesn't support multi-line descriptions&#xa;        raw_title = description[""desc""].strip()&#xa;        lines = raw_title.splitlines()&#xa;        if len(lines):&#xa;          options.title = lines[0]&#xa;&#xa;  def GetGUID(self):&#xa;    """"""For now we don't know how to get repository ID for Perforce""""""&#xa;    return&#xa;&#xa;  def RunPerforceCommandWithReturnCode(self, extra_args, marshal_output=False,&#xa;                                       universal_newlines=True):&#xa;    args = [""p4""]&#xa;    if marshal_output:&#xa;      # -G makes perforce format its output as marshalled python objects&#xa;      args.extend([""-G""])&#xa;    if self.p4_port:&#xa;      args.extend([""-p"", self.p4_port])&#xa;    if self.p4_client:&#xa;      args.extend([""-c"", self.p4_client])&#xa;    if self.p4_user:&#xa;      args.extend([""-u"", self.p4_user])&#xa;    args.extend(extra_args)&#xa;&#xa;    data, retcode = RunShellWithReturnCode(&#xa;        args, print_output=False, universal_newlines=universal_newlines)&#xa;    if marshal_output and data:&#xa;      data = marshal.loads(data)&#xa;    return data, retcode&#xa;&#xa;  def RunPerforceCommand(self, extra_args, marshal_output=False,&#xa;                         universal_newlines=True):&#xa;    # This might be a good place to cache call results, since things like&#xa;    # describe or fstat might get called repeatedly.&#xa;    data, retcode = self.RunPerforceCommandWithReturnCode(&#xa;        extra_args, marshal_output, universal_newlines)&#xa;    if retcode:&#xa;      ErrorExit(""Got error status from %s:\n%s"" % (extra_args, data))&#xa;    return data&#xa;&#xa;  def GetFileProperties(self, property_key_prefix = """", command = ""describe""):&#xa;    description = self.RunPerforceCommand([""describe"", self.p4_changelist],&#xa;                                          marshal_output=True)&#xa;&#xa;    changed_files = {}&#xa;    file_index = 0&#xa;    # Try depotFile0, depotFile1, ... until we don't find a match&#xa;    while True:&#xa;      file_key = ""depotFile%d"" % file_index&#xa;      if file_key in description:&#xa;        filename = description[file_key]&#xa;        change_type = description[property_key_prefix + str(file_index)]&#xa;        changed_files[filename] = change_type&#xa;        file_index += 1&#xa;      else:&#xa;        break&#xa;    return changed_files&#xa;&#xa;  def GetChangedFiles(self):&#xa;    return self.GetFileProperties(""action"")&#xa;&#xa;  def GetUnknownFiles(self):&#xa;    # Perforce doesn't detect new files, they have to be explicitly added&#xa;    return []&#xa;&#xa;  def IsBaseBinary(self, filename):&#xa;    base_filename = self.GetBaseFilename(filename)&#xa;    return self.IsBinaryHelper(base_filename, ""files"")&#xa;&#xa;  def IsPendingBinary(self, filename):&#xa;    return self.IsBinaryHelper(filename, ""describe"")&#xa;&#xa;  def IsBinaryHelper(self, filename, command):&#xa;    file_types = self.GetFileProperties(""type"", command)&#xa;    if not filename in file_types:&#xa;      ErrorExit(""Trying to check binary status of unknown file %s."" % filename)&#xa;    # This treats symlinks, macintosh resource files, temporary objects, and&#xa;    # unicode as binary. See the Perforce docs for more details:&#xa;    # http://www.perforce.com/perforce/doc.current/manuals/cmdref/o.ftypes.html&#xa;    return not file_types[filename].endswith(""text"")&#xa;&#xa;  def GetFileContent(self, filename, revision, is_binary):&#xa;    file_arg = filename&#xa;    if revision:&#xa;      file_arg += ""#"" + revision&#xa;    # -q suppresses the initial line that displays the filename and revision&#xa;    return self.RunPerforceCommand([""print"", ""-q"", file_arg],&#xa;                                   universal_newlines=not is_binary)&#xa;&#xa;  def GetBaseFilename(self, filename):&#xa;    actionsWithDifferentBases = [&#xa;        ""move/add"", # p4 move&#xa;        ""branch"", # p4 integrate (to a new file), similar to hg ""add""&#xa;        ""add"", # p4 integrate (to a new file), after modifying the new file&#xa;    ]&#xa;&#xa;    # We only see a different base for ""add"" if this is a downgraded branch&#xa;    # after a file was branched (integrated), then edited.&#xa;    if self.GetAction(filename) in actionsWithDifferentBases:&#xa;      # -Or shows information about pending integrations/moves&#xa;      fstat_result = self.RunPerforceCommand([""fstat"", ""-Or"", filename],&#xa;                                             marshal_output=True)&#xa;&#xa;      baseFileKey = ""resolveFromFile0"" # I think it's safe to use only file0&#xa;      if baseFileKey in fstat_result:&#xa;        return fstat_result[baseFileKey]&#xa;&#xa;    return filename&#xa;&#xa;  def GetBaseRevision(self, filename):&#xa;    base_filename = self.GetBaseFilename(filename)&#xa;&#xa;    have_result = self.RunPerforceCommand([""have"", base_filename],&#xa;                                          marshal_output=True)&#xa;    if ""haveRev"" in have_result:&#xa;      return have_result[""haveRev""]&#xa;&#xa;  def GetLocalFilename(self, filename):&#xa;    where = self.RunPerforceCommand([""where"", filename], marshal_output=True)&#xa;    if ""path"" in where:&#xa;      return where[""path""]&#xa;&#xa;  def GenerateDiff(self, args):&#xa;    class DiffData:&#xa;      def __init__(self, perforceVCS, filename, action):&#xa;        self.perforceVCS = perforceVCS&#xa;        self.filename = filename&#xa;        self.action = action&#xa;        self.base_filename = perforceVCS.GetBaseFilename(filename)&#xa;&#xa;        self.file_body = None&#xa;        self.base_rev = None&#xa;        self.prefix = None&#xa;        self.working_copy = True&#xa;        self.change_summary = None&#xa;&#xa;    def GenerateDiffHeader(diffData):&#xa;      header = []&#xa;      header.append(""Index: %s"" % diffData.filename)&#xa;      header.append(""="" * 67)&#xa;&#xa;      if diffData.base_filename != diffData.filename:&#xa;        if diffData.action.startswith(""move""):&#xa;          verb = ""rename""&#xa;        else:&#xa;          verb = ""copy""&#xa;        header.append(""%s from %s"" % (verb, diffData.base_filename))&#xa;        header.append(""%s to %s"" % (verb, diffData.filename))&#xa;&#xa;      suffix = ""\t(revision %s)"" % diffData.base_rev&#xa;      header.append(""--- "" + diffData.base_filename + suffix)&#xa;      if diffData.working_copy:&#xa;        suffix = ""\t(working copy)""&#xa;      header.append(""+++ "" + diffData.filename + suffix)&#xa;      if diffData.change_summary:&#xa;        header.append(diffData.change_summary)&#xa;      return header&#xa;&#xa;    def GenerateMergeDiff(diffData, args):&#xa;      # -du generates a unified diff, which is nearly svn format&#xa;      diffData.file_body = self.RunPerforceCommand(&#xa;          [""diff"", ""-du"", diffData.filename] + args)&#xa;      diffData.base_rev = self.GetBaseRevision(diffData.filename)&#xa;      diffData.prefix = """"&#xa;&#xa;      # We have to replace p4's file status output (the lines starting&#xa;      # with +++ or ---) to match svn's diff format&#xa;      lines = diffData.file_body.splitlines()&#xa;      first_good_line = 0&#xa;      while (first_good_line < len(lines) and&#xa;            not lines[first_good_line].startswith(""@@"")):&#xa;        first_good_line += 1&#xa;      diffData.file_body = ""\n"".join(lines[first_good_line:])&#xa;      return diffData&#xa;&#xa;    def GenerateAddDiff(diffData):&#xa;      fstat = self.RunPerforceCommand([""fstat"", diffData.filename],&#xa;                                      marshal_output=True)&#xa;      if ""headRev"" in fstat:&#xa;        diffData.base_rev = fstat[""headRev""] # Re-adding a deleted file&#xa;      else:&#xa;        diffData.base_rev = ""0"" # Brand new file&#xa;      diffData.working_copy = False&#xa;      rel_path = self.GetLocalFilename(diffData.filename)&#xa;      diffData.file_body = open(rel_path, 'r').read()&#xa;      # Replicate svn's list of changed lines&#xa;      line_count = len(diffData.file_body.splitlines())&#xa;      diffData.change_summary = ""@@ -0,0 +1""&#xa;      if line_count > 1:&#xa;          diffData.change_summary += "",%d"" % line_count&#xa;      diffData.change_summary += "" @@""&#xa;      diffData.prefix = ""+""&#xa;      return diffData&#xa;&#xa;    def GenerateDeleteDiff(diffData):&#xa;      diffData.base_rev = self.GetBaseRevision(diffData.filename)&#xa;      is_base_binary = self.IsBaseBinary(diffData.filename)&#xa;      # For deletes, base_filename == filename&#xa;      diffData.file_body = self.GetFileContent(diffData.base_filename,&#xa;          None,&#xa;          is_base_binary)&#xa;      # Replicate svn's list of changed lines&#xa;      line_count = len(diffData.file_body.splitlines())&#xa;      diffData.change_summary = ""@@ -1""&#xa;      if line_count > 1:&#xa;        diffData.change_summary += "",%d"" % line_count&#xa;      diffData.change_summary += "" +0,0 @@""&#xa;      diffData.prefix = ""-""&#xa;      return diffData&#xa;&#xa;    changed_files = self.GetChangedFiles()&#xa;&#xa;    svndiff = []&#xa;    filecount = 0&#xa;    for (filename, action) in changed_files.items():&#xa;      svn_status = self.PerforceActionToSvnStatus(action)&#xa;      if svn_status == ""SKIP"":&#xa;        continue&#xa;&#xa;      diffData = DiffData(self, filename, action)&#xa;      # Is it possible to diff a branched file? Stackoverflow says no:&#xa;      # http://stackoverflow.com/questions/1771314/in-perforce-command-line-how-to-diff-a-file-reopened-for-add&#xa;      if svn_status == ""M"":&#xa;        diffData = GenerateMergeDiff(diffData, args)&#xa;      elif svn_status == ""A"":&#xa;        diffData = GenerateAddDiff(diffData)&#xa;      elif svn_status == ""D"":&#xa;        diffData = GenerateDeleteDiff(diffData)&#xa;      else:&#xa;        ErrorExit(""Unknown file action %s (svn action %s)."" % \&#xa;                  (action, svn_status))&#xa;&#xa;      svndiff += GenerateDiffHeader(diffData)&#xa;&#xa;      for line in diffData.file_body.splitlines():&#xa;        svndiff.append(diffData.prefix + line)&#xa;      filecount += 1&#xa;    if not filecount:&#xa;      ErrorExit(""No valid patches found in output from p4 diff"")&#xa;    return ""\n"".join(svndiff) + ""\n""&#xa;&#xa;  def PerforceActionToSvnStatus(self, status):&#xa;    # Mirroring the list at http://permalink.gmane.org/gmane.comp.version-control.mercurial.devel/28717&#xa;    # Is there something more official?&#xa;    return {&#xa;            ""add"" : ""A"",&#xa;            ""branch"" : ""A"",&#xa;            ""delete"" : ""D"",&#xa;            ""edit"" : ""M"", # Also includes changing file types.&#xa;            ""integrate"" : ""M"",&#xa;            ""move/add"" : ""M"",&#xa;            ""move/delete"": ""SKIP"",&#xa;            ""purge"" : ""D"", # How does a file's status become ""purge""?&#xa;            }[status]&#xa;&#xa;  def GetAction(self, filename):&#xa;    changed_files = self.GetChangedFiles()&#xa;    if not filename in changed_files:&#xa;      ErrorExit(""Trying to get base version of unknown file %s."" % filename)&#xa;&#xa;    return changed_files[filename]&#xa;&#xa;  def GetBaseFile(self, filename):&#xa;    base_filename = self.GetBaseFilename(filename)&#xa;    base_content = """"&#xa;    new_content = None&#xa;&#xa;    status = self.PerforceActionToSvnStatus(self.GetAction(filename))&#xa;&#xa;    if status != ""A"":&#xa;      revision = self.GetBaseRevision(base_filename)&#xa;      if not revision:&#xa;        ErrorExit(""Couldn't find base revision for file %s"" % filename)&#xa;      is_base_binary = self.IsBaseBinary(base_filename)&#xa;      base_content = self.GetFileContent(base_filename,&#xa;                                         revision,&#xa;                                         is_base_binary)&#xa;&#xa;    is_binary = self.IsPendingBinary(filename)&#xa;    if status != ""D"" and status != ""SKIP"":&#xa;      relpath = self.GetLocalFilename(filename)&#xa;      if is_binary:&#xa;        new_content = open(relpath, ""rb"").read()&#xa;&#xa;    return base_content, new_content, is_binary, status&#xa;&#xa;# NOTE: The SplitPatch function is duplicated in engine.py, keep them in sync.&#xa;def SplitPatch(data):&#xa;  """"""Splits a patch into separate pieces for each file.&#xa;&#xa;  Args:&#xa;    data: A string containing the output of svn diff.&#xa;&#xa;  Returns:&#xa;    A list of 2-tuple (filename, text) where text is the svn diff output&#xa;      pertaining to filename.&#xa;  """"""&#xa;  patches = []&#xa;  filename = None&#xa;  diff = []&#xa;  for line in data.splitlines(True):&#xa;    new_filename = None&#xa;    if line.startswith('Index:'):&#xa;      unused, new_filename = line.split(':', 1)&#xa;      new_filename = new_filename.strip()&#xa;    elif line.startswith('Property changes on:'):&#xa;      unused, temp_filename = line.split(':', 1)&#xa;      # When a file is modified, paths use '/' between directories, however&#xa;      # when a property is modified '\' is used on Windows.  Make them the same&#xa;      # otherwise the file shows up twice.&#xa;      temp_filename = temp_filename.strip().replace('\\', '/')&#xa;      if temp_filename != filename:&#xa;        # File has property changes but no modifications, create a new diff.&#xa;        new_filename = temp_filename&#xa;    if new_filename:&#xa;      if filename and diff:&#xa;        patches.append((filename, ''.join(diff)))&#xa;      filename = new_filename&#xa;      diff = [line]&#xa;      continue&#xa;    if diff is not None:&#xa;      diff.append(line)&#xa;  if filename and diff:&#xa;    patches.append((filename, ''.join(diff)))&#xa;  return patches&#xa;&#xa;&#xa;def UploadSeparatePatches(issue, rpc_server, patchset, data, options):&#xa;  """"""Uploads a separate patch for each file in the diff output.&#xa;&#xa;  Returns a list of [patch_key, filename] for each file.&#xa;  """"""&#xa;  patches = SplitPatch(data)&#xa;  rv = []&#xa;  for patch in patches:&#xa;    if len(patch[1]) > MAX_UPLOAD_SIZE:&#xa;      print (""Not uploading the patch for "" + patch[0] +&#xa;             "" because the file is too large."")&#xa;      continue&#xa;    form_fields = [(""filename"", patch[0])]&#xa;    if not options.download_base:&#xa;      form_fields.append((""content_upload"", ""1""))&#xa;    files = [(""data"", ""data.diff"", patch[1])]&#xa;    ctype, body = EncodeMultipartFormData(form_fields, files)&#xa;    url = ""/%d/upload_patch/%d"" % (int(issue), int(patchset))&#xa;    print ""Uploading patch for "" + patch[0]&#xa;    response_body = rpc_server.Send(url, body, content_type=ctype)&#xa;    lines = response_body.splitlines()&#xa;    if not lines or lines[0] != ""OK"":&#xa;      StatusUpdate(""  --> %s"" % response_body)&#xa;      sys.exit(1)&#xa;    rv.append([lines[1], patch[0]])&#xa;  return rv&#xa;&#xa;&#xa;def GuessVCSName(options):&#xa;  """"""Helper to guess the version control system.&#xa;&#xa;  This examines the current directory, guesses which VersionControlSystem&#xa;  we're using, and returns an string indicating which VCS is detected.&#xa;&#xa;  Returns:&#xa;    A pair (vcs, output).  vcs is a string indicating which VCS was detected&#xa;    and is one of VCS_GIT, VCS_MERCURIAL, VCS_SUBVERSION, VCS_PERFORCE,&#xa;    VCS_CVS, or VCS_UNKNOWN.&#xa;    Since local perforce repositories can't be easily detected, this method&#xa;    will only guess VCS_PERFORCE if any perforce options have been specified.&#xa;    output is a string containing any interesting output from the vcs&#xa;    detection routine, or None if there is nothing interesting.&#xa;  """"""&#xa;  for attribute, value in options.__dict__.iteritems():&#xa;    if attribute.startswith(""p4"") and value != None:&#xa;      return (VCS_PERFORCE, None)&#xa;&#xa;  def RunDetectCommand(vcs_type, command):&#xa;    """"""Helper to detect VCS by executing command.&#xa;&#xa;    Returns:&#xa;       A pair (vcs, output) or None. Throws exception on error.&#xa;    """"""&#xa;    try:&#xa;      out, returncode = RunShellWithReturnCode(command)&#xa;      if returncode == 0:&#xa;        return (vcs_type, out.strip())&#xa;    except OSError, (errcode, message):&#xa;      if errcode != errno.ENOENT:  # command not found code&#xa;        raise&#xa;&#xa;  # Mercurial has a command to get the base directory of a repository&#xa;  # Try running it, but don't die if we don't have hg installed.&#xa;  # NOTE: we try Mercurial first as it can sit on top of an SVN working copy.&#xa;  res = RunDetectCommand(VCS_MERCURIAL, [""hg"", ""root""])&#xa;  if res != None:&#xa;    return res&#xa;&#xa;  # Subversion from 1.7 has a single centralized .svn folder&#xa;  # ( see http://subversion.apache.org/docs/release-notes/1.7.html#wc-ng )&#xa;  # That's why we use 'svn info' instead of checking for .svn dir&#xa;  res = RunDetectCommand(VCS_SUBVERSION, [""svn"", ""info""])&#xa;  if res != None:&#xa;    return res&#xa;&#xa;  # Git has a command to test if you're in a git tree.&#xa;  # Try running it, but don't die if we don't have git installed.&#xa;  res = RunDetectCommand(VCS_GIT, [""git"", ""rev-parse"",&#xa;                                   ""--is-inside-work-tree""])&#xa;  if res != None:&#xa;    return res&#xa;&#xa;  # detect CVS repos use `cvs status && $? == 0` rules&#xa;  res = RunDetectCommand(VCS_CVS, [""cvs"", ""status""])&#xa;  if res != None:&#xa;    return res&#xa;&#xa;  return (VCS_UNKNOWN, None)&#xa;&#xa;&#xa;def GuessVCS(options):&#xa;  """"""Helper to guess the version control system.&#xa;&#xa;  This verifies any user-specified VersionControlSystem (by command line&#xa;  or environment variable).  If the user didn't specify one, this examines&#xa;  the current directory, guesses which VersionControlSystem we're using,&#xa;  and returns an instance of the appropriate class.  Exit with an error&#xa;  if we can't figure it out.&#xa;&#xa;  Returns:&#xa;    A VersionControlSystem instance. Exits if the VCS can't be guessed.&#xa;  """"""&#xa;  vcs = options.vcs&#xa;  if not vcs:&#xa;    vcs = os.environ.get(""CODEREVIEW_VCS"")&#xa;  if vcs:&#xa;    v = VCS_ABBREVIATIONS.get(vcs.lower())&#xa;    if v is None:&#xa;      ErrorExit(""Unknown version control system %r specified."" % vcs)&#xa;    (vcs, extra_output) = (v, None)&#xa;  else:&#xa;    (vcs, extra_output) = GuessVCSName(options)&#xa;&#xa;  if vcs == VCS_MERCURIAL:&#xa;    if extra_output is None:&#xa;      extra_output = RunShell([""hg"", ""root""]).strip()&#xa;    return MercurialVCS(options, extra_output)&#xa;  elif vcs == VCS_SUBVERSION:&#xa;    return SubversionVCS(options)&#xa;  elif vcs == VCS_PERFORCE:&#xa;    return PerforceVCS(options)&#xa;  elif vcs == VCS_GIT:&#xa;    return GitVCS(options)&#xa;  elif vcs == VCS_CVS:&#xa;    return CVSVCS(options)&#xa;&#xa;  ErrorExit((""Could not guess version control system. ""&#xa;             ""Are you in a working copy directory?""))&#xa;&#xa;&#xa;def CheckReviewer(reviewer):&#xa;  """"""Validate a reviewer -- either a nickname or an email addres.&#xa;&#xa;  Args:&#xa;    reviewer: A nickname or an email address.&#xa;&#xa;  Calls ErrorExit() if it is an invalid email address.&#xa;  """"""&#xa;  if ""@"" not in reviewer:&#xa;    return  # Assume nickname&#xa;  parts = reviewer.split(""@"")&#xa;  if len(parts) > 2:&#xa;    ErrorExit(""Invalid email address: %r"" % reviewer)&#xa;  assert len(parts) == 2&#xa;  if ""."" not in parts[1]:&#xa;    ErrorExit(""Invalid email address: %r"" % reviewer)&#xa;&#xa;&#xa;def LoadSubversionAutoProperties():&#xa;  """"""Returns the content of [auto-props] section of Subversion's config file as&#xa;  a dictionary.&#xa;&#xa;  Returns:&#xa;    A dictionary whose key-value pair corresponds the [auto-props] section's&#xa;      key-value pair.&#xa;    In following cases, returns empty dictionary:&#xa;      - config file doesn't exist, or&#xa;      - 'enable-auto-props' is not set to 'true-like-value' in [miscellany].&#xa;  """"""&#xa;  if os.name == 'nt':&#xa;    subversion_config = os.environ.get(""APPDATA"") + ""\\Subversion\\config""&#xa;  else:&#xa;    subversion_config = os.path.expanduser(""~/.subversion/config"")&#xa;  if not os.path.exists(subversion_config):&#xa;    return {}&#xa;  config = ConfigParser.ConfigParser()&#xa;  config.read(subversion_config)&#xa;  if (config.has_section(""miscellany"") and&#xa;      config.has_option(""miscellany"", ""enable-auto-props"") and&#xa;      config.getboolean(""miscellany"", ""enable-auto-props"") and&#xa;      config.has_section(""auto-props"")):&#xa;    props = {}&#xa;    for file_pattern in config.options(""auto-props""):&#xa;      props[file_pattern] = ParseSubversionPropertyValues(&#xa;        config.get(""auto-props"", file_pattern))&#xa;    return props&#xa;  else:&#xa;    return {}&#xa;&#xa;def ParseSubversionPropertyValues(props):&#xa;  """"""Parse the given property value which comes from [auto-props] section and&#xa;  returns a list whose element is a (svn_prop_key, svn_prop_value) pair.&#xa;&#xa;  See the following doctest for example.&#xa;&#xa;  >>> ParseSubversionPropertyValues('svn:eol-style=LF')&#xa;  [('svn:eol-style', 'LF')]&#xa;  >>> ParseSubversionPropertyValues('svn:mime-type=image/jpeg')&#xa;  [('svn:mime-type', 'image/jpeg')]&#xa;  >>> ParseSubversionPropertyValues('svn:eol-style=LF;svn:executable')&#xa;  [('svn:eol-style', 'LF'), ('svn:executable', '*')]&#xa;  """"""&#xa;  key_value_pairs = []&#xa;  for prop in props.split("";""):&#xa;    key_value = prop.split(""="")&#xa;    assert len(key_value) <= 2&#xa;    if len(key_value) == 1:&#xa;      # If value is not given, use '*' as a Subversion's convention.&#xa;      key_value_pairs.append((key_value[0], ""*""))&#xa;    else:&#xa;      key_value_pairs.append((key_value[0], key_value[1]))&#xa;  return key_value_pairs&#xa;&#xa;&#xa;def GetSubversionPropertyChanges(filename):&#xa;  """"""Return a Subversion's 'Property changes on ...' string, which is used in&#xa;  the patch file.&#xa;&#xa;  Args:&#xa;    filename: filename whose property might be set by [auto-props] config.&#xa;&#xa;  Returns:&#xa;    A string like 'Property changes on |filename| ...' if given |filename|&#xa;      matches any entries in [auto-props] section. None, otherwise.&#xa;  """"""&#xa;  global svn_auto_props_map&#xa;  if svn_auto_props_map is None:&#xa;    svn_auto_props_map = LoadSubversionAutoProperties()&#xa;&#xa;  all_props = []&#xa;  for file_pattern, props in svn_auto_props_map.items():&#xa;    if fnmatch.fnmatch(filename, file_pattern):&#xa;      all_props.extend(props)&#xa;  if all_props:&#xa;    return FormatSubversionPropertyChanges(filename, all_props)&#xa;  return None&#xa;&#xa;&#xa;def FormatSubversionPropertyChanges(filename, props):&#xa;  """"""Returns Subversion's 'Property changes on ...' strings using given filename&#xa;  and properties.&#xa;&#xa;  Args:&#xa;    filename: filename&#xa;    props: A list whose element is a (svn_prop_key, svn_prop_value) pair.&#xa;&#xa;  Returns:&#xa;    A string which can be used in the patch file for Subversion.&#xa;&#xa;  See the following doctest for example.&#xa;&#xa;  >>> print FormatSubversionPropertyChanges('foo.cc', [('svn:eol-style', 'LF')])&#xa;  Property changes on: foo.cc&#xa;  ___________________________________________________________________&#xa;  Added: svn:eol-style&#xa;     + LF&#xa;  <BLANKLINE>&#xa;  """"""&#xa;  prop_changes_lines = [&#xa;    ""Property changes on: %s"" % filename,&#xa;    ""___________________________________________________________________""]&#xa;  for key, value in props:&#xa;    prop_changes_lines.append(""Added: "" + key)&#xa;    prop_changes_lines.append(""   + "" + value)&#xa;  return ""\n"".join(prop_changes_lines) + ""\n""&#xa;&#xa;&#xa;def RealMain(argv, data=None):&#xa;  """"""The real main function.&#xa;&#xa;  Args:&#xa;    argv: Command line arguments.&#xa;    data: Diff contents. If None (default) the diff is generated by&#xa;      the VersionControlSystem implementation returned by GuessVCS().&#xa;&#xa;  Returns:&#xa;    A 2-tuple (issue id, patchset id).&#xa;    The patchset id is None if the base files are not uploaded by this&#xa;    script (applies only to SVN checkouts).&#xa;  """"""&#xa;  options, args = parser.parse_args(argv[1:])&#xa;  if options.help:&#xa;    if options.verbose < 2:&#xa;      # hide Perforce options&#xa;      parser.epilog = ""Use '--help -v' to show additional Perforce options.""&#xa;      parser.option_groups.remove(parser.get_option_group('--p4_port'))&#xa;    parser.print_help()&#xa;    sys.exit(0)&#xa;&#xa;  global verbosity&#xa;  verbosity = options.verbose&#xa;  if verbosity >= 3:&#xa;    logging.getLogger().setLevel(logging.DEBUG)&#xa;  elif verbosity >= 2:&#xa;    logging.getLogger().setLevel(logging.INFO)&#xa;&#xa;  vcs = GuessVCS(options)&#xa;&#xa;  base = options.base_url&#xa;  if isinstance(vcs, SubversionVCS):&#xa;    # Guessing the base field is only supported for Subversion.&#xa;    # Note: Fetching base files may become deprecated in future releases.&#xa;    guessed_base = vcs.GuessBase(options.download_base)&#xa;    if base:&#xa;      if guessed_base and base != guessed_base:&#xa;        print ""Using base URL \""%s\"" from --base_url instead of \""%s\"""" % \&#xa;            (base, guessed_base)&#xa;    else:&#xa;      base = guessed_base&#xa;&#xa;  if not base and options.download_base:&#xa;    options.download_base = True&#xa;    logging.info(""Enabled upload of base file"")&#xa;  if not options.assume_yes:&#xa;    vcs.CheckForUnknownFiles()&#xa;  if data is None:&#xa;    data = vcs.GenerateDiff(args)&#xa;  data = vcs.PostProcessDiff(data)&#xa;  if options.print_diffs:&#xa;    print ""Rietveld diff start:*****""&#xa;    print data&#xa;    print ""Rietveld diff end:*****""&#xa;  files = vcs.GetBaseFiles(data)&#xa;  if verbosity >= 1:&#xa;    print ""Upload server:"", options.server, ""(change with -s/--server)""&#xa;  rpc_server = GetRpcServer(options.server,&#xa;                            options.email,&#xa;                            options.host,&#xa;                            options.save_cookies,&#xa;                            options.account_type)&#xa;  form_fields = []&#xa;&#xa;  repo_guid = vcs.GetGUID()&#xa;  if repo_guid:&#xa;    form_fields.append((""repo_guid"", repo_guid))&#xa;  if base:&#xa;    b = urlparse.urlparse(base)&#xa;    username, netloc = urllib.splituser(b.netloc)&#xa;    if username:&#xa;      logging.info(""Removed username from base URL"")&#xa;      base = urlparse.urlunparse((b.scheme, netloc, b.path, b.params,&#xa;                                  b.query, b.fragment))&#xa;    form_fields.append((""base"", base))&#xa;  if options.issue:&#xa;    form_fields.append((""issue"", str(options.issue)))&#xa;  if options.email:&#xa;    form_fields.append((""user"", options.email))&#xa;  if options.reviewers:&#xa;    for reviewer in options.reviewers.split(','):&#xa;      CheckReviewer(reviewer)&#xa;    form_fields.append((""reviewers"", options.reviewers))&#xa;  if options.cc:&#xa;    for cc in options.cc.split(','):&#xa;      CheckReviewer(cc)&#xa;    form_fields.append((""cc"", options.cc))&#xa;&#xa;  # Process --message, --title and --file.&#xa;  message = options.message or """"&#xa;  title = options.title or """"&#xa;  if options.file:&#xa;    if options.message:&#xa;      ErrorExit(""Can't specify both message and message file options"")&#xa;    file = open(options.file, 'r')&#xa;    message = file.read()&#xa;    file.close()&#xa;  if options.issue:&#xa;    prompt = ""Title describing this patch set: ""&#xa;  else:&#xa;    prompt = ""New issue subject: ""&#xa;  title = (&#xa;      title or message.split('\n', 1)[0].strip() or raw_input(prompt).strip())&#xa;  if not title and not options.issue:&#xa;    ErrorExit(""A non-empty title is required for a new issue"")&#xa;  # For existing issues, it's fine to give a patchset an empty name. Rietveld&#xa;  # doesn't accept that so use a whitespace.&#xa;  title = title or "" ""&#xa;  if len(title) > 100:&#xa;    title = title[:99] + '…'&#xa;  if title and not options.issue:&#xa;    message = message or title&#xa;&#xa;  form_fields.append((""subject"", title))&#xa;  # If it's a new issue send message as description. Otherwise a new&#xa;  # message is created below on upload_complete.&#xa;  if message and not options.issue:&#xa;    form_fields.append((""description"", message))&#xa;&#xa;  # Send a hash of all the base file so the server can determine if a copy&#xa;  # already exists in an earlier patchset.&#xa;  base_hashes = """"&#xa;  for file, info in files.iteritems():&#xa;    if not info[0] is None:&#xa;      checksum = md5(info[0]).hexdigest()&#xa;      if base_hashes:&#xa;        base_hashes += ""|""&#xa;      base_hashes += checksum + "":"" + file&#xa;  form_fields.append((""base_hashes"", base_hashes))&#xa;  if options.private:&#xa;    if options.issue:&#xa;      print ""Warning: Private flag ignored when updating an existing issue.""&#xa;    else:&#xa;      form_fields.append((""private"", ""1""))&#xa;  if options.send_patch:&#xa;    options.send_mail = True&#xa;  if not options.download_base:&#xa;    form_fields.append((""content_upload"", ""1""))&#xa;  if len(data) > MAX_UPLOAD_SIZE:&#xa;    print ""Patch is large, so uploading file patches separately.""&#xa;    uploaded_diff_file = []&#xa;    form_fields.append((""separate_patches"", ""1""))&#xa;  else:&#xa;    uploaded_diff_file = [(""data"", ""data.diff"", data)]&#xa;  ctype, body = EncodeMultipartFormData(form_fields, uploaded_diff_file)&#xa;  response_body = rpc_server.Send(""/upload"", body, content_type=ctype)&#xa;  patchset = None&#xa;  if not options.download_base or not uploaded_diff_file:&#xa;    lines = response_body.splitlines()&#xa;    if len(lines) >= 2:&#xa;      msg = lines[0]&#xa;      patchset = lines[1].strip()&#xa;      patches = [x.split("" "", 1) for x in lines[2:]]&#xa;    else:&#xa;      msg = response_body&#xa;  else:&#xa;    msg = response_body&#xa;  StatusUpdate(msg)&#xa;  if not response_body.startswith(""Issue created."") and \&#xa;  not response_body.startswith(""Issue updated.""):&#xa;    sys.exit(0)&#xa;  issue = msg[msg.rfind(""/"")+1:]&#xa;&#xa;  if not uploaded_diff_file:&#xa;    result = UploadSeparatePatches(issue, rpc_server, patchset, data, options)&#xa;    if not options.download_base:&#xa;      patches = result&#xa;&#xa;  if not options.download_base:&#xa;    vcs.UploadBaseFiles(issue, rpc_server, patches, patchset, options, files)&#xa;&#xa;  payload = {}  # payload for final request&#xa;  if options.send_mail:&#xa;    payload[""send_mail""] = ""yes""&#xa;    if options.send_patch:&#xa;      payload[""attach_patch""] = ""yes""&#xa;  if options.issue and message:&#xa;    payload[""message""] = message&#xa;  payload = urllib.urlencode(payload)&#xa;  rpc_server.Send(""/"" + issue + ""/upload_complete/"" + (patchset or """"),&#xa;                  payload=payload)&#xa;  return issue, patchset&#xa;&#xa;&#xa;def main():&#xa;  try:&#xa;    logging.basicConfig(format=(""%(asctime).19s %(levelname)s %(filename)s:""&#xa;                                ""%(lineno)s %(message)s ""))&#xa;    os.environ['LC_ALL'] = 'C'&#xa;    RealMain(sys.argv)&#xa;  except KeyboardInterrupt:&#xa;    print&#xa;    StatusUpdate(""Interrupted."")&#xa;    sys.exit(1)&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;  main()&#xa;"
19379636|"# Patchwork - automated patch tracking system&#xa;# Copyright (C) 2008 Jeremy Kerr <jk@ozlabs.org>&#xa;#&#xa;# SPDX-License-Identifier: GPL-2.0-or-later&#xa;&#xa;import email&#xa;from email import message_from_string&#xa;from email.mime.multipart import MIMEMultipart&#xa;from email.mime.text import MIMEText&#xa;from email.utils import make_msgid&#xa;import os&#xa;import unittest&#xa;&#xa;from django.test import TestCase&#xa;from django.test import TransactionTestCase&#xa;from django.utils import six&#xa;&#xa;from patchwork.models import Comment&#xa;from patchwork.models import Patch&#xa;from patchwork.models import Person&#xa;from patchwork.models import State&#xa;from patchwork.parser import clean_subject&#xa;from patchwork.parser import get_or_create_author&#xa;from patchwork.parser import find_patch_content as find_content&#xa;from patchwork.parser import find_comment_content&#xa;from patchwork.parser import find_project&#xa;from patchwork.parser import find_series&#xa;from patchwork.parser import parse_mail as _parse_mail&#xa;from patchwork.parser import parse_pull_request&#xa;from patchwork.parser import parse_series_marker&#xa;from patchwork.parser import parse_version&#xa;from patchwork.parser import split_prefixes&#xa;from patchwork.parser import subject_check&#xa;from patchwork.tests import TEST_MAIL_DIR&#xa;from patchwork.tests import TEST_FUZZ_DIR&#xa;from patchwork.tests.utils import create_project&#xa;from patchwork.tests.utils import create_series&#xa;from patchwork.tests.utils import create_series_reference&#xa;from patchwork.tests.utils import create_state&#xa;from patchwork.tests.utils import create_user&#xa;from patchwork.tests.utils import read_patch&#xa;from patchwork.tests.utils import SAMPLE_DIFF&#xa;&#xa;&#xa;def load_mail(file_path):&#xa;    if six.PY3:&#xa;        with open(file_path, 'rb') as f:&#xa;            mail = email.message_from_binary_file(f)&#xa;    else:&#xa;        with open(file_path) as f:&#xa;            mail = email.message_from_file(f)&#xa;    return mail&#xa;&#xa;&#xa;def read_mail(filename, project=None):&#xa;    """"""Read a mail from a file.""""""&#xa;    file_path = os.path.join(TEST_MAIL_DIR, filename)&#xa;    mail = load_mail(file_path)&#xa;    if 'Message-Id' not in mail:&#xa;        mail['Message-Id'] = make_msgid()&#xa;    if project:&#xa;        mail['List-Id'] = project.listid&#xa;    return mail&#xa;&#xa;&#xa;def _create_email(msg, msgid=None, sender=None, listid=None, in_reply_to=None):&#xa;    msg['Message-Id'] = msgid or make_msgid()&#xa;    msg['Subject'] = 'Test subject'&#xa;    msg['From'] = sender or 'Test Author <test-author@example.com>'&#xa;    msg['List-Id'] = listid or 'test.example.com'&#xa;    if in_reply_to:&#xa;        msg['In-Reply-To'] = in_reply_to&#xa;&#xa;    return msg&#xa;&#xa;&#xa;def create_email(content, msgid=None, sender=None, listid=None,&#xa;                 in_reply_to=None):&#xa;    msg = MIMEText(content, _charset='us-ascii')&#xa;&#xa;    return _create_email(msg, msgid, sender, listid, in_reply_to)&#xa;&#xa;&#xa;def parse_mail(*args, **kwargs):&#xa;    create_state()&#xa;    return _parse_mail(*args, **kwargs)&#xa;&#xa;&#xa;class PatchTest(TestCase):&#xa;&#xa;    def _find_content(self, mbox_filename):&#xa;        mail = read_mail(mbox_filename)&#xa;        diff, message = find_content(mail)&#xa;&#xa;        return diff, message&#xa;&#xa;&#xa;class InlinePatchTest(PatchTest):&#xa;&#xa;    orig_content = 'Test for attached patch'&#xa;    orig_diff = read_patch('0001-add-line.patch')&#xa;&#xa;    def setUp(self):&#xa;        email = create_email(self.orig_content + '\n' + self.orig_diff)&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;    def test_patch_content(self):&#xa;        self.assertEqual(self.diff, self.orig_diff)&#xa;&#xa;    def test_patch_diff(self):&#xa;        self.assertEqual(self.content, self.orig_content)&#xa;&#xa;&#xa;class AttachmentPatchTest(InlinePatchTest):&#xa;&#xa;    orig_content = 'Test for attached patch'&#xa;    content_subtype = 'x-patch'&#xa;&#xa;    def setUp(self):&#xa;        msg = MIMEMultipart()&#xa;        body = MIMEText(self.orig_content, _subtype='plain')&#xa;        attachment = MIMEText(self.orig_diff, _subtype=self.content_subtype)&#xa;        msg.attach(body)&#xa;        msg.attach(attachment)&#xa;        email = _create_email(msg)&#xa;&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;&#xa;class AttachmentXDiffPatchTest(AttachmentPatchTest):&#xa;&#xa;    content_subtype = 'x-diff'&#xa;&#xa;&#xa;class UTF8InlinePatchTest(InlinePatchTest):&#xa;&#xa;    orig_diff = read_patch('0002-utf-8.patch', 'utf-8')&#xa;&#xa;    def setUp(self):&#xa;        msg = MIMEText(self.orig_content + '\n' + self.orig_diff,&#xa;                       _charset='utf-8')&#xa;        email = _create_email(msg)&#xa;&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;&#xa;class NoCharsetInlinePatchTest(InlinePatchTest):&#xa;    """"""Test mails with no content-type or content-encoding header.""""""&#xa;&#xa;    def setUp(self):&#xa;        email = create_email(self.orig_content + '\n' + self.orig_diff)&#xa;        del email['Content-Type']&#xa;        del email['Content-Transfer-Encoding']&#xa;&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;&#xa;class SignatureCommentTest(InlinePatchTest):&#xa;&#xa;    orig_content = 'Test comment\nmore comment'&#xa;&#xa;    def setUp(self):&#xa;        email = create_email(self.orig_content + '\n-- \nsig\n' +&#xa;                             self.orig_diff)&#xa;&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;&#xa;class UpdateSigCommentTest(SignatureCommentTest):&#xa;    """"""Test for '---\nUpdate: v2' style comments to patches, with a sig.""""""&#xa;&#xa;    patch_filename = '0001-add-line.patch'&#xa;    orig_content = 'Test comment\nmore comment\n---\nUpdate: test update'&#xa;&#xa;&#xa;class ListFooterTest(InlinePatchTest):&#xa;&#xa;    orig_content = 'Test comment\nmore comment'&#xa;&#xa;    def setUp(self):&#xa;        email = create_email('\n'.join([&#xa;            self.orig_content,&#xa;            '_______________________________________________',&#xa;            'Linuxppc-dev mailing list',&#xa;            self.orig_diff]))&#xa;&#xa;        self.diff, self.content = find_content(email)&#xa;&#xa;&#xa;class DiffWordInCommentTest(InlinePatchTest):&#xa;&#xa;    orig_content = 'Lines can start with words beginning in ""diff""\n' + \&#xa;                   'difficult\nDifferent'&#xa;&#xa;&#xa;class UpdateCommentTest(InlinePatchTest):&#xa;    """"""Test for '---\nUpdate: v2' style comments to patches.""""""&#xa;&#xa;    orig_content = 'Test comment\nmore comment\n---\nUpdate: test update'&#xa;&#xa;&#xa;class SenderEncodingTest(TestCase):&#xa;    """"""Validate correct handling of encoded recipients.""""""&#xa;&#xa;    @staticmethod&#xa;    def _create_email(from_header):&#xa;        mail = 'Message-Id: %s\n' % make_msgid() + \&#xa;               'From: %s\n' % from_header + \&#xa;               'Subject: test\n\n' + \&#xa;               'test'&#xa;        return message_from_string(mail)&#xa;&#xa;    def _test_encoding(self, from_header, sender_name, sender_email):&#xa;        email = self._create_email(from_header)&#xa;        person = get_or_create_author(email)&#xa;        person.save()&#xa;&#xa;        # ensure it was parsed correctly&#xa;        self.assertEqual(person.name, sender_name)&#xa;        self.assertEqual(person.email, sender_email)&#xa;&#xa;        # ...and that it's queryable from DB&#xa;        db_person = Person.objects.get(name=sender_name)&#xa;        self.assertEqual(person, db_person)&#xa;        db_person = Person.objects.get(email=sender_email)&#xa;        self.assertEqual(person, db_person)&#xa;&#xa;    def test_empty(self):&#xa;        email = self._create_email('')&#xa;        with self.assertRaises(ValueError):&#xa;            get_or_create_author(email)&#xa;&#xa;    def test_ascii_encoding(self):&#xa;        from_header = 'example user <user@example.com>'&#xa;        sender_name = u'example user'&#xa;        sender_email = 'user@example.com'&#xa;        self._test_encoding(from_header, sender_name, sender_email)&#xa;&#xa;    def test_utf8qp_encoding(self):&#xa;        from_header = '=?utf-8?q?=C3=A9xample=20user?= <user@example.com>'&#xa;        sender_name = u'\xe9xample user'&#xa;        sender_email = 'user@example.com'&#xa;        self._test_encoding(from_header, sender_name, sender_email)&#xa;&#xa;    def test_utf8qp_split_encoding(self):&#xa;        from_header = '=?utf-8?q?=C3=A9xample?= user <user@example.com>'&#xa;        sender_name = u'\xe9xample user'&#xa;        sender_email = 'user@example.com'&#xa;        self._test_encoding(from_header, sender_name, sender_email)&#xa;&#xa;    def test_utf8b64_encoding(self):&#xa;        from_header = '=?utf-8?B?w6l4YW1wbGUgdXNlcg==?= <user@example.com>'&#xa;        sender_name = u'\xe9xample user'&#xa;        sender_email = 'user@example.com'&#xa;        self._test_encoding(from_header, sender_name, sender_email)&#xa;&#xa;&#xa;class SenderCorrelationTest(TestCase):&#xa;    """"""Validate correct behavior of the get_or_create_author case.&#xa;&#xa;    Relies of checking the internal state of a Django model object.&#xa;&#xa;    http://stackoverflow.com/a/19379636/613428&#xa;    """"""&#xa;&#xa;    @staticmethod&#xa;    def _create_email(from_header):&#xa;        mail = 'Message-Id: %s\n' % make_msgid() + \&#xa;               'From: %s\n' % from_header + \&#xa;               'Subject: Tests\n\n'\&#xa;               'test\n'&#xa;        return message_from_string(mail)&#xa;&#xa;    def test_existing_sender(self):&#xa;        sender = 'Existing Sender <existing@example.com>'&#xa;        mail = self._create_email(sender)&#xa;&#xa;        # create the person first&#xa;        person_a = get_or_create_author(mail)&#xa;        person_a.save()&#xa;&#xa;        # then attempt to parse email with the same 'From' line&#xa;        person_b = get_or_create_author(mail)&#xa;        self.assertEqual(person_b._state.adding, False)&#xa;        self.assertEqual(person_b.id, person_a.id)&#xa;&#xa;    def test_existing_different_format(self):&#xa;        sender = 'Existing Sender <existing@example.com>'&#xa;        mail = self._create_email(sender)&#xa;&#xa;        # create the person first&#xa;        person_a = get_or_create_author(mail)&#xa;        person_a.save()&#xa;&#xa;        # then attempt to parse email with a new 'From' line&#xa;        mail = self._create_email('existing@example.com')&#xa;        person_b = get_or_create_author(mail)&#xa;        self.assertEqual(person_b._state.adding, False)&#xa;        self.assertEqual(person_b.id, person_a.id)&#xa;&#xa;    def test_existing_different_case(self):&#xa;        sender = 'Existing Sender <existing@example.com>'&#xa;        mail = self._create_email(sender)&#xa;&#xa;        person_a = get_or_create_author(mail)&#xa;        person_a.save()&#xa;&#xa;        mail = self._create_email(sender.upper())&#xa;        person_b = get_or_create_author(mail)&#xa;        self.assertEqual(person_b._state.adding, False)&#xa;        self.assertEqual(person_b.id, person_a.id)&#xa;&#xa;&#xa;class SeriesCorrelationTest(TestCase):&#xa;    """"""Validate correct behavior of find_series.""""""&#xa;&#xa;    @staticmethod&#xa;    def _create_email(msgid, references=None):&#xa;        """"""Create a sample mail.&#xa;&#xa;        Arguments:&#xa;            msgid (str): The message's msgid&#xa;            references (list): A list of preceding messages' msgids,&#xa;                oldest first&#xa;        """"""&#xa;        mail = 'Message-Id: %s\n' % msgid + \&#xa;               'From: example user <user@example.com>\n' + \&#xa;               'Subject: Tests\n'&#xa;&#xa;        if references:&#xa;            mail += 'In-Reply-To: %s\n' % references[-1]&#xa;            mail += 'References: %s\n' % '\n\t'.join(references)&#xa;&#xa;        mail += 'test\n\n' + SAMPLE_DIFF&#xa;        return message_from_string(mail)&#xa;&#xa;    def test_new_series(self):&#xa;        msgid = make_msgid()&#xa;        email = self._create_email(msgid)&#xa;        project = create_project()&#xa;&#xa;        self.assertIsNone(find_series(project, email,&#xa;                                      get_or_create_author(email)))&#xa;&#xa;    def test_first_reply(self):&#xa;        msgid_a = make_msgid()&#xa;        msgid_b = make_msgid()&#xa;        email = self._create_email(msgid_b, [msgid_a])&#xa;&#xa;        # assume msgid_a was already handled&#xa;        ref = create_series_reference(msgid=msgid_a)&#xa;&#xa;        series = find_series(ref.series.project, email,&#xa;                             get_or_create_author(email))&#xa;        self.assertEqual(series, ref.series)&#xa;&#xa;    def test_nested_series(self):&#xa;        """"""Handle a series sent in-reply-to an existing series.""""""&#xa;        # create an old series with a ""cover letter""&#xa;        msgids = [make_msgid()]&#xa;        project = create_project()&#xa;        series_v1 = create_series(project=project)&#xa;        create_series_reference(msgid=msgids[0], series=series_v1)&#xa;&#xa;        # ...and three patches&#xa;        for i in range(3):&#xa;            msgids.append(make_msgid())&#xa;            create_series_reference(msgid=msgids[-1], series=series_v1)&#xa;&#xa;        # now create a new series with ""cover letter""&#xa;        msgids.append(make_msgid())&#xa;        series_v2 = create_series(project=project)&#xa;        ref_v2 = create_series_reference(msgid=msgids[-1], series=series_v2)&#xa;&#xa;        # ...and the ""first patch"" of this new series&#xa;        msgid = make_msgid()&#xa;        email = self._create_email(msgid, msgids)&#xa;        series = find_series(project, email, get_or_create_author(email))&#xa;&#xa;        # this should link to the second series - not the first&#xa;        self.assertEqual(len(msgids), 4 + 1)  # old series + new cover&#xa;        self.assertEqual(series, ref_v2.series)&#xa;&#xa;&#xa;class SubjectEncodingTest(TestCase):&#xa;    """"""Validate correct handling of encoded subjects.""""""&#xa;&#xa;    @staticmethod&#xa;    def _create_email(subject):&#xa;        mail = 'Message-Id: %s\n' % make_msgid() + \&#xa;               'From: example user <user@example.com>\n' + \&#xa;               'Subject: %s\n\n' % subject + \&#xa;               'test\n\n' + SAMPLE_DIFF&#xa;        return message_from_string(mail)&#xa;&#xa;    def _test_encoding(self, subject_header, parsed_subject):&#xa;        email = self._create_email(subject_header)&#xa;        name, _ = clean_subject(email.get('Subject'))&#xa;        self.assertEqual(name, parsed_subject)&#xa;&#xa;    def test_subject_ascii_encoding(self):&#xa;        subject_header = 'test subject'&#xa;        subject = 'test subject'&#xa;        self._test_encoding(subject_header, subject)&#xa;&#xa;    def test_subject_utf8qp_encoding(self):&#xa;        subject_header = '=?utf-8?q?test=20s=c3=bcbject?='&#xa;        subject = u'test s\xfcbject'&#xa;        self._test_encoding(subject_header, subject)&#xa;&#xa;    def test_subject_utf8qp_multiple_encoding(self):&#xa;        subject_header = 'test =?utf-8?q?s=c3=bcbject?='&#xa;        subject = u'test s\xfcbject'&#xa;        self._test_encoding(subject_header, subject)&#xa;&#xa;&#xa;class MultipleProjectPatchTest(TestCase):&#xa;    """"""Test that patches sent to multiple patchwork projects are&#xa;       handled correctly.""""""&#xa;&#xa;    orig_content = 'Test Comment'&#xa;    patch_filename = '0001-add-line.patch'&#xa;    msgid = '<1@example.com>'&#xa;&#xa;    def setUp(self):&#xa;        self.p1 = create_project()&#xa;        self.p2 = create_project()&#xa;&#xa;        patch = read_patch(self.patch_filename)&#xa;        email = create_email(&#xa;            content=''.join([self.orig_content, '\n', patch]),&#xa;            msgid=self.msgid,&#xa;            listid='<%s>' % self.p1.listid)&#xa;        parse_mail(email)&#xa;&#xa;        del email['List-ID']&#xa;        email['List-ID'] = '<%s>' % self.p2.listid&#xa;        parse_mail(email)&#xa;&#xa;    def test_parsed_projects(self):&#xa;        self.assertEqual(Patch.objects.filter(project=self.p1).count(), 1)&#xa;        self.assertEqual(Patch.objects.filter(project=self.p2).count(), 1)&#xa;&#xa;&#xa;class MultipleProjectPatchCommentTest(MultipleProjectPatchTest):&#xa;    """"""Test that followups to multiple-project patches end up on the&#xa;       correct patch.""""""&#xa;&#xa;    comment_msgid = '<2@example.com>'&#xa;    comment_content = 'test comment'&#xa;&#xa;    def setUp(self):&#xa;        super(MultipleProjectPatchCommentTest, self).setUp()&#xa;&#xa;        for project in [self.p1, self.p2]:&#xa;            email = create_email(&#xa;                content=self.comment_content,&#xa;                msgid=self.comment_msgid,&#xa;                listid='<%s>' % project.listid)&#xa;            email['In-Reply-To'] = self.msgid&#xa;            parse_mail(email)&#xa;&#xa;    def test_parsed_comment(self):&#xa;        for project in [self.p1, self.p2]:&#xa;            patch = Patch.objects.filter(project=project)[0]&#xa;            # we should see the reply comment only&#xa;            self.assertEqual(&#xa;                Comment.objects.filter(submission=patch).count(), 1)&#xa;&#xa;&#xa;class ListIdHeaderTest(TestCase):&#xa;    """"""Test that we parse List-Id headers from mails correctly.""""""&#xa;&#xa;    def setUp(self):&#xa;        self.project = create_project()&#xa;&#xa;    def test_no_list_id(self):&#xa;        email = MIMEText('')&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, None)&#xa;&#xa;    def test_blank_list_id(self):&#xa;        email = MIMEText('')&#xa;        email['List-Id'] = ''&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, None)&#xa;&#xa;    def test_whitespace_list_id(self):&#xa;        email = MIMEText('')&#xa;        email['List-Id'] = ' '&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, None)&#xa;&#xa;    def test_substring_list_id(self):&#xa;        email = MIMEText('')&#xa;        email['List-Id'] = 'example.com'&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, None)&#xa;&#xa;    def test_short_list_id(self):&#xa;        """"""Some mailing lists have List-Id headers in short formats, where it&#xa;           is only the list ID itself (without enclosing angle-brackets). """"""&#xa;        email = MIMEText('')&#xa;        email['List-Id'] = self.project.listid&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, self.project)&#xa;&#xa;    def test_long_list_id(self):&#xa;        email = MIMEText('')&#xa;        email['List-Id'] = 'Test text <%s>' % self.project.listid&#xa;        project = find_project(email)&#xa;        self.assertEqual(project, self.project)&#xa;&#xa;&#xa;class PatchParseTest(PatchTest):&#xa;    """"""Test parsing of different patch formats.""""""&#xa;&#xa;    def _test_pull_request_parse(self, mbox_filename):&#xa;        diff, message = self._find_content(mbox_filename)&#xa;        pull_url = parse_pull_request(message)&#xa;        self.assertTrue(diff is None)&#xa;        self.assertTrue(message is not None)&#xa;        self.assertTrue(pull_url is not None)&#xa;&#xa;    def test_git_pull_request(self):&#xa;        self._test_pull_request_parse('0001-git-pull-request.mbox')&#xa;&#xa;    @unittest.skipIf(six.PY3, 'Breaks only on Python 2')&#xa;    def test_git_pull_request_crlf_newlines(self):&#xa;        # verify that we haven't munged the file&#xa;        crlf_file = os.path.join(TEST_MAIL_DIR,&#xa;                                 '0018-git-pull-request-crlf-newlines.mbox')&#xa;        with open(crlf_file) as f:&#xa;            message = f.read()&#xa;            self.assertIn('\r\n', message)&#xa;&#xa;        # verify the file works&#xa;        self._test_pull_request_parse(&#xa;            '0018-git-pull-request-crlf-newlines.mbox')&#xa;&#xa;    def test_git_pull_wrapped_request(self):&#xa;        self._test_pull_request_parse('0002-git-pull-request-wrapped.mbox')&#xa;&#xa;    def test_git_pull_git_ssh_url(self):&#xa;        self._test_pull_request_parse('0004-git-pull-request-git+ssh.mbox')&#xa;&#xa;    def test_git_pull_ssh_url(self):&#xa;        self._test_pull_request_parse('0005-git-pull-request-ssh.mbox')&#xa;&#xa;    def test_git_pull_http_url(self):&#xa;        self._test_pull_request_parse('0006-git-pull-request-http.mbox')&#xa;&#xa;    def test_git_pull_git_2_14_3(self):&#xa;        """"""Handle messages from Git 2.14.3+.&#xa;&#xa;        See: https://github.com/git/git/commit/e66d7c37a&#xa;        """"""&#xa;        self._test_pull_request_parse('0017-git-pull-request-git-2-14-3.mbox')&#xa;&#xa;    def test_git_pull_with_diff(self):&#xa;        diff, message = self._find_content(&#xa;            '0003-git-pull-request-with-diff.mbox')&#xa;        pull_url = parse_pull_request(message)&#xa;        self.assertEqual(&#xa;            'git://git.kernel.org/pub/scm/linux/kernel/git/tip/'&#xa;            'linux-2.6-tip.git x86-fixes-for-linus',&#xa;            pull_url)&#xa;        self.assertTrue(&#xa;            diff.startswith('diff --git a/arch/x86/include/asm/smp.h'),&#xa;            diff)&#xa;&#xa;    def test_git_rename(self):&#xa;        diff, _ = self._find_content('0008-git-rename.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertEqual(diff.count(""\nrename from ""), 2)&#xa;        self.assertEqual(diff.count(""\nrename to ""), 2)&#xa;&#xa;    def test_git_rename_with_diff(self):&#xa;        diff, message = self._find_content('0009-git-rename-with-diff.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertTrue(message is not None)&#xa;        self.assertEqual(diff.count(""\nrename from ""), 2)&#xa;        self.assertEqual(diff.count(""\nrename to ""), 2)&#xa;        self.assertEqual(diff.count('\n-a\n+b'), 1)&#xa;&#xa;    def test_cvs_format(self):&#xa;        diff, message = self._find_content('0007-cvs-format-diff.mbox')&#xa;        self.assertTrue(diff.startswith('Index'))&#xa;&#xa;    def test_invalid_charset(self):&#xa;        """"""Validate behavior with an invalid charset name.&#xa;&#xa;        Ensure that we can parse with one of the fallback encodings.&#xa;        """"""&#xa;        diff, message = self._find_content('0010-invalid-charset.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertTrue(message is not None)&#xa;&#xa;    def test_no_newline(self):&#xa;        """"""Validate behavior when trailing newline is absent.""""""&#xa;        diff, message = self._find_content(&#xa;            '0011-no-newline-at-end-of-file.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertTrue(message is not None)&#xa;        self.assertTrue(diff.startswith(&#xa;            'diff --git a/tools/testing/selftests/powerpc/Makefile'))&#xa;        # Confirm the trailing no newline marker doesn't end up in the comment&#xa;        self.assertFalse(message.rstrip().endswith(&#xa;            r'\ No newline at end of file'))&#xa;        # Confirm it's instead at the bottom of the patch&#xa;        self.assertTrue(diff.rstrip().endswith(&#xa;            r'\ No newline at end of file'))&#xa;        # Confirm we got both markers&#xa;        self.assertEqual(2, diff.count(r'\ No newline at end of file'))&#xa;&#xa;    def test_no_subject(self):&#xa;        """"""Validate parsing a mail with no subject.""""""&#xa;        diff, message = self._find_content('0016-no-subject.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertTrue(message is not None)&#xa;&#xa;    def test_html_multipart(self):&#xa;        """"""Validate parsing a mail with multiple parts.""""""&#xa;        diff, message = self._find_content('0019-multipart-patch.mbox')&#xa;        self.assertTrue(diff is not None)&#xa;        self.assertTrue(message is not None)&#xa;        self.assertFalse('<div' in diff)&#xa;        self.assertFalse('<div' in message)&#xa;&#xa;&#xa;class EncodingParseTest(TestCase):&#xa;    """"""Test parsing of patches with different encoding issues.""""""&#xa;&#xa;    def setUp(self):&#xa;        self.project = create_project()&#xa;&#xa;    def _test_encoded_patch_parse(self, mbox_filename):&#xa;        mail = read_mail(mbox_filename, self.project)&#xa;        parse_mail(mail, list_id=self.project.listid)&#xa;        self.assertEqual(Patch.objects.all().count(), 1)&#xa;&#xa;    def test_invalid_header_char(self):&#xa;        """"""Validate behaviour when an invalid character is in a header.""""""&#xa;        self._test_encoded_patch_parse('0012-invalid-header-char.mbox')&#xa;&#xa;    def test_utf8_mail(self):&#xa;        """"""Validate behaviour when a UTF-8 char is in a message.""""""&#xa;        self._test_encoded_patch_parse('0013-with-utf8-body.mbox')&#xa;&#xa;    def test_utf8_unencoded_headers(self):&#xa;        """"""Validate behaviour when unencoded UTF-8 is in headers,&#xa;        including subject and from.""""""&#xa;        self._test_encoded_patch_parse('0014-with-unencoded-utf8-headers.mbox')&#xa;&#xa;    def test_invalid_utf8_headers(self):&#xa;        """"""Validate behaviour when invalid encoded UTF-8 is in headers.""""""&#xa;        self._test_encoded_patch_parse('0015-with-invalid-utf8-headers.mbox')&#xa;&#xa;&#xa;class CommentParseTest(TestCase):&#xa;    """"""Test parsing of different comment formats.""""""&#xa;&#xa;    @staticmethod&#xa;    def _find_content(mbox_filename):&#xa;        mail = read_mail(mbox_filename)&#xa;        _, message = find_comment_content(mail)&#xa;&#xa;        return message&#xa;&#xa;    def test_html_multipart(self):&#xa;        """"""Validate parsing a mail with multiple parts.""""""&#xa;        message = self._find_content('0020-multipart-comment.mbox')&#xa;        self.assertTrue(message is not None)&#xa;        self.assertFalse('<div' in message)&#xa;&#xa;&#xa;class DelegateRequestTest(TestCase):&#xa;&#xa;    patch_filename = '0001-add-line.patch'&#xa;    msgid = '<1@example.com>'&#xa;    invalid_delegate_email = ""nobody""&#xa;&#xa;    def setUp(self):&#xa;        self.patch = read_patch(self.patch_filename)&#xa;        self.user = create_user()&#xa;        self.project = create_project()&#xa;&#xa;    def _get_email(self):&#xa;        email = create_email(self.patch)&#xa;        del email['List-ID']&#xa;        email['List-ID'] = '<' + self.project.listid + '>'&#xa;        email['Message-Id'] = self.msgid&#xa;        return email&#xa;&#xa;    def assertDelegate(self, delegate):  # noqa&#xa;        query = Patch.objects.filter(project=self.project)&#xa;        self.assertEqual(query.count(), 1)&#xa;        self.assertEqual(query[0].delegate, delegate)&#xa;&#xa;    def test_delegate(self):&#xa;        email = self._get_email()&#xa;        email['X-Patchwork-Delegate'] = self.user.email&#xa;        parse_mail(email)&#xa;        self.assertDelegate(self.user)&#xa;&#xa;    def test_no_delegate(self):&#xa;        email = self._get_email()&#xa;        parse_mail(email)&#xa;        self.assertDelegate(None)&#xa;&#xa;    def test_invalid_delegate(self):&#xa;        email = self._get_email()&#xa;        email['X-Patchwork-Delegate'] = self.invalid_delegate_email&#xa;        parse_mail(email)&#xa;        self.assertDelegate(None)&#xa;&#xa;&#xa;class InitialPatchStateTest(TestCase):&#xa;&#xa;    patch_filename = '0001-add-line.patch'&#xa;    msgid = '<1@example.com>'&#xa;    invalid_state_name = ""Nonexistent Test State""&#xa;&#xa;    def setUp(self):&#xa;        self.default_state = create_state()&#xa;        self.nondefault_state = create_state()&#xa;&#xa;        self.patch = read_patch(self.patch_filename)&#xa;        self.user = create_user()&#xa;        self.project = create_project()&#xa;&#xa;    def _get_email(self):&#xa;        email = create_email(&#xa;            self.patch, msgid=self.msgid, listid='<%s>' % self.project.listid)&#xa;        return email&#xa;&#xa;    def assertState(self, state):  # noqa&#xa;        query = Patch.objects.filter(project=self.project)&#xa;        self.assertEqual(query.count(), 1)&#xa;        self.assertEqual(query[0].state, state)&#xa;&#xa;    def test_non_default_state(self):&#xa;        self.assertNotEqual(self.default_state, self.nondefault_state)&#xa;&#xa;    def test_explicit_non_default_state_request(self):&#xa;        email = self._get_email()&#xa;        email['X-Patchwork-State'] = self.nondefault_state.name&#xa;        parse_mail(email)&#xa;        self.assertState(self.nondefault_state)&#xa;&#xa;    def test_explicit_default_state_request(self):&#xa;        email = self._get_email()&#xa;        email['X-Patchwork-State'] = self.default_state.name&#xa;        parse_mail(email)&#xa;        self.assertState(self.default_state)&#xa;&#xa;    def test_implicit_default_state_request(self):&#xa;        email = self._get_email()&#xa;        parse_mail(email)&#xa;        self.assertState(self.default_state)&#xa;&#xa;    def test_invalid_state(self):&#xa;        # make sure it's actually invalid&#xa;        with self.assertRaises(State.DoesNotExist):&#xa;            State.objects.get(name=self.invalid_state_name)&#xa;&#xa;        email = self._get_email()&#xa;        email['X-Patchwork-State'] = self.invalid_state_name&#xa;        parse_mail(email)&#xa;        self.assertState(self.default_state)&#xa;&#xa;&#xa;class ParseInitialTagsTest(PatchTest):&#xa;&#xa;    fixtures = ['default_tags']&#xa;    patch_filename = '0001-add-line.patch'&#xa;    orig_content = ('test comment\n\n' +&#xa;                    'Tested-by: Test User <test@example.com>\n' +&#xa;                    'Reviewed-by: Test User <test@example.com>\n')&#xa;&#xa;    def setUp(self):&#xa;        project = create_project(listid='test.example.com')&#xa;        self.orig_diff = read_patch(self.patch_filename)&#xa;        email = create_email(self.orig_content + '\n' + self.orig_diff,&#xa;                             listid=project.listid)&#xa;        parse_mail(email)&#xa;&#xa;    def test_tags(self):&#xa;        self.assertEqual(Patch.objects.count(), 1)&#xa;        patch = Patch.objects.all()[0]&#xa;        self.assertEqual(patch.patchtag_set.filter(&#xa;            tag__name='Acked-by').count(), 0)&#xa;        self.assertEqual(patch.patchtag_set.get(&#xa;            tag__name='Reviewed-by').count, 1)&#xa;        self.assertEqual(patch.patchtag_set.get(&#xa;            tag__name='Tested-by').count, 1)&#xa;&#xa;&#xa;class ParseCommentTagsTest(PatchTest):&#xa;    fixtures = ['default_tags']&#xa;    patch_filename = '0001-add-line.patch'&#xa;    comment_content = ('test comment\n\n' +&#xa;                       'Tested-by: Test User <test@example.com>\n' +&#xa;                       'Reviewed-by: Test User <test@example.com>\n')&#xa;&#xa;    def setUp(self):&#xa;        project = create_project(listid='test.example.com')&#xa;        self.orig_diff = read_patch(self.patch_filename)&#xa;        email = create_email(self.orig_diff,&#xa;                             listid=project.listid)&#xa;        parse_mail(email)&#xa;        email2 = create_email(self.comment_content,&#xa;                              in_reply_to=email['Message-Id'])&#xa;        parse_mail(email2)&#xa;&#xa;    def test_tags(self):&#xa;        self.assertEqual(Patch.objects.count(), 1)&#xa;        patch = Patch.objects.all()[0]&#xa;        self.assertEqual(patch.patchtag_set.filter(&#xa;            tag__name='Acked-by').count(), 0)&#xa;        self.assertEqual(patch.patchtag_set.get(&#xa;            tag__name='Reviewed-by').count, 1)&#xa;        self.assertEqual(patch.patchtag_set.get(&#xa;            tag__name='Tested-by').count, 1)&#xa;&#xa;&#xa;class SubjectTest(TestCase):&#xa;&#xa;    def test_clean_subject(self):&#xa;        self.assertEqual(clean_subject('meep'), ('meep', []))&#xa;        self.assertEqual(clean_subject('Re: meep'), ('meep', []))&#xa;        self.assertEqual(clean_subject('[PATCH] meep'), ('meep', []))&#xa;        self.assertEqual(clean_subject(""[PATCH] meep \n meep""),&#xa;                         ('meep meep', []))&#xa;        self.assertEqual(clean_subject('[PATCH RFC] meep'),&#xa;                         ('[RFC] meep', ['RFC']))&#xa;        self.assertEqual(clean_subject('[PATCH,RFC] meep'),&#xa;                         ('[RFC] meep', ['RFC']))&#xa;        self.assertEqual(clean_subject('[PATCH,1/2] meep'),&#xa;                         ('[1/2] meep', ['1/2']))&#xa;        self.assertEqual(clean_subject('[PATCH RFC 1/2] meep'),&#xa;                         ('[RFC,1/2] meep', ['RFC', '1/2']))&#xa;        self.assertEqual(clean_subject('[PATCH] [RFC] meep'),&#xa;                         ('[RFC] meep', ['RFC']))&#xa;        self.assertEqual(clean_subject('[PATCH] [RFC,1/2] meep'),&#xa;                         ('[RFC,1/2] meep', ['RFC', '1/2']))&#xa;        self.assertEqual(clean_subject('[PATCH] [RFC] [1/2] meep'),&#xa;                         ('[RFC,1/2] meep', ['RFC', '1/2']))&#xa;        self.assertEqual(clean_subject('[PATCH] rewrite [a-z] regexes'),&#xa;                         ('rewrite [a-z] regexes', []))&#xa;        self.assertEqual(clean_subject('[PATCH] [RFC] rewrite [a-z] regexes'),&#xa;                         ('[RFC] rewrite [a-z] regexes', ['RFC']))&#xa;        self.assertEqual(clean_subject('[foo] [bar] meep', ['foo']),&#xa;                         ('[bar] meep', ['bar']))&#xa;        self.assertEqual(clean_subject('[FOO] [bar] meep', ['foo']),&#xa;                         ('[bar] meep', ['bar']))&#xa;&#xa;    def test_subject_check(self):&#xa;        self.assertIsNotNone(subject_check('RE: meep'))&#xa;        self.assertIsNotNone(subject_check('Re: meep'))&#xa;        self.assertIsNotNone(subject_check('re: meep'))&#xa;        self.assertIsNotNone(subject_check('RE meep'))&#xa;        self.assertIsNotNone(subject_check('Re meep'))&#xa;        self.assertIsNotNone(subject_check('re meep'))&#xa;&#xa;    def test_split_prefixes(self):&#xa;        self.assertEqual(split_prefixes('PATCH'), ['PATCH'])&#xa;        self.assertEqual(split_prefixes('PATCH,RFC'), ['PATCH', 'RFC'])&#xa;        self.assertEqual(split_prefixes(''), [])&#xa;        self.assertEqual(split_prefixes('PATCH,'), ['PATCH'])&#xa;        self.assertEqual(split_prefixes('PATCH '), ['PATCH'])&#xa;        self.assertEqual(split_prefixes('PATCH,RFC'), ['PATCH', 'RFC'])&#xa;        self.assertEqual(split_prefixes('PATCH 1/2'), ['PATCH', '1/2'])&#xa;&#xa;    def test_series_markers(self):&#xa;        self.assertEqual(parse_series_marker([]), (None, None))&#xa;        self.assertEqual(parse_series_marker(['bar']), (None, None))&#xa;        self.assertEqual(parse_series_marker(['bar', '1/2']), (1, 2))&#xa;        self.assertEqual(parse_series_marker(['bar', '0/12']), (0, 12))&#xa;        self.assertEqual(parse_series_marker(['bar', '1 of 2']), (1, 2))&#xa;        self.assertEqual(parse_series_marker(['bar', '0 of 12']), (0, 12))&#xa;        # Handle people missing the space between PATCH and the markers&#xa;        # e.g. PATCH1/8&#xa;        self.assertEqual(parse_series_marker(['PATCH1/8']), (1, 8))&#xa;        self.assertEqual(parse_series_marker(['PATCH1 of 8']), (1, 8))&#xa;        # verify the prefix-stripping is non-greedy&#xa;        self.assertEqual(parse_series_marker(['PATCH100/123']), (100, 123))&#xa;        # and that it is hard to confuse&#xa;        self.assertEqual(parse_series_marker(['v2PATCH1/4']), (1, 4))&#xa;        self.assertEqual(parse_series_marker(['v2', 'PATCH1/4']), (1, 4))&#xa;        self.assertEqual(parse_series_marker(['v2.3PATCH1/4']), (1, 4))&#xa;&#xa;    def test_version(self):&#xa;        self.assertEqual(parse_version('', []), 1)&#xa;        self.assertEqual(parse_version('Hello, world', []), 1)&#xa;        self.assertEqual(parse_version('Hello, world', ['version']), 1)&#xa;        self.assertEqual(parse_version('Hello, world', ['v2']), 2)&#xa;        self.assertEqual(parse_version('Hello, world', ['V6']), 6)&#xa;        self.assertEqual(parse_version('Hello, world', ['v10']), 10)&#xa;        self.assertEqual(parse_version('Hello, world (v2)', []), 2)&#xa;        self.assertEqual(parse_version('Hello, world (V6)', []), 6)&#xa;&#xa;&#xa;class SubjectMatchTest(TestCase):&#xa;&#xa;    def setUp(self):&#xa;        self.list_id = 'test-subject-match.test.org'&#xa;        self.project_x = create_project(name='PROJECT X',&#xa;                                        listid=self.list_id,&#xa;                                        subject_match=r'.*PROJECT[\s]?X.*')&#xa;        self.default_project = create_project(name='Default',&#xa;                                              listid=self.list_id,&#xa;                                              subject_match=r'')&#xa;        self.keyword_project = create_project(name='keyword',&#xa;                                              listid=self.list_id,&#xa;                                              subject_match=r'keyword')&#xa;&#xa;        self.email = MIMEText('')&#xa;        self.email['List-Id'] = self.list_id&#xa;&#xa;        self.email_no_project = MIMEText('')&#xa;        self.email_no_project['List-Id'] = 'nonexistent-project.test.org'&#xa;        self.email_no_project['Subject'] = '[PATCH keyword]'&#xa;&#xa;    def test_project_with_regex(self):&#xa;        self.email['Subject'] = '[PATCH PROJECT X subsystem]'&#xa;        project = find_project(self.email)&#xa;        self.assertEqual(project, self.project_x)&#xa;&#xa;        self.email['Subject'] = '[PATCH PROJECTX another subsystem]'&#xa;        project = find_project(self.email)&#xa;        self.assertEqual(project, self.project_x)&#xa;&#xa;    def test_project_with_keyword(self):&#xa;        self.email['Subject'] = '[PATCH keyword] subsystem'&#xa;        project = find_project(self.email)&#xa;        self.assertEqual(project, self.keyword_project)&#xa;&#xa;    def test_default_project(self):&#xa;        self.email['Subject'] = '[PATCH unknown project]'&#xa;        project = find_project(self.email)&#xa;        self.assertEqual(project, self.default_project)&#xa;&#xa;        self.email['Subject'] = '[PATCH NOT-PROJECT-X]'&#xa;        project = find_project(self.email)&#xa;        self.assertEqual(project, self.default_project)&#xa;&#xa;    def test_nonexistent_project(self):&#xa;        project = find_project(self.email_no_project)&#xa;        self.assertEqual(project, None)&#xa;&#xa;    def test_list_id_override(self):&#xa;        project = find_project(self.email_no_project,&#xa;                               self.keyword_project.listid)&#xa;        self.assertEqual(project, self.keyword_project)&#xa;&#xa;&#xa;class WeirdMailTest(TransactionTestCase):&#xa;    """"""Test fuzzed or otherwise weird patches.""""""&#xa;&#xa;    def setUp(self):&#xa;        create_project(listid='patchwork.ozlabs.org')&#xa;&#xa;    def _test_patch(self, name):&#xa;        file_path = os.path.join(TEST_FUZZ_DIR, name)&#xa;        m = load_mail(file_path)&#xa;        try:&#xa;            parse_mail(m, list_id='patchwork.ozlabs.org')&#xa;        except ValueError:&#xa;            pass&#xa;&#xa;    @unittest.skipIf(six.PY2, 'Breaks only on Python 3')&#xa;    def test_early_fail(self):&#xa;        file_path = os.path.join(TEST_FUZZ_DIR, 'earlyfail.mbox')&#xa;        with self.assertRaises(AttributeError):&#xa;            load_mail(file_path)&#xa;&#xa;    def test_base64err(self):&#xa;        self._test_patch('base64err.mbox')&#xa;&#xa;    def test_codec(self):&#xa;        self._test_patch('codec-null.mbox')&#xa;        self._test_patch('charset.mbox')&#xa;        self._test_patch('unknown-encoding.mbox')&#xa;        self._test_patch('value2.mbox')&#xa;&#xa;    def test_date(self):&#xa;        self._test_patch('date.mbox')&#xa;        self._test_patch('date-too-long.mbox')&#xa;        self._test_patch('year-out-of-range.mbox')&#xa;        self._test_patch('date-oserror.mbox')&#xa;&#xa;    def test_length_for_db(self):&#xa;        self._test_patch('msgid-len.mbox')&#xa;        self._test_patch('msgid-len2.mbox')&#xa;        self._test_patch('email-len.mbox')&#xa;        self._test_patch('name-len.mbox')&#xa;&#xa;    def test_hdr(self):&#xa;        self._test_patch('refshdr.mbox')&#xa;        self._test_patch('dateheader.mbox')&#xa;        self._test_patch('msgidheader.mbox')&#xa;&#xa;    def test_x_face(self):&#xa;        self._test_patch('x-face.mbox')&#xa;"
18566289|"# Windows implementation of PyAutoGUI functions.&#xa;# BSD license&#xa;# Al Sweigart al@inventwithpython.com&#xa;&#xa;import ctypes&#xa;import ctypes.wintypes&#xa;import pyautogui&#xa;&#xa;import sys&#xa;if sys.platform !=  'win32':&#xa;    raise Exception('The pyautogui_win module should only be loaded on a Windows system.')&#xa;&#xa;""""""&#xa;A lot of this code is probably repeated from win32 extensions module, but I didn't want to have that dependency.&#xa;&#xa;Note: According to http://msdn.microsoft.com/en-us/library/windows/desktop/ms646260(v=vs.85).aspx&#xa;the ctypes.windll.user32.mouse_event() function has been superceded by SendInput.&#xa;&#xa;SendInput() is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646310(v=vs.85).aspx&#xa;&#xa;UPDATE: SendInput() doesn't seem to be working for me. I've switched back to mouse_event().""""""&#xa;&#xa;&#xa;# Event codes to be passed to the mouse_event() win32 function.&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646273(v=vs.85).aspx&#xa;MOUSEEVENTF_LEFTDOWN = 0x0002&#xa;MOUSEEVENTF_LEFTUP = 0x0004&#xa;MOUSEEVENTF_LEFTCLICK = MOUSEEVENTF_LEFTDOWN + MOUSEEVENTF_LEFTUP&#xa;MOUSEEVENTF_RIGHTDOWN = 0x0008&#xa;MOUSEEVENTF_RIGHTUP = 0x0010&#xa;MOUSEEVENTF_RIGHTCLICK = MOUSEEVENTF_RIGHTDOWN + MOUSEEVENTF_RIGHTUP&#xa;MOUSEEVENTF_MIDDLEDOWN = 0x0020&#xa;MOUSEEVENTF_MIDDLEUP = 0x0040&#xa;MOUSEEVENTF_MIDDLECLICK = MOUSEEVENTF_MIDDLEDOWN + MOUSEEVENTF_MIDDLEUP&#xa;&#xa;MOUSEEVENTF_WHEEL = 0x0800&#xa;MOUSEEVENTF_HWHEEL = 0x01000&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646304(v=vs.85).aspx&#xa;KEYEVENTF_KEYUP = 0x0002&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;INPUT_MOUSE = 0&#xa;INPUT_KEYBOARD = 1&#xa;&#xa;&#xa;# This ctypes structure is for a Win32 POINT structure,&#xa;# which is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/dd162805(v=vs.85).aspx&#xa;# The POINT structure is used by GetCursorPos().&#xa;class POINT(ctypes.Structure):&#xa;    _fields_ = [(""x"", ctypes.c_ulong),&#xa;                (""y"", ctypes.c_ulong)]&#xa;&#xa;# These ctypes structures are for Win32 INPUT, MOUSEINPUT, KEYBDINPUT, and HARDWAREINPUT structures,&#xa;# used by SendInput and documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;# Thanks to BSH for this StackOverflow answer: https://stackoverflow.com/questions/18566289/how-would-you-recreate-this-windows-api-structure-with-ctypes&#xa;class MOUSEINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('dx', ctypes.wintypes.LONG),&#xa;        ('dy', ctypes.wintypes.LONG),&#xa;        ('mouseData', ctypes.wintypes.DWORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class KEYBDINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('wVk', ctypes.wintypes.WORD),&#xa;        ('wScan', ctypes.wintypes.WORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class HARDWAREINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('uMsg', ctypes.wintypes.DWORD),&#xa;        ('wParamL', ctypes.wintypes.WORD),&#xa;        ('wParamH', ctypes.wintypes.DWORD)&#xa;    ]&#xa;&#xa;class INPUT(ctypes.Structure):&#xa;    class _I(ctypes.Union):&#xa;        _fields_ = [&#xa;            ('mi', MOUSEINPUT),&#xa;            ('ki', KEYBDINPUT),&#xa;            ('hi', HARDWAREINPUT),&#xa;        ]&#xa;&#xa;    _anonymous_ = ('i', )&#xa;    _fields_ = [&#xa;        ('type', ctypes.wintypes.DWORD),&#xa;        ('i', _I),&#xa;    ]&#xa;# End of the SendInput win32 data structures.&#xa;&#xa;&#xa;&#xa;"""""" Keyboard key mapping for pyautogui:&#xa;Documented at http://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx&#xa;&#xa;The *KB dictionaries in pyautogui map a string that can be passed to keyDown(),&#xa;keyUp(), or press() into the code used for the OS-specific keyboard function.&#xa;&#xa;They should always be lowercase, and the same keys should be used across all OSes.""""""&#xa;keyboardMapping = dict([(key, None) for key in pyautogui.KEY_NAMES])&#xa;keyboardMapping.update({&#xa;    'backspace': 0x08, # VK_BACK&#xa;    '\b': 0x08, # VK_BACK&#xa;    'super': 0x5B, #VK_LWIN&#xa;    'tab': 0x09, # VK_TAB&#xa;    '\t': 0x09, # VK_TAB&#xa;    'clear': 0x0c, # VK_CLEAR&#xa;    'enter': 0x0d, # VK_RETURN&#xa;    '\n': 0x0d, # VK_RETURN&#xa;    'return': 0x0d, # VK_RETURN&#xa;    'shift': 0x10, # VK_SHIFT&#xa;    'ctrl': 0x11, # VK_CONTROL&#xa;    'alt': 0x12, # VK_MENU&#xa;    'pause': 0x13, # VK_PAUSE&#xa;    'capslock': 0x14, # VK_CAPITAL&#xa;    'kana': 0x15, # VK_KANA&#xa;    'hanguel': 0x15, # VK_HANGUEL&#xa;    'hangul': 0x15, # VK_HANGUL&#xa;    'junja': 0x17, # VK_JUNJA&#xa;    'final': 0x18, # VK_FINAL&#xa;    'hanja': 0x19, # VK_HANJA&#xa;    'kanji': 0x19, # VK_KANJI&#xa;    'esc': 0x1b, # VK_ESCAPE&#xa;    'escape': 0x1b, # VK_ESCAPE&#xa;    'convert': 0x1c, # VK_CONVERT&#xa;    'nonconvert': 0x1d, # VK_NONCONVERT&#xa;    'accept': 0x1e, # VK_ACCEPT&#xa;    'modechange': 0x1f, # VK_MODECHANGE&#xa;    ' ': 0x20, # VK_SPACE&#xa;    'space': 0x20,&#xa;    'pgup': 0x21, # VK_PRIOR&#xa;    'pgdn': 0x22, # VK_NEXT&#xa;    'pageup': 0x21, # VK_PRIOR&#xa;    'pagedown': 0x22, # VK_NEXT&#xa;    'end': 0x23, # VK_END&#xa;    'home': 0x24, # VK_HOME&#xa;    'left': 0x25, # VK_LEFT&#xa;    'up': 0x26, # VK_UP&#xa;    'right': 0x27, # VK_RIGHT&#xa;    'down': 0x28, # VK_DOWN&#xa;    'select': 0x29, # VK_SELECT&#xa;    'print': 0x2a, # VK_PRINT&#xa;    'execute': 0x2b, # VK_EXECUTE&#xa;    'prtsc': 0x2c, # VK_SNAPSHOT&#xa;    'prtscr': 0x2c, # VK_SNAPSHOT&#xa;    'prntscrn': 0x2c, # VK_SNAPSHOT&#xa;    'printscreen': 0x2c, # VK_SNAPSHOT&#xa;    'insert': 0x2d, # VK_INSERT&#xa;    'del': 0x2e, # VK_DELETE&#xa;    'delete': 0x2e, # VK_DELETE&#xa;    'help': 0x2f, # VK_HELP&#xa;    'win': 0x5b, # VK_LWIN&#xa;    'winleft': 0x5b, # VK_LWIN&#xa;    'winright': 0x5c, # VK_RWIN&#xa;    'apps': 0x5d, # VK_APPS&#xa;    'sleep': 0x5f, # VK_SLEEP&#xa;    'num0': 0x60, # VK_NUMPAD0&#xa;    'num1': 0x61, # VK_NUMPAD1&#xa;    'num2': 0x62, # VK_NUMPAD2&#xa;    'num3': 0x63, # VK_NUMPAD3&#xa;    'num4': 0x64, # VK_NUMPAD4&#xa;    'num5': 0x65, # VK_NUMPAD5&#xa;    'num6': 0x66, # VK_NUMPAD6&#xa;    'num7': 0x67, # VK_NUMPAD7&#xa;    'num8': 0x68, # VK_NUMPAD8&#xa;    'num9': 0x69, # VK_NUMPAD9&#xa;    'multiply': 0x6a, # VK_MULTIPLY  ??? Is this the numpad *?&#xa;    'add': 0x6b, # VK_ADD  ??? Is this the numpad +?&#xa;    'separator': 0x6c, # VK_SEPARATOR  ??? Is this the numpad enter?&#xa;    'subtract': 0x6d, # VK_SUBTRACT  ??? Is this the numpad -?&#xa;    'decimal': 0x6e, # VK_DECIMAL&#xa;    'divide': 0x6f, # VK_DIVIDE&#xa;    'f1': 0x70, # VK_F1&#xa;    'f2': 0x71, # VK_F2&#xa;    'f3': 0x72, # VK_F3&#xa;    'f4': 0x73, # VK_F4&#xa;    'f5': 0x74, # VK_F5&#xa;    'f6': 0x75, # VK_F6&#xa;    'f7': 0x76, # VK_F7&#xa;    'f8': 0x77, # VK_F8&#xa;    'f9': 0x78, # VK_F9&#xa;    'f10': 0x79, # VK_F10&#xa;    'f11': 0x7a, # VK_F11&#xa;    'f12': 0x7b, # VK_F12&#xa;    'f13': 0x7c, # VK_F13&#xa;    'f14': 0x7d, # VK_F14&#xa;    'f15': 0x7e, # VK_F15&#xa;    'f16': 0x7f, # VK_F16&#xa;    'f17': 0x80, # VK_F17&#xa;    'f18': 0x81, # VK_F18&#xa;    'f19': 0x82, # VK_F19&#xa;    'f20': 0x83, # VK_F20&#xa;    'f21': 0x84, # VK_F21&#xa;    'f22': 0x85, # VK_F22&#xa;    'f23': 0x86, # VK_F23&#xa;    'f24': 0x87, # VK_F24&#xa;    'numlock': 0x90, # VK_NUMLOCK&#xa;    'scrolllock': 0x91, # VK_SCROLL&#xa;    'shiftleft': 0xa0, # VK_LSHIFT&#xa;    'shiftright': 0xa1, # VK_RSHIFT&#xa;    'ctrlleft': 0xa2, # VK_LCONTROL&#xa;    'ctrlright': 0xa3, # VK_RCONTROL&#xa;    'altleft': 0xa4, # VK_LMENU&#xa;    'altright': 0xa5, # VK_RMENU&#xa;    'browserback': 0xa6, # VK_BROWSER_BACK&#xa;    'browserforward': 0xa7, # VK_BROWSER_FORWARD&#xa;    'browserrefresh': 0xa8, # VK_BROWSER_REFRESH&#xa;    'browserstop': 0xa9, # VK_BROWSER_STOP&#xa;    'browsersearch': 0xaa, # VK_BROWSER_SEARCH&#xa;    'browserfavorites': 0xab, # VK_BROWSER_FAVORITES&#xa;    'browserhome': 0xac, # VK_BROWSER_HOME&#xa;    'volumemute': 0xad, # VK_VOLUME_MUTE&#xa;    'volumedown': 0xae, # VK_VOLUME_DOWN&#xa;    'volumeup': 0xaf, # VK_VOLUME_UP&#xa;    'nexttrack': 0xb0, # VK_MEDIA_NEXT_TRACK&#xa;    'prevtrack': 0xb1, # VK_MEDIA_PREV_TRACK&#xa;    'stop': 0xb2, # VK_MEDIA_STOP&#xa;    'playpause': 0xb3, # VK_MEDIA_PLAY_PAUSE&#xa;    'launchmail': 0xb4, # VK_LAUNCH_MAIL&#xa;    'launchmediaselect': 0xb5, # VK_LAUNCH_MEDIA_SELECT&#xa;    'launchapp1': 0xb6, # VK_LAUNCH_APP1&#xa;    'launchapp2': 0xb7, # VK_LAUNCH_APP2&#xa;    #';': 0xba, # VK_OEM_1&#xa;    #'+': 0xbb, # VK_OEM_PLUS&#xa;    #',': 0xbc, # VK_OEM_COMMA&#xa;    #'-': 0xbd, # VK_OEM_MINUS&#xa;    #'.': 0xbe, # VK_OEM_PERIOD&#xa;    #'/': 0xbf, # VK_OEM_2&#xa;    #'~': 0xc0, # VK_OEM_3&#xa;    #'[': 0xdb, # VK_OEM_4&#xa;    #'|': 0xdc, # VK_OEM_5&#xa;    #']': 0xdd, # VK_OEM_6&#xa;    #""'"": 0xde, # VK_OEM_7&#xa;    #'': 0xdf, # VK_OEM_8&#xa;    #'': 0xe7, # VK_PACKET&#xa;    #'': 0xf6, # VK_ATTN&#xa;    #'': 0xf7, # VK_CRSEL&#xa;    #'': 0xf8, # VK_EXSEL&#xa;    #'': 0xf9, # VK_EREOF&#xa;    #'': 0xfa, # VK_PLAY&#xa;    #'': 0xfb, # VK_ZOOM&#xa;    #'': 0xfc, # VK_NONAME&#xa;    #'': 0xfd, # VK_PA1&#xa;    #'': 0xfe, # VK_OEM_CLEAR&#xa;})&#xa;&#xa;# Populate the basic printable ascii characters.&#xa;for c in range(32, 128):&#xa;    keyboardMapping[chr(c)] = ctypes.windll.user32.VkKeyScanA(ctypes.wintypes.WCHAR(chr(c)))&#xa;&#xa;&#xa;def _keyDown(key):&#xa;    """"""Performs a keyboard key press without the release. This will put that&#xa;    key in a held down state.&#xa;&#xa;    NOTE: For some reason, this does not seem to cause key repeats like would&#xa;    happen if a keyboard key was held down on a text field.&#xa;&#xa;    Args:&#xa;      key (str): The key to be pressed down. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;&#xa;    vkCode = keyboardMapping[key]&#xa;    if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;        vkCode -= 0x100&#xa;        needsShift = True&#xa;&#xa;    if needsShift:&#xa;        ctypes.windll.user32.keybd_event(0x10, 0, 0, 0) # 0x10 is VK_SHIFT&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, 0, 0)&#xa;    if needsShift:&#xa;        ctypes.windll.user32.keybd_event(0x10, 0, KEYEVENTF_KEYUP, 0) # 0x10 is VK_SHIFT&#xa;&#xa;&#xa;def _keyUp(key):&#xa;    """"""Performs a keyboard key release (without the press down beforehand).&#xa;&#xa;    Args:&#xa;      key (str): The key to be released up. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;    vkCode = keyboardMapping[key]&#xa;    if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;        vkCode -= 0x100&#xa;        needsShift = True&#xa;&#xa;    if needsShift:&#xa;        ctypes.windll.user32.keybd_event(0x10, 0, 0, 0) # 0x10 is VK_SHIFT&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, KEYEVENTF_KEYUP, 0)&#xa;    if needsShift:&#xa;        ctypes.windll.user32.keybd_event(0x10, 0, KEYEVENTF_KEYUP, 0) # 0x10 is VK_SHIFT&#xa;&#xa;&#xa;def _position():&#xa;    """"""Returns the current xy coordinates of the mouse cursor as a two-integer&#xa;    tuple by calling the GetCursorPos() win32 function.&#xa;&#xa;    Returns:&#xa;      (x, y) tuple of the current xy coordinates of the mouse cursor.&#xa;    """"""&#xa;&#xa;    cursor = POINT()&#xa;    ctypes.windll.user32.GetCursorPos(ctypes.byref(cursor))&#xa;    return (cursor.x, cursor.y)&#xa;&#xa;&#xa;def _size():&#xa;    """"""Returns the width and height of the screen as a two-integer tuple.&#xa;&#xa;    Returns:&#xa;      (width, height) tuple of the screen size, in pixels.&#xa;    """"""&#xa;    return (ctypes.windll.user32.GetSystemMetrics(0), ctypes.windll.user32.GetSystemMetrics(1))&#xa;&#xa;&#xa;def _moveTo(x, y):&#xa;    """"""Send the mouse move event to Windows by calling SetCursorPos() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    ctypes.windll.user32.SetCursorPos(x, y)&#xa;&#xa;&#xa;def _mouseDown(x, y, button):&#xa;    """"""Send the mouse down event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        _sendMouseEvent(MOUSEEVENTF_LEFTDOWN, x, y)&#xa;    elif button == 'middle':&#xa;        _sendMouseEvent(MOUSEEVENTF_MIDDLEDOWN, x, y)&#xa;    elif button == 'right':&#xa;        _sendMouseEvent(MOUSEEVENTF_RIGHTDOWN, x, y)&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _mouseUp(x, y, button):&#xa;    """"""Send the mouse up event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        _sendMouseEvent(MOUSEEVENTF_LEFTUP, x, y)&#xa;    elif button == 'middle':&#xa;        _sendMouseEvent(MOUSEEVENTF_MIDDLEUP, x, y)&#xa;    elif button == 'right':&#xa;        _sendMouseEvent(MOUSEEVENTF_RIGHTUP, x, y)&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _click(x, y, button):&#xa;    """"""Send the mouse click event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        _sendMouseEvent(MOUSEEVENTF_LEFTCLICK, x, y)&#xa;    elif button == 'middle':&#xa;        _sendMouseEvent(MOUSEEVENTF_MIDDLECLICK, x, y)&#xa;    elif button == 'right':&#xa;        _sendMouseEvent(MOUSEEVENTF_RIGHTCLICK, x, y)&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _sendMouseEvent(ev, x, y, dwData=0):&#xa;    """"""The helper function that actually makes the call to the mouse_event()&#xa;    win32 function.&#xa;&#xa;    Args:&#xa;      ev (int): The win32 code for the mouse event. Use one of the MOUSEEVENTF_*&#xa;      constants for this argument.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      dwData (int): The argument for mouse_event()'s dwData parameter. So far&#xa;        this is only used by mouse scrolling.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    assert x != None and y != None, 'x and y cannot be set to None'&#xa;    # TODO: ARG! For some reason, SendInput isn't working for mouse events. I'm switching to using the older mouse_event win32 function.&#xa;    #mouseStruct = MOUSEINPUT()&#xa;    #mouseStruct.dx = x&#xa;    #mouseStruct.dy = y&#xa;    #mouseStruct.mouseData = ev&#xa;    #mouseStruct.time = 0&#xa;    #mouseStruct.dwExtraInfo = ctypes.pointer(ctypes.c_ulong(0)) # according to https://stackoverflow.com/questions/13564851/generate-keyboard-events I can just set this. I don't really care about this value.&#xa;    #inputStruct = INPUT()&#xa;    #inputStruct.mi = mouseStruct&#xa;    #inputStruct.type = INPUT_MOUSE&#xa;    #ctypes.windll.user32.SendInput(1, ctypes.pointer(inputStruct), ctypes.sizeof(inputStruct))&#xa;&#xa;    width, height = _size()&#xa;    convertedX = 65536 * x // width + 1&#xa;    convertedY = 65536 * y // height + 1&#xa;    ctypes.windll.user32.mouse_event(ev, ctypes.c_long(convertedX), ctypes.c_long(convertedY), dwData, 0)&#xa;&#xa;    if ctypes.windll.kernel32.GetLastError() != 0:&#xa;        raise ctypes.WinError()&#xa;&#xa;&#xa;def _scroll(clicks, x=None, y=None):&#xa;    """"""Send the mouse vertical scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    startx, starty = _position()&#xa;    width, height = _size()&#xa;&#xa;    if x is None:&#xa;        x = startx&#xa;    else:&#xa;        if x < 0:&#xa;            x = 0&#xa;        elif x >= width:&#xa;            x = width - 1&#xa;    if y is None:&#xa;        y = starty&#xa;    else:&#xa;        if y < 0:&#xa;            y = 0&#xa;        elif y >= height:&#xa;            y = height - 1&#xa;&#xa;    _sendMouseEvent(MOUSEEVENTF_WHEEL, x, y, dwData=clicks)&#xa;&#xa;&#xa;def _hscroll(clicks, x, y):&#xa;    """"""Send the mouse horizontal scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving right, a negative value is moving left.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;&#xa;def _vscroll(clicks, x, y):&#xa;    """"""A wrapper for _scroll(), which does vertical scrolling.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;"
132058|"# -*- coding: utf-8 -*-&#xa;##############################################################################&#xa;#&#xa;#    OpenERP, Open Source Management Solution&#xa;#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).&#xa;#&#xa;#    This program is free software: you can redistribute it and/or modify&#xa;#    it under the terms of the GNU Affero General Public License as&#xa;#    published by the Free Software Foundation, either version 3 of the&#xa;#    License, or (at your option) any later version.&#xa;#&#xa;#    This program is distributed in the hope that it will be useful,&#xa;#    but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;#    GNU Affero General Public License for more details.&#xa;#&#xa;#    You should have received a copy of the GNU Affero General Public License&#xa;#    along with this program.  If not, see <http://www.gnu.org/licenses/>.&#xa;#&#xa;##############################################################################&#xa;&#xa;""""""&#xa;OpenERP - Server&#xa;OpenERP is an ERP+CRM program for small and medium businesses.&#xa;&#xa;The whole source code is distributed under the terms of the&#xa;GNU Public Licence.&#xa;&#xa;(c) 2003-TODAY, Fabien Pinckaers - OpenERP SA&#xa;""""""&#xa;&#xa;import logging&#xa;import os&#xa;import signal&#xa;import sys&#xa;import threading&#xa;import traceback&#xa;import time&#xa;&#xa;import openerp&#xa;&#xa;from . import Command&#xa;&#xa;__author__ = openerp.release.author&#xa;__version__ = openerp.release.version&#xa;&#xa;# Also use the `openerp` logger for the main script.&#xa;_logger = logging.getLogger('openerp')&#xa;&#xa;def check_root_user():&#xa;    """""" Exit if the process's user is 'root' (on POSIX system).""""""&#xa;    if os.name == 'posix':&#xa;        import pwd&#xa;        if pwd.getpwuid(os.getuid())[0] == 'root' :&#xa;            sys.stderr.write(""Running as user 'root' is a security risk, aborting.\n"")&#xa;            sys.exit(1)&#xa;&#xa;def check_postgres_user():&#xa;    """""" Exit if the configured database user is 'postgres'.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    if config['db_user'] == 'postgres':&#xa;        sys.stderr.write(""Using the database user 'postgres' is a security risk, aborting."")&#xa;        sys.exit(1)&#xa;&#xa;def report_configuration():&#xa;    """""" Log the server version and some configuration values.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    _logger.info(""OpenERP version %s"", __version__)&#xa;    for name, value in [('addons paths', config['addons_path']),&#xa;                        ('database hostname', config['db_host'] or 'localhost'),&#xa;                        ('database port', config['db_port'] or '5432'),&#xa;                        ('database user', config['db_user'])]:&#xa;        _logger.info(""%s: %s"", name, value)&#xa;&#xa;def setup_pid_file():&#xa;    """""" Create a file with the process id written in it.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    if config['pidfile']:&#xa;        fd = open(config['pidfile'], 'w')&#xa;        pidtext = ""%d"" % (os.getpid())&#xa;        fd.write(pidtext)&#xa;        fd.close()&#xa;&#xa;def preload_registry(dbname):&#xa;    """""" Preload a registry, and start the cron.""""""&#xa;    try:&#xa;        update_module = True if openerp.tools.config['init'] or openerp.tools.config['update'] else False&#xa;        db, registry = openerp.pooler.get_db_and_pool(dbname,update_module=update_module)&#xa;    except Exception:&#xa;        _logger.exception('Failed to initialize database `%s`.', dbname)&#xa;&#xa;def run_test_file(dbname, test_file):&#xa;    """""" Preload a registry, possibly run a test file, and start the cron.""""""&#xa;    try:&#xa;        config = openerp.tools.config&#xa;        db, registry = openerp.pooler.get_db_and_pool(dbname, update_module=config['init'] or config['update'])&#xa;        cr = db.cursor()&#xa;        _logger.info('loading test file %s', test_file)&#xa;        openerp.tools.convert_yaml_import(cr, 'base', file(test_file), 'test', {}, 'test', True)&#xa;        cr.rollback()&#xa;        cr.close()&#xa;    except Exception:&#xa;        _logger.exception('Failed to initialize database `%s` and run test file `%s`.', dbname, test_file)&#xa;&#xa;def export_translation():&#xa;    config = openerp.tools.config&#xa;    dbname = config['db_name']&#xa;&#xa;    if config[""language""]:&#xa;        msg = ""language %s"" % (config[""language""],)&#xa;    else:&#xa;        msg = ""new language""&#xa;    _logger.info('writing translation file for %s to %s', msg,&#xa;        config[""translate_out""])&#xa;&#xa;    fileformat = os.path.splitext(config[""translate_out""])[-1][1:].lower()&#xa;    buf = file(config[""translate_out""], ""w"")&#xa;    cr = openerp.pooler.get_db(dbname).cursor()&#xa;    openerp.tools.trans_export(config[""language""],&#xa;        config[""translate_modules""] or [""all""], buf, fileformat, cr)&#xa;    cr.close()&#xa;    buf.close()&#xa;&#xa;    _logger.info('translation file written successfully')&#xa;&#xa;def import_translation():&#xa;    config = openerp.tools.config&#xa;    context = {'overwrite': config[""overwrite_existing_translations""]}&#xa;    dbname = config['db_name']&#xa;&#xa;    cr = openerp.pooler.get_db(dbname).cursor()&#xa;    openerp.tools.trans_load( cr, config[""translate_in""], config[""language""],&#xa;        context=context)&#xa;    cr.commit()&#xa;    cr.close()&#xa;&#xa;# Variable keeping track of the number of calls to the signal handler defined&#xa;# below. This variable is monitored by ``quit_on_signals()``.&#xa;quit_signals_received = 0&#xa;&#xa;def signal_handler(sig, frame):&#xa;    """""" Signal handler: exit ungracefully on the second handled signal.&#xa;&#xa;    :param sig: the signal number&#xa;    :param frame: the interrupted stack frame or None&#xa;    """"""&#xa;    global quit_signals_received&#xa;    quit_signals_received += 1&#xa;    if quit_signals_received > 1:&#xa;        # logging.shutdown was already called at this point.&#xa;        sys.stderr.write(""Forced shutdown.\n"")&#xa;        os._exit(0)&#xa;&#xa;def dumpstacks(sig, frame):&#xa;    """""" Signal handler: dump a stack trace for each existing thread.""""""&#xa;    # code from http://stackoverflow.com/questions/132058/getting-stack-trace-from-a-running-python-application#answer-2569696&#xa;    # modified for python 2.5 compatibility&#xa;    threads_info = dict([(th.ident, {'name': th.name,&#xa;                                    'uid': getattr(th,'uid','n/a')})&#xa;                                for th in threading.enumerate()])&#xa;    code = []&#xa;    for threadId, stack in sys._current_frames().items():&#xa;        thread_info = threads_info.get(threadId)&#xa;        code.append(""\n# Thread: %s (id:%s) (uid:%s)"" % \&#xa;                    (thread_info and thread_info['name'] or 'n/a',&#xa;                     threadId,&#xa;                     thread_info and thread_info['uid'] or 'n/a'))&#xa;        for filename, lineno, name, line in traceback.extract_stack(stack):&#xa;            code.append('File: ""%s"", line %d, in %s' % (filename, lineno, name))&#xa;            if line:&#xa;                code.append(""  %s"" % (line.strip()))&#xa;    _logger.info(""\n"".join(code))&#xa;&#xa;def setup_signal_handlers():&#xa;    """""" Register the signal handler defined above. """"""&#xa;    SIGNALS = map(lambda x: getattr(signal, ""SIG%s"" % x), ""INT TERM"".split())&#xa;    if os.name == 'posix':&#xa;        map(lambda sig: signal.signal(sig, signal_handler), SIGNALS)&#xa;        signal.signal(signal.SIGQUIT, dumpstacks)&#xa;    elif os.name == 'nt':&#xa;        import win32api&#xa;        win32api.SetConsoleCtrlHandler(lambda sig: signal_handler(sig, None), 1)&#xa;&#xa;def quit_on_signals():&#xa;    """""" Wait for one or two signals then shutdown the server.&#xa;&#xa;    The first SIGINT or SIGTERM signal will initiate a graceful shutdown while&#xa;    a second one if any will force an immediate exit.&#xa;&#xa;    """"""&#xa;    # Wait for a first signal to be handled. (time.sleep will be interrupted&#xa;    # by the signal handler.) The try/except is for the win32 case.&#xa;    try:&#xa;        while quit_signals_received == 0:&#xa;            time.sleep(60)&#xa;    except KeyboardInterrupt:&#xa;        pass&#xa;&#xa;    config = openerp.tools.config&#xa;    openerp.service.stop_services()&#xa;&#xa;    if getattr(openerp, 'phoenix', False):&#xa;        # like the phoenix, reborn from ashes...&#xa;        openerp.service._reexec()&#xa;        return&#xa;&#xa;    if config['pidfile']:&#xa;        os.unlink(config['pidfile'])&#xa;    sys.exit(0)&#xa;&#xa;def main(args):&#xa;    check_root_user()&#xa;    openerp.tools.config.parse_config(args)&#xa;&#xa;    check_postgres_user()&#xa;    openerp.netsvc.init_logger()&#xa;    report_configuration()&#xa;&#xa;    config = openerp.tools.config&#xa;&#xa;    setup_signal_handlers()&#xa;&#xa;    if config[""test_file""]:&#xa;        run_test_file(config['db_name'], config['test_file'])&#xa;        sys.exit(0)&#xa;&#xa;    if config[""translate_out""]:&#xa;        export_translation()&#xa;        sys.exit(0)&#xa;&#xa;    if config[""translate_in""]:&#xa;        import_translation()&#xa;        sys.exit(0)&#xa;&#xa;    if not config[""stop_after_init""]:&#xa;        setup_pid_file()&#xa;        # Some module register themselves when they are loaded so we need the&#xa;        # services to be running before loading any registry.&#xa;        if config['workers']:&#xa;            openerp.service.start_services_workers()&#xa;        else:&#xa;            openerp.service.start_services()&#xa;&#xa;    if config['db_name']:&#xa;        for dbname in config['db_name'].split(','):&#xa;            preload_registry(dbname)&#xa;&#xa;    if config[""stop_after_init""]:&#xa;        sys.exit(0)&#xa;&#xa;    _logger.info('OpenERP server is running, waiting for connections...')&#xa;    quit_on_signals()&#xa;&#xa;class Server(Command):&#xa;    def run(self, args):&#xa;        main(args)&#xa;&#xa;# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:&#xa;"
13436167|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# http://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
279237|"#!/usr/bin/env python&#xa;&#xa;# Test whether a client sends a correct SUBSCRIBE to a topic with QoS 0.&#xa;&#xa;# The client should connect to port 1888 with keepalive=60, clean session set,&#xa;# and client id subscribe-qos0-test&#xa;# The test will send a CONNACK message to the client with rc=0. Upon receiving&#xa;# the CONNACK and verifying that rc=0, the client should send a SUBSCRIBE&#xa;# message to subscribe to topic ""qos0/test"" with QoS=0. If rc!=0, the client&#xa;# should exit with an error.&#xa;# Upon receiving the correct SUBSCRIBE message, the test will reply with a&#xa;# SUBACK message with the accepted QoS set to 0. On receiving the SUBACK&#xa;# message, the client should send a DISCONNECT message.&#xa;&#xa;import inspect&#xa;import os&#xa;import subprocess&#xa;import socket&#xa;import sys&#xa;import time&#xa;&#xa;# From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder&#xa;cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],"".."")))&#xa;if cmd_subfolder not in sys.path:&#xa;    sys.path.insert(0, cmd_subfolder)&#xa;&#xa;import mosq_test&#xa;&#xa;rc = 1&#xa;keepalive = 60&#xa;connect_packet = mosq_test.gen_connect(""subscribe-qos0-test"", keepalive=keepalive)&#xa;connack_packet = mosq_test.gen_connack(rc=0)&#xa;&#xa;disconnect_packet = mosq_test.gen_disconnect()&#xa;&#xa;mid = 1&#xa;subscribe_packet = mosq_test.gen_subscribe(mid, ""qos0/test"", 0)&#xa;suback_packet = mosq_test.gen_suback(mid, 0)&#xa;&#xa;sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&#xa;sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)&#xa;sock.settimeout(10)&#xa;sock.bind(('', 1888))&#xa;sock.listen(5)&#xa;&#xa;client_args = sys.argv[1:]&#xa;env = dict(os.environ)&#xa;env['LD_LIBRARY_PATH'] = '../../lib:../../lib/cpp'&#xa;try:&#xa;    pp = env['PYTHONPATH']&#xa;except KeyError:&#xa;    pp = ''&#xa;env['PYTHONPATH'] = '../../lib/python:'+pp&#xa;client = mosq_test.start_client(filename=sys.argv[1].replace('/', '-'), cmd=client_args, env=env)&#xa;&#xa;try:&#xa;    (conn, address) = sock.accept()&#xa;    conn.settimeout(10)&#xa;&#xa;    if mosq_test.expect_packet(conn, ""connect"", connect_packet):&#xa;        conn.send(connack_packet)&#xa;&#xa;        if mosq_test.expect_packet(conn, ""subscribe"", subscribe_packet):&#xa;            conn.send(suback_packet)&#xa;        &#xa;            if mosq_test.expect_packet(conn, ""disconnect"", disconnect_packet):&#xa;                rc = 0&#xa;        &#xa;    conn.close()&#xa;finally:&#xa;    client.terminate()&#xa;    client.wait()&#xa;    sock.close()&#xa;&#xa;exit(rc)&#xa;"
21243834|"from lib.common import helpers&#xa;&#xa;class Module:&#xa;&#xa;    def __init__(self, mainMenu, params=[]):&#xa;&#xa;        # metadata info about the module, not modified during runtime&#xa;        self.info = {&#xa;            # name for the module that will appear in module menus&#xa;            'Name': 'HTTP REST API',&#xa;&#xa;            # list of one or more authors for the module&#xa;            'Author': ['@TweekFawkes',""@scottjpack""],&#xa;&#xa;            # more verbose multi-line description of the module&#xa;            'Description': ('Interacts with a HTTP REST API and returns the results back to the screen.'),&#xa;&#xa;            # True if the module needs to run in the background&#xa;            'Background' : True,&#xa;&#xa;            # File extension to save the file as&#xa;            'OutputExtension': """",&#xa;&#xa;            # if the module needs administrative privileges&#xa;            'NeedsAdmin' : False,&#xa;&#xa;            # True if the method doesn't touch disk/is reasonably opsec safe&#xa;            'OpsecSafe' : True,&#xa;&#xa;            # the module language&#xa;            'Language' : 'python',&#xa;&#xa;            # the minimum language version needed&#xa;            'MinLanguageVersion' : '2.6',&#xa;&#xa;            # list of any references/other comments&#xa;            'Comments': [""Docs: https://mesos.github.io/chronos/docs/api.html"", ""urllib2 DELETE method credits to: http://stackoverflow.com/questions/21243834/doing-put-using-python-urllib2""]&#xa;        }&#xa;&#xa;        # any options needed by the module, settable during runtime&#xa;        self.options = {&#xa;            # format:&#xa;            #   value_name : {description, required, default_value}&#xa;            'Agent' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'Agent to execute module on.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   ''&#xa;            },&#xa;            'Protocol' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'Protocol or Scheme to use.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   'http'&#xa;            },&#xa;            'Target' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'FQDN, domain name, or hostname of the remote target.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   'master.mesos'&#xa;            },&#xa;            'Port' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'The port to connect to.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   '8123'&#xa;            },&#xa;            'Path' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'The path.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   '/v1/version'&#xa;            },&#xa;            'RequMethod' : {&#xa;                # The 'Agent' option is the only one that MUST be in a module&#xa;                'Description'   :   'The HTTP request method to use.',&#xa;                'Required'      :   True,&#xa;                'Value'         :   'GET'&#xa;            }&#xa;        }&#xa;&#xa;        # save off a copy of the mainMenu object to access external functionality&#xa;        #   like listeners/agent handlers/etc.&#xa;        self.mainMenu = mainMenu&#xa;&#xa;        # During instantiation, any settable option parameters&#xa;        #   are passed as an object set to the module and the&#xa;        #   options dictionary is automatically set. This is mostly&#xa;        #   in case options are passed on the command line&#xa;        if params:&#xa;            for param in params:&#xa;                # parameter format is [Name, Value]&#xa;                option, value = param&#xa;                if option in self.options:&#xa;                    self.options[option]['Value'] = value&#xa;&#xa;&#xa;    def generate(self):&#xa;        protocol = self.options['Protocol']['Value']&#xa;        target = self.options['Target']['Value']&#xa;        port = self.options['Port']['Value']&#xa;        path = self.options['Path']['Value']&#xa;        requmethod = self.options['RequMethod']['Value']&#xa;&#xa;        script = """"""&#xa;import urllib2&#xa;&#xa;protocol = ""%s""&#xa;target = ""%s""&#xa;port = ""%s""&#xa;path = ""%s""&#xa;requmethod = ""%s""&#xa;&#xa;url = protocol + ""://"" + target + "":"" + port + path&#xa;&#xa;class MethodRequest(urllib2.Request):&#xa;    def __init__(self, *args, **kwargs):&#xa;        if 'method' in kwargs:&#xa;            self._method = kwargs['method']&#xa;            del kwargs['method']&#xa;        else:&#xa;            self._method = None&#xa;        return urllib2.Request.__init__(self, *args, **kwargs)&#xa;&#xa;    def get_method(self, *args, **kwargs):&#xa;        if self._method is not None:&#xa;            return self._method&#xa;        return urllib2.Request.get_method(self, *args, **kwargs)&#xa;&#xa;try:&#xa;    request = MethodRequest(url, method=requmethod)&#xa;    request.add_header('User-Agent',&#xa;                   'Mozilla/6.0 (X11; Linux x86_64; rv:24.0) '&#xa;                   'Gecko/20140205     Firefox/27.0 Iceweasel/25.3.0')&#xa;    opener = urllib2.build_opener(urllib2.HTTPHandler)&#xa;    content = opener.open(request).read()&#xa;    print str(content)&#xa;except Exception as e:&#xa;    print ""Failure sending payload: "" + str(e)&#xa;&#xa;print ""Finished""&#xa;"""""" %(protocol, target, port, path, requmethod)&#xa;&#xa;        return script&#xa;"
5573409|"# -*- coding: utf-8 -*-&#xa;""""""Tests for jslex.""""""&#xa;# originally from https://bitbucket.org/ned/jslex&#xa;from __future__ import unicode_literals&#xa;&#xa;from django.test import SimpleTestCase&#xa;from django.utils.jslex import JsLexer, prepare_js_for_gettext&#xa;&#xa;&#xa;class JsTokensTest(SimpleTestCase):&#xa;    LEX_CASES = [&#xa;        # ids&#xa;        (""a ABC $ _ a123"", [""id a"", ""id ABC"", ""id $"", ""id _"", ""id a123""]),&#xa;        (""\\u1234 abc\\u0020 \\u0065_\\u0067"", [""id \\u1234"", ""id abc\\u0020"", ""id \\u0065_\\u0067""]),&#xa;        # numbers&#xa;        (""123 1.234 0.123e-3 0 1E+40 1e1 .123"", [""dnum 123"", ""dnum 1.234"", ""dnum 0.123e-3"", ""dnum 0"", ""dnum 1E+40"", ""dnum 1e1"", ""dnum .123""]),&#xa;        (""0x1 0xabCD 0XABcd"", [""hnum 0x1"", ""hnum 0xabCD"", ""hnum 0XABcd""]),&#xa;        (""010 0377 090"", [""onum 010"", ""onum 0377"", ""dnum 0"", ""dnum 90""]),&#xa;        (""0xa123ghi"", [""hnum 0xa123"", ""id ghi""]),&#xa;        # keywords&#xa;        (""function Function FUNCTION"", [""keyword function"", ""id Function"", ""id FUNCTION""]),&#xa;        (""const constructor in inherits"", [""keyword const"", ""id constructor"", ""keyword in"", ""id inherits""]),&#xa;        (""true true_enough"", [""reserved true"", ""id true_enough""]),&#xa;        # strings&#xa;        (''' 'hello' ""hello"" ''', [""string 'hello'"", 'string ""hello""']),&#xa;        (r"""""" 'don\'t' ""don\""t"" '""' ""'"" '\'' ""\"""" """""",&#xa;         [r""""""string 'don\'t'"""""", r'''string ""don\""t""''', r""""""string '""'"""""", r'''string ""'""''', r""""""string '\''"""""", r'''string ""\""""''']),&#xa;        (r'""ƃuıxǝ⅂ ʇdıɹɔsɐʌɐſ\""""', [r'string ""ƃuıxǝ⅂ ʇdıɹɔsɐʌɐſ\""""']),&#xa;        # comments&#xa;        (""a//b"", [""id a"", ""linecomment //b""]),&#xa;        (""/****/a/=2//hello"", [""comment /****/"", ""id a"", ""punct /="", ""dnum 2"", ""linecomment //hello""]),&#xa;        (""/*\n * Header\n */\na=1;"", [""comment /*\n * Header\n */"", ""id a"", ""punct ="", ""dnum 1"", ""punct ;""]),&#xa;        # punctuation&#xa;        (""a+++b"", [""id a"", ""punct ++"", ""punct +"", ""id b""]),&#xa;        # regex&#xa;        (r""a=/a*/,1"", [""id a"", ""punct ="", ""regex /a*/"", ""punct ,"", ""dnum 1""]),&#xa;        (r""a=/a*[^/]+/,1"", [""id a"", ""punct ="", ""regex /a*[^/]+/"", ""punct ,"", ""dnum 1""]),&#xa;        (r""a=/a*\[^/,1"", [""id a"", ""punct ="", r""regex /a*\[^/"", ""punct ,"", ""dnum 1""]),&#xa;        (r""a=/\//,1"", [""id a"", ""punct ="", r""regex /\//"", ""punct ,"", ""dnum 1""]),&#xa;&#xa;        # next two are from http://www.mozilla.org/js/language/js20-2002-04/rationale/syntax.html#regular-expressions&#xa;        (""""""for (var x = a in foo && ""</x>"" || mot ? z:/x:3;x<5;y</g/i) {xyz(x++);}"""""",&#xa;            [""keyword for"", ""punct ("", ""keyword var"", ""id x"", ""punct ="", ""id a"", ""keyword in"",&#xa;            ""id foo"", ""punct &&"", 'string ""</x>""', ""punct ||"", ""id mot"", ""punct ?"", ""id z"",&#xa;            ""punct :"", ""regex /x:3;x<5;y</g"", ""punct /"", ""id i"", ""punct )"", ""punct {"",&#xa;            ""id xyz"", ""punct ("", ""id x"", ""punct ++"", ""punct )"", ""punct ;"", ""punct }""]),&#xa;        (""""""for (var x = a in foo && ""</x>"" || mot ? z/x:3;x<5;y</g/i) {xyz(x++);}"""""",&#xa;            [""keyword for"", ""punct ("", ""keyword var"", ""id x"", ""punct ="", ""id a"", ""keyword in"",&#xa;            ""id foo"", ""punct &&"", 'string ""</x>""', ""punct ||"", ""id mot"", ""punct ?"", ""id z"",&#xa;            ""punct /"", ""id x"", ""punct :"", ""dnum 3"", ""punct ;"", ""id x"", ""punct <"", ""dnum 5"",&#xa;            ""punct ;"", ""id y"", ""punct <"", ""regex /g/i"", ""punct )"", ""punct {"",&#xa;            ""id xyz"", ""punct ("", ""id x"", ""punct ++"", ""punct )"", ""punct ;"", ""punct }""]),&#xa;&#xa;        # Various ""illegal"" regexes that are valid according to the std.&#xa;        (r""""""/????/, /++++/, /[----]/ """""", [""regex /????/"", ""punct ,"", ""regex /++++/"", ""punct ,"", ""regex /[----]/""]),&#xa;&#xa;        # Stress cases from http://stackoverflow.com/questions/5533925/what-javascript-constructs-does-jslex-incorrectly-lex/5573409#5573409&#xa;        (r""""""/\[/"""""", [r""""""regex /\[/""""""]),&#xa;        (r""""""/[i]/"""""", [r""""""regex /[i]/""""""]),&#xa;        (r""""""/[\]]/"""""", [r""""""regex /[\]]/""""""]),&#xa;        (r""""""/a[\]]/"""""", [r""""""regex /a[\]]/""""""]),&#xa;        (r""""""/a[\]]b/"""""", [r""""""regex /a[\]]b/""""""]),&#xa;        (r""""""/[\]/]/gi"""""", [r""""""regex /[\]/]/gi""""""]),&#xa;        (r""""""/\[[^\]]+\]/gi"""""", [r""""""regex /\[[^\]]+\]/gi""""""]),&#xa;        (""""""&#xa;            rexl.re = {&#xa;            NAME: /^(?![0-9])(?:\w)+|^""(?:[^""]|"""")+""/,&#xa;            UNQUOTED_LITERAL: /^@(?:(?![0-9])(?:\w|\:)+|^""(?:[^""]|"""")+"")\[[^\]]+\]/,&#xa;            QUOTED_LITERAL: /^'(?:[^']|'')*'/,&#xa;            NUMERIC_LITERAL: /^[0-9]+(?:\.[0-9]*(?:[eE][-+][0-9]+)?)?/,&#xa;            SYMBOL: /^(?:==|=|<>|<=|<|>=|>|!~~|!~|~~|~|!==|!=|!~=|!~|!|&|\||\.|\:|,|\(|\)|\[|\]|\{|\}|\?|\:|;|@|\^|\/\+|\/|\*|\+|-)/&#xa;            };&#xa;        """""",&#xa;        [""id rexl"", ""punct ."", ""id re"", ""punct ="", ""punct {"",&#xa;         ""id NAME"", ""punct :"", r""""""regex /^(?![0-9])(?:\w)+|^""(?:[^""]|"""")+""/"""""", ""punct ,"",&#xa;         ""id UNQUOTED_LITERAL"", ""punct :"", r""""""regex /^@(?:(?![0-9])(?:\w|\:)+|^""(?:[^""]|"""")+"")\[[^\]]+\]/"""""", ""punct ,"",&#xa;         ""id QUOTED_LITERAL"", ""punct :"", r""""""regex /^'(?:[^']|'')*'/"""""", ""punct ,"",&#xa;         ""id NUMERIC_LITERAL"", ""punct :"", r""""""regex /^[0-9]+(?:\.[0-9]*(?:[eE][-+][0-9]+)?)?/"""""", ""punct ,"",&#xa;         ""id SYMBOL"", ""punct :"", r""""""regex /^(?:==|=|<>|<=|<|>=|>|!~~|!~|~~|~|!==|!=|!~=|!~|!|&|\||\.|\:|,|\(|\)|\[|\]|\{|\}|\?|\:|;|@|\^|\/\+|\/|\*|\+|-)/"""""",&#xa;         ""punct }"", ""punct ;""&#xa;         ]),&#xa;&#xa;        (""""""&#xa;            rexl.re = {&#xa;            NAME: /^(?![0-9])(?:\w)+|^""(?:[^""]|"""")+""/,&#xa;            UNQUOTED_LITERAL: /^@(?:(?![0-9])(?:\w|\:)+|^""(?:[^""]|"""")+"")\[[^\]]+\]/,&#xa;            QUOTED_LITERAL: /^'(?:[^']|'')*'/,&#xa;            NUMERIC_LITERAL: /^[0-9]+(?:\.[0-9]*(?:[eE][-+][0-9]+)?)?/,&#xa;            SYMBOL: /^(?:==|=|<>|<=|<|>=|>|!~~|!~|~~|~|!==|!=|!~=|!~|!|&|\||\.|\:|,|\(|\)|\[|\]|\{|\}|\?|\:|;|@|\^|\/\+|\/|\*|\+|-)/&#xa;            };&#xa;            str = '""';&#xa;        """""",&#xa;        [""id rexl"", ""punct ."", ""id re"", ""punct ="", ""punct {"",&#xa;         ""id NAME"", ""punct :"", r""""""regex /^(?![0-9])(?:\w)+|^""(?:[^""]|"""")+""/"""""", ""punct ,"",&#xa;         ""id UNQUOTED_LITERAL"", ""punct :"", r""""""regex /^@(?:(?![0-9])(?:\w|\:)+|^""(?:[^""]|"""")+"")\[[^\]]+\]/"""""", ""punct ,"",&#xa;         ""id QUOTED_LITERAL"", ""punct :"", r""""""regex /^'(?:[^']|'')*'/"""""", ""punct ,"",&#xa;         ""id NUMERIC_LITERAL"", ""punct :"", r""""""regex /^[0-9]+(?:\.[0-9]*(?:[eE][-+][0-9]+)?)?/"""""", ""punct ,"",&#xa;         ""id SYMBOL"", ""punct :"", r""""""regex /^(?:==|=|<>|<=|<|>=|>|!~~|!~|~~|~|!==|!=|!~=|!~|!|&|\||\.|\:|,|\(|\)|\[|\]|\{|\}|\?|\:|;|@|\^|\/\+|\/|\*|\+|-)/"""""",&#xa;         ""punct }"", ""punct ;"",&#xa;         ""id str"", ""punct ="", """"""string '""'"""""", ""punct ;"",&#xa;         ]),&#xa;&#xa;        (r"""""" this._js = ""e.str(\"""" + this.value.replace(/\\/g, ""\\\\"").replace(/""/g, ""\\\"""") + ""\"")""; """""",&#xa;         [""keyword this"", ""punct ."", ""id _js"", ""punct ="", r'''string ""e.str(\""""''', ""punct +"", ""keyword this"", ""punct ."",&#xa;          ""id value"", ""punct ."", ""id replace"", ""punct ("", r""regex /\\/g"", ""punct ,"", r'string ""\\\\""', ""punct )"",&#xa;          ""punct ."", ""id replace"", ""punct ("", r'regex /""/g', ""punct ,"", r'string ""\\\""""', ""punct )"", ""punct +"",&#xa;          r'string ""\"")""', ""punct ;""]),&#xa;    ]&#xa;&#xa;&#xa;def make_function(input, toks):&#xa;    def test_func(self):&#xa;        lexer = JsLexer()&#xa;        result = [""%s %s"" % (name, tok) for name, tok in lexer.lex(input) if name != 'ws']&#xa;        self.assertListEqual(result, toks)&#xa;    return test_func&#xa;&#xa;for i, (input, toks) in enumerate(JsTokensTest.LEX_CASES):&#xa;    setattr(JsTokensTest, ""test_case_%d"" % i, make_function(input, toks))&#xa;&#xa;&#xa;GETTEXT_CASES = (&#xa;    (&#xa;        r""""""&#xa;            a = 1; /* /[0-9]+/ */&#xa;            b = 0x2a0b / 1; // /[0-9]+/&#xa;            c = 3;&#xa;        """""",&#xa;        r""""""&#xa;            a = 1; /* /[0-9]+/ */&#xa;            b = 0x2a0b / 1; // /[0-9]+/&#xa;            c = 3;&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            a = 1.234e-5;&#xa;            /*&#xa;             * /[0-9+/&#xa;             */&#xa;            b = .0123;&#xa;        """""",&#xa;        r""""""&#xa;            a = 1.234e-5;&#xa;            /*&#xa;             * /[0-9+/&#xa;             */&#xa;            b = .0123;&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            x = y / z;&#xa;            alert(gettext(""hello""));&#xa;            x /= 3;&#xa;        """""",&#xa;        r""""""&#xa;            x = y / z;&#xa;            alert(gettext(""hello""));&#xa;            x /= 3;&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            s = ""Hello \""th/foo/ere\"""";&#xa;            s = 'He\x23llo \'th/foo/ere\'';&#xa;            s = 'slash quote \"", just quote ""';&#xa;        """""",&#xa;        r""""""&#xa;            s = ""Hello \""th/foo/ere\"""";&#xa;            s = ""He\x23llo \'th/foo/ere\'"";&#xa;            s = ""slash quote \"", just quote \"""";&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            s = ""Line continuation\&#xa;            continued /hello/ still the string"";/hello/;&#xa;        """""",&#xa;        r""""""&#xa;            s = ""Line continuation\&#xa;            continued /hello/ still the string"";""REGEX"";&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            var regex = /pattern/;&#xa;            var regex2 = /matter/gm;&#xa;            var regex3 = /[*/]+/gm.foo(""hey"");&#xa;        """""",&#xa;        r""""""&#xa;            var regex = ""REGEX"";&#xa;            var regex2 = ""REGEX"";&#xa;            var regex3 = ""REGEX"".foo(""hey"");&#xa;        """"""&#xa;    ), (&#xa;        r""""""&#xa;            for (var x = a in foo && ""</x>"" || mot ? z:/x:3;x<5;y</g/i) {xyz(x++);}&#xa;            for (var x = a in foo && ""</x>"" || mot ? z/x:3;x<5;y</g/i) {xyz(x++);}&#xa;        """""",&#xa;        r""""""&#xa;            for (var x = a in foo && ""</x>"" || mot ? z:""REGEX""/i) {xyz(x++);}&#xa;            for (var x = a in foo && ""</x>"" || mot ? z/x:3;x<5;y<""REGEX"") {xyz(x++);}&#xa;        """"""&#xa;    ), (&#xa;        """"""&#xa;            \\u1234xyz = gettext('Hello there');&#xa;        """""", r""""""&#xa;            Uu1234xyz = gettext(""Hello there"");&#xa;        """"""&#xa;    )&#xa;)&#xa;&#xa;&#xa;class JsToCForGettextTest(SimpleTestCase):&#xa;    pass&#xa;&#xa;&#xa;def make_function(js, c):&#xa;    def test_func(self):&#xa;        self.assertMultiLineEqual(prepare_js_for_gettext(js), c)&#xa;    return test_func&#xa;&#xa;for i, pair in enumerate(GETTEXT_CASES):&#xa;    setattr(JsToCForGettextTest, ""test_case_%d"" % i, make_function(*pair))&#xa;"
898669|"#!/usr/bin/env python&#xa;&#xa;# Copyright 2015 The Kubernetes Authors.&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;&#xa;from __future__ import print_function&#xa;&#xa;import argparse&#xa;import os&#xa;import re&#xa;import sys&#xa;&#xa;parser = argparse.ArgumentParser()&#xa;parser.add_argument(""filenames"", help=""list of files to check, all files if unspecified"", nargs='*')&#xa;args = parser.parse_args()&#xa;&#xa;# Cargo culted from http://stackoverflow.com/questions/898669/how-can-i-detect-if-a-file-is-binary-non-text-in-python&#xa;def is_binary(pathname):&#xa;    """"""Return true if the given filename is binary.&#xa;    @raise EnvironmentError: if the file does not exist or cannot be accessed.&#xa;    @attention: found @ http://bytes.com/topic/python/answers/21222-determine-file-type-binary-text on 6/08/2010&#xa;    @author: Trent Mick <TrentM@ActiveState.com>&#xa;    @author: Jorge Orpinel <jorge@orpinel.com>""""""&#xa;    try:&#xa;        with open(pathname, 'r') as f:&#xa;            CHUNKSIZE = 1024&#xa;            while 1:&#xa;                chunk = f.read(CHUNKSIZE)&#xa;                if '\0' in chunk: # found null byte&#xa;                    return True&#xa;                if len(chunk) < CHUNKSIZE:&#xa;                    break # done&#xa;    except:&#xa;        return True&#xa;&#xa;    return False&#xa;&#xa;def get_all_files(rootdir):&#xa;    all_files = []&#xa;    for root, dirs, files in os.walk(rootdir):&#xa;        # don't visit certain dirs&#xa;        if 'vendor' in dirs:&#xa;            dirs.remove('vendor')&#xa;        if 'staging' in dirs:&#xa;            dirs.remove('staging')&#xa;        if '_output' in dirs:&#xa;            dirs.remove('_output')&#xa;        if '_gopath' in dirs:&#xa;            dirs.remove('_gopath')&#xa;        if 'third_party' in dirs:&#xa;            dirs.remove('third_party')&#xa;        if '.git' in dirs:&#xa;            dirs.remove('.git')&#xa;        if '.make' in dirs:&#xa;            dirs.remove('.make')&#xa;        if 'BUILD' in files:&#xa;           files.remove('BUILD')&#xa;&#xa;        for name in files:&#xa;            pathname = os.path.join(root, name)&#xa;            if is_binary(pathname):&#xa;                continue&#xa;            all_files.append(pathname)&#xa;    return all_files&#xa;&#xa;# Collects all the flags used in golang files and verifies the flags do&#xa;# not contain underscore. If any flag needs to be excluded from this check,&#xa;# need to add that flag in hack/verify-flags/excluded-flags.txt.&#xa;def check_underscore_in_flags(rootdir, files):&#xa;    # preload the 'known' flags which don't follow the - standard&#xa;    pathname = os.path.join(rootdir, ""hack/verify-flags/excluded-flags.txt"")&#xa;    f = open(pathname, 'r')&#xa;    excluded_flags = set(f.read().splitlines())&#xa;    f.close()&#xa;&#xa;    regexs = [ re.compile('Var[P]?\([^,]*, ""([^""]*)""'),&#xa;               re.compile('.String[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Int[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Bool[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Duration[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.StringSlice[P]?\(""([^""]*)"",[^,]+,[^)]+\)') ]&#xa;&#xa;    new_excluded_flags = set()&#xa;    # walk all the files looking for any flags being declared&#xa;    for pathname in files:&#xa;        if not pathname.endswith("".go""):&#xa;            continue&#xa;        f = open(pathname, 'r')&#xa;        data = f.read()&#xa;        f.close()&#xa;        matches = []&#xa;        for regex in regexs:&#xa;            matches = matches + regex.findall(data)&#xa;        for flag in matches:&#xa;            if any(x in flag for x in excluded_flags):&#xa;                continue&#xa;            if ""_"" in flag:&#xa;                new_excluded_flags.add(flag)&#xa;    if len(new_excluded_flags) != 0:&#xa;        print(""Found a flag declared with an _ but which is not explicitly listed as a valid flag name in hack/verify-flags/excluded-flags.txt"")&#xa;        print(""Are you certain this flag should not have been declared with an - instead?"")&#xa;        l = list(new_excluded_flags)&#xa;        l.sort()&#xa;        print(""%s"" % ""\n"".join(l))&#xa;        sys.exit(1)&#xa;&#xa;def main():&#xa;    rootdir = os.path.dirname(__file__) + ""/../""&#xa;    rootdir = os.path.abspath(rootdir)&#xa;&#xa;    if len(args.filenames) > 0:&#xa;        files = args.filenames&#xa;    else:&#xa;        files = get_all_files(rootdir)&#xa;&#xa;    check_underscore_in_flags(rootdir, files)&#xa;&#xa;if __name__ == ""__main__"":&#xa;  sys.exit(main())&#xa;"
19331550|"import csv&#xa;import datetime&#xa;import shutil&#xa;import sqlite3&#xa;import time&#xa;import enum&#xa;import re&#xa;&#xa;import bwb.bwbglobal&#xa;&#xa;#################&#xa;#&#xa;# Model&#xa;#&#xa;# This module contains everything related to the model for the application:&#xa;# * The db schema&#xa;# * The db connection&#xa;# * Data structure classes (each of which contains functions for reading and writing to the db)&#xa;# * Database creation and setup&#xa;# * Various functions (for backing up the db etc)&#xa;#&#xa;# Notes:&#xa;# * When inserting vales, it's best to use ""VALUES (?, ?)"" because then the sqlite3 module will take care of&#xa;#   escaping strings for us&#xa;#&#xa;#################&#xa;&#xa;SQLITE_FALSE = 0&#xa;SQLITE_TRUE = 1&#xa;SQLITE_NULL = ""NULL""&#xa;TIME_NOT_SET = -1&#xa;NO_REFERENCE = -1&#xa;&#xa;&#xa;class QuestionSetupEnum(enum.Enum):&#xa;    # -only used at setup&#xa;    practice = 1&#xa;    gratitude = 2&#xa;    sharing = 3&#xa;    contribution = 4&#xa;    study = 5&#xa;    self_compassion = 6&#xa;&#xa;&#xa;class MoveDirectionEnum(enum.Enum):&#xa;    up = 1&#xa;    down = 2&#xa;&#xa;&#xa;def get_schema_version(i_db_conn):&#xa;    t_cursor = i_db_conn.execute(""PRAGMA user_version"")&#xa;    return t_cursor.fetchone()[0]&#xa;&#xa;&#xa;def set_schema_version(i_db_conn, i_version_it):&#xa;    i_db_conn.execute(""PRAGMA user_version={:d}"".format(i_version_it))&#xa;&#xa;&#xa;def initial_schema_and_setup(i_db_conn):&#xa;    """"""Auto-increment is not needed in our case: https://www.sqlite.org/autoinc.html&#xa;    """"""&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.QuestionTable.name + ""(""&#xa;        + DbSchemaM.QuestionTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.QuestionTable.Cols.sort_order + "" INTEGER NOT NULL, ""&#xa;        + DbSchemaM.QuestionTable.Cols.title + "" TEXT NOT NULL, ""&#xa;        + DbSchemaM.QuestionTable.Cols.question + "" TEXT NOT NULL DEFAULT '', ""&#xa;        + DbSchemaM.QuestionTable.Cols.archived + "" INTEGER DEFAULT "" + str(SQLITE_FALSE)&#xa;        + "")""&#xa;    )&#xa;&#xa;    i_db_conn.execute(&#xa;        ""INSERT INTO "" + DbSchemaM.QuestionTable.name + ""(""&#xa;        + DbSchemaM.QuestionTable.Cols.id + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.sort_order + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.title + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.question&#xa;        + "") VALUES (?, ?, ?, ?)"", (bwb.bwbglobal.NO_ACTIVE_QUESTION_INT, -1, ""<i>no question</i>"", """")&#xa;    )&#xa;&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.date_added + "" INTEGER, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.favorite + "" INTEGER NOT NULL DEFAULT '""&#xa;        + str(SQLITE_FALSE) + ""', ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "" TEXT, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.question_ref&#xa;        + "" INTEGER REFERENCES "" + DbSchemaM.QuestionTable.name&#xa;        + ""("" + DbSchemaM.QuestionTable.Cols.id + "")""&#xa;        + "" NOT NULL DEFAULT '"" + str(bwb.bwbglobal.NO_ACTIVE_QUESTION_INT) + ""'""&#xa;        + "")""&#xa;    )&#xa;&#xa;    # + "" NOT NULL DEFAULT '"" + str(bwb.bwbglobal.NO_ACTIVE_QUESTION_INT) + ""'""&#xa;&#xa;    """"""&#xa;    i_db_conn.execute(&#xa;        ""CREATE INDEX "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;        + "")""&#xa;    )&#xa;    """"""&#xa;&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.ReminderTable.name + ""(""&#xa;        + DbSchemaM.ReminderTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.ReminderTable.Cols.title + "" TEXT DEFAULT '', ""&#xa;        + DbSchemaM.ReminderTable.Cols.reminder + "" TEXT DEFAULT ''""&#xa;        + "")""&#xa;    )&#xa;&#xa;    if not bwb.bwbglobal.persistent_bool:&#xa;        populate_db_with_test_data()&#xa;&#xa;&#xa;""""""&#xa;Example of db upgrade code:&#xa;def upgrade_1_2(i_db_conn):&#xa;    backup_db_file()&#xa;    i_db_conn.execute(&#xa;        ""ALTER TABLE "" + DbSchemaM.ObservancesTable.name + "" ADD COLUMN ""&#xa;        + DbSchemaM.ObservancesTable.Cols.user_text + "" TEXT DEFAULT ''""&#xa;    )&#xa;""""""&#xa;&#xa;&#xa;upgrade_steps = {&#xa;    1: initial_schema_and_setup,&#xa;}&#xa;&#xa;&#xa;class DbHelperM(object):&#xa;    __db_connection = None  # ""Static""&#xa;&#xa;    # noinspection PyTypeChecker&#xa;    @staticmethod&#xa;    def get_db_connection():&#xa;        if DbHelperM.__db_connection is None:&#xa;            DbHelperM.__db_connection = sqlite3.connect(bwb.bwbglobal.get_database_filename())&#xa;&#xa;            # Upgrading the database&#xa;            # Very good upgrade explanation:&#xa;            # http://stackoverflow.com/questions/19331550/database-change-with-software-update&#xa;            # More info here: https://www.sqlite.org/pragma.html#pragma_schema_version&#xa;            current_db_ver_it = get_schema_version(DbHelperM.__db_connection)&#xa;            target_db_ver_it = max(upgrade_steps)&#xa;            for upgrade_step_it in range(current_db_ver_it + 1, target_db_ver_it + 1):&#xa;                if upgrade_step_it in upgrade_steps:&#xa;                    upgrade_steps[upgrade_step_it](DbHelperM.__db_connection)&#xa;                    set_schema_version(DbHelperM.__db_connection, upgrade_step_it)&#xa;            DbHelperM.__db_connection.commit()&#xa;&#xa;            # TODO: Where do we close the db connection? (Do we need to close it?)&#xa;            # http://stackoverflow.com/questions/3850261/doing-something-before-program-exit&#xa;&#xa;        return DbHelperM.__db_connection&#xa;&#xa;&#xa;class DbSchemaM:&#xa;    class QuestionTable:&#xa;        name = ""question""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            sort_order = ""sort_order""&#xa;            title = ""title""&#xa;            question = ""question""&#xa;            archived = ""archived""&#xa;&#xa;    class DiaryEntryTable:&#xa;        name = ""diary_entry""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            date_added = ""date_added""&#xa;            favorite = ""favorite""&#xa;            diary_entry = ""diary_entry""&#xa;            question_ref = ""question_ref""&#xa;&#xa;    class ReminderTable:&#xa;        name = ""reminder""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            title = ""title""&#xa;            reminder = ""reminder""&#xa;&#xa;&#xa;class QuestionM:&#xa;    def __init__(self, i_id: int, i_order: int, i_title: str, i_question: str, i_archived: bool=False) -> None:&#xa;        self.id_int = i_id&#xa;        self.sort_order_int = i_order&#xa;        self.title_str = i_title&#xa;        self.question_str = i_question&#xa;        self.archived_bl = i_archived&#xa;&#xa;    @staticmethod&#xa;    def add(i_question_id_int: int, i_title_str: str, i_question_str: str) -> None:&#xa;        sort_order = len(QuestionM.get_all())&#xa;        print(""sort_order = "" + str(sort_order))&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.QuestionTable.name + ""(""&#xa;            + DbSchemaM.QuestionTable.Cols.id + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.sort_order + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.title + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.question&#xa;            + "") VALUES (?, ?, ?, ?)"",&#xa;            (i_question_id_int, sort_order, i_title_str, i_question_str)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        journal_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return QuestionM(*journal_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_active_by_sort_order(i_sort_order: int, i_move_direction: MoveDirectionEnum):&#xa;&#xa;        direction_as_lt_gt_str = "">""&#xa;        sort_direction_str = ""DESC""&#xa;        if i_move_direction == MoveDirectionEnum.up:&#xa;            direction_as_lt_gt_str = ""<""&#xa;            sort_direction_str = ""DESC""&#xa;        elif i_move_direction == MoveDirectionEnum.down:&#xa;            direction_as_lt_gt_str = "">""&#xa;            sort_direction_str = ""ASC""&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.sort_order + direction_as_lt_gt_str + str(i_sort_order)&#xa;            + "" AND "" + DbSchemaM.QuestionTable.Cols.archived + ""="" + str(SQLITE_FALSE)&#xa;            + "" ORDER BY "" + DbSchemaM.QuestionTable.Cols.sort_order + "" "" + sort_direction_str&#xa;        )&#xa;        journal_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return QuestionM(*journal_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all(i_show_archived_questions_bool = False):&#xa;        if i_show_archived_questions_bool:&#xa;            show_archived_questions_bool_as_int = SQLITE_TRUE&#xa;        else:&#xa;            show_archived_questions_bool_as_int = SQLITE_FALSE&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.archived + ""="" + str(show_archived_questions_bool_as_int)&#xa;            + "" ORDER BY "" + DbSchemaM.QuestionTable.Cols.sort_order&#xa;        )&#xa;        journal_db_te_list = db_cursor_result.fetchall()&#xa;        db_connection.commit()&#xa;&#xa;        return [QuestionM(*journal_db_te) for journal_db_te in journal_db_te_list]&#xa;&#xa;    @staticmethod&#xa;    def update_active_sort_order_move_up_down(i_id: int, i_move_direction: MoveDirectionEnum) -> None:&#xa;        """"""&#xa;        There is a swap of the sort order value with the question above or below which means that the&#xa;        archived questions can keep their sort orders (the previous solution was to inc/dec the sort&#xa;        order value)&#xa;        """"""&#xa;        main_id_int = i_id&#xa;        main_sort_order_int = QuestionM.get(i_id).sort_order_int&#xa;        if i_move_direction == MoveDirectionEnum.up:&#xa;            if main_sort_order_int == 0 or main_sort_order_int > len(QuestionM.get_all()):&#xa;                return&#xa;        elif i_move_direction == MoveDirectionEnum.down:&#xa;            if main_sort_order_int < 0 or main_sort_order_int >= len(QuestionM.get_all()):&#xa;                return&#xa;        else:&#xa;            pass&#xa;        other = QuestionM.get_active_by_sort_order(main_sort_order_int, i_move_direction)&#xa;        other_id_int = other.id_int&#xa;        other_sort_order_int = other.sort_order_int&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.sort_order + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(other_sort_order_int), str(main_id_int))&#xa;        )&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.sort_order + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(main_sort_order_int), str(other_id_int))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_title(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.title + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_description(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.question + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_archived(i_id_it, i_archived_bool):&#xa;&#xa;        archived_bool_as_int = SQLITE_FALSE&#xa;        if i_archived_bool:&#xa;            archived_bool_as_int = SQLITE_TRUE&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.archived + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(archived_bool_as_int), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_it):&#xa;&#xa;        if i_id_it == bwb.bwbglobal.NO_ACTIVE_QUESTION_INT:&#xa;            raise Exception(""This cannot be removed"")&#xa;            return&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;&#xa;&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + SQLITE_NULL&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_id_it)&#xa;        )&#xa;&#xa;&#xa;&#xa;        db_connection.commit()&#xa;&#xa;&#xa;class DiaryEntryM:&#xa;    def __init__(self, i_id, i_date_added_it, i_favorite_it, i_diary_text, i_question_ref_it):&#xa;        self.id = i_id&#xa;        self.date_added_it = i_date_added_it&#xa;        self.favorite_it = i_favorite_it&#xa;        self.diary_text = i_diary_text&#xa;        self.question_ref_it = i_question_ref_it&#xa;&#xa;    @staticmethod&#xa;    def add(i_date_added_it, i_favorite_it, i_diary_text, i_journal_ref_it: int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.date_added + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.favorite + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.question_ref&#xa;            + "") VALUES (?, ?, ?, ?)"",&#xa;            (i_date_added_it, i_favorite_it, i_diary_text, i_journal_ref_it)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;        # t_diary_id = db_cursor.lastrowid&#xa;&#xa;    @staticmethod&#xa;    def update_note(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_question(i_id_it, i_question_ref_id_int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_question_ref_id_int), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def clear_question(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (None, str(i_id_it))&#xa;            # -Please note: We cannot use ""SQLITE_NULL"" (which is the string ""null"", instead we use None&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_date(i_id_it, i_new_time_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_new_time_it), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_favorite(i_id_it, i_favorite_bool_as_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.favorite + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_favorite_bool_as_it), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name + "" WHERE ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        diary_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return DiaryEntryM(*diary_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all(i_reverse_bl = False):&#xa;        t_direction_sg = ""ASC""&#xa;        if i_reverse_bl:&#xa;            t_direction_sg = ""DESC""&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" "" + t_direction_sg&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_search_term(i_search_term_str: str, i_page_number_int: int):&#xa;        t_direction_sg = ""ASC""&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry&#xa;            + "" LIKE "" + '""%' + i_search_term_str + '%""'&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" "" + t_direction_sg&#xa;            + "" LIMIT "" + str(bwb.bwbglobal.diary_entries_per_page_int)&#xa;            + "" OFFSET "" + str(i_page_number_int * bwb.bwbglobal.diary_entries_per_page_int)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_question_and_month(&#xa;            i_question_id_it, i_start_of_month_as_unix_time_it,&#xa;            i_number_of_days_in_month_it, i_reverse_bl=False):&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(i_start_of_month_as_unix_time_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<""&#xa;            + str(i_start_of_month_as_unix_time_it + 24 * 3600 * i_number_of_days_in_month_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_question_id_it)&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        if i_reverse_bl:&#xa;            ret_diary_list.reverse()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_active_day(i_reverse_bl=False):&#xa;        start_of_day_datetime = datetime.datetime(&#xa;            year=bwb.bwbglobal.active_date_qdate.year(),&#xa;            month=bwb.bwbglobal.active_date_qdate.month(),&#xa;            day=bwb.bwbglobal.active_date_qdate.day()&#xa;        )&#xa;        start_of_day_unixtime_it = int(start_of_day_datetime.timestamp())&#xa;&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(start_of_day_unixtime_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<"" + str(start_of_day_unixtime_it + 24 * 3600)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        if i_reverse_bl:&#xa;            ret_diary_list.reverse()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_for_question_and_active_day(i_question_id: int) -> QuestionM:&#xa;        start_of_day_datetime = datetime.datetime(&#xa;            year=bwb.bwbglobal.active_date_qdate.year(),&#xa;            month=bwb.bwbglobal.active_date_qdate.month(),&#xa;            day=bwb.bwbglobal.active_date_qdate.day()&#xa;        )&#xa;        start_of_day_unixtime_it = int(start_of_day_datetime.timestamp())&#xa;&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(start_of_day_unixtime_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<"" + str(start_of_day_unixtime_it + 24 * 3600)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_question_id)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_tags_or_friends(i_special_char_str: str) -> list:&#xa;        ret_tag_tuple_list_list = []&#xa;        # ret_tag_tuple_list_list: [(""#tag1"", [id1, id2, ___]), (""#tag2"", [id1, id3, ___]), ___]&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry&#xa;            + "" LIKE "" + '""%' + i_special_char_str + '%""'&#xa;        )&#xa;        # -http://sqlite.org/lang_expr.html#like&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            diary_entry = DiaryEntryM(*diary_db_te)&#xa;            string_with_hashtag_str = diary_entry.diary_text&#xa;            t_diary_id_int = diary_entry.id&#xa;            regexp_pattern_obj = re.compile(""\\"" + i_special_char_str + r""\w+"")&#xa;            # Please note: we need to escape the caret (""^"") character becase this is a&#xa;            # special character (""literal"")&#xa;            regexp_search_result_list = regexp_pattern_obj.findall(string_with_hashtag_str)&#xa;            # https://docs.python.org/3/library/re.html&#xa;&#xa;            for t_re_tag_str in regexp_search_result_list:&#xa;                # -regexp_search_result_list: [""#tag1"", ""#tag2"", ___]&#xa;                flag_boolean = False&#xa;                for (t_ret_tag_str, t_ret_diary_id_list) in ret_tag_tuple_list_list:&#xa;                    if t_re_tag_str == t_ret_tag_str:&#xa;                        t_ret_diary_id_list.append(t_diary_id_int)&#xa;                        flag_boolean = True&#xa;                        break&#xa;                if flag_boolean:&#xa;                    break&#xa;                else:&#xa;                    ret_tag_tuple_list_list.append((t_re_tag_str, [t_diary_id_int]))&#xa;&#xa;        db_connection.commit()&#xa;&#xa;        # TODO: Removing duplicates&#xa;&#xa;        return ret_tag_tuple_list_list&#xa;&#xa;&#xa;class ReminderM:&#xa;    def __init__(self, i_id_int: int, i_title_str: str, i_reminder_str: str) -> None:&#xa;        self.id_int = i_id_int&#xa;        self.title_str = i_title_str&#xa;        self.reminder_str = i_reminder_str&#xa;&#xa;    @staticmethod&#xa;    def add(i_title_str: str, i_reminder_str: str) -> None:&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.ReminderTable.name + ""(""&#xa;            + DbSchemaM.ReminderTable.Cols.title + "", ""&#xa;            + DbSchemaM.ReminderTable.Cols.reminder&#xa;            + "") VALUES (?, ?)"", (i_title_str, i_reminder_str)&#xa;        )&#xa;&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_int: int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.ReminderTable.name&#xa;            + "" WHERE "" + DbSchemaM.ReminderTable.Cols.id + ""="" + str(i_id_int)&#xa;        )&#xa;        reminder_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return ReminderM(*reminder_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all():&#xa;        ret_reminder_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.ReminderTable.name&#xa;        )&#xa;        reminder_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in reminder_db_te_list:&#xa;            ret_reminder_list.append(ReminderM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_reminder_list&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.ReminderTable.name&#xa;            + "" WHERE "" + DbSchemaM.ReminderTable.Cols.id + ""="" + str(i_id_int)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;&#xa;def export_all():&#xa;    csv_writer = csv.writer(open(""exported.csv"", ""w""))&#xa;    for diary_item in DiaryEntryM.get_all():&#xa;        time_datetime = datetime.date.fromtimestamp(diary_item.date_added_it)&#xa;        date_str = time_datetime.strftime(""%Y-%m-%d"")&#xa;        csv_writer.writerow((date_str, diary_item.diary_text))&#xa;&#xa;&#xa;def backup_db_file():&#xa;    date_sg = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")&#xa;    new_file_name_sg = bwb.bwbglobal.get_database_filename() + ""_"" + date_sg&#xa;    shutil.copyfile(bwb.bwbglobal.get_database_filename(), new_file_name_sg)&#xa;    return&#xa;&#xa;&#xa;def populate_db_with_test_data():&#xa;    delta_day_it = 24 * 60 * 60&#xa;&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.gratitude.value,&#xa;        QuestionSetupEnum.gratitude.name.capitalize(),&#xa;        ""What did I do to water the seeds of joy in myself today? What posivite things came my way today?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.practice.value,&#xa;        QuestionSetupEnum.practice.name.capitalize(),&#xa;        ""What practices did I do today? Sitting meditation ? Walking meditation? Gathas?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.sharing.value,&#xa;        QuestionSetupEnum.sharing.name.capitalize(),&#xa;        ""Did I share my happiness with others? Did I enjoy the happiness of others?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.contribution.value,&#xa;        QuestionSetupEnum.contribution.name.capitalize(),&#xa;        ""How did I contribute to the well-being on others? Did I share my joy with my friends and family?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.study.value,&#xa;        QuestionSetupEnum.study.name.capitalize(),&#xa;        ""What did I read and listen to today and learn? Dharma talks? Lectures? Books? Sutras?"")&#xa;&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dear Buddha, today i was #practicing #sitting meditation before meeting a friend of mine to be able to be more present during our meeting"",&#xa;        bwb.bwbglobal.NO_ACTIVE_QUESTION_INT)&#xa;&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dear Buddha, i'm #grateful for being able to breathe!"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(time.time() - delta_day_it, SQLITE_FALSE,&#xa;        ""Most difficult today was my negative thinking, #practicing with this by changing the peg from negative thoughts to positive thinking"",&#xa;        QuestionSetupEnum.practice.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 7 * delta_day_it, SQLITE_FALSE,&#xa;        ""Grateful for having a place to live, a roof over my head, food to eat, and people to care for"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 7 * delta_day_it, SQLITE_FALSE,&#xa;        ""Grateful for the blue sky and the white clouds"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 3 * delta_day_it, SQLITE_FALSE,&#xa;        ""Dear Buddha, today i read about the four foundations of mindfulness. Some important parts: 1. Body 2. Feelings 3. Mind 4. Objects of mind"",&#xa;        QuestionSetupEnum.study.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 4 * delta_day_it, SQLITE_FALSE,&#xa;        ""Programming and working on the application. Using Python and Qt. Cooperating with @John and @Emma"",&#xa;        QuestionSetupEnum.contribution.value)&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dharma talk from ^Plum-Village"",&#xa;        QuestionSetupEnum.practice.value)&#xa;&#xa;    ReminderM.add(""Inter-being"",&#xa;        ""All things in the universe inter-are, our suffering and happiness inter-is with the suffernig and happiness of others"")&#xa;    ReminderM.add(""No Mud, no lotus"",&#xa;        ""A lotus flower cannot grow on marble!"")&#xa;&#xa;"
1405971|"#&#xa;# Kivy - Cross-platform UI framework&#xa;# https://kivy.org/&#xa;#&#xa;from __future__ import print_function&#xa;&#xa;import sys&#xa;build_examples = False&#xa;if ""--build_examples"" in sys.argv:&#xa;    build_examples = True&#xa;    sys.argv.remove(""--build_examples"")&#xa;&#xa;from copy import deepcopy&#xa;import os&#xa;from os.path import join, dirname, sep, exists, basename, isdir&#xa;from os import walk, environ&#xa;from distutils.version import LooseVersion&#xa;from distutils.sysconfig import get_python_inc&#xa;from collections import OrderedDict&#xa;from time import sleep&#xa;from subprocess import check_output, CalledProcessError&#xa;from datetime import datetime&#xa;&#xa;if environ.get('KIVY_USE_SETUPTOOLS'):&#xa;    from setuptools import setup, Extension&#xa;    print('Using setuptools')&#xa;else:&#xa;    from distutils.core import setup&#xa;    from distutils.extension import Extension&#xa;    print('Using distutils')&#xa;&#xa;&#xa;PY3 = sys.version > '3'&#xa;&#xa;if PY3:  # fix error with py3's LooseVersion comparisons&#xa;    def ver_equal(self, other):&#xa;        return self.version == other&#xa;&#xa;    LooseVersion.__eq__ = ver_equal&#xa;&#xa;&#xa;def get_version(filename='kivy/version.py'):&#xa;    VERSION = kivy.__version__&#xa;    DATE = datetime.utcnow().strftime('%Y%m%d')&#xa;    try:&#xa;        GIT_REVISION = check_output(&#xa;            ['git', 'rev-parse', 'HEAD']&#xa;        ).strip().decode('ascii')&#xa;    except (CalledProcessError, OSError, IOError) as e:&#xa;        # CalledProcessError has no errno&#xa;        errno = getattr(e, 'errno', None)&#xa;        if errno != 2 and 'CalledProcessError' not in repr(e):&#xa;            raise&#xa;        GIT_REVISION = ""Unknown""&#xa;&#xa;    cnt = (&#xa;        ""# THIS FILE IS GENERATED FROM KIVY SETUP.PY\n""&#xa;        ""__version__ = '%(version)s'\n""&#xa;        ""__hash__ = '%(hash)s'\n""&#xa;        ""__date__ = '%(date)s'\n""&#xa;    )&#xa;&#xa;    with open(filename, 'w') as f:&#xa;        f.write(cnt % {&#xa;            'version': VERSION,&#xa;            'hash': GIT_REVISION,&#xa;            'date': DATE&#xa;        })&#xa;    return VERSION&#xa;&#xa;&#xa;MIN_CYTHON_STRING = '0.23'&#xa;MIN_CYTHON_VERSION = LooseVersion(MIN_CYTHON_STRING)&#xa;MAX_CYTHON_STRING = '0.27.3'&#xa;MAX_CYTHON_VERSION = LooseVersion(MAX_CYTHON_STRING)&#xa;CYTHON_UNSUPPORTED = (&#xa;    # ref https://github.com/cython/cython/issues/1968&#xa;    '0.27', '0.27.2'&#xa;)&#xa;&#xa;&#xa;def getoutput(cmd, env=None):&#xa;    import subprocess&#xa;    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,&#xa;                         stderr=subprocess.PIPE, env=env)&#xa;    p.wait()&#xa;    if p.returncode:  # if not returncode == 0&#xa;        print('WARNING: A problem occurred while running {0} (code {1})\n'&#xa;              .format(cmd, p.returncode))&#xa;        stderr_content = p.stderr.read()&#xa;        if stderr_content:&#xa;            print('{0}\n'.format(stderr_content))&#xa;        return """"&#xa;    return p.stdout.read()&#xa;&#xa;&#xa;def pkgconfig(*packages, **kw):&#xa;    flag_map = {'-I': 'include_dirs', '-L': 'library_dirs', '-l': 'libraries'}&#xa;    lenviron = None&#xa;    pconfig = join(sys.prefix, 'libs', 'pkgconfig')&#xa;&#xa;    if isdir(pconfig):&#xa;        lenviron = environ.copy()&#xa;        lenviron['PKG_CONFIG_PATH'] = '{};{}'.format(&#xa;            environ.get('PKG_CONFIG_PATH', ''), pconfig)&#xa;    cmd = 'pkg-config --libs --cflags {}'.format(' '.join(packages))&#xa;    results = getoutput(cmd, lenviron).split()&#xa;    for token in results:&#xa;        ext = token[:2].decode('utf-8')&#xa;        flag = flag_map.get(ext)&#xa;        if not flag:&#xa;            continue&#xa;        kw.setdefault(flag, []).append(token[2:].decode('utf-8'))&#xa;    return kw&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# Determine on which platform we are&#xa;&#xa;platform = sys.platform&#xa;&#xa;# Detect 32/64bit for OSX (http://stackoverflow.com/a/1405971/798575)&#xa;if sys.platform == 'darwin':&#xa;    if sys.maxsize > 2 ** 32:&#xa;        osx_arch = 'x86_64'&#xa;    else:&#xa;        osx_arch = 'i386'&#xa;&#xa;# Detect Python for android project (http://github.com/kivy/python-for-android)&#xa;ndkplatform = environ.get('NDKPLATFORM')&#xa;if ndkplatform is not None and environ.get('LIBLINK'):&#xa;    platform = 'android'&#xa;kivy_ios_root = environ.get('KIVYIOSROOT', None)&#xa;if kivy_ios_root is not None:&#xa;    platform = 'ios'&#xa;if exists('/opt/vc/include/bcm_host.h'):&#xa;    platform = 'rpi'&#xa;if exists('/usr/lib/arm-linux-gnueabihf/libMali.so'):&#xa;    platform = 'mali'&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# Detect options&#xa;#&#xa;c_options = OrderedDict()&#xa;c_options['use_rpi'] = platform == 'rpi'&#xa;c_options['use_mali'] = platform == 'mali'&#xa;c_options['use_egl'] = False&#xa;c_options['use_opengl_es2'] = None&#xa;c_options['use_opengl_mock'] = environ.get('READTHEDOCS', None) == 'True'&#xa;c_options['use_sdl2'] = None&#xa;c_options['use_ios'] = False&#xa;c_options['use_mesagl'] = False&#xa;c_options['use_x11'] = False&#xa;c_options['use_wayland'] = False&#xa;c_options['use_gstreamer'] = None&#xa;c_options['use_avfoundation'] = platform == 'darwin'&#xa;c_options['use_osx_frameworks'] = platform == 'darwin'&#xa;c_options['debug_gl'] = False&#xa;&#xa;# now check if environ is changing the default values&#xa;for key in list(c_options.keys()):&#xa;    ukey = key.upper()&#xa;    if ukey in environ:&#xa;        value = bool(int(environ[ukey]))&#xa;        print('Environ change {0} -> {1}'.format(key, value))&#xa;        c_options[key] = value&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# Cython check&#xa;# on python-for-android and kivy-ios, cython usage is external&#xa;&#xa;cython_unsupported_append = '''&#xa;&#xa;  Please note that the following versions of Cython are not supported&#xa;  at all: {}&#xa;'''.format(', '.join(map(str, CYTHON_UNSUPPORTED)))&#xa;&#xa;cython_min = '''\&#xa;  This version of Cython is not compatible with Kivy. Please upgrade to&#xa;  at least version {0}, preferably the newest supported version {1}.&#xa;&#xa;  If your platform provides a Cython package, make sure you have upgraded&#xa;  to the newest version. If the newest version available is still too low,&#xa;  please remove it and install the newest supported Cython via pip:&#xa;&#xa;    pip install -I Cython=={1}{2}\&#xa;'''.format(MIN_CYTHON_STRING, MAX_CYTHON_STRING,&#xa;           cython_unsupported_append if CYTHON_UNSUPPORTED else '')&#xa;&#xa;cython_max = '''\&#xa;  This version of Cython is untested with Kivy. While this version may&#xa;  work perfectly fine, it is possible that you may experience issues. If&#xa;  you do have issues, please downgrade to a supported version. It is&#xa;  best to use the newest supported version, {1}, but the minimum&#xa;  supported version is {0}.&#xa;&#xa;  If your platform provides a Cython package, check if you can downgrade&#xa;  to a supported version. Otherwise, uninstall the platform package and&#xa;  install Cython via pip:&#xa;&#xa;    pip install -I Cython=={1}{2}\&#xa;'''.format(MIN_CYTHON_STRING, MAX_CYTHON_STRING,&#xa;           cython_unsupported_append if CYTHON_UNSUPPORTED else '')&#xa;&#xa;cython_unsupported = '''\&#xa;  This version of Cython suffers from known bugs and is unsupported.&#xa;  Please install the newest supported version, {1}, if possible, but&#xa;  the minimum supported version is {0}.&#xa;&#xa;  If your platform provides a Cython package, check if you can install&#xa;  a supported version. Otherwise, uninstall the platform package and&#xa;  install Cython via pip:&#xa;&#xa;    pip install -I Cython=={1}{2}\&#xa;'''.format(MIN_CYTHON_STRING, MAX_CYTHON_STRING,&#xa;           cython_unsupported_append)&#xa;&#xa;have_cython = False&#xa;skip_cython = False&#xa;if platform in ('ios', 'android'):&#xa;    print('\nCython check avoided.')&#xa;    skip_cython = True&#xa;else:&#xa;    try:&#xa;        # check for cython&#xa;        from Cython.Distutils import build_ext&#xa;        have_cython = True&#xa;        import Cython&#xa;        cy_version_str = Cython.__version__&#xa;        cy_ver = LooseVersion(cy_version_str)&#xa;        print('\nDetected Cython version {}'.format(cy_version_str))&#xa;        if cy_ver < MIN_CYTHON_VERSION:&#xa;            print(cython_min)&#xa;            raise ImportError('Incompatible Cython Version')&#xa;        if cy_ver in CYTHON_UNSUPPORTED:&#xa;            print(cython_unsupported)&#xa;            raise ImportError('Incompatible Cython Version')&#xa;        if cy_ver > MAX_CYTHON_VERSION:&#xa;            print(cython_max)&#xa;            sleep(1)&#xa;    except ImportError:&#xa;        print(""\nCython is missing, it's required for compiling kivy !\n\n"")&#xa;        raise&#xa;&#xa;if not have_cython:&#xa;    from distutils.command.build_ext import build_ext&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# Setup classes&#xa;&#xa;# the build path where kivy is being compiled&#xa;src_path = build_path = dirname(__file__)&#xa;&#xa;&#xa;class KivyBuildExt(build_ext):&#xa;&#xa;    def finalize_options(self):&#xa;        retval = build_ext.finalize_options(self)&#xa;        global build_path&#xa;        if (self.build_lib is not None and exists(self.build_lib) and&#xa;                not self.inplace):&#xa;            build_path = self.build_lib&#xa;        return retval&#xa;&#xa;    def build_extensions(self):&#xa;        # build files&#xa;        config_h_fn = ('include', 'config.h')&#xa;        config_pxi_fn = ('include', 'config.pxi')&#xa;        config_py_fn = ('setupconfig.py', )&#xa;&#xa;        # generate headers&#xa;        config_h = '// Autogenerated file for Kivy C configuration\n'&#xa;        config_h += '#define __PY3 {0}\n'.format(int(PY3))&#xa;        config_pxi = '# Autogenerated file for Kivy Cython configuration\n'&#xa;        config_pxi += 'DEF PY3 = {0}\n'.format(int(PY3))&#xa;        config_py = '# Autogenerated file for Kivy configuration\n'&#xa;        config_py += 'PY3 = {0}\n'.format(int(PY3))&#xa;        config_py += 'CYTHON_MIN = {0}\nCYTHON_MAX = {1}\n'.format(&#xa;            repr(MIN_CYTHON_STRING), repr(MAX_CYTHON_STRING))&#xa;        config_py += 'CYTHON_BAD = {0}\n'.format(repr(', '.join(map(&#xa;            str, CYTHON_UNSUPPORTED))))&#xa;&#xa;        # generate content&#xa;        print('Build configuration is:')&#xa;        for opt, value in c_options.items():&#xa;            value = int(bool(value))&#xa;            print(' * {0} = {1}'.format(opt, value))&#xa;            opt = opt.upper()&#xa;            config_h += '#define __{0} {1}\n'.format(opt, value)&#xa;            config_pxi += 'DEF {0} = {1}\n'.format(opt, value)&#xa;            config_py += '{0} = {1}\n'.format(opt, value)&#xa;        debug = bool(self.debug)&#xa;        print(' * debug = {0}'.format(debug))&#xa;&#xa;        config_pxi += 'DEF DEBUG = {0}\n'.format(debug)&#xa;        config_py += 'DEBUG = {0}\n'.format(debug)&#xa;        config_pxi += 'DEF PLATFORM = ""{0}""\n'.format(platform)&#xa;        config_py += 'PLATFORM = ""{0}""\n'.format(platform)&#xa;        for fn, content in (&#xa;                (config_h_fn, config_h), (config_pxi_fn, config_pxi),&#xa;                (config_py_fn, config_py)):&#xa;            build_fn = expand(build_path, *fn)&#xa;            if self.update_if_changed(build_fn, content):&#xa;                print('Updated {}'.format(build_fn))&#xa;            src_fn = expand(src_path, *fn)&#xa;            if src_fn != build_fn and self.update_if_changed(src_fn, content):&#xa;                print('Updated {}'.format(src_fn))&#xa;&#xa;        c = self.compiler.compiler_type&#xa;        print('Detected compiler is {}'.format(c))&#xa;        if c != 'msvc':&#xa;            for e in self.extensions:&#xa;                e.extra_link_args += ['-lm']&#xa;&#xa;        build_ext.build_extensions(self)&#xa;&#xa;    def update_if_changed(self, fn, content):&#xa;        need_update = True&#xa;        if exists(fn):&#xa;            with open(fn) as fd:&#xa;                need_update = fd.read() != content&#xa;        if need_update:&#xa;            with open(fn, 'w') as fd:&#xa;                fd.write(content)&#xa;        return need_update&#xa;&#xa;&#xa;def _check_and_fix_sdl2_mixer(f_path):&#xa;    print(""Check if SDL2_mixer smpeg2 have an @executable_path"")&#xa;    rpath_from = (""@executable_path/../Frameworks/SDL2.framework""&#xa;                  ""/Versions/A/SDL2"")&#xa;    rpath_to = ""@rpath/../../../../SDL2.framework/Versions/A/SDL2""&#xa;    smpeg2_path = (""{}/Versions/A/Frameworks/smpeg2.framework""&#xa;                   ""/Versions/A/smpeg2"").format(f_path)&#xa;    output = getoutput((""otool -L '{}'"").format(smpeg2_path)).decode('utf-8')&#xa;    if ""@executable_path"" not in output:&#xa;        return&#xa;&#xa;    print(""WARNING: Your SDL2_mixer version is invalid"")&#xa;    print(""WARNING: The smpeg2 framework embedded in SDL2_mixer contains a"")&#xa;    print(""WARNING: reference to @executable_path that will fail the"")&#xa;    print(""WARNING: execution of your application."")&#xa;    print(""WARNING: We are going to change:"")&#xa;    print(""WARNING: from: {}"".format(rpath_from))&#xa;    print(""WARNING: to: {}"".format(rpath_to))&#xa;    getoutput(""install_name_tool -change {} {} {}"".format(&#xa;        rpath_from, rpath_to, smpeg2_path))&#xa;&#xa;    output = getoutput((""otool -L '{}'"").format(smpeg2_path))&#xa;    if b""@executable_path"" not in output:&#xa;        print(""WARNING: Change successfully applied!"")&#xa;        print(""WARNING: You'll never see this message again."")&#xa;    else:&#xa;        print(""WARNING: Unable to apply the changes, sorry."")&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# extract version (simulate doc generation, kivy will be not imported)&#xa;environ['KIVY_DOC_INCLUDE'] = '1'&#xa;import kivy&#xa;&#xa;# extra build commands go in the cmdclass dict {'command-name': CommandClass}&#xa;# see tools.packaging.{platform}.build.py for custom build commands for&#xa;# portable packages. Also e.g. we use build_ext command from cython if its&#xa;# installed for c extensions.&#xa;from kivy.tools.packaging.factory import FactoryBuild&#xa;cmdclass = {&#xa;    'build_factory': FactoryBuild,&#xa;    'build_ext': KivyBuildExt}&#xa;&#xa;try:&#xa;    # add build rules for portable packages to cmdclass&#xa;    if platform == 'win32':&#xa;        from kivy.tools.packaging.win32.build import WindowsPortableBuild&#xa;        cmdclass['build_portable'] = WindowsPortableBuild&#xa;    elif platform == 'darwin':&#xa;        from kivy.tools.packaging.osx.build import OSXPortableBuild&#xa;        cmdclass['build_portable'] = OSXPortableBuild&#xa;except ImportError:&#xa;    print('User distribution detected, avoid portable command.')&#xa;&#xa;# Detect which opengl version headers to use&#xa;if platform in ('android', 'darwin', 'ios', 'rpi', 'mali'):&#xa;    c_options['use_opengl_es2'] = True&#xa;elif c_options['use_opengl_es2'] is None:&#xa;    c_options['use_opengl_es2'] = \&#xa;        environ.get('KIVY_GRAPHICS', '').lower() == 'gles'&#xa;&#xa;print('Using this graphics system: {}'.format(&#xa;    ['OpenGL', 'OpenGL ES 2'][int(c_options['use_opengl_es2'] or False)]))&#xa;&#xa;# check if we are in a kivy-ios build&#xa;if platform == 'ios':&#xa;    print('Kivy-IOS project environment detect, use it.')&#xa;    print('Kivy-IOS project located at {0}'.format(kivy_ios_root))&#xa;    c_options['use_ios'] = True&#xa;    c_options['use_sdl2'] = True&#xa;&#xa;elif platform == 'darwin':&#xa;    if c_options['use_osx_frameworks']:&#xa;        if osx_arch == ""i386"":&#xa;            print(""Warning: building with frameworks fail on i386"")&#xa;        else:&#xa;            print(""OSX framework used, force to x86_64 only"")&#xa;            environ[""ARCHFLAGS""] = environ.get(""ARCHFLAGS"", ""-arch x86_64"")&#xa;            print(""OSX ARCHFLAGS are: {}"".format(environ[""ARCHFLAGS""]))&#xa;&#xa;# detect gstreamer, only on desktop&#xa;# works if we forced the options or in autodetection&#xa;if platform not in ('ios', 'android') and (c_options['use_gstreamer']&#xa;                                           in (None, True)):&#xa;    gstreamer_valid = False&#xa;    if c_options['use_osx_frameworks'] and platform == 'darwin':&#xa;        # check the existence of frameworks&#xa;        f_path = '/Library/Frameworks/GStreamer.framework'&#xa;        if not exists(f_path):&#xa;            c_options['use_gstreamer'] = False&#xa;            print('GStreamer framework not found, fallback on pkg-config')&#xa;        else:&#xa;            print('GStreamer framework found')&#xa;            gstreamer_valid = True&#xa;            c_options['use_gstreamer'] = True&#xa;            gst_flags = {&#xa;                'extra_link_args': [&#xa;                    '-F/Library/Frameworks',&#xa;                    '-Xlinker', '-rpath',&#xa;                    '-Xlinker', '/Library/Frameworks',&#xa;                    '-Xlinker', '-headerpad',&#xa;                    '-Xlinker', '190',&#xa;                    '-framework', 'GStreamer'],&#xa;                'include_dirs': [join(f_path, 'Headers')]}&#xa;&#xa;    if not gstreamer_valid:&#xa;        # use pkg-config approach instead&#xa;        gst_flags = pkgconfig('gstreamer-1.0')&#xa;        if 'libraries' in gst_flags:&#xa;            print('GStreamer found via pkg-config')&#xa;            c_options['use_gstreamer'] = True&#xa;&#xa;&#xa;# detect SDL2, only on desktop and iOS, or android if explicitly enabled&#xa;# works if we forced the options or in autodetection&#xa;sdl2_flags = {}&#xa;if c_options['use_sdl2'] or (&#xa;        platform not in ('android',) and c_options['use_sdl2'] is None):&#xa;&#xa;    sdl2_valid = False&#xa;    if c_options['use_osx_frameworks'] and platform == 'darwin':&#xa;        # check the existence of frameworks&#xa;        sdl2_valid = True&#xa;        sdl2_flags = {&#xa;            'extra_link_args': [&#xa;                '-F/Library/Frameworks',&#xa;                '-Xlinker', '-rpath',&#xa;                '-Xlinker', '/Library/Frameworks',&#xa;                '-Xlinker', '-headerpad',&#xa;                '-Xlinker', '190'],&#xa;            'include_dirs': [],&#xa;            'extra_compile_args': ['-F/Library/Frameworks']&#xa;        }&#xa;        for name in ('SDL2', 'SDL2_ttf', 'SDL2_image', 'SDL2_mixer'):&#xa;            f_path = '/Library/Frameworks/{}.framework'.format(name)&#xa;            if not exists(f_path):&#xa;                print('Missing framework {}'.format(f_path))&#xa;                sdl2_valid = False&#xa;                continue&#xa;            sdl2_flags['extra_link_args'] += ['-framework', name]&#xa;            sdl2_flags['include_dirs'] += [join(f_path, 'Headers')]&#xa;            print('Found sdl2 frameworks: {}'.format(f_path))&#xa;            if name == 'SDL2_mixer':&#xa;                _check_and_fix_sdl2_mixer(f_path)&#xa;&#xa;        if not sdl2_valid:&#xa;            c_options['use_sdl2'] = False&#xa;            print('SDL2 frameworks not found, fallback on pkg-config')&#xa;        else:&#xa;            c_options['use_sdl2'] = True&#xa;            print('Activate SDL2 compilation')&#xa;&#xa;    if not sdl2_valid and platform != ""ios"":&#xa;        # use pkg-config approach instead&#xa;        sdl2_flags = pkgconfig('sdl2', 'SDL2_ttf', 'SDL2_image', 'SDL2_mixer')&#xa;        if 'libraries' in sdl2_flags:&#xa;            print('SDL2 found via pkg-config')&#xa;            c_options['use_sdl2'] = True&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# declare flags&#xa;&#xa;&#xa;def get_modulename_from_file(filename):&#xa;    filename = filename.replace(sep, '/')&#xa;    pyx = '.'.join(filename.split('.')[:-1])&#xa;    pyxl = pyx.split('/')&#xa;    while pyxl[0] != 'kivy':&#xa;        pyxl.pop(0)&#xa;    if pyxl[1] == 'kivy':&#xa;        pyxl.pop(0)&#xa;    return '.'.join(pyxl)&#xa;&#xa;&#xa;def expand(root, *args):&#xa;    return join(root, 'kivy', *args)&#xa;&#xa;&#xa;class CythonExtension(Extension):&#xa;&#xa;    def __init__(self, *args, **kwargs):&#xa;        Extension.__init__(self, *args, **kwargs)&#xa;        self.cython_directives = {&#xa;            'c_string_encoding': 'utf-8',&#xa;            'profile': 'USE_PROFILE' in environ,&#xa;            'embedsignature': 'USE_EMBEDSIGNATURE' in environ}&#xa;        # XXX with pip, setuptools is imported before distutils, and change&#xa;        # our pyx to c, then, cythonize doesn't happen. So force again our&#xa;        # sources&#xa;        self.sources = args[1]&#xa;&#xa;&#xa;def merge(d1, *args):&#xa;    d1 = deepcopy(d1)&#xa;    for d2 in args:&#xa;        for key, value in d2.items():&#xa;            value = deepcopy(value)&#xa;            if key in d1:&#xa;                d1[key].extend(value)&#xa;            else:&#xa;                d1[key] = value&#xa;    return d1&#xa;&#xa;&#xa;def determine_base_flags():&#xa;    flags = {&#xa;        'libraries': [],&#xa;        'include_dirs': [join(src_path, 'kivy', 'include')],&#xa;        'library_dirs': [],&#xa;        'extra_link_args': [],&#xa;        'extra_compile_args': []}&#xa;    if c_options['use_ios']:&#xa;        sysroot = environ.get('IOSSDKROOT', environ.get('SDKROOT'))&#xa;        if not sysroot:&#xa;            raise Exception('IOSSDKROOT is not set')&#xa;        flags['include_dirs'] += [sysroot]&#xa;        flags['extra_compile_args'] += ['-isysroot', sysroot]&#xa;        flags['extra_link_args'] += ['-isysroot', sysroot]&#xa;    elif platform.startswith('freebsd'):&#xa;        flags['include_dirs'] += [join(&#xa;            environ.get('LOCALBASE', '/usr/local'), 'include')]&#xa;        flags['library_dirs'] += [join(&#xa;            environ.get('LOCALBASE', '/usr/local'), 'lib')]&#xa;    elif platform == 'darwin':&#xa;        v = os.uname()&#xa;        if v[2] >= '13.0.0':&#xa;            # use xcode-select to search on the right Xcode path&#xa;            # XXX use the best SDK available instead of a specific one&#xa;            import platform as _platform&#xa;            xcode_dev = getoutput('xcode-select -p').splitlines()[0]&#xa;            sdk_mac_ver = '.'.join(_platform.mac_ver()[0].split('.')[:2])&#xa;            print('Xcode detected at {}, and using OS X{} sdk'.format(&#xa;                xcode_dev, sdk_mac_ver))&#xa;            sysroot = join(&#xa;                xcode_dev.decode('utf-8'),&#xa;                'Platforms/MacOSX.platform/Developer/SDKs',&#xa;                'MacOSX{}.sdk'.format(sdk_mac_ver),&#xa;                'System/Library/Frameworks')&#xa;        else:&#xa;            sysroot = ('/System/Library/Frameworks/'&#xa;                       'ApplicationServices.framework/Frameworks')&#xa;        flags['extra_compile_args'] += ['-F%s' % sysroot]&#xa;        flags['extra_link_args'] += ['-F%s' % sysroot]&#xa;    elif platform == 'win32':&#xa;        flags['include_dirs'] += [get_python_inc(prefix=sys.prefix)]&#xa;        flags['library_dirs'] += [join(sys.prefix, ""libs"")]&#xa;    return flags&#xa;&#xa;&#xa;def determine_gl_flags():&#xa;    kivy_graphics_include = join(src_path, 'kivy', 'include')&#xa;    flags = {'include_dirs': [kivy_graphics_include], 'libraries': []}&#xa;    base_flags = {'include_dirs': [kivy_graphics_include], 'libraries': []}&#xa;    if c_options['use_opengl_mock']:&#xa;        return flags, base_flags&#xa;    if platform == 'win32':&#xa;        flags['libraries'] = ['opengl32', 'glew32']&#xa;    elif platform == 'ios':&#xa;        flags['libraries'] = ['GLESv2']&#xa;        flags['extra_link_args'] = ['-framework', 'OpenGLES']&#xa;    elif platform == 'darwin':&#xa;        flags['extra_link_args'] = ['-framework', 'OpenGL', '-arch', osx_arch]&#xa;        flags['extra_compile_args'] = ['-arch', osx_arch]&#xa;    elif platform.startswith('freebsd'):&#xa;        flags['libraries'] = ['GL']&#xa;    elif platform.startswith('openbsd'):&#xa;        flags['include_dirs'] = ['/usr/X11R6/include']&#xa;        flags['library_dirs'] = ['/usr/X11R6/lib']&#xa;        flags['libraries'] = ['GL']&#xa;    elif platform == 'android':&#xa;        flags['include_dirs'] = [join(ndkplatform, 'usr', 'include')]&#xa;        flags['library_dirs'] = [join(ndkplatform, 'usr', 'lib')]&#xa;        flags['libraries'] = ['GLESv2']&#xa;    elif platform == 'rpi':&#xa;        flags['include_dirs'] = [&#xa;            '/opt/vc/include',&#xa;            '/opt/vc/include/interface/vcos/pthreads',&#xa;            '/opt/vc/include/interface/vmcs_host/linux']&#xa;        flags['library_dirs'] = ['/opt/vc/lib']&#xa;        brcm_lib_files = (&#xa;            '/opt/vc/lib/libbrcmEGL.so',&#xa;            '/opt/vc/lib/libbrcmGLESv2.so')&#xa;        if all((exists(lib) for lib in brcm_lib_files)):&#xa;            print(&#xa;                'Found brcmEGL and brcmGLES library files'&#xa;                'for rpi platform at /opt/vc/lib/')&#xa;            gl_libs = ['brcmEGL', 'brcmGLESv2']&#xa;        else:&#xa;            print(&#xa;                'Failed to find brcmEGL and brcmGLESv2 library files'&#xa;                'for rpi platform, falling back to EGL and GLESv2.')&#xa;            gl_libs = ['EGL', 'GLESv2']&#xa;        flags['libraries'] = ['bcm_host'] + gl_libs&#xa;    elif platform == 'mali':&#xa;        flags['include_dirs'] = ['/usr/include/']&#xa;        flags['library_dirs'] = ['/usr/lib/arm-linux-gnueabihf']&#xa;        flags['libraries'] = ['GLESv2']&#xa;        c_options['use_x11'] = True&#xa;        c_options['use_egl'] = True&#xa;    else:&#xa;        flags['libraries'] = ['GL']&#xa;    return flags, base_flags&#xa;&#xa;&#xa;def determine_sdl2():&#xa;    flags = {}&#xa;    if not c_options['use_sdl2']:&#xa;        return flags&#xa;&#xa;    sdl2_path = environ.get('KIVY_SDL2_PATH', None)&#xa;&#xa;    if sdl2_flags and not sdl2_path and platform == 'darwin':&#xa;        return sdl2_flags&#xa;&#xa;    # no pkgconfig info, or we want to use a specific sdl2 path, so perform&#xa;    # manual configuration&#xa;    flags['libraries'] = ['SDL2', 'SDL2_ttf', 'SDL2_image', 'SDL2_mixer']&#xa;    split_chr = ';' if platform == 'win32' else ':'&#xa;    sdl2_paths = sdl2_path.split(split_chr) if sdl2_path else []&#xa;&#xa;    if not sdl2_paths:&#xa;        sdl_inc = join(sys.prefix, 'include', 'SDL2')&#xa;        if isdir(sdl_inc):&#xa;            sdl2_paths = [sdl_inc]&#xa;        sdl2_paths.extend(['/usr/local/include/SDL2', '/usr/include/SDL2'])&#xa;&#xa;    flags['include_dirs'] = sdl2_paths&#xa;    flags['extra_link_args'] = []&#xa;    flags['extra_compile_args'] = []&#xa;    flags['library_dirs'] = (&#xa;        sdl2_paths if sdl2_paths else&#xa;        ['/usr/local/lib/'])&#xa;&#xa;    if sdl2_flags:&#xa;        flags = merge(flags, sdl2_flags)&#xa;&#xa;    # ensure headers for all the SDL2 and sub libraries are available&#xa;    libs_to_check = ['SDL', 'SDL_mixer', 'SDL_ttf', 'SDL_image']&#xa;    can_compile = True&#xa;    for lib in libs_to_check:&#xa;        found = False&#xa;        for d in flags['include_dirs']:&#xa;            fn = join(d, '{}.h'.format(lib))&#xa;            if exists(fn):&#xa;                found = True&#xa;                print('SDL2: found {} header at {}'.format(lib, fn))&#xa;                break&#xa;&#xa;        if not found:&#xa;            print('SDL2: missing sub library {}'.format(lib))&#xa;            can_compile = False&#xa;&#xa;    if not can_compile:&#xa;        c_options['use_sdl2'] = False&#xa;        return {}&#xa;&#xa;    return flags&#xa;&#xa;&#xa;base_flags = determine_base_flags()&#xa;gl_flags, gl_flags_base = determine_gl_flags()&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# sources to compile&#xa;# all the dependencies have been found manually with:&#xa;# grep -inr -E '(cimport|include)' kivy/graphics/context_instructions.{pxd,pyx}&#xa;graphics_dependencies = {&#xa;    'buffer.pyx': ['common.pxi'],&#xa;    'context.pxd': ['instructions.pxd', 'texture.pxd', 'vbo.pxd', 'cgl.pxd'],&#xa;    'cgl.pxd': ['common.pxi', 'config.pxi', 'gl_redirect.h'],&#xa;    'compiler.pxd': ['instructions.pxd'],&#xa;    'compiler.pyx': ['context_instructions.pxd'],&#xa;    'cgl.pyx': ['cgl.pxd'],&#xa;    'cgl_mock.pyx': ['cgl.pxd'],&#xa;    'cgl_sdl2.pyx': ['cgl.pxd'],&#xa;    'cgl_gl.pyx': ['cgl.pxd'],&#xa;    'cgl_glew.pyx': ['cgl.pxd'],&#xa;    'context_instructions.pxd': [&#xa;        'transformation.pxd', 'instructions.pxd', 'texture.pxd'],&#xa;    'fbo.pxd': ['cgl.pxd', 'instructions.pxd', 'texture.pxd'],&#xa;    'fbo.pyx': [&#xa;        'config.pxi', 'opcodes.pxi', 'transformation.pxd', 'context.pxd'],&#xa;    'gl_instructions.pyx': [&#xa;        'config.pxi', 'opcodes.pxi', 'cgl.pxd', 'instructions.pxd'],&#xa;    'instructions.pxd': [&#xa;        'vbo.pxd', 'context_instructions.pxd', 'compiler.pxd', 'shader.pxd',&#xa;        'texture.pxd', '../_event.pxd'],&#xa;    'instructions.pyx': [&#xa;        'config.pxi', 'opcodes.pxi', 'cgl.pxd',&#xa;        'context.pxd', 'common.pxi', 'vertex.pxd', 'transformation.pxd'],&#xa;    'opengl.pyx': [&#xa;        'config.pxi', 'common.pxi', 'cgl.pxd', 'gl_redirect.h'],&#xa;    'opengl_utils.pyx': [&#xa;        'opengl_utils_def.pxi', 'cgl.pxd', ],&#xa;    'shader.pxd': ['cgl.pxd', 'transformation.pxd', 'vertex.pxd'],&#xa;    'shader.pyx': [&#xa;        'config.pxi', 'common.pxi', 'cgl.pxd',&#xa;        'vertex.pxd', 'transformation.pxd', 'context.pxd',&#xa;        'gl_debug_logger.pxi'],&#xa;    'stencil_instructions.pxd': ['instructions.pxd'],&#xa;    'stencil_instructions.pyx': [&#xa;        'config.pxi', 'opcodes.pxi', 'cgl.pxd',&#xa;        'gl_debug_logger.pxi'],&#xa;    'scissor_instructions.pyx': [&#xa;        'config.pxi', 'opcodes.pxi', 'cgl.pxd'],&#xa;    'svg.pyx': ['config.pxi', 'common.pxi', 'texture.pxd', 'instructions.pxd',&#xa;                'vertex_instructions.pxd', 'tesselator.pxd'],&#xa;    'texture.pxd': ['cgl.pxd'],&#xa;    'texture.pyx': [&#xa;        'config.pxi', 'common.pxi', 'opengl_utils_def.pxi', 'context.pxd',&#xa;        'cgl.pxd', 'opengl_utils.pxd',&#xa;        'img_tools.pxi', 'gl_debug_logger.pxi'],&#xa;    'vbo.pxd': ['buffer.pxd', 'cgl.pxd', 'vertex.pxd'],&#xa;    'vbo.pyx': [&#xa;        'config.pxi', 'common.pxi', 'context.pxd',&#xa;        'instructions.pxd', 'shader.pxd', 'gl_debug_logger.pxi'],&#xa;    'vertex.pxd': ['cgl.pxd'],&#xa;    'vertex.pyx': ['config.pxi', 'common.pxi'],&#xa;    'vertex_instructions.pyx': [&#xa;        'config.pxi', 'common.pxi', 'vbo.pxd', 'vertex.pxd',&#xa;        'instructions.pxd', 'vertex_instructions.pxd',&#xa;        'cgl.pxd', 'texture.pxd', 'vertex_instructions_line.pxi'],&#xa;    'vertex_instructions_line.pxi': ['stencil_instructions.pxd']}&#xa;&#xa;sources = {&#xa;    '_event.pyx': merge(base_flags, {'depends': ['properties.pxd']}),&#xa;    '_clock.pyx': {},&#xa;    'weakproxy.pyx': {},&#xa;    'properties.pyx': merge(base_flags, {'depends': ['_event.pxd']}),&#xa;    'graphics/buffer.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/context.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/compiler.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/context_instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/fbo.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/gl_instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/opengl.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/opengl_utils.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/shader.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/stencil_instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/scissor_instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/texture.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/transformation.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/vbo.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/vertex.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/vertex_instructions.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/cgl.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/cgl_backend/cgl_mock.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/cgl_backend/cgl_gl.pyx': merge(base_flags, gl_flags),&#xa;    'graphics/cgl_backend/cgl_glew.pyx': merge(base_flags, gl_flags),&#xa;    'graphics/cgl_backend/cgl_sdl2.pyx': merge(base_flags, gl_flags_base),&#xa;    'graphics/cgl_backend/cgl_debug.pyx': merge(base_flags, gl_flags_base),&#xa;    'core/text/text_layout.pyx': base_flags,&#xa;    'core/window/window_info.pyx': base_flags,&#xa;    'graphics/tesselator.pyx': merge(base_flags, {&#xa;        'include_dirs': ['kivy/lib/libtess2/Include'],&#xa;        'c_depends': [&#xa;            'lib/libtess2/Source/bucketalloc.c',&#xa;            'lib/libtess2/Source/dict.c',&#xa;            'lib/libtess2/Source/geom.c',&#xa;            'lib/libtess2/Source/mesh.c',&#xa;            'lib/libtess2/Source/priorityq.c',&#xa;            'lib/libtess2/Source/sweep.c',&#xa;            'lib/libtess2/Source/tess.c'&#xa;        ]&#xa;    }),&#xa;    'graphics/svg.pyx': merge(base_flags, gl_flags_base)&#xa;}&#xa;&#xa;if c_options[""use_sdl2""]:&#xa;    sdl2_flags = determine_sdl2()&#xa;&#xa;if c_options['use_sdl2'] and sdl2_flags:&#xa;    sources['graphics/cgl_backend/cgl_sdl2.pyx'] = merge(&#xa;        sources['graphics/cgl_backend/cgl_sdl2.pyx'], sdl2_flags)&#xa;    sdl2_depends = {'depends': ['lib/sdl2.pxi']}&#xa;    for source_file in ('core/window/_window_sdl2.pyx',&#xa;                        'core/image/_img_sdl2.pyx',&#xa;                        'core/text/_text_sdl2.pyx',&#xa;                        'core/audio/audio_sdl2.pyx',&#xa;                        'core/clipboard/_clipboard_sdl2.pyx'):&#xa;        sources[source_file] = merge(&#xa;            base_flags, sdl2_flags, sdl2_depends)&#xa;&#xa;if platform in ('darwin', 'ios'):&#xa;    # activate ImageIO provider for our core image&#xa;    if platform == 'ios':&#xa;        osx_flags = {'extra_link_args': [&#xa;            '-framework', 'Foundation',&#xa;            '-framework', 'UIKit',&#xa;            '-framework', 'AudioToolbox',&#xa;            '-framework', 'CoreGraphics',&#xa;            '-framework', 'QuartzCore',&#xa;            '-framework', 'ImageIO',&#xa;            '-framework', 'Accelerate']}&#xa;    else:&#xa;        osx_flags = {'extra_link_args': [&#xa;            '-framework', 'ApplicationServices']}&#xa;    sources['core/image/img_imageio.pyx'] = merge(&#xa;        base_flags, osx_flags)&#xa;&#xa;if c_options['use_avfoundation']:&#xa;    import platform as _platform&#xa;    mac_ver = [int(x) for x in _platform.mac_ver()[0].split('.')[:2]]&#xa;    if mac_ver >= [10, 7]:&#xa;        osx_flags = {&#xa;            'extra_link_args': ['-framework', 'AVFoundation'],&#xa;            'extra_compile_args': ['-ObjC++'],&#xa;            'depends': ['core/camera/camera_avfoundation_implem.m']}&#xa;        sources['core/camera/camera_avfoundation.pyx'] = merge(&#xa;            base_flags, osx_flags)&#xa;    else:&#xa;        print('AVFoundation cannot be used, OSX >= 10.7 is required')&#xa;&#xa;if c_options['use_rpi']:&#xa;    sources['lib/vidcore_lite/egl.pyx'] = merge(&#xa;        base_flags, gl_flags)&#xa;    sources['lib/vidcore_lite/bcm.pyx'] = merge(&#xa;        base_flags, gl_flags)&#xa;&#xa;if c_options['use_x11']:&#xa;    libs = ['Xrender', 'X11']&#xa;    if c_options['use_egl']:&#xa;        libs += ['EGL']&#xa;    else:&#xa;        libs += ['GL']&#xa;    sources['core/window/window_x11.pyx'] = merge(&#xa;        base_flags, gl_flags, {&#xa;            # FIXME add an option to depend on them but not compile them&#xa;            # cause keytab is included in core, and core is included in&#xa;            # window_x11&#xa;            #&#xa;            # 'depends': [&#xa;            #    'core/window/window_x11_keytab.c',&#xa;            #    'core/window/window_x11_core.c'],&#xa;            'libraries': libs})&#xa;&#xa;if c_options['use_gstreamer']:&#xa;    sources['lib/gstplayer/_gstplayer.pyx'] = merge(&#xa;        base_flags, gst_flags, {&#xa;            'depends': ['lib/gstplayer/_gstplayer.h']})&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# extension modules&#xa;&#xa;def get_dependencies(name, deps=None):&#xa;    if deps is None:&#xa;        deps = []&#xa;    for dep in graphics_dependencies.get(name, []):&#xa;        if dep not in deps:&#xa;            deps.append(dep)&#xa;            get_dependencies(dep, deps)&#xa;    return deps&#xa;&#xa;&#xa;def resolve_dependencies(fn, depends):&#xa;    fn = basename(fn)&#xa;    deps = []&#xa;    get_dependencies(fn, deps)&#xa;    get_dependencies(fn.replace('.pyx', '.pxd'), deps)&#xa;&#xa;    deps_final = []&#xa;    paths_to_test = ['graphics', 'include']&#xa;    for dep in deps:&#xa;        found = False&#xa;        for path in paths_to_test:&#xa;            filename = expand(src_path, path, dep)&#xa;            if exists(filename):&#xa;                deps_final.append(filename)&#xa;                found = True&#xa;                break&#xa;        if not found:&#xa;            print('ERROR: Dependency for {} not resolved: {}'.format(&#xa;                fn, dep&#xa;            ))&#xa;&#xa;    return deps_final&#xa;&#xa;&#xa;def get_extensions_from_sources(sources):&#xa;    ext_modules = []&#xa;    if environ.get('KIVY_FAKE_BUILDEXT'):&#xa;        print('Fake build_ext asked, will generate only .h/.c')&#xa;        return ext_modules&#xa;    for pyx, flags in sources.items():&#xa;        is_graphics = pyx.startswith('graphics')&#xa;        pyx = expand(src_path, pyx)&#xa;        depends = [expand(src_path, x) for x in flags.pop('depends', [])]&#xa;        c_depends = [expand(src_path, x) for x in flags.pop('c_depends', [])]&#xa;        if not have_cython:&#xa;            pyx = '%s.c' % pyx[:-4]&#xa;        if is_graphics:&#xa;            depends = resolve_dependencies(pyx, depends)&#xa;        f_depends = [x for x in depends if x.rsplit('.', 1)[-1] in (&#xa;            'c', 'cpp', 'm')]&#xa;        module_name = get_modulename_from_file(pyx)&#xa;        flags_clean = {'depends': depends}&#xa;        for key, value in flags.items():&#xa;            if len(value):&#xa;                flags_clean[key] = value&#xa;        ext_modules.append(CythonExtension(&#xa;            module_name, [pyx] + f_depends + c_depends, **flags_clean))&#xa;    return ext_modules&#xa;&#xa;&#xa;ext_modules = get_extensions_from_sources(sources)&#xa;&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# automatically detect data files&#xa;split_examples = int(environ.get('KIVY_SPLIT_EXAMPLES', '0'))&#xa;data_file_prefix = 'share/kivy-'&#xa;examples = {}&#xa;examples_allowed_ext = ('readme', 'py', 'wav', 'png', 'jpg', 'svg', 'json',&#xa;                        'avi', 'gif', 'txt', 'ttf', 'obj', 'mtl', 'kv', 'mpg',&#xa;                        'glsl', 'zip')&#xa;for root, subFolders, files in walk('examples'):&#xa;    for fn in files:&#xa;        ext = fn.split('.')[-1].lower()&#xa;        if ext not in examples_allowed_ext:&#xa;            continue&#xa;        filename = join(root, fn)&#xa;        directory = '%s%s' % (data_file_prefix, dirname(filename))&#xa;        if directory not in examples:&#xa;            examples[directory] = []&#xa;        examples[directory].append(filename)&#xa;&#xa;binary_deps = []&#xa;binary_deps_path = join(src_path, 'kivy', 'binary_deps')&#xa;if isdir(binary_deps_path):&#xa;    for root, dirnames, filenames in walk(binary_deps_path):&#xa;        for fname in filenames:&#xa;            binary_deps.append(&#xa;                join(root.replace(binary_deps_path, 'binary_deps'), fname))&#xa;&#xa;# -----------------------------------------------------------------------------&#xa;# setup !&#xa;if not build_examples:&#xa;    setup(&#xa;        name='Kivy',&#xa;        version=get_version(),&#xa;        author='Kivy Team and other contributors',&#xa;        author_email='kivy-dev@googlegroups.com',&#xa;        url='http://kivy.org',&#xa;        license='MIT',&#xa;        description=(&#xa;            'A software library for rapid development of '&#xa;            'hardware-accelerated multitouch applications.'),&#xa;        ext_modules=ext_modules,&#xa;        cmdclass=cmdclass,&#xa;        packages=[&#xa;            'kivy',&#xa;            'kivy.adapters',&#xa;            'kivy.core',&#xa;            'kivy.core.audio',&#xa;            'kivy.core.camera',&#xa;            'kivy.core.clipboard',&#xa;            'kivy.core.image',&#xa;            'kivy.core.gl',&#xa;            'kivy.core.spelling',&#xa;            'kivy.core.text',&#xa;            'kivy.core.video',&#xa;            'kivy.core.window',&#xa;            'kivy.deps',&#xa;            'kivy.effects',&#xa;            'kivy.graphics',&#xa;            'kivy.graphics.cgl_backend',&#xa;            'kivy.garden',&#xa;            'kivy.input',&#xa;            'kivy.input.postproc',&#xa;            'kivy.input.providers',&#xa;            'kivy.lang',&#xa;            'kivy.lib',&#xa;            'kivy.lib.osc',&#xa;            'kivy.lib.gstplayer',&#xa;            'kivy.lib.vidcore_lite',&#xa;            'kivy.modules',&#xa;            'kivy.network',&#xa;            'kivy.storage',&#xa;            'kivy.tests',&#xa;            'kivy.tools',&#xa;            'kivy.tools.packaging',&#xa;            'kivy.tools.packaging.pyinstaller_hooks',&#xa;            'kivy.tools.highlight',&#xa;            'kivy.extras',&#xa;            'kivy.uix',&#xa;            'kivy.uix.behaviors',&#xa;            'kivy.uix.recycleview',&#xa;        ],&#xa;        package_dir={'kivy': 'kivy'},&#xa;        package_data={'kivy': [&#xa;            'setupconfig.py'&#xa;            '*.pxd',&#xa;            '*.pxi',&#xa;            'core/text/*.pxd',&#xa;            'core/text/*.pxi',&#xa;            'core/window/*.pxi',&#xa;            'core/window/*.pxd',&#xa;            'graphics/*.pxd',&#xa;            'graphics/*.pxi',&#xa;            'graphics/*.h',&#xa;            'include/*',&#xa;            'lib/vidcore_lite/*.pxd',&#xa;            'lib/vidcore_lite/*.pxi',&#xa;            'data/*.kv',&#xa;            'data/*.json',&#xa;            'data/fonts/*.ttf',&#xa;            'data/images/*.png',&#xa;            'data/images/*.jpg',&#xa;            'data/images/*.gif',&#xa;            'data/images/*.atlas',&#xa;            'data/keyboards/*.json',&#xa;            'data/logo/*.png',&#xa;            'data/glsl/*.png',&#xa;            'data/glsl/*.vs',&#xa;            'data/glsl/*.fs',&#xa;            'tests/*.zip',&#xa;            'tests/*.kv',&#xa;            'tests/*.png',&#xa;            'tests/*.ttf',&#xa;            'tests/*.ogg',&#xa;            'tools/gles_compat/*',&#xa;            'tools/highlight/*',&#xa;            'tools/packaging/README.txt',&#xa;            'tools/packaging/win32/kivy.bat',&#xa;            'tools/packaging/win32/kivyenv.sh',&#xa;            'tools/packaging/win32/README.txt',&#xa;            'tools/packaging/osx/Info.plist',&#xa;            'tools/packaging/osx/InfoPlist.strings',&#xa;            'tools/packaging/osx/kivy.sh',&#xa;            'tools/pep8checker/*',&#xa;            'tools/theming/defaulttheme/*',&#xa;        ] + binary_deps},&#xa;        data_files=[] if split_examples else list(examples.items()),&#xa;        classifiers=[&#xa;            'Development Status :: 5 - Production/Stable',&#xa;            'Environment :: MacOS X',&#xa;            'Environment :: Win32 (MS Windows)',&#xa;            'Environment :: X11 Applications',&#xa;            'Intended Audience :: Developers',&#xa;            'Intended Audience :: End Users/Desktop',&#xa;            'Intended Audience :: Information Technology',&#xa;            'Intended Audience :: Science/Research',&#xa;            'License :: OSI Approved :: MIT License',&#xa;            'Natural Language :: English',&#xa;            'Operating System :: MacOS :: MacOS X',&#xa;            'Operating System :: Microsoft :: Windows',&#xa;            'Operating System :: POSIX :: BSD :: FreeBSD',&#xa;            'Operating System :: POSIX :: Linux',&#xa;            'Programming Language :: Python :: 2.7',&#xa;            'Programming Language :: Python :: 3.3',&#xa;            'Programming Language :: Python :: 3.4',&#xa;            'Programming Language :: Python :: 3.5',&#xa;            'Programming Language :: Python :: 3.6',&#xa;            'Topic :: Artistic Software',&#xa;            'Topic :: Games/Entertainment',&#xa;            'Topic :: Multimedia :: Graphics :: 3D Rendering',&#xa;            'Topic :: Multimedia :: Graphics :: Capture :: Digital Camera',&#xa;            'Topic :: Multimedia :: Graphics :: Presentation',&#xa;            'Topic :: Multimedia :: Graphics :: Viewers',&#xa;            'Topic :: Multimedia :: Sound/Audio :: Players :: MP3',&#xa;            'Topic :: Multimedia :: Video :: Display',&#xa;            'Topic :: Scientific/Engineering :: Human Machine Interfaces',&#xa;            'Topic :: Scientific/Engineering :: Visualization',&#xa;            ('Topic :: Software Development :: Libraries :: '&#xa;             'Application Frameworks'),&#xa;            'Topic :: Software Development :: User Interfaces'],&#xa;        dependency_links=[&#xa;            'https://github.com/kivy-garden/garden/archive/master.zip'],&#xa;        install_requires=['Kivy-Garden>=0.1.4', 'docutils', 'pygments'],&#xa;        setup_requires=[&#xa;            'cython>=' + MIN_CYTHON_STRING&#xa;        ] if not skip_cython else [])&#xa;else:&#xa;    setup(&#xa;        name='Kivy-examples',&#xa;        version=get_version(),&#xa;        author='Kivy Team and other contributors',&#xa;        author_email='kivy-dev@googlegroups.com',&#xa;        url='http://kivy.org',&#xa;        license='MIT',&#xa;        description=('Kivy examples.'),&#xa;        data_files=list(examples.items()))&#xa;"
5658622|"import os&#xa;import re&#xa;import sys&#xa;from distutils import log&#xa;import xml.dom.pulldom&#xa;import shlex&#xa;import locale&#xa;import codecs&#xa;import unicodedata&#xa;import warnings&#xa;from setuptools.compat import unicode&#xa;from xml.sax.saxutils import unescape&#xa;&#xa;try:&#xa;    import urlparse&#xa;except ImportError:&#xa;    import urllib.parse as urlparse&#xa;&#xa;from subprocess import Popen as _Popen, PIPE as _PIPE&#xa;&#xa;#NOTE: Use of the command line options require SVN 1.3 or newer (December 2005)&#xa;#      and SVN 1.3 hasn't been supported by the developers since mid 2008.&#xa;&#xa;#subprocess is called several times with shell=(sys.platform=='win32')&#xa;#see the follow for more information:&#xa;#       http://bugs.python.org/issue8557&#xa;#       http://stackoverflow.com/questions/5658622/&#xa;#              python-subprocess-popen-environment-path&#xa;&#xa;&#xa;def _run_command(args, stdout=_PIPE, stderr=_PIPE, encoding=None, stream=0):&#xa;    #regarding the shell argument, see: http://bugs.python.org/issue8557&#xa;    try:&#xa;        proc = _Popen(args, stdout=stdout, stderr=stderr,&#xa;                      shell=(sys.platform == 'win32'))&#xa;&#xa;        data = proc.communicate()[stream]&#xa;    except OSError:&#xa;        return 1, ''&#xa;&#xa;    #doubled checked and&#xa;    data = decode_as_string(data, encoding)&#xa;&#xa;    #communciate calls wait()&#xa;    return proc.returncode, data&#xa;&#xa;&#xa;def _get_entry_schedule(entry):&#xa;    schedule = entry.getElementsByTagName('schedule')[0]&#xa;    return """".join([t.nodeValue&#xa;                    for t in schedule.childNodes&#xa;                    if t.nodeType == t.TEXT_NODE])&#xa;&#xa;&#xa;def _get_target_property(target):&#xa;    property_text = target.getElementsByTagName('property')[0]&#xa;    return """".join([t.nodeValue&#xa;                    for t in property_text.childNodes&#xa;                    if t.nodeType == t.TEXT_NODE])&#xa;&#xa;&#xa;def _get_xml_data(decoded_str):&#xa;    if sys.version_info < (3, 0):&#xa;        #old versions want an encoded string&#xa;        data = decoded_str.encode('utf-8')&#xa;    else:&#xa;        data = decoded_str&#xa;    return data&#xa;&#xa;&#xa;def joinpath(prefix, *suffix):&#xa;    if not prefix or prefix == '.':&#xa;        return os.path.join(*suffix)&#xa;    return os.path.join(prefix, *suffix)&#xa;&#xa;def determine_console_encoding():&#xa;    try:&#xa;        #try for the preferred encoding&#xa;        encoding = locale.getpreferredencoding()&#xa;&#xa;        #see if the locale.getdefaultlocale returns null&#xa;        #some versions of python\platforms return US-ASCII&#xa;        #when it cannot determine an encoding&#xa;        if not encoding or encoding == ""US-ASCII"":&#xa;            encoding = locale.getdefaultlocale()[1]&#xa;&#xa;        if encoding:&#xa;            codecs.lookup(encoding)  # make sure a lookup error is not made&#xa;&#xa;    except (locale.Error, LookupError):&#xa;        encoding = None&#xa;&#xa;    is_osx = sys.platform == ""darwin""&#xa;    if not encoding:&#xa;        return [""US-ASCII"", ""utf-8""][is_osx]&#xa;    elif encoding.startswith(""mac-"") and is_osx:&#xa;        #certain versions of python would return mac-roman as default&#xa;        #OSX as a left over of earlier mac versions.&#xa;        return ""utf-8""&#xa;    else:&#xa;        return encoding&#xa;&#xa;_console_encoding = determine_console_encoding()&#xa;&#xa;def decode_as_string(text, encoding=None):&#xa;    """"""&#xa;    Decode the console or file output explicitly using getpreferredencoding.&#xa;    The text paraemeter should be a encoded string, if not no decode occurs&#xa;    If no encoding is given, getpreferredencoding is used.  If encoding is&#xa;    specified, that is used instead.  This would be needed for SVN --xml&#xa;    output.  Unicode is explicitly put in composed NFC form.&#xa;&#xa;    --xml should be UTF-8 (SVN Issue 2938) the discussion on the Subversion&#xa;    DEV List from 2007 seems to indicate the same.&#xa;    """"""&#xa;    #text should be a byte string&#xa;&#xa;    if encoding is None:&#xa;        encoding = _console_encoding&#xa;&#xa;    if not isinstance(text, unicode):&#xa;        text = text.decode(encoding)&#xa;&#xa;    text = unicodedata.normalize('NFC', text)&#xa;&#xa;    return text&#xa;&#xa;&#xa;def parse_dir_entries(decoded_str):&#xa;    '''Parse the entries from a recursive info xml'''&#xa;    doc = xml.dom.pulldom.parseString(_get_xml_data(decoded_str))&#xa;    entries = list()&#xa;&#xa;    for event, node in doc:&#xa;        if event == 'START_ELEMENT' and node.nodeName == 'entry':&#xa;            doc.expandNode(node)&#xa;            if not _get_entry_schedule(node).startswith('delete'):&#xa;                entries.append((node.getAttribute('path'),&#xa;                                node.getAttribute('kind')))&#xa;&#xa;    return entries[1:]  # do not want the root directory&#xa;&#xa;&#xa;def parse_externals_xml(decoded_str, prefix=''):&#xa;    '''Parse a propget svn:externals xml'''&#xa;    prefix = os.path.normpath(prefix)&#xa;    prefix = os.path.normcase(prefix)&#xa;&#xa;    doc = xml.dom.pulldom.parseString(_get_xml_data(decoded_str))&#xa;    externals = list()&#xa;&#xa;    for event, node in doc:&#xa;        if event == 'START_ELEMENT' and node.nodeName == 'target':&#xa;            doc.expandNode(node)&#xa;            path = os.path.normpath(node.getAttribute('path'))&#xa;&#xa;            if os.path.normcase(path).startswith(prefix):&#xa;                path = path[len(prefix)+1:]&#xa;&#xa;            data = _get_target_property(node)&#xa;            #data should be decoded already&#xa;            for external in parse_external_prop(data):&#xa;                externals.append(joinpath(path, external))&#xa;&#xa;    return externals  # do not want the root directory&#xa;&#xa;&#xa;def parse_external_prop(lines):&#xa;    """"""&#xa;    Parse the value of a retrieved svn:externals entry.&#xa;&#xa;    possible token setups (with quotng and backscaping in laters versions)&#xa;        URL[@#] EXT_FOLDERNAME&#xa;        [-r#] URL EXT_FOLDERNAME&#xa;        EXT_FOLDERNAME [-r#] URL&#xa;    """"""&#xa;    externals = []&#xa;    for line in lines.splitlines():&#xa;        line = line.lstrip()  # there might be a ""\ ""&#xa;        if not line:&#xa;            continue&#xa;&#xa;        if sys.version_info < (3, 0):&#xa;            #shlex handles NULLs just fine and shlex in 2.7 tries to encode&#xa;            #as ascii automatiically&#xa;            line = line.encode('utf-8')&#xa;        line = shlex.split(line)&#xa;        if sys.version_info < (3, 0):&#xa;            line = [x.decode('utf-8') for x in line]&#xa;&#xa;        #EXT_FOLDERNAME is either the first or last depending on where&#xa;        #the URL falls&#xa;        if urlparse.urlsplit(line[-1])[0]:&#xa;            external = line[0]&#xa;        else:&#xa;            external = line[-1]&#xa;&#xa;        external = decode_as_string(external, encoding=""utf-8"")&#xa;        externals.append(os.path.normpath(external))&#xa;&#xa;    return externals&#xa;&#xa;&#xa;def parse_prop_file(filename, key):&#xa;    found = False&#xa;    f = open(filename, 'rt')&#xa;    data = ''&#xa;    try:&#xa;        for line in iter(f.readline, ''):    # can't use direct iter!&#xa;            parts = line.split()&#xa;            if len(parts) == 2:&#xa;                kind, length = parts&#xa;                data = f.read(int(length))&#xa;                if kind == 'K' and data == key:&#xa;                    found = True&#xa;                elif kind == 'V' and found:&#xa;                    break&#xa;    finally:&#xa;        f.close()&#xa;&#xa;    return data&#xa;&#xa;&#xa;class SvnInfo(object):&#xa;    '''&#xa;    Generic svn_info object.  No has little knowledge of how to extract&#xa;    information.  Use cls.load to instatiate according svn version.&#xa;&#xa;    Paths are not filesystem encoded.&#xa;    '''&#xa;&#xa;    @staticmethod&#xa;    def get_svn_version():&#xa;        code, data = _run_command(['svn', '--version', '--quiet'])&#xa;        if code == 0 and data:&#xa;            return data.strip()&#xa;        else:&#xa;            return ''&#xa;&#xa;    #svnversion return values (previous implementations return max revision)&#xa;    #   4123:4168     mixed revision working copy&#xa;    #   4168M         modified working copy&#xa;    #   4123S         switched working copy&#xa;    #   4123:4168MS   mixed revision, modified, switched working copy&#xa;    revision_re = re.compile(r'(?:([\-0-9]+):)?(\d+)([a-z]*)\s*$', re.I)&#xa;&#xa;    @classmethod&#xa;    def load(cls, dirname=''):&#xa;        normdir = os.path.normpath(dirname)&#xa;        code, data = _run_command(['svn', 'info', normdir])&#xa;        # Must check for some contents, as some use empty directories&#xa;        # in testcases&#xa;        svn_dir = os.path.join(normdir, '.svn')&#xa;        has_svn = (os.path.isfile(os.path.join(svn_dir, 'entries')) or&#xa;                   os.path.isfile(os.path.join(svn_dir, 'dir-props')) or&#xa;                   os.path.isfile(os.path.join(svn_dir, 'dir-prop-base')))&#xa;&#xa;        svn_version = tuple(cls.get_svn_version().split('.'))&#xa;&#xa;        try:&#xa;            base_svn_version = tuple(int(x) for x in svn_version[:2])&#xa;        except ValueError:&#xa;            base_svn_version = tuple()&#xa;&#xa;        if not has_svn:&#xa;            return SvnInfo(dirname)&#xa;&#xa;        if code or not base_svn_version or base_svn_version < (1, 3):&#xa;            warnings.warn((""No SVN 1.3+ command found: falling back ""&#xa;                           ""on pre 1.7 .svn parsing""), DeprecationWarning)&#xa;            return SvnFileInfo(dirname)&#xa;&#xa;        if base_svn_version < (1, 5):&#xa;            return Svn13Info(dirname)&#xa;&#xa;        return Svn15Info(dirname)&#xa;&#xa;    def __init__(self, path=''):&#xa;        self.path = path&#xa;        self._entries = None&#xa;        self._externals = None&#xa;&#xa;    def get_revision(self):&#xa;        'Retrieve the directory revision informatino using svnversion'&#xa;        code, data = _run_command(['svnversion', '-c', self.path])&#xa;        if code:&#xa;            log.warn(""svnversion failed"")&#xa;            return 0&#xa;&#xa;        parsed = self.revision_re.match(data)&#xa;        if parsed:&#xa;            return int(parsed.group(2))&#xa;        else:&#xa;            return 0&#xa;&#xa;    @property&#xa;    def entries(self):&#xa;        if self._entries is None:&#xa;            self._entries = self.get_entries()&#xa;        return self._entries&#xa;&#xa;    @property&#xa;    def externals(self):&#xa;        if self._externals is None:&#xa;            self._externals = self.get_externals()&#xa;        return self._externals&#xa;&#xa;    def iter_externals(self):&#xa;        '''&#xa;        Iterate over the svn:external references in the repository path.&#xa;        '''&#xa;        for item in self.externals:&#xa;            yield item&#xa;&#xa;    def iter_files(self):&#xa;        '''&#xa;        Iterate over the non-deleted file entries in the repository path&#xa;        '''&#xa;        for item, kind in self.entries:&#xa;            if kind.lower() == 'file':&#xa;                yield item&#xa;&#xa;    def iter_dirs(self, include_root=True):&#xa;        '''&#xa;        Iterate over the non-deleted file entries in the repository path&#xa;        '''&#xa;        if include_root:&#xa;            yield self.path&#xa;        for item, kind in self.entries:&#xa;            if kind.lower() == 'dir':&#xa;                yield item&#xa;&#xa;    def get_entries(self):&#xa;        return []&#xa;&#xa;    def get_externals(self):&#xa;        return []&#xa;&#xa;&#xa;class Svn13Info(SvnInfo):&#xa;    def get_entries(self):&#xa;        code, data = _run_command(['svn', 'info', '-R', '--xml', self.path],&#xa;                                  encoding=""utf-8"")&#xa;&#xa;        if code:&#xa;            log.debug(""svn info failed"")&#xa;            return []&#xa;&#xa;        return parse_dir_entries(data)&#xa;&#xa;    def get_externals(self):&#xa;        #Previous to 1.5 --xml was not supported for svn propget and the -R&#xa;        #output format breaks the shlex compatible semantics.&#xa;        cmd = ['svn', 'propget', 'svn:externals']&#xa;        result = []&#xa;        for folder in self.iter_dirs():&#xa;            code, lines = _run_command(cmd + [folder], encoding=""utf-8"")&#xa;            if code != 0:&#xa;                log.warn(""svn propget failed"")&#xa;                return []&#xa;            #lines should a str&#xa;            for external in parse_external_prop(lines):&#xa;                if folder:&#xa;                    external = os.path.join(folder, external)&#xa;                result.append(os.path.normpath(external))&#xa;&#xa;        return result&#xa;&#xa;&#xa;class Svn15Info(Svn13Info):&#xa;    def get_externals(self):&#xa;        cmd = ['svn', 'propget', 'svn:externals', self.path, '-R', '--xml']&#xa;        code, lines = _run_command(cmd, encoding=""utf-8"")&#xa;        if code:&#xa;            log.debug(""svn propget failed"")&#xa;            return []&#xa;        return parse_externals_xml(lines, prefix=os.path.abspath(self.path))&#xa;&#xa;&#xa;class SvnFileInfo(SvnInfo):&#xa;&#xa;    def __init__(self, path=''):&#xa;        super(SvnFileInfo, self).__init__(path)&#xa;        self._directories = None&#xa;        self._revision = None&#xa;&#xa;    def _walk_svn(self, base):&#xa;        entry_file = joinpath(base, '.svn', 'entries')&#xa;        if os.path.isfile(entry_file):&#xa;            entries = SVNEntriesFile.load(base)&#xa;            yield (base, False, entries.parse_revision())&#xa;            for path in entries.get_undeleted_records():&#xa;                path = decode_as_string(path)&#xa;                path = joinpath(base, path)&#xa;                if os.path.isfile(path):&#xa;                    yield (path, True, None)&#xa;                elif os.path.isdir(path):&#xa;                    for item in self._walk_svn(path):&#xa;                        yield item&#xa;&#xa;    def _build_entries(self):&#xa;        entries = list()&#xa;&#xa;        rev = 0&#xa;        for path, isfile, dir_rev in self._walk_svn(self.path):&#xa;            if isfile:&#xa;                entries.append((path, 'file'))&#xa;            else:&#xa;                entries.append((path, 'dir'))&#xa;                rev = max(rev, dir_rev)&#xa;&#xa;        self._entries = entries&#xa;        self._revision = rev&#xa;&#xa;    def get_entries(self):&#xa;        if self._entries is None:&#xa;            self._build_entries()&#xa;        return self._entries&#xa;&#xa;    def get_revision(self):&#xa;        if self._revision is None:&#xa;            self._build_entries()&#xa;        return self._revision&#xa;&#xa;    def get_externals(self):&#xa;        prop_files = [['.svn', 'dir-prop-base'],&#xa;                      ['.svn', 'dir-props']]&#xa;        externals = []&#xa;&#xa;        for dirname in self.iter_dirs():&#xa;            prop_file = None&#xa;            for rel_parts in prop_files:&#xa;                filename = joinpath(dirname, *rel_parts)&#xa;                if os.path.isfile(filename):&#xa;                    prop_file = filename&#xa;&#xa;            if prop_file is not None:&#xa;                ext_prop = parse_prop_file(prop_file, 'svn:externals')&#xa;                #ext_prop should be utf-8 coming from svn:externals&#xa;                ext_prop = decode_as_string(ext_prop, encoding=""utf-8"")&#xa;                externals.extend(parse_external_prop(ext_prop))&#xa;&#xa;        return externals&#xa;&#xa;&#xa;def svn_finder(dirname=''):&#xa;    #combined externals due to common interface&#xa;    #combined externals and entries due to lack of dir_props in 1.7&#xa;    info = SvnInfo.load(dirname)&#xa;    for path in info.iter_files():&#xa;        yield path&#xa;&#xa;    for path in info.iter_externals():&#xa;        sub_info = SvnInfo.load(path)&#xa;        for sub_path in sub_info.iter_files():&#xa;            yield sub_path&#xa;&#xa;&#xa;class SVNEntriesFile(object):&#xa;    def __init__(self, data):&#xa;        self.data = data&#xa;&#xa;    @classmethod&#xa;    def load(class_, base):&#xa;        filename = os.path.join(base, '.svn', 'entries')&#xa;        f = open(filename)&#xa;        try:&#xa;            result = SVNEntriesFile.read(f)&#xa;        finally:&#xa;            f.close()&#xa;        return result&#xa;&#xa;    @classmethod&#xa;    def read(class_, fileobj):&#xa;        data = fileobj.read()&#xa;        is_xml = data.startswith('<?xml')&#xa;        class_ = [SVNEntriesFileText, SVNEntriesFileXML][is_xml]&#xa;        return class_(data)&#xa;&#xa;    def parse_revision(self):&#xa;        all_revs = self.parse_revision_numbers() + [0]&#xa;        return max(all_revs)&#xa;&#xa;&#xa;class SVNEntriesFileText(SVNEntriesFile):&#xa;    known_svn_versions = {&#xa;        '1.4.x': 8,&#xa;        '1.5.x': 9,&#xa;        '1.6.x': 10,&#xa;    }&#xa;&#xa;    def __get_cached_sections(self):&#xa;        return self.sections&#xa;&#xa;    def get_sections(self):&#xa;        SECTION_DIVIDER = '\f\n'&#xa;        sections = self.data.split(SECTION_DIVIDER)&#xa;        sections = [x for x in map(str.splitlines, sections)]&#xa;        try:&#xa;            # remove the SVN version number from the first line&#xa;            svn_version = int(sections[0].pop(0))&#xa;            if not svn_version in self.known_svn_versions.values():&#xa;                log.warn(""Unknown subversion verson %d"", svn_version)&#xa;        except ValueError:&#xa;            return&#xa;        self.sections = sections&#xa;        self.get_sections = self.__get_cached_sections&#xa;        return self.sections&#xa;&#xa;    def is_valid(self):&#xa;        return bool(self.get_sections())&#xa;&#xa;    def get_url(self):&#xa;        return self.get_sections()[0][4]&#xa;&#xa;    def parse_revision_numbers(self):&#xa;        revision_line_number = 9&#xa;        rev_numbers = [&#xa;            int(section[revision_line_number])&#xa;            for section in self.get_sections()&#xa;            if (len(section) > revision_line_number&#xa;                and section[revision_line_number])&#xa;        ]&#xa;        return rev_numbers&#xa;&#xa;    def get_undeleted_records(self):&#xa;        undeleted = lambda s: s and s[0] and (len(s) < 6 or s[5] != 'delete')&#xa;        result = [&#xa;            section[0]&#xa;            for section in self.get_sections()&#xa;            if undeleted(section)&#xa;        ]&#xa;        return result&#xa;&#xa;&#xa;class SVNEntriesFileXML(SVNEntriesFile):&#xa;    def is_valid(self):&#xa;        return True&#xa;&#xa;    def get_url(self):&#xa;        ""Get repository URL""&#xa;        urlre = re.compile('url=""([^""]+)""')&#xa;        return urlre.search(self.data).group(1)&#xa;&#xa;    def parse_revision_numbers(self):&#xa;        revre = re.compile(r'committed-rev=""(\d+)""')&#xa;        return [&#xa;            int(m.group(1))&#xa;            for m in revre.finditer(self.data)&#xa;        ]&#xa;&#xa;    def get_undeleted_records(self):&#xa;        entries_pattern = \&#xa;            re.compile(r'name=""([^""]+)""(?![^>]+deleted=""true"")', re.I)&#xa;        results = [&#xa;            unescape(match.group(1))&#xa;            for match in entries_pattern.finditer(self.data)&#xa;        ]&#xa;        return results&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    for name in svn_finder(sys.argv[1]):&#xa;        print(name)&#xa;"
31689645|"# ----------------------------------------------------------------------&#xa;# Numenta Platform for Intelligent Computing (NuPIC)&#xa;# Copyright (C) 2017, Numenta, Inc.  Unless you have an agreement&#xa;# with Numenta, Inc., for a separate license for this software code, the&#xa;# following terms and conditions apply:&#xa;#&#xa;# This program is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU Affero Public License version 3 as&#xa;# published by the Free Software Foundation.&#xa;#&#xa;# This program is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.&#xa;# See the GNU Affero Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU Affero Public License&#xa;# along with this program.  If not, see http://www.gnu.org/licenses.&#xa;#&#xa;# http://numenta.org/licenses/&#xa;# ----------------------------------------------------------------------&#xa;import itertools&#xa;&#xa;import plotly.offline as py&#xa;import plotly.graph_objs as go&#xa;&#xa;import matplotlib.pyplot as plt&#xa;import numpy as np&#xa;&#xa;&#xa;&#xa;def show_values(pc, fmt=""%.2f"", **kw):&#xa;  """"""&#xa;  Heatmap with text in each cell with matplotlib's pyplot&#xa;  Source: http://stackoverflow.com/a/25074150/395857 &#xa;  By HYRY&#xa;  """"""&#xa;  from itertools import izip&#xa;  pc.update_scalarmappable()&#xa;  ax = pc.axes&#xa;  for p, color, value in izip(pc.get_paths(), pc.get_facecolors(),&#xa;                              pc.get_array()):&#xa;    x, y = p.vertices[:-2, :].mean(0)&#xa;    if np.all(color[:3] > 0.5):&#xa;      color = (0.0, 0.0, 0.0)&#xa;    else:&#xa;      color = (1.0, 1.0, 1.0)&#xa;    ax.text(x, y, fmt % value, ha=""center"", va=""center"", color=color, **kw)&#xa;&#xa;&#xa;&#xa;def cm2inch(*tupl):&#xa;  """"""&#xa;  Specify figure size in centimeter in matplotlib&#xa;  Source: http://stackoverflow.com/a/22787457/395857&#xa;  By gns-ank&#xa;  """"""&#xa;  inch = 2.54&#xa;  if type(tupl[0]) == tuple:&#xa;    return tuple(i / inch for i in tupl[0])&#xa;  else:&#xa;    return tuple(i / inch for i in tupl)&#xa;&#xa;&#xa;&#xa;def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels,&#xa;            figure_width=40, figure_height=20, correct_orientation=False,&#xa;            cmap='RdBu'):&#xa;  """"""&#xa;  Inspired by:&#xa;  - http://stackoverflow.com/a/16124677/395857 &#xa;  - http://stackoverflow.com/a/25074150/395857&#xa;  """"""&#xa;&#xa;  # Plot it out&#xa;  fig, ax = plt.subplots()&#xa;  # c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', &#xa;  # linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)&#xa;  c = ax.pcolor(AUC, edgecolors='k', linestyle='dashed', linewidths=0.2,&#xa;                cmap=cmap)&#xa;&#xa;  # put the major ticks at the middle of each cell&#xa;  ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)&#xa;  ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)&#xa;&#xa;  # set tick labels&#xa;  # ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)&#xa;  ax.set_xticklabels(xticklabels, minor=False)&#xa;  ax.set_yticklabels(yticklabels, minor=False)&#xa;&#xa;  # set title and x/y labels&#xa;  plt.title(title)&#xa;  plt.xlabel(xlabel)&#xa;  plt.ylabel(ylabel)&#xa;&#xa;  # Remove last blank column&#xa;  plt.xlim((0, AUC.shape[1]))&#xa;&#xa;  # Turn off all the ticks&#xa;  ax = plt.gca()&#xa;  for t in ax.xaxis.get_major_ticks():&#xa;    t.tick1On = False&#xa;    t.tick2On = False&#xa;  for t in ax.yaxis.get_major_ticks():&#xa;    t.tick1On = False&#xa;    t.tick2On = False&#xa;&#xa;  # Add color bar&#xa;  plt.colorbar(c)&#xa;&#xa;  # Add text in each cell &#xa;  show_values(c)&#xa;&#xa;  # Proper orientation (origin at the top left instead of bottom left)&#xa;  if correct_orientation:&#xa;    ax.invert_yaxis()&#xa;    ax.xaxis.tick_top()&#xa;&#xa;    # resize &#xa;  fig = plt.gcf()&#xa;  # fig.set_size_inches(cm2inch(40, 20))&#xa;  # fig.set_size_inches(cm2inch(40*4, 20*4))&#xa;  fig.set_size_inches(cm2inch(figure_width, figure_height))&#xa;&#xa;&#xa;&#xa;def plot_classification_report(classification_report, filename,&#xa;                               title='Classification report ', cmap='RdBu'):&#xa;  """"""&#xa;  Plot scikit-learn classification report.&#xa;  Extension based on http://stackoverflow.com/a/31689645/395857 &#xa;  """"""&#xa;  lines = classification_report.split('\n')&#xa;&#xa;  classes = []&#xa;  plotMat = []&#xa;  support = []&#xa;  class_names = []&#xa;  for line in lines[2: (len(lines) - 2)]:&#xa;    t = line.strip().split()&#xa;    if len(t) < 2: continue&#xa;    classes.append(t[0])&#xa;    v = [float(x) for x in t[1: len(t) - 1]]&#xa;    support.append(int(t[-1]))&#xa;    class_names.append(t[0])&#xa;    plotMat.append(v)&#xa;&#xa;  xlabel = 'Metrics'&#xa;  ylabel = 'Classes'&#xa;  xticklabels = ['Precision', 'Recall', 'F1-score']&#xa;  yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup in&#xa;                 enumerate(support)]&#xa;  figure_width = 25&#xa;  figure_height = len(class_names) + 7&#xa;  correct_orientation = False&#xa;  heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels,&#xa;          figure_width, figure_height, correct_orientation, cmap=cmap)&#xa;  plt.savefig(filename,&#xa;              dpi=200,&#xa;              format='png',&#xa;              bbox_inches='tight')&#xa;  plt.close()&#xa;&#xa;&#xa;&#xa;def plot_train_history(epochs, acc, loss, output_file):&#xa;  trace0 = go.Scatter(x=epochs, y=loss, name='Loss')&#xa;  trace1 = go.Scatter(x=epochs, y=acc, name='Accuracy')&#xa;&#xa;  layout = go.Layout(showlegend=True, title='Training history')&#xa;  fig = go.Figure(data=[trace0, trace1], layout=layout)&#xa;&#xa;  py.plot(fig,&#xa;          filename=output_file,&#xa;          auto_open=False,&#xa;          link_text=False)&#xa;&#xa;&#xa;&#xa;def plot_data(X, y_labels, t, title):&#xa;  unique_labels = np.unique(y_labels)&#xa;  print('unique labels (%s): %s' % (title, unique_labels))&#xa;&#xa;  colors = ['grey', 'blue', 'black', 'orange', 'yellow', 'pink']&#xa;  # Plot input data&#xa;  traces = []&#xa;  for label in unique_labels:&#xa;    trace = go.Scatter(x=t[np.where(y_labels == label)[0]],&#xa;                       y=X[np.where(y_labels == label)[0]][:, 0],&#xa;                       name='Data (class %s)' % label,&#xa;                       mode='markers',&#xa;                       marker={'color': colors[int(label)]})&#xa;&#xa;    traces.append(trace)&#xa;&#xa;  layout = go.Layout(showlegend=True, title='Data (%s)' % title)&#xa;  fig = go.Figure(data=traces, layout=layout)&#xa;  py.plot(fig,&#xa;          filename='%s_data.html' % title,&#xa;          auto_open=False,&#xa;          link_text=False)&#xa;&#xa;&#xa;&#xa;def plot_predictions(t, X_values, y_true, y_pred, output_file_path, title):&#xa;  """"""&#xa;  Plot prediction results (correct and incorrect)  &#xa;  &#xa;  :param t: (list) timesteps&#xa;  :param X_values: (list) input scalar values (before any encoding)&#xa;  :param y_true: (list) true labels&#xa;  :param y_pred: (list) predicted labels&#xa;  :param output_file_path: (str) path to output file&#xa;  :param title: (str) title of the plot.&#xa;  """"""&#xa;  if type(t) != np.ndarray:&#xa;    t = np.array(t)&#xa;  if type(X_values) != np.ndarray:&#xa;    X_values = np.array(X_values)&#xa;  if type(y_true) != np.ndarray:&#xa;    y_true = np.array(y_true)&#xa;  if type(y_pred) != np.ndarray:&#xa;    y_pred = np.array(y_pred)&#xa;&#xa;  correct = []&#xa;  incorrect = []&#xa;  for prediction in y_true == y_pred:&#xa;    correct.append(prediction)&#xa;    incorrect.append(not prediction)&#xa;&#xa;  trace0 = go.Scatter(x=t[correct], &#xa;                      y=X_values[correct],&#xa;                      name='Correct predictions',&#xa;                      mode='markers', marker={'color': 'green'})&#xa;&#xa;  trace1 = go.Scatter(x=t[incorrect], &#xa;                      y=X_values[incorrect],&#xa;                      name='Incorrect predictions',&#xa;                      mode='markers', marker={'color': 'red'})&#xa;&#xa;  layout = go.Layout(showlegend=True, title=title)&#xa;  fig = go.Figure(data=[trace0, trace1], layout=layout)&#xa;&#xa;  py.plot(fig,&#xa;          filename=output_file_path,&#xa;          auto_open=False,&#xa;          link_text=False)&#xa;&#xa;&#xa;&#xa;def plot_confusion_matrix(cm,&#xa;                          filename,&#xa;                          classes,&#xa;                          normalize=True,&#xa;                          title='Confusion matrix',&#xa;                          cmap=plt.cm.Blues):&#xa;  """"""&#xa;  This function prints and plots the confusion matrix.&#xa;  Normalization can be applied by setting `normalize=True`.&#xa;  """"""&#xa;  &#xa;  if normalize:&#xa;    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]&#xa;  &#xa;  # Use only 2 decimal numbers&#xa;  cm = np.around(cm, 2)&#xa;&#xa;  plt.figure()&#xa;  plt.imshow(cm, interpolation='nearest', cmap=cmap)&#xa;  plt.title(title)&#xa;  plt.colorbar()&#xa;  tick_marks = np.arange(len(classes))&#xa;  plt.xticks(tick_marks, classes, rotation=30)&#xa;  plt.yticks(tick_marks, classes)&#xa;&#xa;  thresh = cm.max() / 2.&#xa;  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):&#xa;    plt.text(j, i, cm[i, j],&#xa;             horizontalalignment=""center"",&#xa;             color=""white"" if cm[i, j] > thresh else ""black"")&#xa;&#xa;  plt.tight_layout()&#xa;  plt.ylabel('True label')&#xa;  plt.xlabel('Predicted label')&#xa;  plt.savefig(filename)&#xa;"
774316|"# coding=utf-8&#xa;import difflib&#xa;import json&#xa;import re&#xa;import urllib&#xa;import urlparse&#xa;&#xa;import jinja2&#xa;from pyquery import PyQuery as pq&#xa;from tidylib import tidy_document&#xa;from tower import ugettext as _&#xa;&#xa;from django.conf import settings&#xa;from django.contrib.sites.models import Site&#xa;from django.core.serializers.json import DjangoJSONEncoder&#xa;from django.utils.html import conditional_escape&#xa;&#xa;import constance.config&#xa;from jingo import register&#xa;from teamwork.shortcuts import build_policy_admin_links&#xa;&#xa;from kuma.core.urlresolvers import reverse&#xa;from .constants import DIFF_WRAP_COLUMN&#xa;from .jobs import DocumentZoneStackJob&#xa;from .models import Document, memcache&#xa;&#xa;register.function(build_policy_admin_links)&#xa;&#xa;&#xa;def compare_url(doc, from_id, to_id):&#xa;    return (reverse('wiki.compare_revisions', args=[doc.full_path],&#xa;                    locale=doc.locale)&#xa;            + '?' +&#xa;            urllib.urlencode({'from': from_id, 'to': to_id})&#xa;           )&#xa;&#xa;&#xa;# http://stackoverflow.com/q/774316/571420&#xa;def show_diff(seqm):&#xa;    """"""Unify operations between two compared strings&#xa;seqm is a difflib.SequenceMatcher instance whose a & b are strings""""""&#xa;    lines = constance.config.FEED_DIFF_CONTEXT_LINES&#xa;    full_output = []&#xa;    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():&#xa;        if opcode == 'equal':&#xa;            full_output.append(seqm.a[a0:a1])&#xa;        elif opcode == 'insert':&#xa;            full_output.append(""<ins>"" + seqm.b[b0:b1] + ""</ins>"")&#xa;        elif opcode == 'delete':&#xa;            full_output.append(""<del>"" + seqm.a[a0:a1] + ""</del>"")&#xa;        elif opcode == 'replace':&#xa;            full_output.append(""&nbsp;<del>"" + seqm.a[a0:a1] + ""</del>&nbsp;"")&#xa;            full_output.append(""&nbsp;<ins>"" + seqm.b[b0:b1] + ""</ins>&nbsp;"")&#xa;        else:&#xa;            raise RuntimeError(""unexpected opcode"")&#xa;    output = []&#xa;    whitespace_change = False&#xa;    for piece in full_output:&#xa;        if '<ins>' in piece or '<del>' in piece:&#xa;            # a change&#xa;            if re.match('<(ins|del)>\W+</(ins|del)>', piece):&#xa;                # the change is whitespace,&#xa;                # ignore it and remove preceding context&#xa;                output = output[:-lines]&#xa;                whitespace_change = True&#xa;                continue&#xa;            else:&#xa;                output.append(piece)&#xa;        else:&#xa;            context_lines = piece.splitlines()&#xa;            if output == []:&#xa;                # first context only shows preceding lines for next change&#xa;                context = ['<p>...</p>'] + context_lines[-lines:]&#xa;            elif whitespace_change:&#xa;                # context shows preceding lines for next change&#xa;                context = ['<p>...</p>'] + context_lines[-lines:]&#xa;                whitespace_change = False&#xa;            else:&#xa;                # context shows subsequent lines&#xa;                # and preceding lines for next change&#xa;                context = (context_lines[:lines]&#xa;                           + ['<p>...</p>']&#xa;                           + context_lines[-lines:])&#xa;            output = output + context&#xa;    # remove extra context from the very end, unless its the only context&#xa;    if len(output) > lines + 1:  # context lines and the change line&#xa;        output = output[:-lines]&#xa;    return ''.join(output)&#xa;&#xa;&#xa;def _massage_diff_content(content):&#xa;    tidy_options = {'output-xhtml': 0, 'force-output': 1}&#xa;    content = tidy_document(content, options=tidy_options)&#xa;    return content&#xa;&#xa;&#xa;@register.filter&#xa;def bugize_text(content):&#xa;    content = jinja2.escape(content)&#xa;    regex = re.compile('(bug)\s+#?(\d+)', re.IGNORECASE)&#xa;    content = regex.sub(&#xa;        jinja2.Markup('<a href=""https://bugzilla.mozilla.org/'&#xa;                      'show_bug.cgi?id=\\2"" '&#xa;                      'target=""_blank"">\\1 \\2</a>'),&#xa;                  content)&#xa;    return content&#xa;&#xa;&#xa;@register.function&#xa;def format_comment(rev):&#xa;    """""" Massages revision comment content after the fact """"""&#xa;&#xa;    prev_rev = getattr(rev, 'previous_revision', None)&#xa;    if prev_rev is None:&#xa;        prev_rev = rev.get_previous()&#xa;    comment = bugize_text(rev.comment if rev.comment else """")&#xa;&#xa;    # If a page move, say so&#xa;    if prev_rev and prev_rev.slug != rev.slug:&#xa;        comment += jinja2.Markup(&#xa;            '<span class=""slug-change"">'&#xa;            '<span>%s</span>'&#xa;            ' <i class=""icon-long-arrow-right"" aria-hidden=""true""></i> '&#xa;            '<span>%s</span></span>') % (prev_rev.slug, rev.slug)&#xa;&#xa;    return comment&#xa;&#xa;&#xa;@register.function&#xa;def revisions_unified_diff(from_revision, to_revision):&#xa;    if from_revision is None or to_revision is None:&#xa;        return ""Diff is unavailable.""&#xa;    fromfile = u'[%s] %s #%s' % (from_revision.document.locale,&#xa;                                 from_revision.document.title,&#xa;                                 from_revision.id)&#xa;    tofile = u'[%s] %s #%s' % (to_revision.document.locale,&#xa;                               to_revision.document.title,&#xa;                               to_revision.id)&#xa;    tidy_from, errors = _massage_diff_content(from_revision.content)&#xa;    tidy_to, errors = _massage_diff_content(to_revision.content)&#xa;    diff = u'\n'.join(difflib.unified_diff(&#xa;        tidy_from.splitlines(),&#xa;        tidy_to.splitlines(),&#xa;        fromfile=fromfile,&#xa;        tofile=tofile&#xa;    ))&#xa;    return diff&#xa;&#xa;&#xa;@register.function&#xa;def diff_table(content_from, content_to, prev_id, curr_id):&#xa;    """"""Creates an HTML diff of the passed in content_from and content_to.""""""&#xa;    tidy_from, errors = _massage_diff_content(content_from)&#xa;    tidy_to, errors = _massage_diff_content(content_to)&#xa;    html_diff = difflib.HtmlDiff(wrapcolumn=DIFF_WRAP_COLUMN)&#xa;    from_lines = tidy_from.splitlines()&#xa;    to_lines = tidy_to.splitlines()&#xa;    try:&#xa;        diff = html_diff.make_table(from_lines, to_lines,&#xa;                                _(""Revision %s"") % prev_id,&#xa;                                _(""Revision %s"") % curr_id,&#xa;                                context=True,&#xa;                                numlines=constance.config.DIFF_CONTEXT_LINES&#xa;                               )&#xa;    except RuntimeError:&#xa;        # some diffs hit a max recursion error&#xa;        message = _(u'There was an error generating the content.')&#xa;        diff = '<div class=""warning""><p>%s</p></div>' % message&#xa;    return jinja2.Markup(diff)&#xa;&#xa;&#xa;@register.function&#xa;def diff_inline(content_from, content_to):&#xa;    tidy_from, errors = _massage_diff_content(content_from)&#xa;    tidy_to, errors = _massage_diff_content(content_to)&#xa;    sm = difflib.SequenceMatcher(None, tidy_from, tidy_to)&#xa;    diff = show_diff(sm)&#xa;    return jinja2.Markup(diff)&#xa;&#xa;&#xa;@register.function&#xa;def tag_diff_table(prev_tags, curr_tags, prev_id, curr_id):&#xa;    html_diff = difflib.HtmlDiff(wrapcolumn=DIFF_WRAP_COLUMN)&#xa;    prev_tag_lines = [prev_tags]&#xa;    curr_tag_lines = [curr_tags]&#xa;&#xa;    diff = html_diff.make_table(prev_tag_lines, curr_tag_lines,&#xa;                                _(""Revision %s"") % prev_id,&#xa;                                _(""Revision %s"") % curr_id)&#xa;&#xa;    # Simple formatting update: 784877&#xa;    diff = diff.replace('"",', '""<br />').replace('<td', '<td valign=""top""')&#xa;    return jinja2.Markup(diff)&#xa;&#xa;&#xa;@register.function&#xa;def colorize_diff(diff):&#xa;    # we're doing something horrible here because this will show up&#xa;    # in feed reader and other clients that don't load CSS files&#xa;    diff = diff.replace('<span class=""diff_add""', '<span class=""diff_add"" '&#xa;                'style=""background-color: #afa; text-decoration: none;""')&#xa;    diff = diff.replace('<span class=""diff_sub""', '<span class=""diff_sub"" '&#xa;                'style=""background-color: #faa; text-decoration: none;""')&#xa;    diff = diff.replace('<span class=""diff_chg""', '<span class=""diff_chg"" '&#xa;                'style=""background-color: #fe0; text-decoration: none;""')&#xa;    return diff&#xa;&#xa;&#xa;@register.filter&#xa;def wiki_bleach(val):&#xa;    from kuma.wiki.models import Document&#xa;    return jinja2.Markup(Document.objects.clean_content(val))&#xa;&#xa;&#xa;@register.filter&#xa;def selector_content_find(document, selector):&#xa;    """"""&#xa;    Provided a selector, returns the relevant content from the document&#xa;    """"""&#xa;    summary = ''&#xa;    try:&#xa;        page = pq(document.rendered_html)&#xa;        summary = page.find(selector).text()&#xa;    except:&#xa;        pass&#xa;    return summary&#xa;&#xa;&#xa;def _recursive_escape(value, esc=conditional_escape):&#xa;    """"""&#xa;    Recursively escapes strings in an object.&#xa;&#xa;    Traverses dict, list and tuples. These are the data structures supported&#xa;    by the JSON encoder.&#xa;    """"""&#xa;    if isinstance(value, dict):&#xa;        return type(value)((esc(k), _recursive_escape(v))&#xa;                           for (k, v) in value.iteritems())&#xa;    elif isinstance(value, (list, tuple)):&#xa;        return type(value)(_recursive_escape(v) for v in value)&#xa;    elif isinstance(value, basestring):&#xa;        return esc(value)&#xa;    elif isinstance(value, (int, long, float)) or value in (True, False, None):&#xa;        return value&#xa;    # We've exhausted all the types acceptable by the default JSON encoder.&#xa;    # Django's improved JSON encoder handles a few other types, all of which&#xa;    # are represented by strings. For these types, we apply JSON encoding&#xa;    # immediately and then escape the result.&#xa;    return esc(DjangoJSONEncoder().default(value))&#xa;&#xa;&#xa;@register.filter&#xa;def tojson(value):&#xa;    """"""&#xa;    Returns the JSON representation of the value.&#xa;    """"""&#xa;    try:&#xa;        # If value contains custom subclasses of int, str, datetime, etc.&#xa;        # arbitrary exceptions may be raised during escaping or serialization.&#xa;        result = json.dumps(_recursive_escape(value), cls=DjangoJSONEncoder)&#xa;    except Exception:&#xa;        return ''&#xa;    return jinja2.Markup(result)&#xa;&#xa;&#xa;@register.function&#xa;def document_zone_management_links(user, document):&#xa;    links = {'add': None, 'change': None}&#xa;    stack = DocumentZoneStackJob().get(document.pk)&#xa;    zone = (len(stack) > 0) and stack[0] or None&#xa;&#xa;    # Enable ""add"" link if there is no zone for this document, or if there's a&#xa;    # zone but the document is not itself the root (ie. to add sub-zones).&#xa;    if ((not zone or zone.document != document) and&#xa;            user.has_perm('wiki.add_documentzone')):&#xa;        links['add'] = '%s?document=%s' % (&#xa;            reverse('admin:wiki_documentzone_add'), document.id)&#xa;&#xa;    # Enable ""change"" link if there's a zone, and the user has permission.&#xa;    if zone and user.has_perm('wiki.change_documentzone'):&#xa;        links['change'] = reverse('admin:wiki_documentzone_change',&#xa;                                  args=(zone.id,))&#xa;&#xa;    return links&#xa;&#xa;&#xa;@register.filter&#xa;def absolutify(url, site=None):&#xa;    """"""&#xa;    Joins a base ``Site`` URL with a URL path.&#xa;&#xa;    If no site provided it gets the current site from Site.&#xa;&#xa;    """"""&#xa;    if url.startswith('http'):&#xa;        return url&#xa;&#xa;    if not site:&#xa;        site = Site.objects.get_current()&#xa;&#xa;    parts = urlparse.urlsplit(url)&#xa;&#xa;    scheme = 'https'&#xa;    netloc = site.domain&#xa;    path = parts.path&#xa;    query = parts.query&#xa;    fragment = parts.fragment&#xa;&#xa;    if path == '':&#xa;        path = '/'&#xa;&#xa;    return urlparse.urlunparse([scheme, netloc, path, None, query, fragment])&#xa;&#xa;&#xa;@register.function&#xa;@jinja2.contextfunction&#xa;def wiki_url(context, path):&#xa;    """"""&#xa;    Create a URL pointing to Kuma.&#xa;    Look for a wiki page in the current locale, or default to given path&#xa;    """"""&#xa;    default_locale = settings.WIKI_DEFAULT_LANGUAGE&#xa;    locale = getattr(context['request'], 'locale', default_locale)&#xa;&#xa;    # let's first check if the cache is already filled&#xa;    url = memcache.get(u'wiki_url:%s:%s' % (locale, path))&#xa;    if url:&#xa;        # and return the URL right away if yes&#xa;        return url&#xa;&#xa;    # shortcut for when the current locale is the default one (English)&#xa;    url = reverse('wiki.document', locale=default_locale, args=[path])&#xa;&#xa;    if locale != default_locale:&#xa;        # in case the current request's locale is *not* the default, e.g. 'de'&#xa;        try:&#xa;            # check if there are any translated documents in the request's&#xa;            # locale of a document with the given path and the default locale&#xa;            translation = Document.objects.get(locale=locale,&#xa;                                               parent__slug=path,&#xa;                                               parent__locale=default_locale)&#xa;&#xa;            # look if the document is actual just a redirect&#xa;            redirect_url = translation.redirect_url()&#xa;            if redirect_url is None:&#xa;                # if no, build the URL of the translation&#xa;                url = translation.get_absolute_url()&#xa;            else:&#xa;                # use the redirect URL instead&#xa;                url = redirect_url&#xa;        except Document.DoesNotExist:&#xa;            # otherwise use the already defined url to the English document&#xa;            pass&#xa;&#xa;    # finally cache the reversed document URL for a bit&#xa;    memcache.set(u'wiki_url:%s:%s' % (locale, path), url, 60 * 5)&#xa;    return url&#xa;"
17199950|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# https://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	# Skip non-scalar values.&#xa;	if codePoint >= 0xD800 and codePoint <= 0xDFFF:&#xa;		continue&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = ln -f ""$<"" ""$@"" 2>/dev/null || (rm -rf ""$@"" && cp -af ""$<"" ""$@"")&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;    if (self.type == 'shared_library' and&#xa;        (self.flavor != 'mac' or self.toolset != 'target')):&#xa;      # Install all shared libs into a common directory (per toolset) for&#xa;      # convenient access with LD_LIBRARY_PATH.&#xa;      return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host',), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CPPFLAGS) $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CPPFLAGS) $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?= $(CPPFLAGS_host) $(CFLAGS_host)&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?= $(CPPFLAGS_host) $(CXXFLAGS_host)&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = rm -rf ""$@"" && cp %(copy_archive_args)s ""$<"" ""$@""&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'netbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;&#xa;    # XXX(TooTallNate): disabling this code since we don't want this behavior...&#xa;    #if (self.type == 'shared_library' and&#xa;    #    (self.flavor != 'mac' or self.toolset != 'target')):&#xa;    #  # Install all shared libs into a common directory (per toolset) for&#xa;    #  # convenient access with LD_LIBRARY_PATH.&#xa;    #  return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  copy_archive_arguments = '-af'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;      'copy_archive_args': copy_archive_arguments,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'openbsd':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host', 'CC'), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host', 'AR'), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host', 'CXX'), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host', 'LINK'), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
19998912|from django import template&#xa;&#xa;register = template.Library()&#xa;&#xa;&#xa;# See: http://stackoverflow.com/questions/19998912/django-templatetag-return-true-or-false&#xa;@register.filter&#xa;def show_django_tracker(request):&#xa;    if not request:&#xa;        return False&#xa;&#xa;    if hasattr(request, 'user'):&#xa;        # Django 1.8 and lower&#xa;        user = request.user&#xa;    else:&#xa;        user = request.request.user&#xa;    return user.is_superuser or user.groups.filter(name='django_tracker').exists()&#xa;
12317940|"# -*- coding: utf-8 -*-&#xa;&#xa;import threading&#xa;import re&#xa;import sys&#xa;import six&#xa;from six import string_types&#xa;&#xa;# Python3 queue support.&#xa;&#xa;try:&#xa;    import Queue&#xa;except ImportError:&#xa;    import queue as Queue&#xa;&#xa;from telebot import logger&#xa;&#xa;&#xa;class WorkerThread(threading.Thread):&#xa;        count = 0&#xa;&#xa;        def __init__(self, exception_callback=None, queue=None, name=None):&#xa;            if not name:&#xa;                name = ""WorkerThread{0}"".format(self.__class__.count + 1)&#xa;                self.__class__.count += 1&#xa;            if not queue:&#xa;                queue = Queue.Queue()&#xa;&#xa;            threading.Thread.__init__(self, name=name)&#xa;            self.queue = queue&#xa;            self.daemon = True&#xa;&#xa;            self.received_task_event = threading.Event()&#xa;            self.done_event = threading.Event()&#xa;            self.exception_event = threading.Event()&#xa;            self.continue_event = threading.Event()&#xa;&#xa;            self.exception_callback = exception_callback&#xa;            self.exc_info = None&#xa;            self._running = True&#xa;            self.start()&#xa;&#xa;        def run(self):&#xa;            while self._running:&#xa;                try:&#xa;                    task, args, kwargs = self.queue.get(block=True, timeout=.5)&#xa;                    self.continue_event.clear()&#xa;                    self.received_task_event.clear()&#xa;                    self.done_event.clear()&#xa;                    self.exception_event.clear()&#xa;                    logger.debug(""Received task"")&#xa;                    self.received_task_event.set()&#xa;&#xa;                    task(*args, **kwargs)&#xa;                    logger.debug(""Task complete"")&#xa;                    self.done_event.set()&#xa;                except Queue.Empty:&#xa;                    pass&#xa;                except:&#xa;                    logger.debug(""Exception occurred"")&#xa;                    self.exc_info = sys.exc_info()&#xa;                    self.exception_event.set()&#xa;&#xa;                    if self.exception_callback:&#xa;                        self.exception_callback(self, self.exc_info)&#xa;                    self.continue_event.wait()&#xa;&#xa;        def put(self, task, *args, **kwargs):&#xa;            self.queue.put((task, args, kwargs))&#xa;&#xa;        def raise_exceptions(self):&#xa;            if self.exception_event.is_set():&#xa;                six.reraise(self.exc_info[0], self.exc_info[1], self.exc_info[2])&#xa;&#xa;        def clear_exceptions(self):&#xa;            self.exception_event.clear()&#xa;            self.continue_event.set()&#xa;&#xa;        def stop(self):&#xa;            self._running = False&#xa;&#xa;&#xa;class ThreadPool:&#xa;&#xa;    def __init__(self, num_threads=2):&#xa;        self.tasks = Queue.Queue()&#xa;        self.workers = [WorkerThread(self.on_exception, self.tasks) for _ in range(num_threads)]&#xa;        self.num_threads = num_threads&#xa;&#xa;        self.exception_event = threading.Event()&#xa;        self.exc_info = None&#xa;&#xa;    def put(self, func, *args, **kwargs):&#xa;        self.tasks.put((func, args, kwargs))&#xa;&#xa;    def on_exception(self, worker_thread, exc_info):&#xa;        self.exc_info = exc_info&#xa;        self.exception_event.set()&#xa;        worker_thread.continue_event.set()&#xa;&#xa;    def raise_exceptions(self):&#xa;        if self.exception_event.is_set():&#xa;            six.reraise(self.exc_info[0], self.exc_info[1], self.exc_info[2])&#xa;&#xa;    def clear_exceptions(self):&#xa;        self.exception_event.clear()&#xa;&#xa;    def close(self):&#xa;        for worker in self.workers:&#xa;            worker.stop()&#xa;        for worker in self.workers:&#xa;            worker.join()&#xa;&#xa;&#xa;class AsyncTask:&#xa;    def __init__(self, target, *args, **kwargs):&#xa;        self.target = target&#xa;        self.args = args&#xa;        self.kwargs = kwargs&#xa;&#xa;        self.done = False&#xa;        self.thread = threading.Thread(target=self._run)&#xa;        self.thread.start()&#xa;&#xa;    def _run(self):&#xa;        try:&#xa;            self.result = self.target(*self.args, **self.kwargs)&#xa;        except:&#xa;            self.result = sys.exc_info()&#xa;        self.done = True&#xa;&#xa;    def wait(self):&#xa;        if not self.done:&#xa;            self.thread.join()&#xa;        if isinstance(self.result, BaseException):&#xa;            six.reraise(self.result[0], self.result[1], self.result[2])&#xa;        else:&#xa;            return self.result&#xa;&#xa;&#xa;def async():&#xa;    def decorator(fn):&#xa;        def wrapper(*args, **kwargs):&#xa;            return AsyncTask(fn, *args, **kwargs)&#xa;&#xa;        return wrapper&#xa;&#xa;    return decorator&#xa;&#xa;&#xa;def is_string(var):&#xa;    return isinstance(var, string_types)&#xa;&#xa;def is_command(text):&#xa;    """"""&#xa;    Checks if `text` is a command. Telegram chat commands start with the '/' character.&#xa;    :param text: Text to check.&#xa;    :return: True if `text` is a command, else False.&#xa;    """"""&#xa;    return text.startswith('/')&#xa;&#xa;&#xa;def extract_command(text):&#xa;    """"""&#xa;    Extracts the command from `text` (minus the '/') if `text` is a command (see is_command).&#xa;    If `text` is not a command, this function returns None.&#xa;&#xa;    Examples:&#xa;    extract_command('/help'): 'help'&#xa;    extract_command('/help@BotName'): 'help'&#xa;    extract_command('/search black eyed peas'): 'search'&#xa;    extract_command('Good day to you'): None&#xa;&#xa;    :param text: String to extract the command from&#xa;    :return: the command if `text` is a command (according to is_command), else None.&#xa;    """"""&#xa;    return text.split()[0].split('@')[0][1:] if is_command(text) else None&#xa;&#xa;&#xa;def split_string(text, chars_per_string):&#xa;    """"""&#xa;    Splits one string into multiple strings, with a maximum amount of `chars_per_string` characters per string.&#xa;    This is very useful for splitting one giant message into multiples.&#xa;&#xa;    :param text: The text to split&#xa;    :param chars_per_string: The number of characters per line the text is split into.&#xa;    :return: The splitted text as a list of strings.&#xa;    """"""&#xa;    return [text[i:i + chars_per_string] for i in range(0, len(text), chars_per_string)]&#xa;&#xa;# CREDITS TO http://stackoverflow.com/questions/12317940#answer-12320352&#xa;def or_set(self):&#xa;    self._set()&#xa;    self.changed()&#xa;&#xa;&#xa;def or_clear(self):&#xa;    self._clear()&#xa;    self.changed()&#xa;&#xa;&#xa;def orify(e, changed_callback):&#xa;    e._set = e.set&#xa;    e._clear = e.clear&#xa;    e.changed = changed_callback&#xa;    e.set = lambda: or_set(e)&#xa;    e.clear = lambda: or_clear(e)&#xa;&#xa;def OrEvent(*events):&#xa;    or_event = threading.Event()&#xa;    def changed():&#xa;        bools = [e.is_set() for e in events]&#xa;        if any(bools):&#xa;            or_event.set()&#xa;        else:&#xa;            or_event.clear()&#xa;&#xa;    def busy_wait():&#xa;        while not or_event.is_set():&#xa;            or_event._wait(3)&#xa;&#xa;    for e in events:&#xa;        orify(e, changed)&#xa;    or_event._wait = or_event.wait&#xa;    or_event.wait = busy_wait&#xa;    changed()&#xa;    return or_event&#xa;&#xa;def extract_arguments(text):&#xa;    """"""&#xa;    Returns the argument after the command.&#xa;    &#xa;    Examples:&#xa;    extract_arguments(""/get name""): 'name'&#xa;    extract_arguments(""/get""): ''&#xa;    extract_arguments(""/get@botName name""): 'name'&#xa;    &#xa;    :param text: String to extract the arguments from a command&#xa;    :return: the arguments if `text` is a command (according to is_command), else None.&#xa;    """"""&#xa;    regexp = re.compile(""\/\w*(@\w*)*\s*([\s\S]*)"",re.IGNORECASE)&#xa;    result = regexp.match(text)&#xa;    return result.group(2) if is_command(text) else None&#xa;"
26003573|"# Support new str.format syntax in log messages&#xa;#&#xa;# Based on http://stackoverflow.com/a/25433007 and&#xa;# http://stackoverflow.com/a/26003573 and logging cookbook&#xa;# https://docs.python.org/3/howto/logging-cookbook.html#use-of-alternative-formatting-styles&#xa;#&#xa;# It's worth noting that this implementation has problems if key words&#xa;# used for brace substitution include level, msg, args, exc_info,&#xa;# extra or stack_info. These are argument names used by the log method&#xa;# of Logger. If you need to one of these names then modify process to&#xa;# exclude these names or just remove log_kwargs from the _log call. On&#xa;# a further note, this implementation also silently ignores misspelled&#xa;# keywords meant for the Logger (eg. ectra).&#xa;#&#xa;&#xa;&#xa;import logging&#xa;&#xa;&#xa;class NewStyleLogMessage(object):&#xa;    def __init__(self, message, *args, **kwargs):&#xa;        self.message = message&#xa;        self.args = args&#xa;        self.kwargs = kwargs&#xa;&#xa;    def __str__(self):&#xa;        args = (i() if callable(i) else i for i in self.args)&#xa;        kwargs = dict((k, v() if callable(v) else v)&#xa;                      for k, v in self.kwargs.items())&#xa;&#xa;        return self.message.format(*args, **kwargs)&#xa;&#xa;N = NewStyleLogMessage&#xa;&#xa;&#xa;class StyleAdapter(logging.LoggerAdapter):&#xa;    def __init__(self, logger, extra=None):&#xa;        super(StyleAdapter, self).__init__(logger, extra or {})&#xa;&#xa;    def log(self, level, msg, *args, **kwargs):&#xa;        if self.isEnabledFor(level):&#xa;            msg, log_kwargs = self.process(msg, kwargs)&#xa;            self.logger._log(level, N(msg, *args, **kwargs), (),&#xa;                             **log_kwargs)&#xa;&#xa;&#xa;logger = StyleAdapter(logging.getLogger(""project""))&#xa;#   Emits ""Lazily formatted log entry: 123 foo"" in log&#xa;# logger.debug('Lazily formatted entry: {0} {keyword}', 123, keyword='foo')&#xa;"
4124220|"from __future__ import absolute_import, unicode_literals&#xa;&#xa;import re&#xa;from collections import OrderedDict&#xa;&#xa;from django import template&#xa;from django.template import loader&#xa;from django.utils import six&#xa;from django.utils.encoding import force_text, iri_to_uri&#xa;from django.utils.html import escape, format_html, smart_urlquote&#xa;from django.utils.safestring import SafeData, mark_safe&#xa;&#xa;from rest_framework.compat import (&#xa;    NoReverseMatch, markdown, pygments_highlight, reverse, template_render&#xa;)&#xa;from rest_framework.renderers import HTMLFormRenderer&#xa;from rest_framework.utils.urls import replace_query_param&#xa;&#xa;register = template.Library()&#xa;&#xa;# Regex for adding classes to html snippets&#xa;class_re = re.compile(r'(?<=class=[""\'])(.*)(?=[""\'])')&#xa;&#xa;&#xa;@register.tag(name='code')&#xa;def highlight_code(parser, token):&#xa;    code = token.split_contents()[-1]&#xa;    nodelist = parser.parse(('endcode',))&#xa;    parser.delete_first_token()&#xa;    return CodeNode(code, nodelist)&#xa;&#xa;&#xa;class CodeNode(template.Node):&#xa;    style = 'emacs'&#xa;&#xa;    def __init__(self, lang, code):&#xa;        self.lang = lang&#xa;        self.nodelist = code&#xa;&#xa;    def render(self, context):&#xa;        text = self.nodelist.render(context)&#xa;        return pygments_highlight(text, self.lang, self.style)&#xa;&#xa;&#xa;@register.filter()&#xa;def with_location(fields, location):&#xa;    return [&#xa;        field for field in fields&#xa;        if field.location == location&#xa;    ]&#xa;&#xa;&#xa;@register.simple_tag&#xa;def form_for_link(link):&#xa;    import coreschema&#xa;    properties = OrderedDict([&#xa;        (field.name, field.schema or coreschema.String())&#xa;        for field in link.fields&#xa;    ])&#xa;    required = [&#xa;        field.name&#xa;        for field in link.fields&#xa;        if field.required&#xa;    ]&#xa;    schema = coreschema.Object(properties=properties, required=required)&#xa;    return mark_safe(coreschema.render_to_form(schema))&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_markdown(markdown_text):&#xa;    if not markdown:&#xa;        return markdown_text&#xa;    return mark_safe(markdown.markdown(markdown_text))&#xa;&#xa;&#xa;@register.simple_tag&#xa;def get_pagination_html(pager):&#xa;    return pager.to_html()&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_form(serializer, template_pack=None):&#xa;    style = {'template_pack': template_pack} if template_pack else {}&#xa;    renderer = HTMLFormRenderer()&#xa;    return renderer.render(serializer.data, None, {'style': style})&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_field(field, style):&#xa;    renderer = style.get('renderer', HTMLFormRenderer())&#xa;    return renderer.render_field(field, style)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_login(request):&#xa;    """"""&#xa;    Include a login snippet if REST framework's login view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        login_url = reverse('rest_framework:login')&#xa;    except NoReverseMatch:&#xa;        return ''&#xa;&#xa;    snippet = ""<li><a href='{href}?next={next}'>Log in</a></li>""&#xa;    snippet = format_html(snippet, href=login_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_docs_login(request):&#xa;    """"""&#xa;    Include a login snippet if REST framework's login view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        login_url = reverse('rest_framework:login')&#xa;    except NoReverseMatch:&#xa;        return 'log in'&#xa;&#xa;    snippet = ""<a href='{href}?next={next}'>log in</a>""&#xa;    snippet = format_html(snippet, href=login_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_logout(request, user):&#xa;    """"""&#xa;    Include a logout snippet if REST framework's logout view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        logout_url = reverse('rest_framework:logout')&#xa;    except NoReverseMatch:&#xa;        snippet = format_html('<li class=""navbar-text"">{user}</li>', user=escape(user))&#xa;        return mark_safe(snippet)&#xa;&#xa;    snippet = """"""<li class=""dropdown"">&#xa;        <a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">&#xa;            {user}&#xa;            <b class=""caret""></b>&#xa;        </a>&#xa;        <ul class=""dropdown-menu"">&#xa;            <li><a href='{href}?next={next}'>Log out</a></li>&#xa;        </ul>&#xa;    </li>""""""&#xa;    snippet = format_html(snippet, user=escape(user), href=logout_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def add_query_param(request, key, val):&#xa;    """"""&#xa;    Add a query parameter to the current request url, and return the new url.&#xa;    """"""&#xa;    iri = request.get_full_path()&#xa;    uri = iri_to_uri(iri)&#xa;    return escape(replace_query_param(uri, key, val))&#xa;&#xa;&#xa;@register.filter&#xa;def as_string(value):&#xa;    if value is None:&#xa;        return ''&#xa;    return '%s' % value&#xa;&#xa;&#xa;@register.filter&#xa;def as_list_of_strings(value):&#xa;    return [&#xa;        '' if (item is None) else ('%s' % item)&#xa;        for item in value&#xa;    ]&#xa;&#xa;&#xa;@register.filter&#xa;def add_class(value, css_class):&#xa;    """"""&#xa;    http://stackoverflow.com/questions/4124220/django-adding-css-classes-when-rendering-form-fields-in-a-template&#xa;&#xa;    Inserts classes into template variables that contain HTML tags,&#xa;    useful for modifying forms without needing to change the Form objects.&#xa;&#xa;    Usage:&#xa;&#xa;        {{ field.label_tag|add_class:""control-label"" }}&#xa;&#xa;    In the case of REST Framework, the filter is used to add Bootstrap-specific&#xa;    classes to the forms.&#xa;    """"""&#xa;    html = six.text_type(value)&#xa;    match = class_re.search(html)&#xa;    if match:&#xa;        m = re.search(r'^%s$|^%s\s|\s%s\s|\s%s$' % (css_class, css_class,&#xa;                                                    css_class, css_class),&#xa;                      match.group(1))&#xa;        if not m:&#xa;            return mark_safe(class_re.sub(match.group(1) + "" "" + css_class,&#xa;                                          html))&#xa;    else:&#xa;        return mark_safe(html.replace('>', ' class=""%s"">' % css_class, 1))&#xa;    return value&#xa;&#xa;&#xa;@register.filter&#xa;def format_value(value):&#xa;    if getattr(value, 'is_hyperlink', False):&#xa;        name = six.text_type(value.obj)&#xa;        return mark_safe('<a href=%s>%s</a>' % (value, escape(name)))&#xa;    if value is None or isinstance(value, bool):&#xa;        return mark_safe('<code>%s</code>' % {True: 'true', False: 'false', None: 'null'}[value])&#xa;    elif isinstance(value, list):&#xa;        if any([isinstance(item, (list, dict)) for item in value]):&#xa;            template = loader.get_template('rest_framework/admin/list_value.html')&#xa;        else:&#xa;            template = loader.get_template('rest_framework/admin/simple_list_value.html')&#xa;        context = {'value': value}&#xa;        return template_render(template, context)&#xa;    elif isinstance(value, dict):&#xa;        template = loader.get_template('rest_framework/admin/dict_value.html')&#xa;        context = {'value': value}&#xa;        return template_render(template, context)&#xa;    elif isinstance(value, six.string_types):&#xa;        if (&#xa;            (value.startswith('http:') or value.startswith('https:')) and not&#xa;            re.search(r'\s', value)&#xa;        ):&#xa;            return mark_safe('<a href=""{value}"">{value}</a>'.format(value=escape(value)))&#xa;        elif '@' in value and not re.search(r'\s', value):&#xa;            return mark_safe('<a href=""mailto:{value}"">{value}</a>'.format(value=escape(value)))&#xa;        elif '\n' in value:&#xa;            return mark_safe('<pre>%s</pre>' % escape(value))&#xa;    return six.text_type(value)&#xa;&#xa;&#xa;@register.filter&#xa;def items(value):&#xa;    """"""&#xa;    Simple filter to return the items of the dict. Useful when the dict may&#xa;    have a key 'items' which is resolved first in Django tempalte dot-notation&#xa;    lookup.  See issue #4931&#xa;    Also see: https://stackoverflow.com/questions/15416662/django-template-loop-over-dictionary-items-with-items-as-key&#xa;    """"""&#xa;    return value.items()&#xa;&#xa;&#xa;@register.filter&#xa;def add_nested_class(value):&#xa;    if isinstance(value, dict):&#xa;        return 'class=nested'&#xa;    if isinstance(value, list) and any([isinstance(item, (list, dict)) for item in value]):&#xa;        return 'class=nested'&#xa;    return ''&#xa;&#xa;&#xa;# Bunch of stuff cloned from urlize&#xa;TRAILING_PUNCTUATION = ['.', ',', ':', ';', '.)', '""', ""']"", ""'}"", ""'""]&#xa;WRAPPING_PUNCTUATION = [('(', ')'), ('<', '>'), ('[', ']'), ('&lt;', '&gt;'),&#xa;                        ('""', '""'), (""'"", ""'"")]&#xa;word_split_re = re.compile(r'(\s+)')&#xa;simple_url_re = re.compile(r'^https?://\[?\w', re.IGNORECASE)&#xa;simple_url_2_re = re.compile(r'^www\.|^(?!http)\w[^@]+\.(com|edu|gov|int|mil|net|org)$', re.IGNORECASE)&#xa;simple_email_re = re.compile(r'^\S+@\S+\.\S+$')&#xa;&#xa;&#xa;def smart_urlquote_wrapper(matched_url):&#xa;    """"""&#xa;    Simple wrapper for smart_urlquote. ValueError(""Invalid IPv6 URL"") can&#xa;    be raised here, see issue #1386&#xa;    """"""&#xa;    try:&#xa;        return smart_urlquote(matched_url)&#xa;    except ValueError:&#xa;        return None&#xa;&#xa;&#xa;@register.filter&#xa;def urlize_quoted_links(text, trim_url_limit=None, nofollow=True, autoescape=True):&#xa;    """"""&#xa;    Converts any URLs in text into clickable links.&#xa;&#xa;    Works on http://, https://, www. links, and also on links ending in one of&#xa;    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).&#xa;    Links can have trailing punctuation (periods, commas, close-parens) and&#xa;    leading punctuation (opening parens) and it'll still do the right thing.&#xa;&#xa;    If trim_url_limit is not None, the URLs in link text longer than this limit&#xa;    will truncated to trim_url_limit-3 characters and appended with an ellipsis.&#xa;&#xa;    If nofollow is True, the URLs in link text will get a rel=""nofollow""&#xa;    attribute.&#xa;&#xa;    If autoescape is True, the link text and URLs will get autoescaped.&#xa;    """"""&#xa;    def trim_url(x, limit=trim_url_limit):&#xa;        return limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x&#xa;&#xa;    safe_input = isinstance(text, SafeData)&#xa;    words = word_split_re.split(force_text(text))&#xa;    for i, word in enumerate(words):&#xa;        if '.' in word or '@' in word or ':' in word:&#xa;            # Deal with punctuation.&#xa;            lead, middle, trail = '', word, ''&#xa;            for punctuation in TRAILING_PUNCTUATION:&#xa;                if middle.endswith(punctuation):&#xa;                    middle = middle[:-len(punctuation)]&#xa;                    trail = punctuation + trail&#xa;            for opening, closing in WRAPPING_PUNCTUATION:&#xa;                if middle.startswith(opening):&#xa;                    middle = middle[len(opening):]&#xa;                    lead = lead + opening&#xa;                # Keep parentheses at the end only if they're balanced.&#xa;                if (&#xa;                    middle.endswith(closing) and&#xa;                    middle.count(closing) == middle.count(opening) + 1&#xa;                ):&#xa;                    middle = middle[:-len(closing)]&#xa;                    trail = closing + trail&#xa;&#xa;            # Make URL we want to point to.&#xa;            url = None&#xa;            nofollow_attr = ' rel=""nofollow""' if nofollow else ''&#xa;            if simple_url_re.match(middle):&#xa;                url = smart_urlquote_wrapper(middle)&#xa;            elif simple_url_2_re.match(middle):&#xa;                url = smart_urlquote_wrapper('http://%s' % middle)&#xa;            elif ':' not in middle and simple_email_re.match(middle):&#xa;                local, domain = middle.rsplit('@', 1)&#xa;                try:&#xa;                    domain = domain.encode('idna').decode('ascii')&#xa;                except UnicodeError:&#xa;                    continue&#xa;                url = 'mailto:%s@%s' % (local, domain)&#xa;                nofollow_attr = ''&#xa;&#xa;            # Make link.&#xa;            if url:&#xa;                trimmed = trim_url(middle)&#xa;                if autoescape and not safe_input:&#xa;                    lead, trail = escape(lead), escape(trail)&#xa;                    url, trimmed = escape(url), escape(trimmed)&#xa;                middle = '<a href=""%s""%s>%s</a>' % (url, nofollow_attr, trimmed)&#xa;                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))&#xa;            else:&#xa;                if safe_input:&#xa;                    words[i] = mark_safe(word)&#xa;                elif autoescape:&#xa;                    words[i] = escape(word)&#xa;        elif safe_input:&#xa;            words[i] = mark_safe(word)&#xa;        elif autoescape:&#xa;            words[i] = escape(word)&#xa;    return ''.join(words)&#xa;&#xa;&#xa;@register.filter&#xa;def break_long_headers(header):&#xa;    """"""&#xa;    Breaks headers longer than 160 characters (~page length)&#xa;    when possible (are comma separated)&#xa;    """"""&#xa;    if len(header) > 160 and ',' in header:&#xa;        header = mark_safe('<br> ' + ', <br>'.join(header.split(',')))&#xa;    return header&#xa;"
12523516|"# (c) 2012, Michael DeHaan <michael.dehaan@gmail.com>&#xa;#&#xa;# This file is part of Ansible&#xa;#&#xa;# Ansible is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Ansible is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.&#xa;from __future__ import (absolute_import, division, print_function)&#xa;__metaclass__ = type&#xa;&#xa;import ast&#xa;import sys&#xa;&#xa;from six.moves import builtins&#xa;&#xa;from ansible import constants as C&#xa;from ansible.plugins import filter_loader&#xa;&#xa;def safe_eval(expr, locals={}, include_exceptions=False):&#xa;    '''&#xa;    This is intended for allowing things like:&#xa;    with_items: a_list_variable&#xa;&#xa;    Where Jinja2 would return a string but we do not want to allow it to&#xa;    call functions (outside of Jinja2, where the env is constrained). If&#xa;    the input data to this function came from an untrusted (remote) source,&#xa;    it should first be run through _clean_data_struct() to ensure the data&#xa;    is further sanitized prior to evaluation.&#xa;&#xa;    Based on:&#xa;    http://stackoverflow.com/questions/12523516/using-ast-and-whitelists-to-make-pythons-eval-safe&#xa;    '''&#xa;&#xa;    # this is the whitelist of AST nodes we are going to&#xa;    # allow in the evaluation. Any node type other than&#xa;    # those listed here will raise an exception in our custom&#xa;    # visitor class defined below.&#xa;    SAFE_NODES = set(&#xa;        (&#xa;            ast.Add,&#xa;            ast.BinOp,&#xa;            ast.Call,&#xa;            ast.Compare,&#xa;            ast.Dict,&#xa;            ast.Div,&#xa;            ast.Expression,&#xa;            ast.List,&#xa;            ast.Load,&#xa;            ast.Mult,&#xa;            ast.Num,&#xa;            ast.Name,&#xa;            ast.Str,&#xa;            ast.Sub,&#xa;            ast.Tuple,&#xa;            ast.UnaryOp,&#xa;        )&#xa;    )&#xa;&#xa;    # AST node types were expanded after 2.6&#xa;    if not sys.version.startswith('2.6'):&#xa;        SAFE_NODES.union(&#xa;            set(&#xa;                (ast.Set,)&#xa;            )&#xa;        )&#xa;&#xa;    filter_list = []&#xa;    for filter in filter_loader.all():&#xa;        filter_list.extend(filter.filters().keys())&#xa;&#xa;    CALL_WHITELIST = C.DEFAULT_CALLABLE_WHITELIST + filter_list&#xa;&#xa;    class CleansingNodeVisitor(ast.NodeVisitor):&#xa;        def generic_visit(self, node, inside_call=False):&#xa;            if type(node) not in SAFE_NODES:&#xa;                raise Exception(""invalid expression (%s)"" % expr)&#xa;            elif isinstance(node, ast.Call):&#xa;                inside_call = True&#xa;            elif isinstance(node, ast.Name) and inside_call:&#xa;                if hasattr(builtins, node.id) and node.id not in CALL_WHITELIST:&#xa;                    raise Exception(""invalid function: %s"" % node.id)&#xa;            # iterate over all child nodes&#xa;            for child_node in ast.iter_child_nodes(node):&#xa;                self.generic_visit(child_node, inside_call)&#xa;&#xa;    if not isinstance(expr, basestring):&#xa;        # already templated to a datastructure, perhaps?&#xa;        if include_exceptions:&#xa;            return (expr, None)&#xa;        return expr&#xa;&#xa;    cnv = CleansingNodeVisitor()&#xa;    try:&#xa;        parsed_tree = ast.parse(expr, mode='eval')&#xa;        cnv.visit(parsed_tree)&#xa;        compiled = compile(parsed_tree, expr, 'eval')&#xa;        result = eval(compiled, {}, locals)&#xa;&#xa;        if include_exceptions:&#xa;            return (result, None)&#xa;        else:&#xa;            return result&#xa;    except SyntaxError as e:&#xa;        # special handling for syntax errors, we just return&#xa;        # the expression string back as-is&#xa;        if include_exceptions:&#xa;            return (expr, None)&#xa;        return expr&#xa;    except Exception as e:&#xa;        if include_exceptions:&#xa;            return (expr, e)&#xa;        return expr&#xa;&#xa;"
13436167|#!/usr/bin/env python&#xa;&#xa;import re&#xa;import json&#xa;&#xa;# https://mathiasbynens.be/notes/javascript-encoding#surrogate-formulae&#xa;# http://stackoverflow.com/a/13436167/96656&#xa;def unisymbol(codePoint):&#xa;	if codePoint >= 0x0000 and codePoint <= 0xFFFF:&#xa;		return unichr(codePoint)&#xa;	elif codePoint >= 0x010000 and codePoint <= 0x10FFFF:&#xa;		highSurrogate = int((codePoint - 0x10000) / 0x400) + 0xD800&#xa;		lowSurrogate = int((codePoint - 0x10000) % 0x400) + 0xDC00&#xa;		return unichr(highSurrogate) + unichr(lowSurrogate)&#xa;	else:&#xa;		return 'Error'&#xa;&#xa;def hexify(codePoint):&#xa;	return 'U+' + hex(codePoint)[2:].upper().zfill(6)&#xa;&#xa;def writeFile(filename, contents):&#xa;	print filename&#xa;	with open(filename, 'w') as f:&#xa;		f.write(contents.strip() + '\n')&#xa;&#xa;data = []&#xa;for codePoint in range(0x000000, 0x10FFFF + 1):&#xa;	# Skip non-scalar values.&#xa;	if codePoint >= 0xD800 and codePoint <= 0xDFFF:&#xa;		continue&#xa;	symbol = unisymbol(codePoint)&#xa;	# http://stackoverflow.com/a/17199950/96656&#xa;	bytes = symbol.encode('utf8').decode('latin1')&#xa;	data.append({&#xa;		'codePoint': codePoint,&#xa;		'decoded': symbol,&#xa;		'encoded': bytes&#xa;	});&#xa;&#xa;jsonData = json.dumps(data, sort_keys=False, indent=2, separators=(',', ': '))&#xa;# Use tabs instead of double spaces for indentation&#xa;jsonData = jsonData.replace('  ', '\t')&#xa;# Escape hexadecimal digits in escape sequences&#xa;jsonData = re.sub(&#xa;	r'\\u([a-fA-F0-9]{4})',&#xa;	lambda match: r'\u{}'.format(match.group(1).upper()),&#xa;	jsonData&#xa;)&#xa;&#xa;writeFile('data.json', jsonData)&#xa;
4474395|"# Copyright (c) 2016, Diego Alonso Cortez, diego@mathisart.org&#xa;# Code released under the BSD 2-Clause License&#xa;&#xa;""""""# mathIsART utilities!&#xa;&#xa;**mathisart.util v0.0.5**&#xa;&#xa;Welcome to the mathIsART utilities library! Just some helper functions, nothing fancy.&#xa;&#xa;## Index of functions:&#xa;&#xa;**Name**                **Short description**&#xa;printz                  A printing format I use way too much...&#xa;properties              See basic metadata of objects, specially arrays/lists&#xa;timeit                  Time execution of code. Call using `with`&#xa;silence                 A bazooka that silences all errors inside a `with` block&#xa;cartesian               Cartesian product of 1-arrays. Efficient implmementation&#xa;add_row                 NumPy's equivalent of: 'Add a row to your spreadsheet!'&#xa;add_col                 NumPy's equivalent of: 'Add a column to your spreadsheet!'&#xa;save                    Save a NumPy array as .npy or .7z&#xa;load                    Load a NumPy array, either from .npy or .7z&#xa;imshow                  Show images (using OpenCV)&#xa;palette                 A bunch of colors I like&#xa;hex_to_rgb              Convert a hex string to an RGB 3-tuple&#xa;rgb_to_hex              Convert an RGB 3-tuple to a hex string&#xa;&#xa;## Index of classes:&#xa;&#xa;Infix                   Ugly hack to get (non-Pythonic) infix operators&#xa;Bunch                   A ""list"" indexed by an arbitrary set (not just the natural numbers)&#xa;abstractstaticmethod    Call on abstract methods to force their concretizations to be static&#xa;singleton               Singleton design pattern, decorator form&#xa;Singleton               Singleton design pattern, metaclass form&#xa;&#xa;## Definitions/notation:&#xa;&#xa;Let *n* be a **nonnegative integer**. Then:&#xa;&#xa;- An **n-list** is a **sequence with n elements**, indexed from 0 to n-1. Equivalently, it's an&#xa;n-dimensional vector, ie. an element of an n-dimensional vector space. In Python, an n-list may&#xa;be implemented as a `list()` or as a `numpy.ndarray` of shape `(n,)`&#xa;- An **n-array** is an **n-dimensional array**, meaning its **SHAPE** is an *n-dimensional vector*&#xa;- A **matrix** is a **2-array**&#xa;&#xa;**Remarks**:&#xa;&#xa;    - An n-list is a 1-array&#xa;    - Notice that an array of shape `(n,)` is *not* an n-array, but rather a 1-array. An&#xa;    n-dimensional array corresponds usually to a vector of much higher dimension. Eg. a 1080 x 1920&#xa;    matrix (corresponding to a Full HD grayscale image) is a 2-array because its *shape* is&#xa;    `(1080, 1920)` and so its *shape* is a 2-dimensional vector, but the matrix itself is a&#xa;    2,073,600-dimensional vector&#xa;""""""&#xa;&#xa;import numpy as np     # none(4x4, 2.5s), blosc(4x4,3.4s)&#xa;import inspect&#xa;import contextlib      # AWESOME with statements!&#xa;import shutil          # To check if something is in the PATH&#xa;import time            # For the timeit() function!&#xa;import os&#xa;import functools&#xa;import sys&#xa;import subprocess&#xa;&#xa;# Try to import non-standard libraries&#xa;with contextlib.suppress(ImportError):&#xa;    import cv2  # mathisart.util.imshow uses this&#xa;&#xa;np.set_printoptions(precision=2)&#xa;&#xa;&#xa;###################################################################################################&#xa;def printz(*args, sep=False):&#xa;    """"""Print each arg in a new line.&#xa;&#xa;    Args:&#xa;        *args: Stuff to be printed&#xa;        sep (bool): Set to True to print a full-line divider between args!&#xa;&#xa;    Returns:&#xa;        None&#xa;    """"""&#xa;    end = '\n' + 79*'-' + '\n\n'&#xa;    if sep:&#xa;        print(*args, sep='\n' + 79*'-' + '\n', end=end)&#xa;    else:&#xa;        print(*args, sep='\n', end=end)&#xa;&#xa;&#xa;def properties(*objects, show=False):&#xa;    """"""A catch-all attribute-printing function. Input any object!&#xa;&#xa;    Args:&#xa;        objects: Comma-separated objects&#xa;        show (bool, optional): Set to False to not print the object itself, nor its base!&#xa;&#xa;    Returns:&#xa;        None&#xa;&#xa;    TODO: make this less ugly, while keeping independent error-checking of each statement&#xa;    """"""&#xa;    for obj in objects:&#xa;&#xa;        print('PROPERTIES of object:')&#xa;        print(type(obj))&#xa;&#xa;        with silence():&#xa;            print('Shape:\t\t', obj.shape)&#xa;        with silence():&#xa;            print('dtype:\t\t', obj.dtype)&#xa;        with silence():&#xa;            print('Size:\t\t', obj.size)&#xa;        with silence():&#xa;            print('Bytes:\t\t', obj.nbytes)&#xa;        with silence():&#xa;            print('Sys bytes:\t', sys.getsizeof(obj))&#xa;        with silence():&#xa;            print('Dimension:\t', obj.ndim)&#xa;        with silence():&#xa;            print('Itemsize:\t', obj.itemsize)&#xa;        with silence():&#xa;            print('Strides:\t', obj.strides)&#xa;        if show:&#xa;            with silence():&#xa;                print('Base:\t\t', obj.base)&#xa;        with silence():&#xa;            print('Data:\t\t', obj.data)&#xa;        with silence():&#xa;            print('CPU ID:\t\t', id(obj))&#xa;        with silence():&#xa;            print('GPU ID:\t\t', obj.ptr)&#xa;        with silence():&#xa;            print('dbuffer:\t', obj.__array_interface__['data'][0])&#xa;        with silence():&#xa;            print('C_contig:\t', obj.flags.c_contiguous)&#xa;        with silence():&#xa;            print('F_contig:\t', obj.flags.f_contiguous)&#xa;&#xa;        with silence():&#xa;            print('Signature:\t', inspect.signature(obj))&#xa;        with silence():&#xa;            print('MRO:\t\t', obj.__mro__)&#xa;&#xa;        with silence():&#xa;            print('Length:\t\t', len(obj))&#xa;        with silence():&#xa;            print('Length:\t\t', obj.length())&#xa;        with silence():&#xa;            print('Length:\t\t', obj.length)&#xa;&#xa;        if show:&#xa;            print('Object:', obj)&#xa;        print(79*'-', '\n')&#xa;&#xa;&#xa;@contextlib.contextmanager  # Create factory function for *with* context managers&#xa;def timeit():&#xa;    """"""Time execution of code, based on time.clock(). Call using a *with* statement!&#xa;&#xa;    Examples:&#xa;        with timeit():&#xa;            a @ a&#xa;    """"""&#xa;    start = time.clock()&#xa;    yield  # The decorated func must return a generator-iterator&#xa;    print('Operation took: %0.6f seconds!' % (time.clock() - start))&#xa;&#xa;&#xa;def silence(error=Exception):&#xa;    """"""Wrapper for contextlib.suppress(). Call using a *with* statement!&#xa;&#xa;    Examples:&#xa;        with supress():&#xa;            2.shape&#xa;    """"""&#xa;    return contextlib.suppress(error)&#xa;&#xa;&#xa;def cartesian(*arrays, out=None):&#xa;    """"""Generate the n-fold Cartesian product of n 1-arrays. Efficient implementation.&#xa;&#xa;    Args:&#xa;        *arrays: Comma-separated 1-arrays&#xa;&#xa;    Returns:&#xa;        ndarray: A 2-array of shape (s1 * s2 * ... * s3, n)&#xa;&#xa;    Source:&#xa;        http://stackoverflow.com/questions/1208118/using-numpy-to-build-an-array-of-all-combinations-of-two-arrays&#xa;    """"""&#xa;    arrays = [np.asarray(x) for x in arrays]&#xa;    dtype = arrays[0].dtype&#xa;&#xa;    n = np.prod([x.size for x in arrays])&#xa;    if out is None:&#xa;        out = np.zeros([n, len(arrays)], dtype=dtype)&#xa;&#xa;    m = n / arrays[0].size&#xa;    out[:, 0] = np.repeat(arrays[0], m)&#xa;    if arrays[1:]:&#xa;        cartesian(arrays[1:], out=out[0:m, 1:])&#xa;        for j in range(1, arrays[0].size):&#xa;            out[j*m:(j+1)*m, 1:] = out[0:m, 1:]&#xa;    return out&#xa;&#xa;&#xa;def add_row(grid, row):&#xa;    """"""Expand a 2-array across axis 0 (eg. from shape (100, 15) to shape (101, 15)). Excel lingo:&#xa;    'add a new row to your spreadsheet'.&#xa;&#xa;    Args:&#xa;        grid (ndarray): The original grid to which you want to add a row&#xa;        row (ndarray): Row to be appended&#xa;&#xa;    Returns:&#xa;        ndarray&#xa;    """"""&#xa;    if grid.ndim != 2:&#xa;        print('Base grid is not a 2-array!')&#xa;        return&#xa;    elif row.ndim > 2:  # Notice 1 or 2 dimensions can work&#xa;        print('New row is too high-dimensional!')&#xa;        return&#xa;&#xa;    row = row.row(1, row.size)  # From shape (n, ) to (1, n)&#xa;    return np.vstack((grid, row))&#xa;&#xa;&#xa;def add_col(grid, col):&#xa;    """"""Expand a 2-array across axis 1 —eg. from shape (100, 15) to shape (100, 16)—. Excel lingo:&#xa;    'add a new column to your spreadsheet'&#xa;&#xa;    Args:&#xa;        grid (ndarray): The original grid to which you want to add a column&#xa;        col (ndarray): The column to be appended&#xa;&#xa;    Returns:&#xa;        ndarray&#xa;    """"""&#xa;    if grid.ndim != 2:&#xa;        print('Base grid is not a 2-array!')&#xa;        return&#xa;    elif col.ndim > 2:  # Notice 1 or 2 dimensions can work&#xa;        print('New column is too high-dimensional!')&#xa;        return&#xa;&#xa;    col = col.reshape(col.size, 1)  # From shape (n, ) to (n, 1)&#xa;    return np.hstack((grid, col))&#xa;&#xa;&#xa;def save_ndarray(ndarray, filename, compress=False):&#xa;    """"""Store a single NumPy array on an .npy binary file. Tries to keep allow_pickle at False.&#xa;    Optional multithreaded compression using 7-zip (needs to be in the PATH).&#xa;&#xa;    Args:&#xa;        ndarray (ndarray): Array to be stored on disk.&#xa;        filename (str): The file's name on disk. No need for .npy extension&#xa;        compress (bool, optional): Set to True to save array as .npy and then compress it to .7z&#xa;&#xa;    Returns:&#xa;        None&#xa;    """"""&#xa;    if filename.endswith('.npy'):&#xa;        file_sans = filename[:-4]&#xa;    else:&#xa;        file_sans = filename  # String assignments are always copies!&#xa;        filename += '.npy'&#xa;    print('Saving {0!s}-array ({1:,} bytes) to disk...' .format(ndarray.shape, ndarray.nbytes))&#xa;&#xa;    if compress:&#xa;        status_compression = 'compressed'&#xa;        if shutil.which('7z') is None:&#xa;            raise FileNotFoundError('7z not found on the PATH!')&#xa;&#xa;        try:&#xa;            np.save(filename, ndarray, allow_pickle=False)&#xa;            status_pickle = 'unpickled'&#xa;        except ValueError:&#xa;            np.save(filename, ndarray, allow_pickle=True)&#xa;            status_pickle = 'pickled'&#xa;&#xa;        subprocess.Popen('7z a ' + file_sans + '.7z ' + file_sans + '.npy -mmt', shell=True).wait()&#xa;        os.remove(filename)&#xa;        filename = file_sans + '.7z '&#xa;        print()&#xa;&#xa;    else:&#xa;        status_compression = 'uncompressed'&#xa;        try:&#xa;            np.save(filename, ndarray, allow_pickle=False)&#xa;            status_pickle = 'unpickled'&#xa;        except ValueError:&#xa;            np.save(filename, ndarray, allow_pickle=True)&#xa;            status_pickle = 'pickled'&#xa;&#xa;    print('Successfully saved {0!s}-array ({1:,} bytes) as [{2!s}] in [{3}], [{4}] form!'&#xa;          .format(ndarray.shape, ndarray.nbytes, filename, status_compression, status_pickle))&#xa;&#xa;&#xa;def load_ndarray(filename):&#xa;    """"""Load a NumPy array from disk into memory, with extension .npy or .7z. If no extension is&#xa;    included in the argument, first assume it's .npy, then .7z.&#xa;&#xa;    Args:&#xa;        filename (str): Name of the NumPy array (in disk).&#xa;&#xa;    Returns:&#xa;        ndarray&#xa;    """"""&#xa;    if filename.endswith('.npy'):&#xa;        compress = False&#xa;        status = 'uncompressed'  # Everything OK!&#xa;    elif filename.endswith('.7z'):&#xa;        compress = True&#xa;        status = 'compressed'&#xa;    else:&#xa;        file_npy = filename + '.npy'&#xa;        if file_npy in os.listdir():&#xa;            filename = file_npy&#xa;            compress = False&#xa;            status = 'uncompressed'&#xa;        else:&#xa;            file_7z = filename + '.7z'&#xa;            if file_7z in os.listdir():&#xa;                filename = file_7z&#xa;                compress = True&#xa;                status = 'compressed'&#xa;            else:&#xa;                raise FileNotFoundError&#xa;&#xa;    # ---------------------------------&#xa;    size = os.stat(filename).st_size&#xa;    print('Loading {0:,} [{1}] bytes from disk... File: {2}'&#xa;          .format(size, status, filename))&#xa;&#xa;    if compress:&#xa;        if shutil.which('7z') is None:&#xa;            raise FileNotFoundError('7z not found on the PATH!')&#xa;        subprocess.Popen('7z e ' + filename + ' -mmt', shell=True).wait()&#xa;        ndarray = np.load(filename[:-3] + '.npy')&#xa;    else:&#xa;        ndarray = np.load(filename)&#xa;        print('Succesfully loaded {0!s}-array ({1:,} bytes) from {2}!'&#xa;              .format(ndarray.shape, size, filename))&#xa;&#xa;    return ndarray&#xa;&#xa;&#xa;def imshow(*images):&#xa;    """"""Show images! Uses OpenCV.""""""&#xa;&#xa;    print('Showing %d image(s)...' % (len(images)))&#xa;&#xa;    for i, img in enumerate(images):&#xa;        print('Image %d of shape %s, dtype %s.' % (i + 1, img.shape, img.dtype))&#xa;        print(img, '\n')&#xa;        cv2.imshow('Image ' + str(i + 1), img)&#xa;    print('Press Q to close image(s)...')&#xa;    cv2.waitKey(0)&#xa;&#xa;&#xa;def palette(color=None, index=0, space='rgba'):&#xa;    """"""A collection of colors! Input color name, return color RGBA (default). Supported color&#xa;    spaces: RGB, RGBA, hex.&#xa;&#xa;    Examples:&#xa;        palette()  # Random color from main collection&#xa;        palette('red')  # Red color, main collection&#xa;        palette('mono2', 2)  # third color, mono2 collection&#xa;&#xa;    TODO: Add support for more color spaces.&#xa;    """"""&#xa;&#xa;    # Main collection, indexed by name!&#xa;    single = Bunch(&#xa;        main='#2196f3',&#xa;        alt='#0088cb',&#xa;        black='#222',&#xa;        red='#e74c3c',&#xa;        green='#2ecc71',&#xa;        blue='#3498db',&#xa;        purple='#9b59b6',&#xa;        darkblue='#34495e',&#xa;        orange='#e67e22',&#xa;        yellow='#f1c40f')&#xa;&#xa;    # Other collections, indexed by number!&#xa;    multi = Bunch(&#xa;        mono1=['#00557f', '#006698', '#0077b2', '#0088cb', '#0099e5',&#xa;               '#00aafe', '#19b3ff'],&#xa;        mono2=['#00567f', '#4cc5ff', '#00Acff', '#26627f'],&#xa;        analogous=['#0dafff', '#0c9fe8', '#00abff', '#0c9fe8'],&#xa;        darker1=['#007ab6', '#006ca2', '#005f8e', '#005179', '#004465',&#xa;                 '#003651', '#00283c', '#001b28', '#000d14'],&#xa;        darker2=['#0c83e1', '#0b79ce', '#0a6dbb', '#0962a9', '#085796',&#xa;                 '#074c84', '#064271', '#05375e'],&#xa;        lighter1=['#1993d0', '#329fd5', '#4cabda', '#66b7df', '#7fc3e5',&#xa;                  '#99cfea', '#b2dbef', '#cce7f4', '#e5f3f9'],&#xa;        lighter2=['#eef7fe', '#dbeefd', '#c8e5fc', '#b6dcfb', '#a3d4fa',&#xa;                  '#91cbf9', '#7ec2f8', '#6bb9f7', '#59b0f6'])&#xa;&#xa;    if color is None:  # If you don't know what you want, anything is good&#xa;        color = np.random.choice(list(single.values()))&#xa;    else:&#xa;        try:  # First look on the first palette&#xa;            color = single[color]&#xa;        except KeyError:  # Then look on the second one&#xa;            color = multi[color][index]&#xa;&#xa;    space = space.lower()  # Make string a tad safer&#xa;    if space == 'rgba':&#xa;        return hex_to_rgba(color)&#xa;    elif space == 'hex':&#xa;        return color&#xa;    elif space == 'rgb':&#xa;        return hex_to_rgb(color)&#xa;    elif space in {'hsv', 'hsl', 'lab'}:&#xa;        raise NotImplementedError  # TODO&#xa;    else:&#xa;        raise AttributeError('Invalid color space!')&#xa;&#xa;&#xa;def hex_to_rgb(color):&#xa;    """"""Hex string to RGB 3-list in one fell swoop! Like Macduff.""""""&#xa;    hx = color.lstrip('#')  # Remove the hash symbol!&#xa;&#xa;    if len(hx) == 3:  # If hex is in shortform, expand it to fullform&#xa;        hx = hx[0]*2 + hx[1]*2 + hx[2]*2&#xa;&#xa;    color = [int(hx[i:i+2], 16) for i in (0, 2, 4)]&#xa;    return np.array(color)&#xa;&#xa;&#xa;def hex_to_rgba(hx):&#xa;    """"""Hex string to RGB 4-list in one fell swoop!""""""&#xa;    return np.append(hex_to_rgb(hx), 1)&#xa;&#xa;&#xa;def rgb_to_hex(color):&#xa;    """"""RGB 3-list or RGBA 4-list to hex string! But ignore alpha channel.""""""&#xa;    color = tuple((color[0], color[1], color[2]))  # remove alpha channel!&#xa;    return '#%02x%02x%02x' % color&#xa;&#xa;&#xa;###################################################################################################&#xa;""""""Experimental functions!""""""&#xa;&#xa;&#xa;def cache(function):&#xa;    """"""Cache results of static functions (ie. function that always return the same value)! Call as&#xa;    a decorator!&#xa;&#xa;    Source: Raymond Hettinger&#xa;        https://www.youtube.com/watch?v=OSGv2VnC0go&#xa;&#xa;    Example:&#xa;        @cache&#xa;        def web_lookup(url):&#xa;            return urllib.urlopen(url).read()&#xa;&#xa;    TODO: make it work&#xa;    """"""&#xa;    saved = {}&#xa;&#xa;    # functools.wraps allows decorated functions to keep their own docstrings!&#xa;    @functools.wraps(function)&#xa;    def wrapper(*args):&#xa;        if args in saved:&#xa;            return wrapper(*args)&#xa;        result = function(*args)&#xa;        saved[args] = result&#xa;        return result&#xa;    return wrapper&#xa;&#xa;&#xa;def os_filenames(path, extension='jpg'):&#xa;    """"""Returns a list of filenames of a given extension in a directory.""""""&#xa;    ext = extension.lower()  # Sanitize a bit&#xa;    return [os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)]&#xa;&#xa;&#xa;def bytes_from_file(filename, chunksize=8192):&#xa;    """"""Read binary files efficiently!&#xa;&#xa;    Source:&#xa;        http://stackoverflow.com/questions/1035340/reading-binary-file-in-python-and-looping-over-each-byte&#xa;    """"""&#xa;    with open(filename, 'rb') as f:&#xa;        while True:&#xa;            chunk = f.read(chunksize)&#xa;            if chunk:&#xa;                for b in chunk:&#xa;                    yield b&#xa;            else:&#xa;                break&#xa;&#xa;&#xa;def invert_color(img):&#xa;    return 255 - img&#xa;&#xa;&#xa;def mapz(function, list_of_args):&#xa;    """"""Just the *map* function with the *list* function applied afterwards. Ah, GwR...""""""&#xa;    return list(map(function, list_of_args))&#xa;&#xa;&#xa;###################################################################################################&#xa;class Infix:&#xa;    """"""Definition of an infix operator class. Call as a decorator. The new operator can't be a&#xa;    Python operator. '|' is the operator with the lowest priority still practical to use.&#xa;&#xa;    Examples:&#xa;&#xa;        # 2-fold product&#xa;        @Infix&#xa;        def x(a, b):&#xa;            return a * b&#xa;        c = 2 |x| 4&#xa;        print(c)&#xa;        COUT: 8&#xa;&#xa;        # Functional programming&#xa;        def curry(f,x):&#xa;            def curried_function(*args, **kw):&#xa;                return f(*((x,)+args),**kw)&#xa;            return curried_function&#xa;        curry=Infix(curry)&#xa;&#xa;        # More stuff&#xa;        add5= operator.add |curry| 5&#xa;        print add5(6)&#xa;        COUT: 11&#xa;&#xa;    Source:&#xa;        http://code.activestate.com/recipes/384122/&#xa;    """"""&#xa;&#xa;    def __init__(self, function):&#xa;        self.function = function&#xa;&#xa;    def __or__(self, other):&#xa;        return self.function(other)&#xa;&#xa;    def __ror__(self, other):&#xa;        # partial() turns a func of n args into func of n-k args&#xa;        return Infix(functools.partial(self.function, other))&#xa;&#xa;    def __call__(self, value1, value2):&#xa;        return self.function(value1, value2)&#xa;&#xa;&#xa;class Bunch(dict):&#xa;    """"""A dict allowing access using method notation and dict notation! Sexier printing for the&#xa;    whole dict.&#xa;&#xa;    Examples:&#xa;        data = Bunch(bananas=10, unicorns=1337)&#xa;        print(data.bananas)&#xa;&#xa;    Source:&#xa;        http://code.activestate.com/recipes/52308/&#xa;&#xa;    TODO: accept zip argument, like a standard dict. Why doesn't it already...?&#xa;    """"""&#xa;&#xa;    def __init__(self, **kwargs):&#xa;        dict.__init__(self, kwargs)  # Access a Bunch object like a dict!&#xa;        self.__dict__ = self&#xa;&#xa;    def __str__(self):&#xa;        """"""When we print self, we get a nice string representation!""""""&#xa;        state = [""%s=%r"" % (attribute, value)&#xa;                 for (attribute, value)&#xa;                 in self.__dict__.items()]&#xa;        return '\n'.join(state)&#xa;&#xa;&#xa;class abstractstaticmethod(staticmethod):&#xa;    """"""Use this decorator on methods of an abstract base class to force them to be static on every&#xa;    subclass! Now there's *no* need to *also* call the @staticmethod decorator on them!&#xa;&#xa;    Examples:&#xa;        class MyAbstractClass(metaclass=abc.ABCMeta):&#xa;            @abstractstaticmethod&#xa;            def foo():&#xa;&#xa;        class Child(MyAbstractClass):&#xa;            # Now this method will be static even *without* the @staticmethod decorator!&#xa;            def foo():&#xa;                print(5)&#xa;&#xa;        child = Child&#xa;        child.foo()&#xa;&#xa;    Source:&#xa;        http://stackoverflow.com/questions/4474395/staticmethod-and-abc-abstractmethod-will-it-blend&#xa;    """"""&#xa;    __slots__ = ()&#xa;&#xa;    def __init__(self, function):&#xa;        super(abstractstaticmethod, self).__init__(function)&#xa;        function.__isabstractmethod__ = True&#xa;&#xa;    __isabstractmethod__ = True&#xa;&#xa;&#xa;def singleton(cls):&#xa;    """"""The singleton decorator! Decorate your classes with this to make them singletons.&#xa;&#xa;    Examples:&#xa;        @singleton&#xa;        class IWantThisClassToBeASingleton:&#xa;            pass&#xa;&#xa;        x = IWantToBeASingleton()&#xa;        y = IWantToBeASingleton()&#xa;        print(id(x) == id(y))  # Prints True!&#xa;&#xa;    Source:&#xa;        http://intermediatepythonista.com/metaclasses-abc-class-decorators&#xa;    """"""&#xa;    instances = {}&#xa;&#xa;    def get_instance():&#xa;        if cls not in instances:&#xa;            instances[cls] = cls()&#xa;        return instances[cls]&#xa;&#xa;    return get_instance&#xa;&#xa;&#xa;class Singleton(type):&#xa;    """"""The Singleton metaclass! Inherit from this class to become a singleton!&#xa;&#xa;    Examples:&#xa;        class IWantToBeASingleton(metaclass=Singleton):&#xa;            pass&#xa;&#xa;        x = IWantToBeASingleton()&#xa;        y = IWantToBeASingleton()&#xa;        print(id(x) == id(y))  # Prints True!&#xa;    """"""&#xa;    _instances = {}&#xa;&#xa;    def __call__(cls, *args, **kwargs):&#xa;        if cls not in cls._instances:&#xa;            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)&#xa;        return cls._instances[cls]&#xa;&#xa;&#xa;###################################################################################################&#xa;""""""# mathisart.triangles v0.0.1&#xa;&#xa;Welcome to the mathIsART Triangles module! Some helper functions for computation triangles.&#xa;Initially intended for Euclidean 2-space, eventually it should include general n-dimensional&#xa;inner product spaces over (non)Archimedean fields.&#xa;&#xa;""""""&#xa;&#xa;import math&#xa;&#xa;&#xa;def long_side(diagonal, aspect_ratio_long=16, aspect_ratio_short=9):&#xa;    numerator = diagonal**2 * aspect_ratio_long**2&#xa;    denominator = aspect_ratio_short**2 + aspect_ratio_long**2&#xa;    return math.sqrt(numerator / denominator)&#xa;&#xa;&#xa;def short_side(diagonal, aspect_ratio_long=16, aspect_ratio_short=9):&#xa;    numerator = diagonal**2 * aspect_ratio_short**2&#xa;    denominator = aspect_ratio_short**2 + aspect_ratio_long**2&#xa;    return math.sqrt(numerator / denominator)&#xa;&#xa;&#xa;###################################################################################################&#xa;""""""Unit tests!!""""""&#xa;&#xa;# if __name__ == '__main__':&#xa;#     a = np.arange(100)&#xa;#     save(a, 'lala', compress=False)&#xa;#     save(a, 'lala.npy', compress=False)&#xa;#     save(a, 'lala', compress=True)&#xa;#     save(a, 'lala.npy', compress=True)&#xa;#     a = load('lala')&#xa;#     a = load('lala.npy')&#xa;#     a = load('lala.7z')&#xa;#     print(a)&#xa;"
6796492|"#!/usr/bin/env python&#xa;&#xa;## \file redirect.py&#xa;#  \brief python package for file redirection &#xa;#  \author T. Lukaczyk, F. Palacios&#xa;#  \version 5.0.0 ""Raven""&#xa;#&#xa;# SU2 Original Developers: Dr. Francisco D. Palacios.&#xa;#                          Dr. Thomas D. Economon.&#xa;#&#xa;# SU2 Developers: Prof. Juan J. Alonso's group at Stanford University.&#xa;#                 Prof. Piero Colonna's group at Delft University of Technology.&#xa;#                 Prof. Nicolas R. Gauger's group at Kaiserslautern University of Technology.&#xa;#                 Prof. Alberto Guardone's group at Polytechnic University of Milan.&#xa;#                 Prof. Rafael Palacios' group at Imperial College London.&#xa;#                 Prof. Edwin van der Weide's group at the University of Twente.&#xa;#                 Prof. Vincent Terrapon's group at the University of Liege.&#xa;#&#xa;# Copyright (C) 2012-2017 SU2, the open-source CFD code.&#xa;#&#xa;# SU2 is free software; you can redistribute it and/or&#xa;# modify it under the terms of the GNU Lesser General Public&#xa;# License as published by the Free Software Foundation; either&#xa;# version 2.1 of the License, or (at your option) any later version.&#xa;#&#xa;# SU2 is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU&#xa;# Lesser General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU Lesser General Public&#xa;# License along with SU2. If not, see <http://www.gnu.org/licenses/>.&#xa;&#xa;# ----------------------------------------------------------------------&#xa;#  Imports&#xa;# ----------------------------------------------------------------------&#xa;&#xa;import os, sys, shutil, copy, glob&#xa;from .tools import add_suffix, make_link, expand_part&#xa;&#xa;# -------------------------------------------------------------------&#xa;#  Output Redirection &#xa;# -------------------------------------------------------------------&#xa;# original source: http://stackoverflow.com/questions/6796492/python-temporarily-redirect-stdout-stderr&#xa;class output(object):&#xa;    ''' with SU2.io.redirect_output(stdout,stderr)&#xa;    &#xa;        Temporarily redirects sys.stdout and sys.stderr when used in&#xa;        a 'with' contextmanager&#xa;        &#xa;        Example:&#xa;        with SU2.io.redirect_output('stdout.txt','stderr.txt'):&#xa;            sys.stdout.write(""standard out"")&#xa;            sys.stderr.write(""stanrard error"")&#xa;            # code&#xa;        #: with output redirection&#xa;        &#xa;        Inputs:&#xa;            stdout - None, a filename, or a file stream&#xa;            stderr - None, a filename, or a file stream&#xa;        None will not redirect outptu&#xa;        &#xa;    '''&#xa;    def __init__(self, stdout=None, stderr=None):&#xa;        &#xa;        _newout = False&#xa;        _newerr = False&#xa;        &#xa;        if isinstance(stdout,str):&#xa;            stdout = open(stdout,'a')&#xa;            _newout = True            &#xa;        if isinstance(stderr,str):&#xa;            stderr = open(stderr,'a')&#xa;            _newerr = True                   &#xa;                &#xa;        self._stdout = stdout or sys.stdout&#xa;        self._stderr = stderr or sys.stderr&#xa;        self._newout = _newout&#xa;        self._newerr = _newerr&#xa;&#xa;    def __enter__(self):&#xa;        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr&#xa;        self.old_stdout.flush(); self.old_stderr.flush()&#xa;        sys.stdout, sys.stderr = self._stdout, self._stderr&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        self._stdout.flush(); self._stderr.flush()&#xa;        sys.stdout = self.old_stdout&#xa;        sys.stderr = self.old_stderr&#xa;        &#xa;        if self._newout:&#xa;            self._stdout.close()&#xa;        if self._newerr:&#xa;            self._stderr.close()           &#xa;&#xa;#: class output()&#xa;&#xa;&#xa;# -------------------------------------------------------------------&#xa;#  Folder Redirection &#xa;# -------------------------------------------------------------------&#xa;class folder(object):&#xa;    ''' with SU2.io.redirect_folder(folder,pull,link,force) as push&#xa;    &#xa;        Temporarily redirects to a working folder, pulling &#xa;        and pushing needed files&#xa;        &#xa;        Example:&#xa;        &#xa;        folder = 'temp'                    &#xa;        pull   = ['file1.txt','file2.txt'] &#xa;        link   = ['file3.big']             &#xa;        force  = True                      &#xa;        &#xa;        # original path&#xa;        import os&#xa;        print os.getcwd()&#xa;        &#xa;        # enter folder&#xa;        with SU2.io.redirect_folder(folder,pull,link,force) as push:&#xa;            print os.getcwd()&#xa;            # code&#xa;            push.append('file4.txt')&#xa;        #: with folder redirection&#xa;        &#xa;        # returned to original path&#xa;        print os.getcwd()&#xa;        &#xa;        Inputs:&#xa;            folder - working folder, relative or absolute&#xa;            pull   - list of files to pull (copy to working folder)&#xa;            link   - list of files to link (symbolic link in working folder)&#xa;            force  - True/False overwrite existing files in working folder&#xa;        &#xa;        Targets:&#xa;            push   - list of files to push (copy to originating path)&#xa;        &#xa;        Notes:&#xa;            push must be appended or extended, not overwritten&#xa;            links in Windows not supported, will simply copy&#xa;    '''&#xa;    &#xa;    def __init__(self, folder, pull=None, link=None, force=True ):&#xa;        ''' folder redirection initialization&#xa;            see help( folder ) for more info&#xa;        '''&#xa;        &#xa;        if pull is None: pull = []&#xa;        if link is None: link = []&#xa;        &#xa;        if not isinstance(pull,list) : pull = [pull]&#xa;        if not isinstance(link,list) : link = [link]&#xa;        &#xa;        origin = os.getcwd()&#xa;        origin = os.path.abspath(origin).rstrip('/')+'/'&#xa;        folder = os.path.abspath(folder).rstrip('/')+'/'&#xa;        &#xa;        self.origin = origin&#xa;        self.folder = folder&#xa;        self.pull   = copy.deepcopy(pull)&#xa;        self.push   = []&#xa;        self.link   = copy.deepcopy(link)&#xa;        self.force  = force&#xa;&#xa;    def __enter__(self): &#xa;        &#xa;        origin = self.origin  # absolute path&#xa;        folder = self.folder  # absolute path&#xa;        pull   = self.pull&#xa;        push   = self.push&#xa;        link   = self.link&#xa;        force  = self.force&#xa;        &#xa;        # check for no folder change&#xa;        if folder == origin:&#xa;            return []&#xa;        &#xa;        # relative folder path&#xa;        #relative = os.path.relpath(folder,origin)&#xa;        &#xa;        # check, make folder&#xa;        if not os.path.exists(folder):&#xa;            os.makedirs(folder)&#xa;        &#xa;        # copy pull files&#xa;        for name in pull:&#xa;            old_name = os.path.abspath(name)&#xa;            new_name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(folder,new_name)&#xa;            if old_name == new_name: continue&#xa;            if os.path.exists( new_name ): &#xa;                if force: os.remove( new_name )&#xa;                else: continue&#xa;            shutil.copy(old_name,new_name)&#xa;&#xa;        # make links&#xa;        for name in link:&#xa;            old_name = os.path.abspath(name)&#xa;            new_name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(folder,new_name)&#xa;            if old_name == new_name: continue&#xa;            if os.path.exists( new_name ): &#xa;                if force: os.remove( new_name )&#xa;                else: continue&#xa;            make_link(old_name,new_name)&#xa;            &#xa;        # change directory&#xa;        os.chdir(folder)&#xa;        &#xa;        # return empty list to append with files to push to super folder&#xa;        return push&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        &#xa;        origin = self.origin&#xa;        folder = self.folder&#xa;        push   = self.push&#xa;        force  = self.force&#xa;        &#xa;        # check for no folder change&#xa;        if folder == origin:&#xa;            return&#xa;        &#xa;        # move assets&#xa;        for name in push:&#xa;            &#xa;            old_name = os.path.abspath(name)&#xa;            name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(origin,name)&#xa;            &#xa;            # links&#xa;            if os.path.islink(old_name):&#xa;                source = os.path.realpath(old_name)&#xa;                if source == new_name: continue&#xa;                if os.path.exists( new_name ):&#xa;                    if force: os.remove( new_name )&#xa;                    else: continue&#xa;                make_link(source,new_name)&#xa;            &#xa;            # moves&#xa;            else:&#xa;                if old_name == new_name: continue&#xa;                if os.path.exists( new_name ):&#xa;                    if force: os.remove( new_name )&#xa;                    else: continue&#xa;                shutil.move(old_name,new_name)&#xa;            &#xa;        # change directory&#xa;        os.chdir(origin)&#xa;        &#xa;#: class folder()&#xa;"
23932488|"# Copyright 2017 QuantRocket - All Rights Reserved&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;&#xa;import six&#xa;import sys&#xa;&#xa;def write_response_to_filepath_or_buffer(filepath_or_buffer, response):&#xa;    """"""&#xa;    Writes the response content to the filepath or buffer.&#xa;    """"""&#xa;    if hasattr(filepath_or_buffer, ""write""):&#xa;        if six.PY3 and filepath_or_buffer is sys.stdout:&#xa;            # Write bytes to stdout (https://stackoverflow.com/a/23932488)&#xa;            filepath_or_buffer = filepath_or_buffer.buffer&#xa;        mode = getattr(filepath_or_buffer, ""mode"", ""w"")&#xa;        for chunk in response.iter_content(chunk_size=1024):&#xa;            if chunk:&#xa;                if ""b"" not in mode and six.PY3:&#xa;                    chunk = chunk.decode(""utf-8"")&#xa;                filepath_or_buffer.write(chunk)&#xa;        if filepath_or_buffer.seekable():&#xa;            filepath_or_buffer.seek(0)&#xa;    else:&#xa;        with open(filepath_or_buffer, ""wb"") as f:&#xa;            for chunk in response.iter_content(chunk_size=1024):&#xa;                if chunk:&#xa;                    f.write(chunk)&#xa;"
22061082|"from lib.test_base import *&#xa;&#xa;&#xa;class TestExtractWorker(TestGeneric):&#xa;&#xa;    def setUp(self):&#xa;&#xa;        super(TestExtractWorker, self).setUp()&#xa;&#xa;        self.test_publish = os.path.join(PROJ_HOME, 'tests/test_integration/stub_data/fulltext_xml_doc_with_acknowledgement.links')&#xa;        self.expected_paths = self.calculate_expected_folders(self.test_publish)&#xa;&#xa;    def tearDown(self):&#xa;&#xa;        self.clean_up_path(self.expected_paths)&#xa;&#xa;        super(TestExtractWorker, self).tearDown()&#xa;&#xa;    def test_extraction_of_non_extracted(self):&#xa;&#xa;        # user loads the list of full text files and publishes them to the first queue&#xa;        records = read_links_from_file(self.test_publish)&#xa;&#xa;        self.helper_get_details(self.test_publish)&#xa;        self.assertEqual(len(records.bibcode), self.nor,&#xa;                         ""The number of records should match the number of lines. It does not: %d [%d]""&#xa;                         % (len(records.bibcode),self.nor))&#xa;&#xa;        # The pipeline converts the input into a payload expected by the workers&#xa;        records.make_payload()&#xa;        self.assertTrue(len(records.payload)>0)&#xa;&#xa;        # External worker publishes the payload created before to the RabbitMQ queue&#xa;        # for the workers to start consuming&#xa;        ret = publish(self.publish_worker, records.payload, exchange='FulltextExtractionExchange',&#xa;                      routing_key='CheckIfExtractRoute')&#xa;        self.assertTrue(ret)&#xa;        time.sleep(10)&#xa;&#xa;        # Worker receives packet of information and checks to see if it needs to be updated&#xa;        ## see: http://stackoverflow.com/questions/22061082/\&#xa;        ## getting-pika-exceptions-connectionclosed-error-while-using-rabbitmq-in-python&#xa;        print('starting check worker...')&#xa;        self.check_worker.run()&#xa;&#xa;        # We pause to simulate the asynchronous running of the workers. This is not needed when the workers&#xa;        # are listening continuously.&#xa;        time.sleep(10)&#xa;&#xa;        # Check to see if the correct number of updates got published to the next queue&#xa;        ## Re-declare the queue with passive flag&#xa;        standard_queue = self.check_worker.channel.queue_declare(&#xa;            queue=""StandardFileExtractorQueue"",&#xa;            passive=True&#xa;            )&#xa;&#xa;        pdf_queue = self.check_worker.channel.queue_declare(&#xa;            queue=""PDFFileExtractorQueue"",&#xa;            passive=True&#xa;            )&#xa;&#xa;        self.assertTrue(standard_queue.method.message_count == self.number_of_standard_files,&#xa;                        ""Standard queue should have at least %d message, but it has: %d"" %&#xa;                        (self.number_of_standard_files, standard_queue.method.message_count))&#xa;        self.assertTrue(pdf_queue.method.message_count == self.number_of_PDFs,&#xa;                        ""PDF queue should have at least %d message, but it has: %d"" %&#xa;                        (self.number_of_PDFs, pdf_queue.method.message_count))&#xa;&#xa;        # Double check with the worker output&#xa;        pdf_res = json.loads(self.check_worker.results[""PDF""])&#xa;        standard_res = json.loads(self.check_worker.results[""Standard""])&#xa;&#xa;        self.assertTrue('NOT_EXTRACTED_BEFORE',&#xa;                        'This should be NOT_EXTRACTED_BEFORE, but is in fact: %s' % standard_res[0][CONSTANTS['UPDATE']])&#xa;&#xa;        if pdf_res:&#xa;            pdf_res = len(pdf_res)&#xa;        else:&#xa;            pdf_res = 0&#xa;&#xa;        self.assertEqual(pdf_res, self.number_of_PDFs, 'Expected number of PDFs: %d' % self.number_of_PDFs)&#xa;        self.assertEqual(len(standard_res), self.number_of_standard_files, 'Expected number of normal formats: %d' %&#xa;                         self.number_of_standard_files)&#xa;&#xa;        # There should be no errors at this stage&#xa;        queue_error = self.check_worker.channel.queue_declare(&#xa;            queue=""ErrorHandlerQueue"",&#xa;            passive=True&#xa;            )&#xa;        self.assertTrue(queue_error.method.message_count == 0,&#xa;                        ""Should be 0, but it is: %d"" % queue_error.method.message_count)&#xa;&#xa;        # Now the next worker collects the list of files that need to be extracted. The Standard&#xa;        # Extractor should extract the content of the given payload and so the number of outputs&#xa;        # should match the number before. Given we don't expect any errors here!&#xa;        print('starting extractor worker')&#xa;        self.standard_worker.run()&#xa;        number_of_standard_files_2 = len(json.loads(self.standard_worker.results))&#xa;        self.assertTrue(number_of_standard_files_2, self.number_of_standard_files)&#xa;&#xa;        # After the extractor, the meta writer should write all the payloads to disk in the correct&#xa;        # folders&#xa;        print('starting meta writer...')&#xa;        self.meta_writer.run()&#xa;&#xa;        time.sleep(5)&#xa;&#xa;        for path in self.expected_paths:&#xa;            self.assertTrue(os.path.exists(os.path.join(path, 'meta.json')), ""Meta file not created: %s"" % path)&#xa;            self.assertTrue(os.path.exists(os.path.join(path, 'fulltext.txt')), ""Full text file not created: %s"" % path)&#xa;            self.assertTrue(os.path.exists(os.path.join(path, 'acknowledgements.txt')), ""Acknowledgements file not created: %s"" % path)&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;    unittest.main()"
319293|"#!/usr/bin/python&#xa;# -*- coding: utf-8 -*-&#xa;&#xa;# LittleBits Hue Controller by Jeremy Blum&#xa;# Copyright 2014 Jeremy Blum, Blum Idea Labs, LLC.&#xa;# http://www.jeremyblum.com&#xa;# File: LittleBits-Hue-Controller.py&#xa;# License: GPL v3 (http://www.gnu.org/licenses/gpl.html)&#xa;&#xa;# Import Libraries&#xa;import ConfigParser, argparse, serial, atexit, os, signal, sys, logging, subprocess, time, re&#xa;from phue import Bridge&#xa;from crontab import CronTab&#xa;from serial.tools import list_ports&#xa;&#xa;# Initialize LittleBits Serial Object so we can exit cleanly&#xa;littleBits = serial.Serial()&#xa;&#xa;# Read config options&#xa;config = ConfigParser.SafeConfigParser(allow_no_value = True)&#xa;config_dataset = config.read(os.path.dirname(os.path.abspath(__file__)) + ""/config.ini"")&#xa;&#xa;# Prevents Exit Traceback&#xa;signal.signal(signal.SIGINT, lambda x,y: sys.exit(0))&#xa;&#xa;# Don't log phue to console&#xa;logger = logging.getLogger('phue')&#xa;logger.setLevel(logging.ERROR)&#xa;logger.propagate = False&#xa;&#xa;# Preconfigured Settings&#xa;baud_rate      = '9600'&#xa;default_bri    = '254'&#xa;default_mood   = '0'&#xa;default_tt_sec = '1'&#xa;mood0          = '0.4448, 0.4066'&#xa;mood1          = '0.5128, 0.4147'&#xa;mood2          = '0.6728, 0.3217'&#xa;mood3          = '0.3151, 0.3252'&#xa;mood4          = '0.4083, 0.5162'&#xa;mood5          = '0.6728, 0.3217'&#xa;mood6          = '0.2870, 0.1075'&#xa;mood7          = '0.1752, 0.0552'&#xa;mood8          = '0.5085, 0.2923'&#xa;mood9          = '0.4083, 0.5162'&#xa;&#xa;# Main program execution&#xa;def main():&#xa;    # Parse optional input arguments&#xa;    parser = argparse.ArgumentParser()&#xa;    parser.add_argument('-s', '--setup', action='store_true', help=""Runs setup Mode"", required=False)&#xa;    args = vars(parser.parse_args())&#xa;&#xa;    # Enter Setup Mode&#xa;    if args.has_key('setup') and args['setup']:&#xa;        setup()&#xa;&#xa;    # Run in normal continuous mode, waiting for incoming data&#xa;    else:&#xa;        # Check to see if config file exists&#xa;        if not config_dataset:&#xa;            print 'Config file not found or empty. Please run with -s argument to generate a config file.'&#xa;            exit()&#xa;        loop()&#xa;&#xa;# Runs setup mode for this script&#xa;def setup():&#xa;    # Intro&#xa;    print ''&#xa;    print 'Welcome to the LittleBits Hue Controller Setup!'&#xa;    print 'This program is open source, so feel free to hack it.'&#xa;    print '(c) 2014 Jeremy Blum, Blum Idea Labs (www.jeremyblum.com)'&#xa;&#xa;    print ''&#xa;    print 'Follow the prompts. If you need to change your setup in the future, just run this script in setup mode again.'&#xa;    print 'You can also manually edit the config file that this setup script will generate.'&#xa;&#xa;    # Hue Hub IP Address&#xa;    print ''&#xa;    print 'We need to be able to communicate with your Hue lighting hub.'&#xa;    print 'If it\'s not already, consider setting your hub to a static IP, or a reserved DHCP IP address.'&#xa;    valid_IP = False&#xa;    hub_found = False&#xa;    while not valid_IP or not hub_found:&#xa;        ip = raw_input('Enter the IPv4 Address of your hub (ie. 192.168.0.150): ')&#xa;        if is_valid_ipv4(ip):&#xa;            valid_IP = True&#xa;            print 'Now, go press the ""connect"" button on the top of your hub'&#xa;            raw_input('Once you\'ve done that, hit enter.')&#xa;            print 'Searching for Hub at ' + ip + '...',&#xa;            sys.stdout.flush()&#xa;            try:&#xa;                bridge = Bridge(ip)&#xa;                bridge.connect()&#xa;            except:&#xa;                print 'Failed!'&#xa;                print 'A Hue Bridge could not be found at that address. Try again.'&#xa;            else:&#xa;                print 'Found!'&#xa;                hub_found = True&#xa;        else:&#xa;            print 'IP Address is invalid.'&#xa;&#xa;    # Light Choice&#xa;    print ''&#xa;    print 'Now, we need to choose what lights this will control.'&#xa;    print 'Go apply power only to the lights you want this to control.'&#xa;    print 'Switch off, unplug, or unscrew Hue lights that you DON\'T want to control.'&#xa;    raw_input('Press enter once you\'ve done that...')&#xa;    print 'Allowing 10 seconds for the hue api to refresh...',&#xa;    sys.stdout.flush()&#xa;    time.sleep(10)&#xa;    print 'Done.'&#xa;    light_ids = hue_get_active_light_ids(bridge)&#xa;    light_names = hue_get_light_names(bridge, light_ids)&#xa;    group_id = hue_get_group_id(bridge, light_ids)&#xa;    print 'Great, lighting group ' + str(group_id) + ' has been added.'&#xa;    print 'We\'ll be controlling lights with these IDs/Names:'&#xa;    for light_id, light_name in zip(light_ids, light_names):&#xa;            print 'Light ID: ' + str(light_id) + ' - ' + light_name&#xa;    # TODO: Add some error checking (list length zero, for example)&#xa;&#xa;    # Saving Config&#xa;    print ''&#xa;    print 'Writing setup info to config file...',&#xa;    sys.stdout.flush()&#xa;    with open(os.path.dirname(os.path.abspath(__file__)) + ""/config.ini"", 'w') as f:&#xa;            write_config_header(f)&#xa;            if not config.has_section('LittleBits'): config.add_section('LittleBits')&#xa;            if not config.has_option('LittleBits', 'baud_rate'): config.set('LittleBits', 'baud_rate', baud_rate)&#xa;&#xa;            if not config.has_section('PhilipsHue'): config.add_section('PhilipsHue')&#xa;            config.set('PhilipsHue', 'bridge_ip', ip)&#xa;            config.set('PhilipsHue', 'group_id',  group_id)&#xa;            if not config.has_option('PhilipsHue', 'default_bri'):    config.set('PhilipsHue', 'default_bri',    default_bri)&#xa;            if not config.has_option('PhilipsHue', 'default_mood'):   config.set('PhilipsHue', 'default_mood',   default_mood)&#xa;            if not config.has_option('PhilipsHue', 'default_tt_sec'): config.set('PhilipsHue', 'default_tt_sec', default_tt_sec)&#xa;&#xa;            if not config.has_section('HueMoods'): config.add_section('HueMoods')&#xa;            if not config.has_option('HueMoods', '0'): config.set('HueMoods', '0', mood0)&#xa;            if not config.has_option('HueMoods', '1'): config.set('HueMoods', '1', mood1)&#xa;            if not config.has_option('HueMoods', '2'): config.set('HueMoods', '2', mood2)&#xa;            if not config.has_option('HueMoods', '3'): config.set('HueMoods', '3', mood3)&#xa;            if not config.has_option('HueMoods', '4'): config.set('HueMoods', '4', mood4)&#xa;            if not config.has_option('HueMoods', '5'): config.set('HueMoods', '5', mood5)&#xa;            if not config.has_option('HueMoods', '6'): config.set('HueMoods', '6', mood6)&#xa;            if not config.has_option('HueMoods', '7'): config.set('HueMoods', '7', mood7)&#xa;            if not config.has_option('HueMoods', '8'): config.set('HueMoods', '8', mood8)&#xa;            if not config.has_option('HueMoods', '9'): config.set('HueMoods', '9', mood9)&#xa;&#xa;            config.write(f)&#xa;    print 'Done!'&#xa;&#xa;    # Make script run at system boot in background&#xa;    print ''&#xa;    print 'Setting up cron service to launch the service at boot...',&#xa;    sys.stdout.flush()&#xa;    cron = CronTab(user=True)&#xa;    cron.remove_all(comment='littlebits')&#xa;    cron_command = os.path.abspath(__file__)&#xa;    job = cron.new(command=cron_command,comment='littlebits')&#xa;    job.enable()&#xa;    job.every_reboot()&#xa;    cron.write()&#xa;    print 'Done!'&#xa;&#xa;    print ''&#xa;    print 'Setup is now complete. The listening service will launch automatically at system boot.'&#xa;    print 'You can test it interactively now by running this script without the -s argument.'&#xa;&#xa;                                                                       &#xa;# Normal Program Execution:&#xa;def loop():        &#xa;&#xa;    # Create Hue Bridge Object&#xa;    bridge = Bridge(config.get('PhilipsHue', 'bridge_ip'))&#xa;&#xa;    # Setup Exit Handler to Kill Serial connection on exit&#xa;    atexit.register(exit_handler)&#xa;&#xa;    print 'Running in Continuous mode...'&#xa;    while True:&#xa;        # Automatically Locate and Connect to LittleBits Controller&#xa;        alert_once = False&#xa;        while not littleBits.isOpen():&#xa;            if not alert_once:&#xa;                alert_once = True&#xa;                print 'Waiting for valid LittleBits controller to be connected via Serial...'&#xa;            connect_to_littleBits()&#xa;        try:&#xa;            trigger = littleBits.readline()&#xa;            data = trigger.strip().split("","")&#xa;            if data[0] == ""on"":&#xa;                result = hue_control(bridge, config.getint('PhilipsHue', 'group_id'), data[0], int(data[1]), int(data[2])*51)&#xa;            elif data[0] == ""off"":&#xa;                result = hue_control(bridge, config.getint('PhilipsHue', 'group_id'), data[0])&#xa;            else:&#xa;                result = ""Malformed Command.""&#xa;            print result&#xa;            littleBits.flush()&#xa;        except serial.SerialException:&#xa;            littleBits.close()&#xa;&#xa;# Control Hue lights.&#xa;# bridge   = bridge object&#xa;# group_id = ID of the light group to control&#xa;# state    = ""on"" or ""off""&#xa;# bri      = brightness (0-255)&#xa;# mood     = mood ID (from 0-9)&#xa;# tt       = transition time (seconds)&#xa;def hue_control(bridge, group_id, state, mood=None, bri=None, tt=None):  &#xa;    if state == ""on"":&#xa;        hue_state = True&#xa;    elif state == ""off"":&#xa;        hue_state = False&#xa;&#xa;    if mood is None:&#xa;        mood = int(config.getint('PhilipsHue', 'default_mood'))&#xa;    if bri is None: &#xa;        bri  = int(config.getint('PhilipsHue', 'default_bri'))&#xa;    if tt is None:&#xa;        tt = int(config.getint('PhilipsHue', 'default_tt_sec'))&#xa;&#xa;    xy = config.get('HueMoods',str(mood))&#xa;    xy = xy.split("", "")&#xa;    xy[0] = float(xy[0])&#xa;    xy[1] = float(xy[1])&#xa;            &#xa;    command =  {'transitiontime' : tt*10,&#xa;                'on'             : hue_state,&#xa;                'bri'            : bri,&#xa;                'xy'             : xy}&#xa;&#xa;    bridge.set_group(group_id, command)&#xa;    result = ""Turned lights "" + state + "".""&#xa;    if state == ""on"":&#xa;        result = result + "" (Brightess = "" + str(bri) + "", Mood = "" + str(mood) + "")""&#xa;            &#xa;    return result&#xa;&#xa;# Returns a list of all the currently connected hue lights&#xa;def hue_get_active_light_ids(bridge):&#xa;    lights = bridge.get_light_objects('id')&#xa;    active_light_ids = []&#xa;    for light_id in lights:&#xa;        if bridge.get_light(light_id,'reachable'):&#xa;            active_light_ids.append(light_id)&#xa;&#xa;    return active_light_ids&#xa;&#xa;# Return list of the light names associated with list of the provided IDs&#xa;def hue_get_light_names(bridge, light_ids):&#xa;    light_names = []&#xa;    for light_id in light_ids:&#xa;        light_names.append(bridge.get_light(light_id, 'name'))&#xa;&#xa;    return light_names&#xa;&#xa;# Checks if ""LittleBits Lights"" group already exists, and deletes and remakes it with the provided light IDs&#xa;# Makes new group if one doesn't exist&#xa;# Returns ID of group&#xa;def hue_get_group_id(bridge, light_ids):&#xa;    GROUP_NAME = ""LittleBits Lights""&#xa;&#xa;    #Delete any groups with this name&#xa;    group_id = bridge.get_group_id_by_name(GROUP_NAME)&#xa;    while group_id:&#xa;        bridge.delete_group(group_id)&#xa;        group_id = bridge.get_group_id_by_name(GROUP_NAME)&#xa;&#xa;    #Create a new group with the desired lights&#xa;    bridge.create_group(GROUP_NAME, light_ids)&#xa;    return bridge.get_group_id_by_name(GROUP_NAME)&#xa;&#xa;# Determine the dev location of the LittleBits Hue Controller&#xa;# Leaves correct device open in global littleBits serial object&#xa;def connect_to_littleBits():&#xa;    IDENTIFIER = ""LittleBits-Hue-Controller""&#xa;    dev_list = list_ports.comports()&#xa;    connected = False&#xa;    for dev in dev_list:&#xa;        try:&#xa;            # In the event at that we try to open while the OS is still registering the new port, we'll have issues&#xa;            littleBits.baudrate = config.getint('LittleBits', 'baud_rate')&#xa;            littleBits.port     = dev[0]&#xa;            littleBits.timeout  = .5 #Allow 500ms for response&#xa;            littleBits.open()&#xa;            littleBits.write('?') # Ask device to identify itself&#xa;            reply = littleBits.readline()&#xa;        except:&#xa;            littleBits.close()&#xa;            time.sleep(.5)&#xa;        else:&#xa;            if reply.rstrip() != IDENTIFIER:&#xa;                littleBits.close()&#xa;            else:&#xa;                print ""Connected to LittleBits on "" + dev[0]&#xa;                connected = True&#xa;        if connected:&#xa;            littleBits.flush()&#xa;            littleBits.timeout  = None&#xa;            break&#xa;&#xa;# Write comment header to config file (pass file handle)&#xa;# Assumes empty file&#xa;def write_config_header(f):&#xa;    f.write('# LittleBits Hue Controller by Jeremy Blum\n')&#xa;    f.write('# Copyright 2014 Jeremy Blum, Blum Idea Labs, LLC.\n')&#xa;    f.write('# http://www.jeremyblum.com\n')&#xa;    f.write('# File: config.ini\n')&#xa;    f.write('# License: GPL v3 (http://www.gnu.org/licenses/gpl.html)\n\n')&#xa;&#xa;# Validates IPv4 addresses&#xa;# http://stackoverflow.com/a/319293&#xa;def is_valid_ipv4(ip):&#xa;    pattern = re.compile(r""""""&#xa;        ^&#xa;        (?:&#xa;          # Dotted variants:&#xa;          (?:&#xa;            # Decimal 1-255 (no leading 0's)&#xa;            [3-9]\d?|2(?:5[0-5]|[0-4]?\d)?|1\d{0,2}&#xa;          |&#xa;            0x0*[0-9a-f]{1,2}  # Hexadecimal 0x0 - 0xFF (possible leading 0's)&#xa;          |&#xa;            0+[1-3]?[0-7]{0,2} # Octal 0 - 0377 (possible leading 0's)&#xa;          )&#xa;          (?:                  # Repeat 0-3 times, separated by a dot&#xa;            \.&#xa;            (?:&#xa;              [3-9]\d?|2(?:5[0-5]|[0-4]?\d)?|1\d{0,2}&#xa;            |&#xa;              0x0*[0-9a-f]{1,2}&#xa;            |&#xa;              0+[1-3]?[0-7]{0,2}&#xa;            )&#xa;          ){0,3}&#xa;        |&#xa;          0x0*[0-9a-f]{1,8}    # Hexadecimal notation, 0x0 - 0xffffffff&#xa;        |&#xa;          0+[0-3]?[0-7]{0,10}  # Octal notation, 0 - 037777777777&#xa;        |&#xa;          # Decimal notation, 1-4294967295:&#xa;          429496729[0-5]|42949672[0-8]\d|4294967[01]\d\d|429496[0-6]\d{3}|&#xa;          42949[0-5]\d{4}|4294[0-8]\d{5}|429[0-3]\d{6}|42[0-8]\d{7}|&#xa;          4[01]\d{8}|[1-3]\d{0,9}|[4-9]\d{0,8}&#xa;        )&#xa;        $&#xa;    """""", re.VERBOSE | re.IGNORECASE)&#xa;    return pattern.match(ip) is not None&#xa;&#xa;# Closes the serial stream when the program exits&#xa;def exit_handler():&#xa;    littleBits.close()&#xa;    print '\nApplication Terminated. Serial Connection Closed.'&#xa;&#xa;# Run the Main funtion when this python script is executed&#xa;if __name__ == '__main__':&#xa;    main()&#xa;"
