postid|data
651794|"'''&#xa;Created on Sep 15, 2012&#xa;&#xa;@author: gaprice@lbl.gov&#xa;'''&#xa;&#xa;from collections import defaultdict as _defaultdict&#xa;&#xa;&#xa;class AutoVivifingDict(dict):&#xa;    """"""Implementation of perl's autovivification feature.""""""&#xa;# see http://stackoverflow.com/questions/651794/whats-the-best-way-to-&#xa;# initialize-a-dict-of-dicts-in-python&#xa;&#xa;    def __getitem__(self, item):&#xa;        try:&#xa;            return dict.__getitem__(self, item)&#xa;        except KeyError:&#xa;            value = self[item] = type(self)()&#xa;            return value&#xa;&#xa;&#xa;class DictListWithSortedIterator(object):&#xa;    ''' Implements a dict-like object, the values of which are lists of&#xa;  objects. The iterator returned from the __iter__ method traverses the lists&#xa;  in order of 1) the sorted list of keys and 2) the order the items were added&#xa;  to the list. Once iteration is started, the DLWSI cannot be modified until&#xa;  the iterator is exhausted or the discard() method is called on the&#xa;  iterator.'''&#xa;&#xa;    def __init__(self):&#xa;        self._store = _defaultdict(list)&#xa;        self._itercount = 0&#xa;        self._len = 0&#xa;&#xa;    def __setitem__(self, key, value):&#xa;        '''Add value to the end of the list stored at key'''&#xa;        self._check_iter_ok()&#xa;        self._store[key].append(value)&#xa;        self._len += 1&#xa;&#xa;    def __delitem__(self, key):&#xa;        '''Delete the list stored at key.'''&#xa;        self._check_iter_ok()&#xa;        if key in self._store:&#xa;            l = len(self._store[key])&#xa;            del self._store[key]&#xa;            self._len -= l&#xa;&#xa;    def __getitem__(self, key):&#xa;        '''Returns the list stored at key as a tuple.'''&#xa;        if self._store[key]:&#xa;            return tuple(self._store[key])&#xa;        else:&#xa;            raise KeyError(str(key))&#xa;&#xa;    def get(self, key, default=None):&#xa;        '''Returns the list stored at key as a tuple. The default argument&#xa;    specifies the object to return if key does not exist (default None).'''&#xa;        if self._store[key]:&#xa;            return tuple(self._store[key])&#xa;        return default&#xa;&#xa;    def keys(self):&#xa;        '''Returns an unsorted list of keys.'''&#xa;        return self._store.keys()&#xa;&#xa;    def __len__(self):&#xa;        return self._len&#xa;&#xa;    def clear(self):&#xa;        '''Removes all keys and values from the DLWSI.'''&#xa;        self._check_iter_ok()&#xa;        self._store = _defaultdict(list)&#xa;        self._len = 0&#xa;&#xa;    def merge(self, dictlist):&#xa;        '''Adds all key value pairs of the passed in DLWSI to this DLWSI. Any&#xa;    keys in this DLWSI that have matching names to keys in the passed in DLWSI&#xa;    will be overwritten.'''&#xa;        self._check_iter_ok()&#xa;        for k, v in dictlist:&#xa;            self.__setitem__(k, v)&#xa;&#xa;    def _check_iter_ok(self):&#xa;        if self._itercount:&#xa;            raise RuntimeError('Attempt to modify while iterating')&#xa;&#xa;    def __iter__(self):&#xa;        '''Returns an iterator over this DLWSI. The iterator proceeds through&#xa;    each list in the order of the sorted keys and returns a key / list item&#xa;    pair for each next call. Thus if a particular key has 3 list items that&#xa;    key will be returned 3 times in succession, once with each list item.&#xa;    The DLWSI cannot be modified while iterating. To allow modification without&#xa;    exhausting the iterator call the discard() method on the iterator.'''&#xa;        if not self._itercount:&#xa;            self._sortedKeys = sorted(self._store.keys())&#xa;        self._itercount += 1&#xa;        return self._ObjIter(self)&#xa;&#xa;    class _ObjIter(object):&#xa;&#xa;        def __init__(self, objStore):&#xa;            self._ostore = objStore&#xa;            self._dictindex = 0&#xa;            self._listindex = -1&#xa;            self._notexhausted = True&#xa;&#xa;        def next(self):&#xa;            if self._notexhausted and self._has_next():&#xa;                self._advance_index()&#xa;                return self._get_current_key_val_tuple()&#xa;            else:&#xa;                self._dec_iter_count()&#xa;                raise StopIteration&#xa;&#xa;        def discard(self):&#xa;            self._dec_iter_count()&#xa;&#xa;        def _dec_iter_count(self):&#xa;            if self._notexhausted:&#xa;                self._ostore._itercount -= 1&#xa;            self._notexhausted = False&#xa;&#xa;        def _has_next(self):&#xa;            if len(self._ostore._store) == 0:&#xa;                return False&#xa;            dictI = self._dictindex&#xa;            if self._listindex + 1 >= len(self._get_list(&#xa;                                          self._get_sorted_key(dictI))):&#xa;                dictI += 1&#xa;            return dictI < len(self._ostore._sortedKeys)&#xa;&#xa;        def _advance_index(self):&#xa;            if self._listindex + 1 >= len(self._get_list(self._get_sorted_key(&#xa;                                                         self._dictindex))):&#xa;                self._dictindex += 1&#xa;                self._listindex = 0&#xa;            else:&#xa;                self._listindex += 1&#xa;&#xa;        def _get_current_key_val_tuple(self):&#xa;            key = self._get_sorted_key(self._dictindex)&#xa;            return key, self._get_list(key)[self._listindex]&#xa;&#xa;        def _get_sorted_key(self, index):&#xa;            return self._ostore._sortedKeys[index]&#xa;&#xa;        def _get_list(self, key):&#xa;            return self._ostore._store[key]&#xa;&#xa;        def __next__(self):&#xa;            return self.next()&#xa;"
952302|"# Copyright 2010 Benjamin Dumke&#xa;# &#xa;# This file is part of Unicornify&#xa;# &#xa;# Unicornify is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU Affero General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;# &#xa;# Unicornify is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;# &#xa;# You should have received a copy of the NU Affero General Public License&#xa;# along with Unicornify; see the file COPYING. If not, see&#xa;# <http://www.gnu.org/licenses/>.&#xa;    &#xa;from math import sin, cos, pi, sqrt&#xa;import functools&#xa;from graphics import hls_to_rgb&#xa;&#xa;class Data(object):&#xa;    def __getattribute__(self, attr):&#xa;        try:&#xa;            return super(Data, self).__getattribute__(attr)&#xa;        except AttributeError:&#xa;            if attr in self._data:&#xa;                return self._data[attr]&#xa;            elif attr.endswith(""_col""):&#xa;                part = attr[:-4]&#xa;                func = lambda lightness: hls_to_rgb(self._data[part + ""_hue""], lightness, self._data[part + ""_sat""])&#xa;                self._data[attr] = func&#xa;                return func&#xa;            else:&#xa;                raise KeyError(""Unknown parameter %s"" % attr)&#xa;        &#xa;    def __setattr__(self, attr, value):&#xa;        if attr == ""_data"":&#xa;            super(Data, self).__setattr__(attr, value)&#xa;        elif attr in self._data:&#xa;            self._data[attr] = value&#xa;        else:&#xa;            raise KeyError(""Unknown parameter %s"" % attr)&#xa;&#xa;&#xa;class Rect(object):&#xa;    def __init__(self, left, top, right, bottom):&#xa;        self.coords = left, top, right, bottom&#xa;        self.left = left&#xa;        self.right = right&#xa;        self.top = top&#xa;        self.bottom = bottom&#xa;        &#xa;    def __add__(self, other):&#xa;        if other is None:&#xa;            return self&#xa;        zipped = zip(self.coords, other.coords)&#xa;        lt = map(min, zipped[:2])&#xa;        rb = map(max, zipped[2:])&#xa;        return Rect(*(lt + rb))&#xa;&#xa;    def __radd__(self, other):&#xa;        return self + other&#xa;        &#xa;    def intersects(self, other):&#xa;        hori = other.left <= self.left <= other.right or other.left <= self.right <= other.right or (self.left <= other.left and self.right >= other.right)&#xa;        vert = other.top <= self.top <= other.bottom or other.top <= self.bottom <= other.bottom or (self.top <= other.top and self.bottom >= other.bottom)&#xa;        return hori and vert&#xa;&#xa;class WorldView(object):&#xa;    """"""Note that projecting does not depend on shift, hence unlike&#xa;       the other parameters, shift may be changed without re-projecting.""""""&#xa;    def __init__(self, angle_y, angle_x, rotation_center, shift):&#xa;        self.angle_y = angle_y&#xa;        self.angle_x = angle_x&#xa;        self.rotation_center = rotation_center&#xa;        self.shift = shift&#xa;&#xa;class Ball(object):&#xa;    def __init__(self, center, radius, color):&#xa;        self.center = tuple(map(float, center))&#xa;        self.radius = float(radius)&#xa;        self.color = color&#xa;        &#xa;        self.projection = None&#xa;&#xa;    def project(self, worldview):&#xa;        rad_y = worldview.angle_y * pi / 180&#xa;        rad_x = worldview.angle_x * pi / 180&#xa;        x1, y1, z1 = (self.center[i] - worldview.rotation_center[i] for i in xrange(3))&#xa;        x2 = x1 * cos(rad_y) - z1 * sin(rad_y)&#xa;        y2 = y1&#xa;        z2 = x1 * sin(rad_y) + z1 * cos(rad_y)&#xa;        x3 = x2&#xa;        y3 = y2 * cos(rad_x) - z2 * sin(rad_x)&#xa;        z3 = y2 * sin(rad_x) + z2 * cos(rad_x)&#xa;        self.projection = tuple(c[0] + c[1] for c in zip((x3, y3, z3), worldview.rotation_center))&#xa;&#xa;    def rotate(self, angle, other, axis = 2):&#xa;        """"""Rotate this ball around the ball ""other"", leaving the ""axis"" coordinate&#xa;           as is. (default z, i.e. rotation is in the x-y-plane)""""""&#xa;        rad = angle * pi / 180&#xa;        swap = [(1, 2, 0), (0, 2, 1), (0, 1, 2)][axis]&#xa;        reverse = [(2, 0, 1), (0, 2, 1), (0, 1, 2)][axis]&#xa;        # the letters are the correct ones for the default case axis = 2&#xa;        x1, y1, z1 = (self.center[i] - other.center[i] for i in swap)&#xa;        x2 = x1 * cos(rad) - y1 * sin(rad)&#xa;        y2 = x1 * sin(rad) + y1 * cos(rad)&#xa;        z2 = z1&#xa;        self.center = tuple((x2, y2, z2)[reverse[i]] + other.center[i] for i in xrange(3))&#xa;           &#xa;    def set_distance(self, distance, other):&#xa;        """"""Move this ball to have the given distance to ""other"" while not changing the&#xa;           direction. This is the distance of the ball centers.""""""&#xa;        span = tuple(c[0] - c[1] for c in zip(self.center, other.center))&#xa;        stretch = distance / sqrt(sum(c * c for c in span))&#xa;        self.center = tuple(c[0] + stretch * c[1] for c in zip(other.center, span))&#xa;        &#xa;    def set_gap(self, gap, other):&#xa;        self.set_distance(gap + self.radius + other.radius, other)&#xa;        &#xa;    def move_to_sphere(self, other):&#xa;        self.set_distance(other.radius, other)&#xa;            &#xa;    def twoD(self):&#xa;        return self.projection[:2]            &#xa;            &#xa;    def draw(self, image, worldview):&#xa;        x, y = map(sum, zip(worldview.shift, self.twoD()))&#xa;        r = self.radius&#xa;        image.circle((x, y), r, self.color)&#xa;        &#xa;    def __sub__(self, other):&#xa;        tup1 = self.center&#xa;        if isinstance(other, Ball):&#xa;            tup2 = other.center&#xa;        else:&#xa;            tup2 = other&#xa;        return tuple(c[0] - c[1] for c in zip(tup1, tup2))&#xa;        &#xa;    def bounding(self):&#xa;        x, y, r = self.twoD() + (self.radius, )&#xa;        return Rect(x - r, y - r, x + r, y + r)&#xa;        &#xa;    def balls(self):&#xa;        yield self&#xa;        &#xa;    def sort(self, worldview):&#xa;        return&#xa;        &#xa;def identity(x):&#xa;    return x&#xa;&#xa;class Bone(object):&#xa;    def __init__(self, ball1, ball2):&#xa;        self._balls = [ball1, ball2]&#xa;        &#xa;    def draw(self, image, worldview, xfunc = identity, yfunc = identity):&#xa;        """"""xfunc and / or yfunc should map [0,1] -> [0,1] if the parameter ""step""&#xa;           should not be applied linearly to the coordinates. Note that these x and&#xa;           y are screen, i.e. 2D, coordinates. This is currently used to make the hair&#xa;           wavy.""""""&#xa;&#xa;        calc = lambda c1, c2, factor: c1 + (c2 - c1) * factor&#xa;        x1, y1 = map(sum, zip(self[0].twoD(), worldview.shift))&#xa;        x2, y2 = map(sum, zip(self[1].twoD(), worldview.shift))&#xa;        steps = max(*map(abs, (x2-x1, y2-y1)))&#xa;        # The centers might be very close, but the radii my be more apart,&#xa;        # hence the following step. without it, the eye/iris gradient sometimes&#xa;        # only has two or three steps&#xa;        steps = max(steps, abs(self[0].radius - self[1].radius))&#xa;        colors = zip(self[0].color, self[1].color)&#xa;&#xa;        # the old way is faster for smaller images, the new one for larger ones.&#xa;        # This table shows test measurings. Note that the size is the final size,&#xa;        # i.e. half the drawing size. O means the old one is faster, N the new one.&#xa;        #&#xa;        # zoom    test hash         32  64  128 256 512&#xa;        # ----------------------------------------------&#xa;        # close   21b96dcc68138     O   N   N   N!  N!&#xa;        # medium  18011847b11145af  O!  O   =   N   N&#xa;        # far     1895854ba5a70     O!  O!  O   =   N&#xa;&#xa;        if xfunc is identity and yfunc is identity:&#xa;            if steps > 80: # based on tests, this number seems roughly to be the break-even point&#xa;                image.connect_circles((x1, y1), self[0].radius, self[0].color, (x2, y2), self[1].radius, self[1].color)&#xa;                return &#xa;&#xa;        for step in xrange(int(steps + 1)):&#xa;            factor = float(step) / steps&#xa;            color = tuple(map(int, (calc(c[0], c[1], factor) for c in colors)))&#xa;            x, y, r = calc(x1, x2, xfunc(factor)), calc(y1, y2, yfunc(factor)), calc(self[0].radius, self[1].radius, factor)&#xa;            image.circle((x, y), r, color)&#xa;&#xa;    def __getitem__(self, index):&#xa;        return self._balls[index]&#xa;        &#xa;    def balls(self):&#xa;        return iter(self._balls)&#xa;        &#xa;    def project(self, worldview):&#xa;        for ball in self._balls:&#xa;            ball.project(worldview)&#xa;            &#xa;    def sort(self, worldview):&#xa;        self._balls.sort(key = lambda ball: ball.projection[2], reverse = True)&#xa;        &#xa;    def span(self):&#xa;        return self[1] - self[0]&#xa;        &#xa;    def bounding(self):&#xa;        return self[0].bounding() + self[1].bounding()&#xa;&#xa;def reverse(func):&#xa;    def result(v):&#xa;        return 1 - func(1 - v)&#xa;    return result&#xa;&#xa;class NonLinBone(Bone):        &#xa;    def __init__(self, ball1, ball2, xfunc = identity, yfunc = identity):&#xa;        self._balls = [ball1, ball2]&#xa;        self._xfunc = xfunc&#xa;        self._yfunc = yfunc&#xa;        &#xa;    def draw(self, image, worldview):&#xa;        super(NonLinBone, self).draw(image, worldview, self._xfunc, self._yfunc)&#xa;&#xa;    def sort(self, worldview):&#xa;        previous = self._balls[:]&#xa;        super(NonLinBone, self).sort(worldview)&#xa;        if previous != self._balls:&#xa;            self._xfunc = reverse(self._xfunc)&#xa;            self._yfunc = reverse(self._yfunc)&#xa;&#xa;def compare(worldview, first, second):&#xa;    """"""Compares two objects (balls or bones) to determine which one is behind the other.&#xa;       Note that although the worldview must be given, the objects still must&#xa;       have already been projected.""""""&#xa;       &#xa;    # FIXME: currently, this should be okay most of the time, because the&#xa;    # only subfigure used so far is unicorn.hairs. &#xa;    if isinstance(first, Figure):&#xa;        return 1&#xa;    elif isinstance(second, Figure):&#xa;        return -1&#xa;    &#xa;       &#xa;    if isinstance(first, Ball) and isinstance(second, Ball):&#xa;        return cmp(first.projection[2], second.projection[2])&#xa;    elif isinstance(first, Bone) and isinstance(second, Ball):&#xa;        # see Definition 1.1 at http://en.wikibooks.org/wiki/Linear_Algebra/Orthogonal_Projection_Into_a_Line&#xa;        span = first.span()&#xa;        lensquare = float(sum(c**2 for c in span))&#xa;        factor = sum(c[0] * c[1] for c in zip(second - first[0], span)) / lensquare&#xa;        if factor < 0:&#xa;            return compare(worldview, first[0], second)&#xa;        elif factor > 1:&#xa;            return compare(worldview, first[1], second)&#xa;        else: # the projection is within the bone&#xa;            proj = tuple(c[0] * factor + c[1] for c in zip(span, first[0].center))&#xa;            proj_ball = Ball(proj, 1, None)&#xa;            proj_ball.project(worldview)&#xa;            return compare(worldview, proj_ball, second)&#xa;    elif isinstance(first, Ball) and isinstance(second, Bone):&#xa;        return -compare(worldview, second, first)&#xa;    elif isinstance(first, Bone) and isinstance(second, Bone):&#xa;        set1 = set(first.balls())&#xa;        set2 = set(second.balls())&#xa;        if set1 & set2: # they share a ball&#xa;            # which bone is longer?&#xa;            l1, l2 = (sum((b[1].projection[i] - b[0].projection[i])**2 for i in (0, 1, 2)) for b in (first, second))&#xa;            if l1 > l2:&#xa;                return compare(worldview, first, (set2 - set1).pop())&#xa;            else:&#xa;                return compare(worldview, (set1 - set2).pop(), second)&#xa;        else:&#xa;            # check for the simple case: is there a pair of balls (one from&#xa;            # first, one from second that we can compare instead?&#xa;            for ball1 in first.balls():&#xa;                for ball2 in second.balls():&#xa;                    if ball1.bounding().intersects(ball2.bounding()):&#xa;                        result = compare(worldview, ball1, ball2)&#xa;                        if result != 0:&#xa;                            return result&#xa;        &#xa;            # find the point where the bones intersect *on the screen*. t1&#xa;            # and t2 are the parameters such that, say, ball1_x + t1 * (ball2_x-ball1_x)&#xa;            # is the x-coordinate refereced by t1. t_ < 0 or t_ > 1 means&#xa;            # that the ""intersection"" isn't on the line itself.&#xa;            &#xa;            s1x, s1y, s1z = first[0].projection&#xa;            d1x, d1y, d1z = (first[1].projection[i] - first[0].projection[i] for i in (0, 1, 2))&#xa;            s2x, s2y, s2z = second[0].projection&#xa;            d2x, d2y, d2z = (second[1].projection[i] - second[0].projection[i] for i in (0, 1, 2))&#xa;            &#xa;            # this number is zero if and only if the lines are parallel&#xa;            # (again, their screen projections -- not neccessarily&#xa;            # parallel in 3d space)&#xa;            denom = d1x * d2y - d2x * d1y&#xa;            &#xa;            if abs(denom) < 1e-4:&#xa;                return 0 #FIXME later?&#xa;                &#xa;            t2 = (d1y * (s2x - s1x) - d1x * (s2y - s1y)) / denom&#xa;            if abs(d1x) > 1e-4:&#xa;                t1 = (s2x + t2 * d2x - s1x) / d1x&#xa;            elif abs(d1y) > 1e-4:&#xa;                t1 = (s2y + t2 * d2y - s1y) / d1y&#xa;            else:&#xa;                return 0 #FIXME later?&#xa;            &#xa;                &#xa;            if t1 < -.5 or t1 > 1.5 or t2 < -1 or t2 > 2:&#xa;                return 0&#xa;                &#xa;                &#xa;            return cmp(s1z + t1 * d1z, s2z + t2 * d2z) #FIXME: zero case?&#xa;    else:&#xa;        raise ValueError(""Can't compare %s and %s"" % (first, second))&#xa;&#xa;def two_combinations(l):&#xa;    """"""itertools.combinations was introduced in Python 2.6; the app engine&#xa;       runs 2.5.""""""&#xa;    for i, first in enumerate(l):&#xa;        for second in l[i+1:]:&#xa;            yield (first, second)&#xa;&#xa;# used in Figure.sort() to determine which thing should be drawn next if&#xa;# it's not possible to fullfill all draw_after constraints&#xa;def evilness(thing):&#xa;    z = thing.projection[2] if isinstance(thing, Ball) else max(b.projection[2] for b in thing.balls())&#xa;    return -z&#xa;&#xa;class Figure(object):&#xa;    def __init__(self):&#xa;        self._things = []&#xa;        &#xa;    def add(self, *things):&#xa;        self._things.extend(things)&#xa;        &#xa;    def project(self, worldview):&#xa;        for thing in self._things:&#xa;            thing.project(worldview)&#xa;            &#xa;    def sort(self, worldview):&#xa;        """"""this assumes that projection has already happened!""""""&#xa;        comp = functools.partial(compare, worldview)&#xa;        &#xa;        # values of this dict are lists of all things that have&#xa;        # to be drawn before the corresponding key&#xa;        draw_after = dict((thing, []) for thing in self._things) &#xa;&#xa;        for first, second in two_combinations(self._things):&#xa;            if second not in draw_after[first] and first not in draw_after[second]:&#xa;                if first.bounding().intersects(second.bounding()):&#xa;                    c = comp(first, second)&#xa;                    if c < 0:&#xa;                        # first is in front of second&#xa;                        draw_after[first].append(second)&#xa;                    elif c > 0:&#xa;                        draw_after[second].append(first)&#xa;        &#xa;        # this is pretty much the algorithm from http://stackoverflow.com/questions/952302/&#xa;        sorted_things = []&#xa;        queue = []&#xa;        for thing, deps in draw_after.items():&#xa;            if not deps:&#xa;                queue.append(thing)&#xa;                del draw_after[thing]&#xa;&#xa;        while draw_after:&#xa;            while queue:&#xa;                popped = queue.pop()&#xa;                sorted_things.append(popped)&#xa;                for thing, deps in draw_after.items():&#xa;                    if popped in deps:&#xa;                        deps.remove(popped)&#xa;                        if not deps:&#xa;                            queue.append(thing)&#xa;                            del draw_after[thing]&#xa;&#xa;            if draw_after:&#xa;                # if the sorting couldn't fullfill all ""draw after"" contraints,&#xa;                # we remove the ball / bone which lies farthest in the back&#xa;                # and try again&#xa;&#xa;                least_evil = min((thing for thing in draw_after.iterkeys()),&#xa;                                 key = evilness&#xa;                                )&#xa;                sorted_things.append(least_evil)&#xa;                del draw_after[least_evil]&#xa;                for thing, deps in draw_after.items():&#xa;                    if least_evil in deps:&#xa;                        deps.remove(least_evil)&#xa;                        if not deps:&#xa;                            queue.append(thing)&#xa;                            del draw_after[thing]&#xa;                &#xa;        self._things = sorted_things            &#xa;&#xa;        for thing in self._things:&#xa;            thing.sort(worldview)&#xa;            &#xa;    def draw(self, image, worldview):&#xa;        viewrect = Rect(*(tuple(-c for c in worldview.shift) + tuple(image.size - c for c in worldview.shift)))&#xa;        for thing in self._things:&#xa;            if thing.bounding().intersects(viewrect):&#xa;                thing.draw(image, worldview)&#xa;&#xa;    def balls(self):&#xa;        for thing in self._things:&#xa;            for ball in thing.balls():&#xa;                yield ball&#xa;&#xa;    def ball_set(self):&#xa;        """"""Returns all balls that are either directly in this figure or in&#xa;           on of its bones. It returns a set; i.e. each ball is returned&#xa;           exactly once.""""""&#xa;        return set(self.balls())&#xa;        &#xa;    def scale(self, factor):&#xa;        for ball in self.ball_set():&#xa;            ball.radius *= factor&#xa;            ball.center = tuple(c * factor for c in ball.center)&#xa;            &#xa;    def bounding(self):&#xa;        return sum((thing.bounding() for thing in self._things), None)&#xa;        &#xa;"
2801882|"""""""&#xa;This file is part of VDISCOVER.&#xa;&#xa;VDISCOVER is free software: you can redistribute it and/or modify&#xa;it under the terms of the GNU General Public License as published by&#xa;the Free Software Foundation, either version 3 of the License, or&#xa;(at your option) any later version.&#xa;&#xa;VDISCOVER is distributed in the hope that it will be useful,&#xa;but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the&#xa;GNU General Public License for more details.&#xa;&#xa;You should have received a copy of the GNU General Public License&#xa;along with VDISCOVER. If not, see <http://www.gnu.org/licenses/>.&#xa;&#xa;Copyright 2014 by G.Grieco&#xa;""""""&#xa;&#xa;import random&#xa;import gzip&#xa;import sys&#xa;import csv&#xa;import subprocess&#xa;import pickle&#xa;import numpy as np&#xa;import matplotlib as mpl&#xa;&#xa;# hack from https://stackoverflow.com/questions/2801882/generating-a-png-with-matplotlib-when-display-is-undefined to avoid using X&#xa;# mpl.use('Agg')&#xa;import matplotlib.pyplot as plt&#xa;&#xa;from Utils import *&#xa;from Pipeline import *&#xa;&#xa;&#xa;# def Cluster(X, labels)&#xa;""""""&#xa;  assert(len(X_red) == len(labels))&#xa;&#xa;  from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;  bandwidth = estimate_bandwidth(X, quantile=0.2)&#xa;  print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;  af = MeanShift(bandwidth=bandwidth/1).fit(X_red)&#xa;&#xa;  cluster_centers = af.cluster_centers_&#xa;  cluster_labels = af.labels_&#xa;  n_clusters = len(cluster_centers)&#xa;&#xa;  plt.figure()&#xa;&#xa;  for ([x,y],label, cluster_label) in zip(X_red,labels, cluster_labels):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    plt.scatter(x, y, c = colors[cluster_label % ncolors])&#xa;    #plt.text(x-0.05, y+0.01, label.split(""/"")[-1])&#xa;&#xa;  for i,[x,y] in enumerate(cluster_centers):&#xa;    plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;             markeredgecolor='k', markersize=7)&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters)&#xa;""""""&#xa;# return zip(labels, cluster_labels)&#xa;&#xa;&#xa;batch_size = 25&#xa;window_size = 32&#xa;maxlen = window_size&#xa;&#xa;embedding_dims = 5&#xa;nb_filters = 50&#xa;filter_length = 3&#xa;hidden_dims = 50&#xa;nb_epoch = 3&#xa;&#xa;&#xa;def ClusterCnn(model_file, train_file, valid_file, ftype, nsamples, outdir):&#xa;&#xa;    f = open(model_file + "".pre"")&#xa;    preprocessor = pickle.load(f)&#xa;&#xa;    import h5py&#xa;    f = h5py.File(model_file + "".wei"")&#xa;&#xa;    layers = []&#xa;    for k in range(f.attrs['nb_layers']):&#xa;        g = f['layer_{}'.format(k)]&#xa;        layers.append([g['param_{}'.format(p)]&#xa;                       for p in range(g.attrs['nb_params'])])&#xa;&#xa;    max_features = len(preprocessor.tokenizer.word_counts)&#xa;&#xa;    print ""Reading and sampling data to train..""&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples, cut=None)&#xa;    train_size = len(train_features)&#xa;&#xa;    #y = train_programs&#xa;    X_train, y_train, labels = preprocessor.preprocess_traces(&#xa;        train_features, y_data=train_classes, labels=train_programs)&#xa;    new_model = make_cluster_cnn(&#xa;        ""test"",&#xa;        max_features,&#xa;        maxlen,&#xa;        embedding_dims,&#xa;        nb_filters,&#xa;        filter_length,&#xa;        hidden_dims,&#xa;        None,&#xa;        weights=layers)&#xa;&#xa;    train_dict = dict()&#xa;    train_dict[ftype] = new_model.predict(X_train)&#xa;&#xa;    model = make_cluster_pipeline_subtraces(ftype)&#xa;    X_red_comp = model.fit_transform(train_dict)&#xa;    explained_var = np.var(X_red_comp, axis=0)&#xa;    print explained_var&#xa;&#xa;    X_red = X_red_comp[:, 0:2]&#xa;    X_red_next = X_red_comp[:, 2:4]&#xa;&#xa;    colors = mpl.colors.cnames.keys()&#xa;    progs = list(set(labels))&#xa;    ncolors = len(colors)&#xa;    size = len(labels)&#xa;    print ""Plotting..""&#xa;&#xa;    for prog, [x, y] in zip(labels, X_red):&#xa;        # for prog,[x,y] in sample(zip(labels, X_red), min(size, 1000)):&#xa;        x = gauss(0, 0.05) + x&#xa;        y = gauss(0, 0.05) + y&#xa;        color = 'r'&#xa;        plt.scatter(x, y, c=color)&#xa;&#xa;    """"""&#xa;  if valid_file is not None:&#xa;    valid_programs, valid_features, valid_classes = read_traces(valid_file, None, cut=None, maxsize=window_size) #None)&#xa;    valid_dict = dict()&#xa;&#xa;    X_valid, _, valid_labels = preprocessor.preprocess_traces(valid_features, y_data=None, labels=valid_programs)&#xa;    valid_dict[ftype] = new_model.predict(X_valid)&#xa;    X_red_valid_comp = model.transform(valid_dict)&#xa;&#xa;    X_red_valid = X_red_valid_comp[:,0:2]&#xa;    X_red_valid_next = X_red_valid_comp[:,2:4]&#xa;&#xa;    for prog,[x,y] in zip(valid_labels, X_red_valid):&#xa;      x = gauss(0,0.05) + x&#xa;      y = gauss(0,0.05) + y&#xa;      plt.scatter(x, y, c='b')&#xa;      plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;  plt.show()&#xa;  """"""&#xa;    plt.savefig(train_file.replace("".gz"", """") + "".png"")&#xa;    print ""Bandwidth estimation..""&#xa;    from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;    X_red_sample = X_red[:min(size, 1000)]&#xa;    bandwidth = estimate_bandwidth(X_red_sample, quantile=0.2)&#xa;    print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;    #X_red = np.vstack((X_red,X_red_valid))&#xa;    #X_red_next = np.vstack((X_red_next,X_red_valid_next))&#xa;    #labels = labels + valid_labels&#xa;&#xa;    print X_red.shape, len(X_red), len(labels)&#xa;    # print valid_labels&#xa;&#xa;    af = MeanShift(bandwidth=bandwidth / 1).fit(X_red)&#xa;&#xa;    cluster_centers = af.cluster_centers_&#xa;    cluster_labels = af.labels_&#xa;    n_clusters = len(cluster_centers)&#xa;&#xa;    plt.figure()&#xa;    for ([x, y], label, cluster_label) in zip(X_red, labels, cluster_labels):&#xa;        # for ([x,y],label, cluster_label) in sample(zip(X_red,labels,&#xa;        # cluster_labels), min(size, 1000)):&#xa;        x = gauss(0, 0.1) + x&#xa;        y = gauss(0, 0.1) + y&#xa;        plt.scatter(x, y, c=colors[cluster_label % ncolors])&#xa;        # print label&#xa;        # if label in valid_labels:&#xa;        #  plt.text(x-0.05, y+0.01, label.split(""/"")[-1])&#xa;&#xa;    for i, [x, y] in enumerate(cluster_centers):&#xa;        plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;                 markeredgecolor='k', markersize=7)&#xa;&#xa;    """"""&#xa;  #for prog,[x,y] in zip(valid_labels, X_red_valid):&#xa;    #x = gauss(0,0.1) + x&#xa;    #y = gauss(0,0.1) + y&#xa;    #plt.scatter(x, y, c='black')&#xa;    #plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters)&#xa;&#xa;  #plt.savefig(""clusters.png"")&#xa;  plt.show()&#xa;  """"""&#xa;    plt.savefig(train_file.replace("".gz"", """") + "".clusters.png"")&#xa;&#xa;    clustered_traces = zip(labels, cluster_labels)&#xa;    writer = open_csv(train_file.replace("".gz"", """") + "".clusters"")&#xa;    for label, cluster in clustered_traces:&#xa;        writer.writerow([label, cluster])&#xa;&#xa;    """"""&#xa;&#xa;  clusters = dict()&#xa;  for label, cluster in clustered_traces:&#xa;    clusters[cluster] = clusters.get(cluster, []) + [label]&#xa;&#xa;  for cluster, traces in clusters.items():&#xa;    plt.figure()&#xa;    plt.title('Cluster %d' % cluster)&#xa;    #X_clus = []&#xa;&#xa;    #for prog in traces:&#xa;    #  i = labels.index(prog)&#xa;    #  X_clus.append(X_train[i])&#xa;&#xa;    #train_dict = dict()&#xa;    #train_dict[ftype] = X_clus&#xa;&#xa;    #model = make_cluster_pipeline_subtraces(ftype)&#xa;    #X_red = model.fit_transform(train_dict)&#xa;&#xa;    #for [x,y],prog in zip(X_red,traces):&#xa;    for prog in traces:&#xa;&#xa;      i = labels.index(prog)&#xa;      assert(i>=0)&#xa;      [x,y] = X_red_next[i]&#xa;      x = gauss(0,0.1) + x&#xa;      y = gauss(0,0.1) + y&#xa;      plt.scatter(x, y, c='r')&#xa;&#xa;      #if prog in valid_labels:&#xa;      plt.text(x-0.05, y+0.01, prog.split(""/"")[-1])&#xa;&#xa;      #plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;&#xa;    plt.show()&#xa;    #plt.savefig('cluster-%d.png' % cluster)&#xa;  """"""&#xa;&#xa;    # return clustered_traces&#xa;&#xa;&#xa;def TrainCnn(model_file, train_file, valid_file, ftype, nsamples):&#xa;&#xa;    csvreader = open_csv(train_file)&#xa;&#xa;    train_features = []&#xa;    train_programs = []&#xa;    train_classes = []&#xa;&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples, cut=None)&#xa;    train_size = len(train_features)&#xa;&#xa;    from keras.preprocessing.text import Tokenizer&#xa;&#xa;    tokenizer = Tokenizer(nb_words=None, filters="""", lower=False, split="" "")&#xa;    # print type(train_features[0])&#xa;    tokenizer.fit_on_texts(train_features)&#xa;    max_features = len(tokenizer.word_counts)&#xa;&#xa;    preprocessor = DeepReprPreprocessor(tokenizer, window_size, batch_size)&#xa;    X_train, y_train = preprocessor.preprocess(train_features, 10000)&#xa;    nb_classes = len(preprocessor.classes)&#xa;    print preprocessor.classes&#xa;&#xa;    model = make_cluster_cnn(&#xa;        ""train"",&#xa;        max_features,&#xa;        maxlen,&#xa;        embedding_dims,&#xa;        nb_filters,&#xa;        filter_length,&#xa;        hidden_dims,&#xa;        nb_classes)&#xa;    model.fit(X_train, y_train, validation_split=0.1,&#xa;              batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True)&#xa;&#xa;    model.mypreprocessor = preprocessor&#xa;    #model_file = model_file + "".wei""&#xa;    #modelfile = open_model(model_file)&#xa;    print ""Saving model to"", model_file + "".wei""&#xa;    model.save_weights(model_file + "".wei"")&#xa;&#xa;    #model_file = model_file + "".pre""&#xa;    modelfile = open_model(model_file + "".pre"")&#xa;    print ""Saving preprocessor to"", model_file + "".pre""&#xa;    # model.save_weights(model_file)&#xa;    modelfile.write(pickle.dumps(preprocessor, protocol=2))&#xa;&#xa;""""""&#xa;def ClusterDoc2Vec(model_file, train_file, valid_file, ftype, nsamples, param):&#xa;&#xa;  train_programs, train_features, train_classes = read_traces(train_file, nsamples)&#xa;  train_size = len(train_programs)&#xa;&#xa;  print ""using"", train_size,""examples to train.""&#xa;&#xa;  from gensim.models.doc2vec import TaggedDocument&#xa;  from gensim.models import Doc2Vec&#xa;&#xa;  print ""Vectorizing traces..""&#xa;  sentences = []&#xa;&#xa;  for (prog,trace) in zip(train_programs,train_features):&#xa;     sentences.append(TaggedDocument(trace.split("" ""), [prog]))&#xa;&#xa;  model = Doc2Vec(dm=2, min_count=1, window=5, size=100, sample=1e-4, negative=5, workers=8, iter=1)&#xa;  model.build_vocab(sentences)&#xa;&#xa;  for epoch in range(20):&#xa;    #print model&#xa;    model.train(sentences)&#xa;    shuffle(sentences)&#xa;&#xa;  train_dict = dict()&#xa;&#xa;  vec_train_features = []&#xa;  for prog in train_programs:&#xa;    #print prog, model.docvecs[prog]&#xa;    vec_train_features.append(model.docvecs[prog])&#xa;&#xa;  train_dict[ftype] = vec_train_features&#xa;&#xa;  print ""Transforming data and fitting model..""&#xa;  model = make_cluster_pipeline_doc2vec(ftype)&#xa;  X_red = model.fit_transform(train_dict)&#xa;&#xa;  #mpl.rcParams.update({'font.size': 10})&#xa;  plt.figure()&#xa;  colors = 'brgcmykbgrcmykbgrcmykbgrcmyk'&#xa;  ncolors = len(colors)&#xa;&#xa;  for prog,[x,y],cl in zip(train_programs, X_red, train_classes):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    try:&#xa;        plt.scatter(x, y, c=colors[int(cl)])&#xa;        plt.text(x, y+0.02, prog.split(""/"")[-1])&#xa;    except ValueError:&#xa;        plt.text(x, y+0.02, cl)&#xa;&#xa;  #plt.show()&#xa;  plt.savefig(train_file.replace("".gz"","""")+"".png"")&#xa;&#xa;  from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;  bandwidth = estimate_bandwidth(X_red, quantile=0.2)&#xa;  print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;  af = MeanShift(bandwidth=bandwidth*param).fit(X_red)&#xa;&#xa;  cluster_centers = af.cluster_centers_&#xa;  labels = af.labels_&#xa;  n_clusters_ = len(cluster_centers)&#xa;&#xa;  plt.close('all')&#xa;  plt.figure(1)&#xa;  plt.clf()&#xa;&#xa;  for ([x,y],label, cluster_label) in zip(X_red,train_programs, labels):&#xa;    x = gauss(0,0.1) + x&#xa;    y = gauss(0,0.1) + y&#xa;    plt.scatter(x, y, c = colors[cluster_label % ncolors])&#xa;&#xa;  for i,[x,y] in enumerate(cluster_centers):&#xa;    plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;             markeredgecolor='k', markersize=7)&#xa;&#xa;  plt.title('Estimated number of clusters: %d' % n_clusters_)&#xa;  plt.savefig(train_file.replace("".gz"","""")+"".clusters.png"")&#xa;&#xa;  #plt.show()&#xa;&#xa;  clustered_traces = zip(train_programs, labels)&#xa;  writer = write_csv(train_file.replace("".gz"","""")+"".clusters"")&#xa;  for label, cluster in clustered_traces:&#xa;     writer.writerow([label.split(""/"")[-1], cluster])&#xa;&#xa;""""""&#xa;&#xa;&#xa;def ClusterScikit(&#xa;        model_file,&#xa;        train_file,&#xa;        valid_file,&#xa;        ftype,&#xa;        nsamples,&#xa;        vectorizer,&#xa;        reducer,&#xa;        param):&#xa;&#xa;    train_programs, train_features, train_classes = read_traces(&#xa;        train_file, nsamples)&#xa;    train_size = len(train_programs)&#xa;    print ""using"", train_size, ""examples to train.""&#xa;&#xa;    if vectorizer == ""bow"":&#xa;&#xa;        train_dict = dict()&#xa;        train_dict[ftype] = train_features&#xa;        #batch_size = 16&#xa;        #window_size = 20&#xa;&#xa;        print ""Transforming data and fitting model..""&#xa;        model = make_cluster_pipeline_bow(ftype, reducer)&#xa;        X_red = model.fit_transform(train_dict)&#xa;&#xa;    elif vectorizer == ""doc2vec"":&#xa;&#xa;        from gensim.models.doc2vec import TaggedDocument&#xa;        from gensim.models import Doc2Vec&#xa;&#xa;        print ""Vectorizing traces..""&#xa;        sentences = []&#xa;&#xa;        for (prog, trace) in zip(train_programs, train_features):&#xa;            sentences.append(TaggedDocument(trace.split("" ""), [prog]))&#xa;&#xa;        model = Doc2Vec(dm=2, min_count=1, window=5, size=100,&#xa;                        sample=1e-4, negative=5, workers=8, iter=1)&#xa;        model.build_vocab(sentences)&#xa;&#xa;        for epoch in range(20):&#xa;            # print model&#xa;            model.train(sentences)&#xa;            shuffle(sentences)&#xa;&#xa;        train_dict = dict()&#xa;&#xa;        vec_train_features = []&#xa;        for prog in train_programs:&#xa;            # print prog, model.docvecs[prog]&#xa;            vec_train_features.append(model.docvecs[prog])&#xa;&#xa;        train_dict[ftype] = vec_train_features&#xa;&#xa;        print ""Transforming data and fitting model..""&#xa;        model = make_cluster_pipeline_doc2vec(ftype, reducer)&#xa;        X_red = model.fit_transform(train_dict)&#xa;&#xa;    #pl.rcParams.update({'font.size': 10})&#xa;    if isinstance(X_red, list):&#xa;        X_red = np.vstack(X_red)&#xa;        print X_red.shape&#xa;&#xa;    if X_red.shape[1] == 2:&#xa;&#xa;        plt.figure()&#xa;        colors = 'brgcmykbgrcmykbgrcmykbgrcmyk'&#xa;        ncolors = len(colors)&#xa;&#xa;        for prog, [x, y], cl in zip(train_programs, X_red, train_classes):&#xa;            x = gauss(0, 0.1) + x&#xa;            y = gauss(0, 0.1) + y&#xa;            try:&#xa;                plt.scatter(x, y, c=colors[int(cl)])&#xa;                plt.text(x, y + 0.02, prog.split(""/"")[-1])&#xa;            except ValueError:&#xa;                plt.text(x, y + 0.02, cl)&#xa;&#xa;        if valid_file is not None:&#xa;            valid_programs, valid_features, valid_classes = read_traces(&#xa;                valid_file, None)&#xa;            valid_dict = dict()&#xa;            valid_dict[ftype] = valid_features&#xa;&#xa;            X_red = model.transform(valid_dict)&#xa;            for prog, [x, y], cl in zip(valid_programs, X_red, valid_classes):&#xa;                x = gauss(0, 0.1) + x&#xa;                y = gauss(0, 0.1) + y&#xa;                plt.scatter(x, y, c=colors[cl + 1])&#xa;                plt.text(x, y + 0.02, prog.split(""/"")[-1])&#xa;&#xa;        # plt.show()&#xa;        plt.savefig(train_file.replace("".gz"", """") + "".png"")&#xa;&#xa;    from sklearn.cluster import MeanShift, estimate_bandwidth&#xa;&#xa;    bandwidth = estimate_bandwidth(X_red, quantile=0.2)&#xa;    print ""Clustering with bandwidth:"", bandwidth&#xa;&#xa;    af = MeanShift(bandwidth=bandwidth * param).fit(X_red)&#xa;&#xa;    cluster_centers = af.cluster_centers_&#xa;    labels = af.labels_&#xa;    n_clusters_ = len(cluster_centers)&#xa;&#xa;    if X_red.shape[1] == 2:&#xa;&#xa;        plt.close('all')&#xa;        plt.figure(1)&#xa;        plt.clf()&#xa;&#xa;        for ([x, y], label, cluster_label) in zip(&#xa;                X_red, train_programs, labels):&#xa;            x = gauss(0, 0.1) + x&#xa;            y = gauss(0, 0.1) + y&#xa;            plt.scatter(x, y, c=colors[cluster_label % ncolors])&#xa;&#xa;        for i, [x, y] in enumerate(cluster_centers):&#xa;            plt.plot(x, y, 'o', markerfacecolor=colors[i % ncolors],&#xa;                     markeredgecolor='k', markersize=7)&#xa;&#xa;        plt.title('Estimated number of clusters: %d' % n_clusters_)&#xa;        plt.savefig(train_file.replace("".gz"", """") + "".clusters.png"")&#xa;&#xa;    # plt.show()&#xa;&#xa;    clustered_traces = zip(train_programs, labels)&#xa;    writer = write_csv(train_file.replace("".gz"", """") + "".clusters"")&#xa;    for label, cluster in clustered_traces:&#xa;        writer.writerow([label.split(""/"")[-1], cluster])&#xa;"
963965|"# http://stackoverflow.com/questions/963965/how-is-this-strategy-pattern&#xa;# -written-in-python-the-sample-in-wikipedia&#xa;""""""&#xa;In most of other languages Strategy pattern is implemented via creating some&#xa;base strategy interface/abstract class and subclassing it with a number of&#xa;concrete strategies (as we can see at&#xa;http://en.wikipedia.org/wiki/Strategy_pattern), however Python supports&#xa;higher-order functions and allows us to have only one class and inject&#xa;functions into it's instances, as shown in this example.&#xa;""""""&#xa;import types&#xa;&#xa;&#xa;class StrategyExample:&#xa;    def __init__(self, func=None):&#xa;        self.name = 'Strategy Example 0'&#xa;        if func is not None:&#xa;            self.execute = types.MethodType(func, self)&#xa;&#xa;    def execute(self):&#xa;        print(self.name)&#xa;&#xa;&#xa;def execute_replacement1(self):&#xa;    print(self.name + ' from execute 1')&#xa;&#xa;&#xa;def execute_replacement2(self):&#xa;    print(self.name + ' from execute 2')&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    strat0 = StrategyExample()&#xa;&#xa;    strat1 = StrategyExample(execute_replacement1)&#xa;    strat1.name = 'Strategy Example 1'&#xa;&#xa;    strat2 = StrategyExample(execute_replacement2)&#xa;    strat2.name = 'Strategy Example 2'&#xa;&#xa;    strat0.execute()&#xa;    strat1.execute()&#xa;    strat2.execute()&#xa;"
24717027|"""""""&#xa;pytest local configuration plug-in&#xa;""""""&#xa;&#xa;import gc&#xa;import warnings&#xa;&#xa;import pytest&#xa;&#xa;@pytest.yield_fixture(scope='function', autouse=True)&#xa;def error_on_ResourceWarning():&#xa;    """"""This fixture captures ResourceWarning's and reports an ""error""&#xa;    describing the file handles left open.&#xa;    &#xa;    This is shown regardless of how successful the test was, if a test fails&#xa;    and leaves files open then those files will be reported.  Ideally, even&#xa;    those files should be closed properly after a test failure or exception.&#xa;&#xa;    Since only Python 3 and PyPy3 have ResourceWarning's, this context will&#xa;    have no effect when running tests on Python 2 or PyPy.&#xa;&#xa;    Because of autouse=True, this function will be automatically enabled for&#xa;    all test_* functions in this module.&#xa;&#xa;    This code is primarily based on the examples found here:&#xa;    https://stackoverflow.com/questions/24717027/convert-python-3-resourcewarnings-into-exception&#xa;    """"""&#xa;    try:&#xa;        ResourceWarning&#xa;    except NameError:&#xa;        # Python 2, PyPy&#xa;        yield&#xa;        return&#xa;    # Python 3, PyPy3&#xa;    with warnings.catch_warnings(record=True) as caught:&#xa;        warnings.resetwarnings() # clear all filters&#xa;        warnings.simplefilter('ignore') # ignore all&#xa;        warnings.simplefilter('always', ResourceWarning) # add filter&#xa;        yield # run tests in this context&#xa;        gc.collect() # run garbage collection (for pypy3)&#xa;        if not caught:&#xa;            return&#xa;        pytest.fail('The following file descriptors were not closed properly:\n' +&#xa;                    '\n'.join((str(warning.message) for warning in caught)),&#xa;                    pytrace=False)&#xa;"
1838699|"""""""Miscellaneous utility functions.""""""&#xa;&#xa;from __future__ import absolute_import, division, with_statement&#xa;&#xa;import zlib&#xa;&#xa;&#xa;class ObjectDict(dict):&#xa;    """"""Makes a dictionary behave like an object.""""""&#xa;    def __getattr__(self, name):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            raise AttributeError(name)&#xa;&#xa;    def __setattr__(self, name, value):&#xa;        self[name] = value&#xa;&#xa;&#xa;class GzipDecompressor(object):&#xa;    """"""Streaming gzip decompressor.&#xa;&#xa;    The interface is like that of `zlib.decompressobj` (without the&#xa;    optional arguments, but it understands gzip headers and checksums.&#xa;    """"""&#xa;    def __init__(self):&#xa;        # Magic parameter makes zlib module understand gzip header&#xa;        # http://stackoverflow.com/questions/1838699/how-can-i-decompress-a-gzip-stream-with-zlib&#xa;        # This works on cpython and pypy, but not jython.&#xa;        self.decompressobj = zlib.decompressobj(16 + zlib.MAX_WBITS)&#xa;&#xa;    def decompress(self, value):&#xa;        """"""Decompress a chunk, returning newly-available data.&#xa;&#xa;        Some data may be buffered for later processing; `flush` must&#xa;        be called when there is no more input data to ensure that&#xa;        all data was processed.&#xa;        """"""&#xa;        return self.decompressobj.decompress(value)&#xa;&#xa;    def flush(self):&#xa;        """"""Return any remaining buffered data not yet returned by decompress.&#xa;&#xa;        Also checks for errors such as truncated input.&#xa;        No other methods may be called on this object after `flush`.&#xa;        """"""&#xa;        return self.decompressobj.flush()&#xa;&#xa;&#xa;def import_object(name):&#xa;    """"""Imports an object by name.&#xa;&#xa;    import_object('x.y.z') is equivalent to 'from x.y import z'.&#xa;&#xa;    >>> import tornado.escape&#xa;    >>> import_object('tornado.escape') is tornado.escape&#xa;    True&#xa;    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8&#xa;    True&#xa;    """"""&#xa;    parts = name.split('.')&#xa;    obj = __import__('.'.join(parts[:-1]), None, None, [parts[-1]], 0)&#xa;    return getattr(obj, parts[-1])&#xa;&#xa;# Fake byte literal support:  In python 2.6+, you can say b""foo"" to get&#xa;# a byte literal (str in 2.x, bytes in 3.x).  There's no way to do this&#xa;# in a way that supports 2.5, though, so we need a function wrapper&#xa;# to convert our string literals.  b() should only be applied to literal&#xa;# latin1 strings.  Once we drop support for 2.5, we can remove this function&#xa;# and just use byte literals.&#xa;if str is unicode:&#xa;    def b(s):&#xa;        return s.encode('latin1')&#xa;    bytes_type = bytes&#xa;else:&#xa;    def b(s):&#xa;        return s&#xa;    bytes_type = str&#xa;&#xa;&#xa;def raise_exc_info(exc_info):&#xa;    """"""Re-raise an exception (with original traceback) from an exc_info tuple.&#xa;&#xa;    The argument is a ``(type, value, traceback)`` tuple as returned by&#xa;    `sys.exc_info`.&#xa;    """"""&#xa;    # 2to3 isn't smart enough to convert three-argument raise&#xa;    # statements correctly in some cases.&#xa;    if isinstance(exc_info[1], exc_info[0]):&#xa;        raise exc_info[1], None, exc_info[2]&#xa;        # After 2to3: raise exc_info[1].with_traceback(exc_info[2])&#xa;    else:&#xa;        # I think this branch is only taken for string exceptions,&#xa;        # which were removed in Python 2.6.&#xa;        raise exc_info[0], exc_info[1], exc_info[2]&#xa;        # After 2to3: raise exc_info[0](exc_info[1]).with_traceback(exc_info[2])&#xa;&#xa;&#xa;def doctests():&#xa;    import doctest&#xa;    return doctest.DocTestSuite()&#xa;"
13564851|"# Windows implementation of PyAutoGUI functions.&#xa;# BSD license&#xa;# Al Sweigart al@inventwithpython.com&#xa;&#xa;import ctypes&#xa;import ctypes.wintypes&#xa;import pyautogui&#xa;&#xa;import sys&#xa;if sys.platform !=  'win32':&#xa;    raise Exception('The pyautogui_win module should only be loaded on a Windows system.')&#xa;&#xa;&#xa;# Fixes the scaling issues where PyAutoGUI was reporting the wrong resolution:&#xa;try:&#xa;   ctypes.windll.user32.SetProcessDPIAware()&#xa;except AttributeError:&#xa;    pass # Windows XP doesn't support this, so just do nothing.&#xa;&#xa;&#xa;""""""&#xa;A lot of this code is probably repeated from win32 extensions module, but I didn't want to have that dependency.&#xa;&#xa;Note: According to http://msdn.microsoft.com/en-us/library/windows/desktop/ms646260(v=vs.85).aspx&#xa;the ctypes.windll.user32.mouse_event() function has been superceded by SendInput.&#xa;&#xa;SendInput() is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646310(v=vs.85).aspx&#xa;&#xa;UPDATE: SendInput() doesn't seem to be working for me. I've switched back to mouse_event().""""""&#xa;&#xa;&#xa;# Event codes to be passed to the mouse_event() win32 function.&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646273(v=vs.85).aspx&#xa;MOUSEEVENTF_LEFTDOWN = 0x0002&#xa;MOUSEEVENTF_LEFTUP = 0x0004&#xa;MOUSEEVENTF_LEFTCLICK = MOUSEEVENTF_LEFTDOWN + MOUSEEVENTF_LEFTUP&#xa;MOUSEEVENTF_RIGHTDOWN = 0x0008&#xa;MOUSEEVENTF_RIGHTUP = 0x0010&#xa;MOUSEEVENTF_RIGHTCLICK = MOUSEEVENTF_RIGHTDOWN + MOUSEEVENTF_RIGHTUP&#xa;MOUSEEVENTF_MIDDLEDOWN = 0x0020&#xa;MOUSEEVENTF_MIDDLEUP = 0x0040&#xa;MOUSEEVENTF_MIDDLECLICK = MOUSEEVENTF_MIDDLEDOWN + MOUSEEVENTF_MIDDLEUP&#xa;&#xa;MOUSEEVENTF_WHEEL = 0x0800&#xa;MOUSEEVENTF_HWHEEL = 0x01000&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646304(v=vs.85).aspx&#xa;KEYEVENTF_KEYUP = 0x0002&#xa;&#xa;# Documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;INPUT_MOUSE = 0&#xa;INPUT_KEYBOARD = 1&#xa;&#xa;&#xa;# This ctypes structure is for a Win32 POINT structure,&#xa;# which is documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/dd162805(v=vs.85).aspx&#xa;# The POINT structure is used by GetCursorPos().&#xa;class POINT(ctypes.Structure):&#xa;    _fields_ = [(""x"", ctypes.c_ulong),&#xa;                (""y"", ctypes.c_ulong)]&#xa;&#xa;# These ctypes structures are for Win32 INPUT, MOUSEINPUT, KEYBDINPUT, and HARDWAREINPUT structures,&#xa;# used by SendInput and documented here: http://msdn.microsoft.com/en-us/library/windows/desktop/ms646270(v=vs.85).aspx&#xa;# Thanks to BSH for this StackOverflow answer: https://stackoverflow.com/questions/18566289/how-would-you-recreate-this-windows-api-structure-with-ctypes&#xa;class MOUSEINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('dx', ctypes.wintypes.LONG),&#xa;        ('dy', ctypes.wintypes.LONG),&#xa;        ('mouseData', ctypes.wintypes.DWORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class KEYBDINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('wVk', ctypes.wintypes.WORD),&#xa;        ('wScan', ctypes.wintypes.WORD),&#xa;        ('dwFlags', ctypes.wintypes.DWORD),&#xa;        ('time', ctypes.wintypes.DWORD),&#xa;        ('dwExtraInfo', ctypes.POINTER(ctypes.wintypes.ULONG)),&#xa;    ]&#xa;&#xa;class HARDWAREINPUT(ctypes.Structure):&#xa;    _fields_ = [&#xa;        ('uMsg', ctypes.wintypes.DWORD),&#xa;        ('wParamL', ctypes.wintypes.WORD),&#xa;        ('wParamH', ctypes.wintypes.DWORD)&#xa;    ]&#xa;&#xa;class INPUT(ctypes.Structure):&#xa;    class _I(ctypes.Union):&#xa;        _fields_ = [&#xa;            ('mi', MOUSEINPUT),&#xa;            ('ki', KEYBDINPUT),&#xa;            ('hi', HARDWAREINPUT),&#xa;        ]&#xa;&#xa;    _anonymous_ = ('i', )&#xa;    _fields_ = [&#xa;        ('type', ctypes.wintypes.DWORD),&#xa;        ('i', _I),&#xa;    ]&#xa;# End of the SendInput win32 data structures.&#xa;&#xa;&#xa;&#xa;"""""" Keyboard key mapping for pyautogui:&#xa;Documented at http://msdn.microsoft.com/en-us/library/windows/desktop/dd375731(v=vs.85).aspx&#xa;&#xa;The *KB dictionaries in pyautogui map a string that can be passed to keyDown(),&#xa;keyUp(), or press() into the code used for the OS-specific keyboard function.&#xa;&#xa;They should always be lowercase, and the same keys should be used across all OSes.""""""&#xa;keyboardMapping = dict([(key, None) for key in pyautogui.KEY_NAMES])&#xa;keyboardMapping.update({&#xa;    'backspace': 0x08, # VK_BACK&#xa;    '\b': 0x08, # VK_BACK&#xa;    'super': 0x5B, #VK_LWIN&#xa;    'tab': 0x09, # VK_TAB&#xa;    '\t': 0x09, # VK_TAB&#xa;    'clear': 0x0c, # VK_CLEAR&#xa;    'enter': 0x0d, # VK_RETURN&#xa;    '\n': 0x0d, # VK_RETURN&#xa;    'return': 0x0d, # VK_RETURN&#xa;    'shift': 0x10, # VK_SHIFT&#xa;    'ctrl': 0x11, # VK_CONTROL&#xa;    'alt': 0x12, # VK_MENU&#xa;    'pause': 0x13, # VK_PAUSE&#xa;    'capslock': 0x14, # VK_CAPITAL&#xa;    'kana': 0x15, # VK_KANA&#xa;    'hanguel': 0x15, # VK_HANGUEL&#xa;    'hangul': 0x15, # VK_HANGUL&#xa;    'junja': 0x17, # VK_JUNJA&#xa;    'final': 0x18, # VK_FINAL&#xa;    'hanja': 0x19, # VK_HANJA&#xa;    'kanji': 0x19, # VK_KANJI&#xa;    'esc': 0x1b, # VK_ESCAPE&#xa;    'escape': 0x1b, # VK_ESCAPE&#xa;    'convert': 0x1c, # VK_CONVERT&#xa;    'nonconvert': 0x1d, # VK_NONCONVERT&#xa;    'accept': 0x1e, # VK_ACCEPT&#xa;    'modechange': 0x1f, # VK_MODECHANGE&#xa;    ' ': 0x20, # VK_SPACE&#xa;    'space': 0x20,&#xa;    'pgup': 0x21, # VK_PRIOR&#xa;    'pgdn': 0x22, # VK_NEXT&#xa;    'pageup': 0x21, # VK_PRIOR&#xa;    'pagedown': 0x22, # VK_NEXT&#xa;    'end': 0x23, # VK_END&#xa;    'home': 0x24, # VK_HOME&#xa;    'left': 0x25, # VK_LEFT&#xa;    'up': 0x26, # VK_UP&#xa;    'right': 0x27, # VK_RIGHT&#xa;    'down': 0x28, # VK_DOWN&#xa;    'select': 0x29, # VK_SELECT&#xa;    'print': 0x2a, # VK_PRINT&#xa;    'execute': 0x2b, # VK_EXECUTE&#xa;    'prtsc': 0x2c, # VK_SNAPSHOT&#xa;    'prtscr': 0x2c, # VK_SNAPSHOT&#xa;    'prntscrn': 0x2c, # VK_SNAPSHOT&#xa;    'printscreen': 0x2c, # VK_SNAPSHOT&#xa;    'insert': 0x2d, # VK_INSERT&#xa;    'del': 0x2e, # VK_DELETE&#xa;    'delete': 0x2e, # VK_DELETE&#xa;    'help': 0x2f, # VK_HELP&#xa;    'win': 0x5b, # VK_LWIN&#xa;    'winleft': 0x5b, # VK_LWIN&#xa;    'winright': 0x5c, # VK_RWIN&#xa;    'apps': 0x5d, # VK_APPS&#xa;    'sleep': 0x5f, # VK_SLEEP&#xa;    'num0': 0x60, # VK_NUMPAD0&#xa;    'num1': 0x61, # VK_NUMPAD1&#xa;    'num2': 0x62, # VK_NUMPAD2&#xa;    'num3': 0x63, # VK_NUMPAD3&#xa;    'num4': 0x64, # VK_NUMPAD4&#xa;    'num5': 0x65, # VK_NUMPAD5&#xa;    'num6': 0x66, # VK_NUMPAD6&#xa;    'num7': 0x67, # VK_NUMPAD7&#xa;    'num8': 0x68, # VK_NUMPAD8&#xa;    'num9': 0x69, # VK_NUMPAD9&#xa;    'multiply': 0x6a, # VK_MULTIPLY  ??? Is this the numpad *?&#xa;    'add': 0x6b, # VK_ADD  ??? Is this the numpad +?&#xa;    'separator': 0x6c, # VK_SEPARATOR  ??? Is this the numpad enter?&#xa;    'subtract': 0x6d, # VK_SUBTRACT  ??? Is this the numpad -?&#xa;    'decimal': 0x6e, # VK_DECIMAL&#xa;    'divide': 0x6f, # VK_DIVIDE&#xa;    'f1': 0x70, # VK_F1&#xa;    'f2': 0x71, # VK_F2&#xa;    'f3': 0x72, # VK_F3&#xa;    'f4': 0x73, # VK_F4&#xa;    'f5': 0x74, # VK_F5&#xa;    'f6': 0x75, # VK_F6&#xa;    'f7': 0x76, # VK_F7&#xa;    'f8': 0x77, # VK_F8&#xa;    'f9': 0x78, # VK_F9&#xa;    'f10': 0x79, # VK_F10&#xa;    'f11': 0x7a, # VK_F11&#xa;    'f12': 0x7b, # VK_F12&#xa;    'f13': 0x7c, # VK_F13&#xa;    'f14': 0x7d, # VK_F14&#xa;    'f15': 0x7e, # VK_F15&#xa;    'f16': 0x7f, # VK_F16&#xa;    'f17': 0x80, # VK_F17&#xa;    'f18': 0x81, # VK_F18&#xa;    'f19': 0x82, # VK_F19&#xa;    'f20': 0x83, # VK_F20&#xa;    'f21': 0x84, # VK_F21&#xa;    'f22': 0x85, # VK_F22&#xa;    'f23': 0x86, # VK_F23&#xa;    'f24': 0x87, # VK_F24&#xa;    'numlock': 0x90, # VK_NUMLOCK&#xa;    'scrolllock': 0x91, # VK_SCROLL&#xa;    'shiftleft': 0xa0, # VK_LSHIFT&#xa;    'shiftright': 0xa1, # VK_RSHIFT&#xa;    'ctrlleft': 0xa2, # VK_LCONTROL&#xa;    'ctrlright': 0xa3, # VK_RCONTROL&#xa;    'altleft': 0xa4, # VK_LMENU&#xa;    'altright': 0xa5, # VK_RMENU&#xa;    'browserback': 0xa6, # VK_BROWSER_BACK&#xa;    'browserforward': 0xa7, # VK_BROWSER_FORWARD&#xa;    'browserrefresh': 0xa8, # VK_BROWSER_REFRESH&#xa;    'browserstop': 0xa9, # VK_BROWSER_STOP&#xa;    'browsersearch': 0xaa, # VK_BROWSER_SEARCH&#xa;    'browserfavorites': 0xab, # VK_BROWSER_FAVORITES&#xa;    'browserhome': 0xac, # VK_BROWSER_HOME&#xa;    'volumemute': 0xad, # VK_VOLUME_MUTE&#xa;    'volumedown': 0xae, # VK_VOLUME_DOWN&#xa;    'volumeup': 0xaf, # VK_VOLUME_UP&#xa;    'nexttrack': 0xb0, # VK_MEDIA_NEXT_TRACK&#xa;    'prevtrack': 0xb1, # VK_MEDIA_PREV_TRACK&#xa;    'stop': 0xb2, # VK_MEDIA_STOP&#xa;    'playpause': 0xb3, # VK_MEDIA_PLAY_PAUSE&#xa;    'launchmail': 0xb4, # VK_LAUNCH_MAIL&#xa;    'launchmediaselect': 0xb5, # VK_LAUNCH_MEDIA_SELECT&#xa;    'launchapp1': 0xb6, # VK_LAUNCH_APP1&#xa;    'launchapp2': 0xb7, # VK_LAUNCH_APP2&#xa;    #';': 0xba, # VK_OEM_1&#xa;    #'+': 0xbb, # VK_OEM_PLUS&#xa;    #',': 0xbc, # VK_OEM_COMMA&#xa;    #'-': 0xbd, # VK_OEM_MINUS&#xa;    #'.': 0xbe, # VK_OEM_PERIOD&#xa;    #'/': 0xbf, # VK_OEM_2&#xa;    #'~': 0xc0, # VK_OEM_3&#xa;    #'[': 0xdb, # VK_OEM_4&#xa;    #'|': 0xdc, # VK_OEM_5&#xa;    #']': 0xdd, # VK_OEM_6&#xa;    #""'"": 0xde, # VK_OEM_7&#xa;    #'': 0xdf, # VK_OEM_8&#xa;    #'': 0xe7, # VK_PACKET&#xa;    #'': 0xf6, # VK_ATTN&#xa;    #'': 0xf7, # VK_CRSEL&#xa;    #'': 0xf8, # VK_EXSEL&#xa;    #'': 0xf9, # VK_EREOF&#xa;    #'': 0xfa, # VK_PLAY&#xa;    #'': 0xfb, # VK_ZOOM&#xa;    #'': 0xfc, # VK_NONAME&#xa;    #'': 0xfd, # VK_PA1&#xa;    #'': 0xfe, # VK_OEM_CLEAR&#xa;})&#xa;&#xa;# Populate the basic printable ascii characters.&#xa;for c in range(32, 128):&#xa;    keyboardMapping[chr(c)] = ctypes.windll.user32.VkKeyScanA(ctypes.wintypes.WCHAR(chr(c)))&#xa;&#xa;&#xa;def _keyDown(key):&#xa;    """"""Performs a keyboard key press without the release. This will put that&#xa;    key in a held down state.&#xa;&#xa;    NOTE: For some reason, this does not seem to cause key repeats like would&#xa;    happen if a keyboard key was held down on a text field.&#xa;&#xa;    Args:&#xa;      key (str): The key to be pressed down. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;    mods, vkCode = divmod(keyboardMapping[key], 0x100)&#xa;&#xa;    for apply_mod, vk_mod in [(mods & 4, 0x12), (mods & 2, 0x11),&#xa;        (mods & 1 or needsShift, 0x10)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, 0, 0) #&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, 0, 0)&#xa;    for apply_mod, vk_mod in [(mods & 1 or needsShift, 0x10), (mods & 2, 0x11),&#xa;        (mods & 4, 0x12)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, KEYEVENTF_KEYUP, 0) #&#xa;&#xa;&#xa;def _keyUp(key):&#xa;    """"""Performs a keyboard key release (without the press down beforehand).&#xa;&#xa;    Args:&#xa;      key (str): The key to be released up. The valid names are listed in&#xa;      pyautogui.KEY_NAMES.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if key not in keyboardMapping or keyboardMapping[key] is None:&#xa;        return&#xa;&#xa;    needsShift = pyautogui.isShiftCharacter(key)&#xa;    """"""&#xa;    # OLD CODE: The new code relies on having all keys be loaded in keyboardMapping from the start.&#xa;    if key in keyboardMapping.keys():&#xa;        vkCode = keyboardMapping[key]&#xa;    elif len(key) == 1:&#xa;        # note: I could use this case to update keyboardMapping to cache the VkKeyScan results, but I've decided not to just to make any possible bugs easier to reproduce.&#xa;        vkCode = ctypes.windll.user32.VkKeyScanW(ctypes.wintypes.WCHAR(key))&#xa;        if vkCode == -1:&#xa;            raise ValueError('There is no VK code for key ""%s""' % (key))&#xa;        if vkCode > 0x100: # the vk code will be > 0x100 if it needs shift&#xa;            vkCode -= 0x100&#xa;            needsShift = True&#xa;    """"""&#xa;    mods, vkCode = divmod(keyboardMapping[key], 0x100)&#xa;&#xa;    for apply_mod, vk_mod in [(mods & 4, 0x12), (mods & 2, 0x11),&#xa;        (mods & 1 or needsShift, 0x10)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, 0, 0) #&#xa;    ctypes.windll.user32.keybd_event(vkCode, 0, KEYEVENTF_KEYUP, 0)&#xa;    for apply_mod, vk_mod in [(mods & 1 or needsShift, 0x10), (mods & 2, 0x11),&#xa;        (mods & 4, 0x12)]: #HANKAKU not suported! mods & 8&#xa;        if apply_mod:&#xa;            ctypes.windll.user32.keybd_event(vk_mod, 0, KEYEVENTF_KEYUP, 0) #&#xa;&#xa;&#xa;def _position():&#xa;    """"""Returns the current xy coordinates of the mouse cursor as a two-integer&#xa;    tuple by calling the GetCursorPos() win32 function.&#xa;&#xa;    Returns:&#xa;      (x, y) tuple of the current xy coordinates of the mouse cursor.&#xa;    """"""&#xa;&#xa;    cursor = POINT()&#xa;    ctypes.windll.user32.GetCursorPos(ctypes.byref(cursor))&#xa;    return (cursor.x, cursor.y)&#xa;&#xa;&#xa;def _size():&#xa;    """"""Returns the width and height of the screen as a two-integer tuple.&#xa;&#xa;    Returns:&#xa;      (width, height) tuple of the screen size, in pixels.&#xa;    """"""&#xa;    return (ctypes.windll.user32.GetSystemMetrics(0), ctypes.windll.user32.GetSystemMetrics(1))&#xa;&#xa;&#xa;def _moveTo(x, y):&#xa;    """"""Send the mouse move event to Windows by calling SetCursorPos() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    ctypes.windll.user32.SetCursorPos(x, y)&#xa;&#xa;&#xa;def _mouseDown(x, y, button):&#xa;    """"""Send the mouse down event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLEDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTDOWN, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _mouseUp(x, y, button):&#xa;    """"""Send the mouse up event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLEUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTUP, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _click(x, y, button):&#xa;    """"""Send the mouse click event to Windows by calling the mouse_event() win32&#xa;    function.&#xa;&#xa;    Args:&#xa;      button (str): The mouse button, either 'left', 'middle', or 'right'&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    if button == 'left':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_LEFTCLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'middle':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_MIDDLECLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    elif button == 'right':&#xa;        try:&#xa;            _sendMouseEvent(MOUSEEVENTF_RIGHTCLICK, x, y)&#xa;        except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;    else:&#xa;        assert False, ""button argument not in ('left', 'middle', 'right')""&#xa;&#xa;&#xa;def _sendMouseEvent(ev, x, y, dwData=0):&#xa;    """"""The helper function that actually makes the call to the mouse_event()&#xa;    win32 function.&#xa;&#xa;    Args:&#xa;      ev (int): The win32 code for the mouse event. Use one of the MOUSEEVENTF_*&#xa;      constants for this argument.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;      dwData (int): The argument for mouse_event()'s dwData parameter. So far&#xa;        this is only used by mouse scrolling.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    assert x != None and y != None, 'x and y cannot be set to None'&#xa;    # TODO: ARG! For some reason, SendInput isn't working for mouse events. I'm switching to using the older mouse_event win32 function.&#xa;    #mouseStruct = MOUSEINPUT()&#xa;    #mouseStruct.dx = x&#xa;    #mouseStruct.dy = y&#xa;    #mouseStruct.mouseData = ev&#xa;    #mouseStruct.time = 0&#xa;    #mouseStruct.dwExtraInfo = ctypes.pointer(ctypes.c_ulong(0)) # according to https://stackoverflow.com/questions/13564851/generate-keyboard-events I can just set this. I don't really care about this value.&#xa;    #inputStruct = INPUT()&#xa;    #inputStruct.mi = mouseStruct&#xa;    #inputStruct.type = INPUT_MOUSE&#xa;    #ctypes.windll.user32.SendInput(1, ctypes.pointer(inputStruct), ctypes.sizeof(inputStruct))&#xa;&#xa;    width, height = _size()&#xa;    convertedX = 65536 * x // width + 1&#xa;    convertedY = 65536 * y // height + 1&#xa;    ctypes.windll.user32.mouse_event(ev, ctypes.c_long(convertedX), ctypes.c_long(convertedY), dwData, 0)&#xa;&#xa;    # TODO: Too many false positives with this code: See: https://github.com/asweigart/pyautogui/issues/108&#xa;    #if ctypes.windll.kernel32.GetLastError() != 0:&#xa;    #    raise ctypes.WinError()&#xa;&#xa;&#xa;def _scroll(clicks, x=None, y=None):&#xa;    """"""Send the mouse vertical scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    startx, starty = _position()&#xa;    width, height = _size()&#xa;&#xa;    if x is None:&#xa;        x = startx&#xa;    else:&#xa;        if x < 0:&#xa;            x = 0&#xa;        elif x >= width:&#xa;            x = width - 1&#xa;    if y is None:&#xa;        y = starty&#xa;    else:&#xa;        if y < 0:&#xa;            y = 0&#xa;        elif y >= height:&#xa;            y = height - 1&#xa;&#xa;    try:&#xa;        _sendMouseEvent(MOUSEEVENTF_WHEEL, x, y, dwData=clicks)&#xa;    except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60&#xa;            pass&#xa;&#xa;&#xa;def _hscroll(clicks, x, y):&#xa;    """"""Send the mouse horizontal scroll event to Windows by calling the&#xa;    mouse_event() win32 function.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving right, a negative value is moving left.&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;&#xa;def _vscroll(clicks, x, y):&#xa;    """"""A wrapper for _scroll(), which does vertical scrolling.&#xa;&#xa;    Args:&#xa;      clicks (int): The amount of scrolling to do. A positive value is the mouse&#xa;      wheel moving forward (scrolling up), a negative value is backwards (down).&#xa;      x (int): The x position of the mouse event.&#xa;      y (int): The y position of the mouse event.&#xa;&#xa;    Returns:&#xa;      None&#xa;    """"""&#xa;    return _scroll(clicks, x, y)&#xa;&#xa;"
279237|"#!/usr/bin/env python&#xa;&#xa;# Test whether a client produces a correct connect with a will, username and password.&#xa;&#xa;# The client should connect to port 1888 with keepalive=60, clean session set,&#xa;# client id 01-will-unpwd-set , will topic set to ""will-topic"", will payload&#xa;# set to ""will message"", will qos=2, will retain not set, username set to&#xa;# ""oibvvwqw"" and password set to ""#'^2hg9a&nm38*us"".&#xa;&#xa;import inspect&#xa;import os&#xa;import socket&#xa;import sys&#xa;&#xa;# From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder&#xa;cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],"".."")))&#xa;if cmd_subfolder not in sys.path:&#xa;    sys.path.insert(0, cmd_subfolder)&#xa;&#xa;import mosq_test&#xa;&#xa;rc = 1&#xa;keepalive = 60&#xa;connect_packet = mosq_test.gen_connect(""01-will-unpwd-set"",&#xa;        keepalive=keepalive, username=""oibvvwqw"", password=""#'^2hg9a&nm38*us"",&#xa;        will_topic=""will-topic"", will_qos=2, will_payload=""will message"")&#xa;&#xa;sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&#xa;sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)&#xa;sock.settimeout(10)&#xa;sock.bind(('', 1888))&#xa;sock.listen(5)&#xa;&#xa;client_args = sys.argv[1:]&#xa;env = dict(os.environ)&#xa;env['LD_LIBRARY_PATH'] = '../../lib:../../lib/cpp'&#xa;try:&#xa;    pp = env['PYTHONPATH']&#xa;except KeyError:&#xa;    pp = ''&#xa;env['PYTHONPATH'] = '../../lib/python:'+pp&#xa;client = mosq_test.start_client(filename=sys.argv[1].replace('/', '-'), cmd=client_args, env=env)&#xa;&#xa;try:&#xa;    (conn, address) = sock.accept()&#xa;    conn.settimeout(10)&#xa;&#xa;    if mosq_test.expect_packet(conn, ""connect"", connect_packet):&#xa;        rc = 0&#xa;&#xa;    conn.close()&#xa;finally:&#xa;    client.terminate()&#xa;    client.wait()&#xa;    sock.close()&#xa;&#xa;exit(rc)&#xa;&#xa;"
2407126|"# -*- coding: utf-'8' ""-*-""&#xa;&#xa;import base64&#xa;import json&#xa;import logging&#xa;import urlparse&#xa;import werkzeug.urls&#xa;import urllib2&#xa;&#xa;from openerp.addons.payment.models.payment_acquirer import ValidationError&#xa;from openerp.addons.payment_paypal.controllers.main import PaypalController&#xa;from openerp.osv import osv, fields&#xa;from openerp.tools.float_utils import float_compare&#xa;from openerp import SUPERUSER_ID&#xa;from openerp.tools.translate import _&#xa;&#xa;_logger = logging.getLogger(__name__)&#xa;&#xa;&#xa;class AcquirerPaypal(osv.Model):&#xa;    _inherit = 'payment.acquirer'&#xa;&#xa;    def _get_paypal_urls(self, cr, uid, environment, context=None):&#xa;        """""" Paypal URLS """"""&#xa;        if environment == 'prod':&#xa;            return {&#xa;                'paypal_form_url': 'https://www.paypal.com/cgi-bin/webscr',&#xa;                'paypal_rest_url': 'https://api.paypal.com/v1/oauth2/token',&#xa;            }&#xa;        else:&#xa;            return {&#xa;                'paypal_form_url': 'https://www.sandbox.paypal.com/cgi-bin/webscr',&#xa;                'paypal_rest_url': 'https://api.sandbox.paypal.com/v1/oauth2/token',&#xa;            }&#xa;&#xa;    def _get_providers(self, cr, uid, context=None):&#xa;        providers = super(AcquirerPaypal, self)._get_providers(cr, uid, context=context)&#xa;        providers.append(['paypal', 'Paypal'])&#xa;        return providers&#xa;&#xa;    _columns = {&#xa;        'paypal_email_account': fields.char('Paypal Email ID', required_if_provider='paypal'),&#xa;        'paypal_seller_account': fields.char(&#xa;            'Paypal Merchant ID',&#xa;            help='The Merchant ID is used to ensure communications coming from Paypal are valid and secured.'),&#xa;        'paypal_use_ipn': fields.boolean('Use IPN', help='Paypal Instant Payment Notification'),&#xa;        # Server 2 server&#xa;        'paypal_api_enabled': fields.boolean('Use Rest API'),&#xa;        'paypal_api_username': fields.char('Rest API Username'),&#xa;        'paypal_api_password': fields.char('Rest API Password'),&#xa;        'paypal_api_access_token': fields.char('Access Token'),&#xa;        'paypal_api_access_token_validity': fields.datetime('Access Token Validity'),&#xa;    }&#xa;&#xa;    _defaults = {&#xa;        'paypal_use_ipn': True,&#xa;        'fees_active': False,&#xa;        'fees_dom_fixed': 0.35,&#xa;        'fees_dom_var': 3.4,&#xa;        'fees_int_fixed': 0.35,&#xa;        'fees_int_var': 3.9,&#xa;        'paypal_api_enabled': False,&#xa;    }&#xa;&#xa;    def _migrate_paypal_account(self, cr, uid, context=None):&#xa;        """""" COMPLETE ME """"""&#xa;        cr.execute('SELECT id, paypal_account FROM res_company')&#xa;        res = cr.fetchall()&#xa;        for (company_id, company_paypal_account) in res:&#xa;            if company_paypal_account:&#xa;                company_paypal_ids = self.search(cr, uid, [('company_id', '=', company_id), ('provider', '=', 'paypal')], limit=1, context=context)&#xa;                if company_paypal_ids:&#xa;                    self.write(cr, uid, company_paypal_ids, {'paypal_email_account': company_paypal_account}, context=context)&#xa;                else:&#xa;                    paypal_view = self.pool['ir.model.data'].get_object(cr, uid, 'payment_paypal', 'paypal_acquirer_button')&#xa;                    self.create(cr, uid, {&#xa;                        'name': 'Paypal',&#xa;                        'provider': 'paypal',&#xa;                        'paypal_email_account': company_paypal_account,&#xa;                        'view_template_id': paypal_view.id,&#xa;                    }, context=context)&#xa;        return True&#xa;&#xa;    def paypal_compute_fees(self, cr, uid, id, amount, currency_id, country_id, context=None):&#xa;        """""" Compute paypal fees.&#xa;&#xa;            :param float amount: the amount to pay&#xa;            :param integer country_id: an ID of a res.country, or None. This is&#xa;                                       the customer's country, to be compared to&#xa;                                       the acquirer company country.&#xa;            :return float fees: computed fees&#xa;        """"""&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;        if not acquirer.fees_active:&#xa;            return 0.0&#xa;        country = self.pool['res.country'].browse(cr, uid, country_id, context=context)&#xa;        if country and acquirer.company_id.country_id.id == country.id:&#xa;            percentage = acquirer.fees_dom_var&#xa;            fixed = acquirer.fees_dom_fixed&#xa;        else:&#xa;            percentage = acquirer.fees_int_var&#xa;            fixed = acquirer.fees_int_fixed&#xa;        fees = (percentage / 100.0 * amount + fixed ) / (1 - percentage / 100.0)&#xa;        return fees&#xa;&#xa;    def paypal_form_generate_values(self, cr, uid, id, values, context=None):&#xa;        base_url = self.pool['ir.config_parameter'].get_param(cr, SUPERUSER_ID, 'web.base.url')&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;&#xa;        paypal_tx_values = dict(values)&#xa;        paypal_tx_values.update({&#xa;            'cmd': '_xclick',&#xa;            'business': acquirer.paypal_email_account,&#xa;            'item_name': '%s: %s' % (acquirer.company_id.name, values['reference']),&#xa;            'item_number': values['reference'],&#xa;            'amount': values['amount'],&#xa;            'currency_code': values['currency'] and values['currency'].name or '',&#xa;            'address1': values.get('partner_address'),&#xa;            'city': values.get('partner_city'),&#xa;            'country': values.get('partner_country') and values.get('partner_country').name or '',&#xa;            'state': values.get('partner_state') and values.get('partner_state').name or '',&#xa;            'email': values.get('partner_email'),&#xa;            'zip_code': values.get('partner_zip'),&#xa;            'first_name': values.get('partner_first_name'),&#xa;            'last_name': values.get('partner_last_name'),&#xa;            'paypal_return': '%s' % urlparse.urljoin(base_url, PaypalController._return_url),&#xa;            'notify_url': '%s' % urlparse.urljoin(base_url, PaypalController._notify_url),&#xa;            'cancel_return': '%s' % urlparse.urljoin(base_url, PaypalController._cancel_url),&#xa;            'handling': '%.2f' % paypal_tx_values.pop('fees', 0.0) if acquirer.fees_active else False,&#xa;            'custom': json.dumps({'return_url': '%s' % paypal_tx_values.pop('return_url')}) if paypal_tx_values.get('return_url') else False,&#xa;        })&#xa;        return paypal_tx_values&#xa;&#xa;    def paypal_get_form_action_url(self, cr, uid, id, context=None):&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;        return self._get_paypal_urls(cr, uid, acquirer.environment, context=context)['paypal_form_url']&#xa;&#xa;    def _paypal_s2s_get_access_token(self, cr, uid, ids, context=None):&#xa;        """"""&#xa;        Note: see # see http://stackoverflow.com/questions/2407126/python-urllib2-basic-auth-problem&#xa;        for explanation why we use Authorization header instead of urllib2&#xa;        password manager&#xa;        """"""&#xa;        res = dict.fromkeys(ids, False)&#xa;        parameters = werkzeug.url_encode({'grant_type': 'client_credentials'})&#xa;&#xa;        for acquirer in self.browse(cr, uid, ids, context=context):&#xa;            tx_url = self._get_paypal_urls(cr, uid, acquirer.environment)['paypal_rest_url']&#xa;            request = urllib2.Request(tx_url, parameters)&#xa;&#xa;            # add other headers (https://developer.paypal.com/webapps/developer/docs/integration/direct/make-your-first-call/)&#xa;            request.add_header('Accept', 'application/json')&#xa;            request.add_header('Accept-Language', 'en_US')&#xa;&#xa;            # add authorization header&#xa;            base64string = base64.encodestring('%s:%s' % (&#xa;                acquirer.paypal_api_username,&#xa;                acquirer.paypal_api_password)&#xa;            ).replace('\n', '')&#xa;            request.add_header(""Authorization"", ""Basic %s"" % base64string)&#xa;&#xa;            request = urllib2.urlopen(request)&#xa;            result = request.read()&#xa;            res[acquirer.id] = json.loads(result).get('access_token')&#xa;            request.close()&#xa;        return res&#xa;&#xa;&#xa;class TxPaypal(osv.Model):&#xa;    _inherit = 'payment.transaction'&#xa;&#xa;    _columns = {&#xa;        'paypal_txn_type': fields.char('Transaction type'),&#xa;    }&#xa;&#xa;    # --------------------------------------------------&#xa;    # FORM RELATED METHODS&#xa;    # --------------------------------------------------&#xa;&#xa;    def _paypal_form_get_tx_from_data(self, cr, uid, data, context=None):&#xa;        reference, txn_id = data.get('item_number'), data.get('txn_id')&#xa;        if not reference or not txn_id:&#xa;            error_msg = _('Paypal: received data with missing reference (%s) or txn_id (%s)') % (reference, txn_id)&#xa;            _logger.info(error_msg)&#xa;            raise ValidationError(error_msg)&#xa;&#xa;        # find tx -> @TDENOTE use txn_id ?&#xa;        tx_ids = self.pool['payment.transaction'].search(cr, uid, [('reference', '=', reference)], context=context)&#xa;        if not tx_ids or len(tx_ids) > 1:&#xa;            error_msg = 'Paypal: received data for reference %s' % (reference)&#xa;            if not tx_ids:&#xa;                error_msg += '; no order found'&#xa;            else:&#xa;                error_msg += '; multiple order found'&#xa;            _logger.info(error_msg)&#xa;            raise ValidationError(error_msg)&#xa;        return self.browse(cr, uid, tx_ids[0], context=context)&#xa;&#xa;    def _paypal_form_get_invalid_parameters(self, cr, uid, tx, data, context=None):&#xa;        invalid_parameters = []&#xa;        _logger.info('Received a notification from Paypal with IPN version %s', data.get('notify_version'))&#xa;        if data.get('test_ipn'):&#xa;            _logger.warning(&#xa;                'Received a notification from Paypal using sandbox'&#xa;            ),&#xa;&#xa;        # TODO: txn_id: shoudl be false at draft, set afterwards, and verified with txn details&#xa;        if tx.acquirer_reference and data.get('txn_id') != tx.acquirer_reference:&#xa;            invalid_parameters.append(('txn_id', data.get('txn_id'), tx.acquirer_reference))&#xa;        # check what is buyed&#xa;        if float_compare(float(data.get('mc_gross', '0.0')), (tx.amount + tx.fees), 2) != 0:&#xa;            invalid_parameters.append(('mc_gross', data.get('mc_gross'), '%.2f' % tx.amount))  # mc_gross is amount + fees&#xa;        if data.get('mc_currency') != tx.currency_id.name:&#xa;            invalid_parameters.append(('mc_currency', data.get('mc_currency'), tx.currency_id.name))&#xa;        if 'handling_amount' in data and float_compare(float(data.get('handling_amount')), tx.fees, 2) != 0:&#xa;            invalid_parameters.append(('handling_amount', data.get('handling_amount'), tx.fees))&#xa;        # check buyer&#xa;        if tx.payment_method_id and data.get('payer_id') != tx.payment_method_id.acquirer_ref:&#xa;            invalid_parameters.append(('payer_id', data.get('payer_id'), tx.payment_method_id.acquirer_ref))&#xa;        # check seller&#xa;        if data.get('receiver_id') and tx.acquirer_id.paypal_seller_account and data['receiver_id'] != tx.acquirer_id.paypal_seller_account:&#xa;            invalid_parameters.append(('receiver_id', data.get('receiver_id'), tx.acquirer_id.paypal_seller_account))&#xa;        if not data.get('receiver_id') or not tx.acquirer_id.paypal_seller_account:&#xa;            # Check receiver_email only if receiver_id was not checked.&#xa;            # In Paypal, this is possible to configure as receiver_email a different email than the business email (the login email)&#xa;            # In Odoo, there is only one field for the Paypal email: the business email. This isn't possible to set a receiver_email&#xa;            # different than the business email. Therefore, if you want such a configuration in your Paypal, you are then obliged to fill&#xa;            # the Merchant ID in the Paypal payment acquirer in Odoo, so the check is performed on this variable instead of the receiver_email.&#xa;            # At least one of the two checks must be done, to avoid fraudsters.&#xa;            if data.get('receiver_email') != tx.acquirer_id.paypal_email_account:&#xa;                invalid_parameters.append(('receiver_email', data.get('receiver_email'), tx.acquirer_id.paypal_email_account))&#xa;&#xa;        return invalid_parameters&#xa;&#xa;    def _paypal_form_validate(self, cr, uid, tx, data, context=None):&#xa;        status = data.get('payment_status')&#xa;        res = {&#xa;            'acquirer_reference': data.get('txn_id'),&#xa;            'paypal_txn_type': data.get('payment_type'),&#xa;        }&#xa;        if status in ['Completed', 'Processed']:&#xa;            _logger.info('Validated Paypal payment for tx %s: set as done' % (tx.reference))&#xa;            res.update(state='done', date_validate=data.get('payment_date', fields.datetime.now()))&#xa;            return tx.write(res)&#xa;        elif status in ['Pending', 'Expired']:&#xa;            _logger.info('Received notification for Paypal payment %s: set as pending' % (tx.reference))&#xa;            res.update(state='pending', state_message=data.get('pending_reason', ''))&#xa;            return tx.write(res)&#xa;        else:&#xa;            error = 'Received unrecognized status for Paypal payment %s: %s, set as error' % (tx.reference, status)&#xa;            _logger.info(error)&#xa;            res.update(state='error', state_message=error)&#xa;            return tx.write(res)&#xa;&#xa;    # --------------------------------------------------&#xa;    # SERVER2SERVER RELATED METHODS&#xa;    # --------------------------------------------------&#xa;&#xa;    def _paypal_try_url(self, request, tries=3, context=None):&#xa;        """""" Try to contact Paypal. Due to some issues, internal service errors&#xa;        seem to be quite frequent. Several tries are done before considering&#xa;        the communication as failed.&#xa;&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        done, res = False, None&#xa;        while (not done and tries):&#xa;            try:&#xa;                res = urllib2.urlopen(request)&#xa;                done = True&#xa;            except urllib2.HTTPError as e:&#xa;                res = e.read()&#xa;                e.close()&#xa;                if tries and res and json.loads(res)['name'] == 'INTERNAL_SERVICE_ERROR':&#xa;                    _logger.warning('Failed contacting Paypal, retrying (%s remaining)' % tries)&#xa;            tries = tries - 1&#xa;        if not res:&#xa;            pass&#xa;            # raise openerp.exceptions.&#xa;        result = res.read()&#xa;        res.close()&#xa;        return result&#xa;&#xa;    def _paypal_s2s_send(self, cr, uid, values, cc_values, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        tx_id = self.create(cr, uid, values, context=context)&#xa;        tx = self.browse(cr, uid, tx_id, context=context)&#xa;&#xa;        headers = {&#xa;            'Content-Type': 'application/json',&#xa;            'Authorization': 'Bearer %s' % tx.acquirer_id._paypal_s2s_get_access_token()[tx.acquirer_id.id],&#xa;        }&#xa;        data = {&#xa;            'intent': 'sale',&#xa;            'transactions': [{&#xa;                'amount': {&#xa;                    'total': '%.2f' % tx.amount,&#xa;                    'currency': tx.currency_id.name,&#xa;                },&#xa;                'description': tx.reference,&#xa;            }]&#xa;        }&#xa;        if cc_values:&#xa;            data['payer'] = {&#xa;                'payment_method': 'credit_card',&#xa;                'funding_instruments': [{&#xa;                    'credit_card': {&#xa;                        'number': cc_values['number'],&#xa;                        'type': cc_values['brand'],&#xa;                        'expire_month': cc_values['expiry_mm'],&#xa;                        'expire_year': cc_values['expiry_yy'],&#xa;                        'cvv2': cc_values['cvc'],&#xa;                        'first_name': tx.partner_name,&#xa;                        'last_name': tx.partner_name,&#xa;                        'billing_address': {&#xa;                            'line1': tx.partner_address,&#xa;                            'city': tx.partner_city,&#xa;                            'country_code': tx.partner_country_id.code,&#xa;                            'postal_code': tx.partner_zip,&#xa;                        }&#xa;                    }&#xa;                }]&#xa;            }&#xa;        else:&#xa;            # TODO: complete redirect URLs&#xa;            data['redirect_urls'] = {&#xa;                # 'return_url': 'http://example.com/your_redirect_url/',&#xa;                # 'cancel_url': 'http://example.com/your_cancel_url/',&#xa;            },&#xa;            data['payer'] = {&#xa;                'payment_method': 'paypal',&#xa;            }&#xa;        data = json.dumps(data)&#xa;&#xa;        request = urllib2.Request('https://api.sandbox.paypal.com/v1/payments/payment', data, headers)&#xa;        result = self._paypal_try_url(request, tries=3, context=context)&#xa;        return (tx_id, result)&#xa;&#xa;    def _paypal_s2s_get_invalid_parameters(self, cr, uid, tx, data, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        invalid_parameters = []&#xa;        return invalid_parameters&#xa;&#xa;    def _paypal_s2s_validate(self, cr, uid, tx, data, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        values = json.loads(data)&#xa;        status = values.get('state')&#xa;        if status in ['approved']:&#xa;            _logger.info('Validated Paypal s2s payment for tx %s: set as done' % (tx.reference))&#xa;            tx.write({&#xa;                'state': 'done',&#xa;                'date_validate': values.get('udpate_time', fields.datetime.now()),&#xa;                'paypal_txn_id': values['id'],&#xa;            })&#xa;            return True&#xa;        elif status in ['pending', 'expired']:&#xa;            _logger.info('Received notification for Paypal s2s payment %s: set as pending' % (tx.reference))&#xa;            tx.write({&#xa;                'state': 'pending',&#xa;                # 'state_message': data.get('pending_reason', ''),&#xa;                'paypal_txn_id': values['id'],&#xa;            })&#xa;            return True&#xa;        else:&#xa;            error = 'Received unrecognized status for Paypal s2s payment %s: %s, set as error' % (tx.reference, status)&#xa;            _logger.info(error)&#xa;            tx.write({&#xa;                'state': 'error',&#xa;                # 'state_message': error,&#xa;                'paypal_txn_id': values['id'],&#xa;            })&#xa;            return False&#xa;&#xa;    def _paypal_s2s_get_tx_status(self, cr, uid, tx, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        # TDETODO: check tx.paypal_txn_id is set&#xa;        headers = {&#xa;            'Content-Type': 'application/json',&#xa;            'Authorization': 'Bearer %s' % tx.acquirer_id._paypal_s2s_get_access_token()[tx.acquirer_id.id],&#xa;        }&#xa;        url = 'https://api.sandbox.paypal.com/v1/payments/payment/%s' % (tx.paypal_txn_id)&#xa;        request = urllib2.Request(url, headers=headers)&#xa;        data = self._paypal_try_url(request, tries=3, context=context)&#xa;        return self.s2s_feedback(cr, uid, tx.id, data, context=context)&#xa;"
264224|"#!/usr/bin/env python&#xa;&#xa;""""""&#xa;Given an input path and an output path, will put&#xa;Gzipped versions of all files from the input path&#xa;to the output path.&#xa;&#xa;If the file is not gzippable it will be copied&#xa;uncompressed.&#xa;""""""&#xa;&#xa;from fnmatch import fnmatch&#xa;import gzip&#xa;import os&#xa;import shutil&#xa;import sys&#xa;&#xa;class FakeTime:&#xa;    def time(self):&#xa;        return 1261130520.0&#xa;&#xa;# Hack to override gzip's time implementation&#xa;# See: http://stackoverflow.com/questions/264224/setting-the-gzip-timestamp-from-python&#xa;gzip.time = FakeTime()&#xa;&#xa;def is_compressable(filename, gzip_globs):&#xa;    """"""&#xa;    Determine if a filename is a gzippable type&#xa;    by comparing to a known list.&#xa;    """"""&#xa;    return any([fnmatch(filename, glob) for glob in gzip_globs])&#xa;&#xa;def compress(file_path):&#xa;    """"""&#xa;    Gzip a single file in place.&#xa;    """"""&#xa;    f_in = open(file_path, 'rb')&#xa;    contents = f_in.readlines()&#xa;    f_in.close()&#xa;    f_out = gzip.open(file_path, 'wb')&#xa;    f_out.writelines(contents)&#xa;    f_out.close()&#xa;&#xa;def main():&#xa;    in_path = sys.argv[1]&#xa;    out_path = sys.argv[2]&#xa;&#xa;    with open('gzip_types.txt') as f:&#xa;        gzip_globs = [glob.strip() for glob in f]&#xa;&#xa;    # Folders&#xa;    if os.path.isdir(in_path):&#xa;        shutil.rmtree(out_path, ignore_errors=True)&#xa;        shutil.copytree(in_path, out_path)&#xa;&#xa;        for path, dirs, files in os.walk(sys.argv[2]):&#xa;            for filename in files:&#xa;                # Is it a gzippable file type?&#xa;                if not is_compressable(filename, gzip_globs):&#xa;                    continue&#xa;&#xa;                file_path = os.path.join(path, filename)&#xa;&#xa;                compress(file_path)&#xa;    # Single files&#xa;    else:&#xa;        filename = os.path.split(in_path)[-1]&#xa;&#xa;        try:&#xa;            os.remove(out_path)&#xa;        except OSError:&#xa;            pass&#xa;&#xa;        shutil.copy(in_path, out_path)&#xa;&#xa;        if not is_compressable(filename, gzip_globs):&#xa;            return &#xa;&#xa;        compress(out_path)&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    main()&#xa;"
3828723|"""""""&#xa;Python part of radio playout (pypo)&#xa;""""""&#xa;&#xa;from optparse import OptionParser&#xa;from datetime import datetime&#xa;&#xa;import telnetlib&#xa;&#xa;import time&#xa;import sys&#xa;import signal&#xa;import logging&#xa;import locale&#xa;import os&#xa;import re&#xa;&#xa;from Queue import Queue&#xa;from threading import Lock&#xa;&#xa;from pypopush import PypoPush&#xa;from pypofetch import PypoFetch&#xa;from pypofile import PypoFile&#xa;from recorder import Recorder&#xa;from listenerstat import ListenerStat&#xa;from pypomessagehandler import PypoMessageHandler&#xa;from pypoliquidsoap import PypoLiquidsoap&#xa;from timeout import ls_timeout&#xa;&#xa;from pypo.media.update.replaygainupdater import ReplayGainUpdater&#xa;from pypo.media.update.silananalyzer import SilanAnalyzer&#xa;&#xa;from configobj import ConfigObj&#xa;&#xa;# custom imports&#xa;from api_clients import api_client&#xa;#from std_err_override import LogWriter&#xa;import pure&#xa;&#xa;LOG_PATH = '/var/log/airtime/pypo/pypo.log'&#xa;LOG_LEVEL = logging.INFO&#xa;&#xa;# Set up command-line options&#xa;parser = OptionParser()&#xa;&#xa;# help screen / info&#xa;usage = ""%prog [options]"" + "" - python playout system""&#xa;parser = OptionParser(usage=usage)&#xa;&#xa;# Options&#xa;parser.add_option(""-v"", ""--compat"",&#xa;        help=""Check compatibility with server API version"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""check_compat"")&#xa;&#xa;parser.add_option(""-t"", ""--test"",&#xa;        help=""Do a test to make sure everything is working properly."",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""test"")&#xa;&#xa;parser.add_option(""-b"",&#xa;        ""--cleanup"",&#xa;        help=""Cleanup"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""cleanup"")&#xa;&#xa;parser.add_option(""-c"",&#xa;        ""--check"",&#xa;        help=""Check the cached schedule and exit"",&#xa;        default=False,&#xa;        action=""store_true"",&#xa;        dest=""check"")&#xa;&#xa;# parse options&#xa;(options, args) = parser.parse_args()&#xa;&#xa;LIQUIDSOAP_MIN_VERSION = ""1.1.1""&#xa;&#xa;PYPO_HOME='/var/tmp/airtime/pypo/'&#xa;&#xa;def configure_environment():&#xa;    os.environ[""HOME""] = PYPO_HOME&#xa;    os.environ[""TERM""] = 'xterm'&#xa;&#xa;configure_environment()&#xa;&#xa;# need to wait for Python 2.7 for this..&#xa;logging.captureWarnings(True)&#xa;&#xa;# configure logging&#xa;try:&#xa;    # Set up logging&#xa;    logFormatter = logging.Formatter(""%(asctime)s [%(module)s] [%(levelname)-5.5s]  %(message)s"")&#xa;    rootLogger = logging.getLogger()&#xa;    rootLogger.setLevel(LOG_LEVEL)&#xa;    logger = rootLogger&#xa;&#xa;    consoleHandler = logging.StreamHandler()&#xa;    consoleHandler.setFormatter(logFormatter)&#xa;    rootLogger.addHandler(consoleHandler)&#xa;except Exception, e:&#xa;    print ""Couldn't configure logging"", e&#xa;    sys.exit(1)&#xa;&#xa;def configure_locale():&#xa;    """"""&#xa;    Silly hacks to force Python 2.x to run in UTF-8 mode. Not portable at all,&#xa;    however serves our purpose at the moment.&#xa;&#xa;    More information available here:&#xa;    http://stackoverflow.com/questions/3828723/why-we-need-sys-setdefaultencodingutf-8-in-a-py-script&#xa;    """"""&#xa;    logger.debug(""Before %s"", locale.nl_langinfo(locale.CODESET))&#xa;    current_locale = locale.getlocale()&#xa;&#xa;    if current_locale[1] is None:&#xa;        logger.debug(""No locale currently set. Attempting to get default locale."")&#xa;        default_locale = locale.getdefaultlocale()&#xa;&#xa;        if default_locale[1] is None:&#xa;            logger.debug(""No default locale exists. Let's try loading from \&#xa;                    /etc/default/locale"")&#xa;            if os.path.exists(""/etc/default/locale""):&#xa;                locale_config = ConfigObj('/etc/default/locale')&#xa;                lang = locale_config.get('LANG')&#xa;                new_locale = lang&#xa;            else:&#xa;                logger.error(""/etc/default/locale could not be found! Please \&#xa;                        run 'sudo update-locale' from command-line."")&#xa;                sys.exit(1)&#xa;        else:&#xa;            new_locale = default_locale&#xa;&#xa;        logger.info(""New locale set to: %s"", \&#xa;                locale.setlocale(locale.LC_ALL, new_locale))&#xa;&#xa;    reload(sys)&#xa;    sys.setdefaultencoding(""UTF-8"")&#xa;    current_locale_encoding = locale.getlocale()[1].lower()&#xa;    logger.debug(""sys default encoding %s"", sys.getdefaultencoding())&#xa;    logger.debug(""After %s"", locale.nl_langinfo(locale.CODESET))&#xa;&#xa;    if current_locale_encoding not in ['utf-8', 'utf8']:&#xa;        logger.error(""Need a UTF-8 locale. Currently '%s'. Exiting..."" % \&#xa;                current_locale_encoding)&#xa;        sys.exit(1)&#xa;&#xa;&#xa;configure_locale()&#xa;&#xa;# loading config file&#xa;try:&#xa;    config = ConfigObj('/etc/airtime/airtime.conf')&#xa;except Exception, e:&#xa;    logger.error('Error loading config file: %s', e)&#xa;    sys.exit(1)&#xa;&#xa;class Global:&#xa;    def __init__(self, api_client):&#xa;        self.api_client = api_client&#xa;&#xa;    def selfcheck(self):&#xa;        return self.api_client.is_server_compatible()&#xa;&#xa;    def test_api(self):&#xa;        self.api_client.test()&#xa;&#xa;def keyboardInterruptHandler(signum, frame):&#xa;    logger = logging.getLogger()&#xa;    logger.info('\nKeyboard Interrupt\n')&#xa;    sys.exit(0)&#xa;&#xa;@ls_timeout&#xa;def liquidsoap_get_info(telnet_lock, host, port, logger):&#xa;    logger.debug(""Checking to see if Liquidsoap is running"")&#xa;    try:&#xa;        telnet_lock.acquire()&#xa;        tn = telnetlib.Telnet(host, port)&#xa;        msg = ""version\n""&#xa;        tn.write(msg)&#xa;        tn.write(""exit\n"")&#xa;        response = tn.read_all()&#xa;    except Exception, e:&#xa;        logger.error(str(e))&#xa;        return None&#xa;    finally:&#xa;        telnet_lock.release()&#xa;&#xa;    return get_liquidsoap_version(response)&#xa;&#xa;def get_liquidsoap_version(version_string):&#xa;    m = re.match(r""Liquidsoap (\d+.\d+.\d+)"", version_string)&#xa;&#xa;    if m:&#xa;        return m.group(1)&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;    if m:&#xa;        current_version = m.group(1)&#xa;        return pure.version_cmp(current_version, LIQUIDSOAP_MIN_VERSION) >= 0&#xa;    return False&#xa;&#xa;def liquidsoap_startup_test():&#xa;&#xa;    liquidsoap_version_string = \&#xa;            liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;    while not liquidsoap_version_string:&#xa;        logger.warning(""Liquidsoap doesn't appear to be running!, "" + \&#xa;               ""Sleeping and trying again"")&#xa;        time.sleep(1)&#xa;        liquidsoap_version_string = \&#xa;                liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;&#xa;    while pure.version_cmp(liquidsoap_version_string, LIQUIDSOAP_MIN_VERSION) < 0:&#xa;        logger.warning(""Liquidsoap is running but in incorrect version! "" + \&#xa;                ""Make sure you have at least Liquidsoap %s installed"" % LIQUIDSOAP_MIN_VERSION)&#xa;        time.sleep(1)&#xa;        liquidsoap_version_string = \&#xa;                liquidsoap_get_info(telnet_lock, ls_host, ls_port, logger)&#xa;&#xa;    logger.info(""Liquidsoap version string found %s"" % liquidsoap_version_string)&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    logger.info('###########################################')&#xa;    logger.info('#             *** pypo  ***               #')&#xa;    logger.info('#   Liquidsoap Scheduled Playout System   #')&#xa;    logger.info('###########################################')&#xa;&#xa;    #Although all of our calculations are in UTC, it is useful to know what timezone&#xa;    #the local machine is, so that we have a reference for what time the actual&#xa;    #log entries were made&#xa;    logger.info(""Timezone: %s"" % str(time.tzname))&#xa;    logger.info(""UTC time: %s"" % str(datetime.utcnow()))&#xa;&#xa;    signal.signal(signal.SIGINT, keyboardInterruptHandler)&#xa;&#xa;    api_client = api_client.AirtimeApiClient()&#xa;    g = Global(api_client)&#xa;&#xa;    while not g.selfcheck():&#xa;        time.sleep(5)&#xa;&#xa;    success = False&#xa;    while not success:&#xa;        try:&#xa;            api_client.register_component('pypo')&#xa;            success = True&#xa;        except Exception, e:&#xa;            logger.error(str(e))&#xa;            time.sleep(10)&#xa;&#xa;    telnet_lock = Lock()&#xa;&#xa;    ls_host = config['pypo']['ls_host']&#xa;    ls_port = config['pypo']['ls_port']&#xa;&#xa;    liquidsoap_startup_test()&#xa;&#xa;    if options.test:&#xa;        g.test_api()&#xa;        sys.exit(0)&#xa;&#xa;&#xa;    ReplayGainUpdater.start_reply_gain(api_client)&#xa;    SilanAnalyzer.start_silan(api_client, logger)&#xa;&#xa;    pypoFetch_q = Queue()&#xa;    recorder_q = Queue()&#xa;    pypoPush_q = Queue()&#xa;&#xa;    pypo_liquidsoap = PypoLiquidsoap(logger, telnet_lock,\&#xa;            ls_host, ls_port)&#xa;&#xa;    """"""&#xa;    This queue is shared between pypo-fetch and pypo-file, where pypo-file&#xa;    is the consumer. Pypo-fetch will send every schedule it gets to pypo-file&#xa;    and pypo will parse this schedule to determine which file has the highest&#xa;    priority, and retrieve it.&#xa;    """"""&#xa;    media_q = Queue()&#xa;&#xa;    # Pass only the configuration sections needed; PypoMessageHandler only needs rabbitmq settings&#xa;    pmh = PypoMessageHandler(pypoFetch_q, recorder_q, config['rabbitmq'])&#xa;    pmh.daemon = True&#xa;    pmh.start()&#xa;&#xa;    pfile = PypoFile(media_q, config['pypo'])&#xa;    pfile.daemon = True&#xa;    pfile.start()&#xa;&#xa;    pf = PypoFetch(pypoFetch_q, pypoPush_q, media_q, telnet_lock, pypo_liquidsoap, config['pypo'])&#xa;    pf.daemon = True&#xa;    pf.start()&#xa;&#xa;    pp = PypoPush(pypoPush_q, telnet_lock, pypo_liquidsoap, config['pypo'])&#xa;    pp.daemon = True&#xa;    pp.start()&#xa;&#xa;    recorder = Recorder(recorder_q)&#xa;    recorder.daemon = True&#xa;    recorder.start()&#xa;&#xa;    stat = ListenerStat()&#xa;    stat.daemon = True&#xa;    stat.start()&#xa;&#xa;    # Just sleep the main thread, instead of blocking on pf.join().&#xa;    # This allows CTRL-C to work!&#xa;    while True:&#xa;        time.sleep(1)&#xa;&#xa;    logger.info(""System exit"")&#xa;"
14381940|"""""""&#xa;Module to exercize virsh attach-device command with various devices/options&#xa;""""""&#xa;&#xa;import os&#xa;import os.path&#xa;import logging&#xa;import aexpect&#xa;&#xa;from string import ascii_lowercase&#xa;&#xa;from autotest.client.shared import error&#xa;&#xa;from virttest import virt_vm, virsh, remote, utils_misc, data_dir&#xa;from virttest.libvirt_xml.vm_xml import VMXML&#xa;from virttest.staging.backports import itertools&#xa;&#xa;from provider import libvirt_version&#xa;&#xa;# TODO: Move all these helper classes someplace else&#xa;&#xa;&#xa;class TestParams(object):&#xa;&#xa;    """"""&#xa;    Organize test parameters and decouple from params names&#xa;    """"""&#xa;&#xa;    def __init__(self, params, env, test, test_prefix='vadu_dev_obj_'):&#xa;        self.test_prefix = test_prefix&#xa;        self.test = test&#xa;        self.vmxml = None  # Can't be known yet&#xa;        self.virsh = None  # Can't be known yet&#xa;        self._e = env&#xa;        self._p = params&#xa;&#xa;    @property&#xa;    def start_vm(self):&#xa;        # Required parameter&#xa;        return bool('yes' == self._p['start_vm'])&#xa;&#xa;    @property&#xa;    def main_vm(self):&#xa;        # Required parameter&#xa;        return self._e.get_vm(self._p[""main_vm""])&#xa;&#xa;    @property&#xa;    def file_ref(self):&#xa;        default = ""normal""&#xa;        return self._p.get('vadu_file_ref', default)&#xa;&#xa;    @property&#xa;    def dom_ref(self):&#xa;        default = ""name""&#xa;        return self._p.get('vadu_dom_ref', default)&#xa;&#xa;    @property&#xa;    def dom_value(self):&#xa;        default = None&#xa;        return self._p.get('vadu_dom_value', default)&#xa;&#xa;    @property&#xa;    def extra(self):&#xa;        default = None&#xa;        return self._p.get('vadu_extra', default)&#xa;&#xa;    @property&#xa;    def status_error(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('status_error', default))&#xa;&#xa;    @property&#xa;    def mmconfig(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_config_option', default))&#xa;&#xa;    @property&#xa;    def preboot_function_error(self):&#xa;        return bool(""yes"" == self._p['vadu_preboot_function_error'])&#xa;&#xa;    @property&#xa;    def pstboot_function_error(self):&#xa;        return bool(""yes"" == self._p['vadu_pstboot_function_error'])&#xa;&#xa;    @property&#xa;    def domain_positional(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_domain_positional', default))&#xa;&#xa;    @property&#xa;    def file_positional(self):&#xa;        default = 'no'&#xa;        return bool('yes' == self._p.get('vadu_file_positional', default))&#xa;&#xa;    @property&#xa;    def devs(self):&#xa;        return self._p.objects('vadu_dev_objs')  # mandatory parameter&#xa;&#xa;    def dev_params(self, class_name):&#xa;        """"""&#xa;        Return Dictionary after parsing out prefix + class name postfix&#xa;&#xa;        e.g. vadu_dev_obj_meg_VirtualDisk = 100&#xa;             ^^^^^^^^^^^^^ ^  ^^^^^^^^^^    ^&#xa;        strip   prefix     |  classname     |&#xa;                           |                |&#xa;                           |                |&#xa;        Return        key--+         value--+&#xa;        """"""&#xa;&#xa;        # Roll up all keys with '_class_name' into top-level&#xa;        # keys with same name and no '_class_name' postfix.&#xa;        #       See Params.object_params() docstring&#xa;        object_params = self._p.object_params(class_name)&#xa;        # Return variable to hold modified key names&#xa;        device_params = {}&#xa;        for prefixed_key, original_value in object_params.items():&#xa;            # They get unrolled, but originals always left behind, skip them&#xa;            if prefixed_key.count(class_name):&#xa;                continue&#xa;            if prefixed_key.startswith(self.test_prefix):&#xa;                stripped_key = prefixed_key[len(self.test_prefix):]&#xa;                device_params[stripped_key] = original_value&#xa;        # The 'count' key required by all VADU AttachDeviceBase subclasses&#xa;        if 'count' not in device_params.keys():&#xa;            # stick prefix back on so error message has meaning&#xa;            raise error.TestError('%scount is a required parameter'&#xa;                                  % (self.test_prefix))&#xa;        return device_params&#xa;&#xa;    @staticmethod&#xa;    def cleanup(test_dev_list):&#xa;        xcpt_list = []&#xa;        for device in test_dev_list:&#xa;            try:&#xa;                device.cleanup()&#xa;                # Attempt to finish entire list before raising&#xa;                # any exceptions that occurred&#xa;            # ignore pylint W0703 - exception acumulated and raised below&#xa;            except Exception, xcept_obj:&#xa;                xcpt_list.append(xcept_obj)&#xa;        if xcpt_list:&#xa;            raise RuntimeError(""One or more exceptions occurred during ""&#xa;                               ""cleanup: %s"" % str(xcpt_list))&#xa;&#xa;&#xa;class TestDeviceBase(object):&#xa;&#xa;    """"""&#xa;    Base class for test devices creator and verification subclasses&#xa;    """"""&#xa;&#xa;    # Class-specific unique string&#xa;    identifier = None&#xa;    # Parameters that come in from Cartesian:&#xa;    count = 0  # number of devices to make/test&#xa;    # flag for use in test and by operate() & function() methods&#xa;    booted = False&#xa;&#xa;    def __init__(self, test_params):&#xa;        """"""&#xa;        Setup one or more device xml for a device based on TestParams instance&#xa;        """"""&#xa;        if self.__class__.identifier is None:&#xa;            identifier = utils_misc.generate_random_string(4)&#xa;            self.__class__.identifier = identifier&#xa;        # how many of this type of device to make&#xa;        self.test_params = test_params&#xa;        # Copy params for this class into attributes&#xa;        cls_name = self.__class__.__name__&#xa;        # Already have test-prefix stripped off&#xa;        for key, value in self.test_params.dev_params(cls_name).items():&#xa;            # Any keys with _anything are not used by this class&#xa;            if key.count('_') > 0:&#xa;                logging.debug(""Removing key: %s from params for class %s"",&#xa;                              test_params.test_prefix + key, cls_name)&#xa;                continue&#xa;            # Attempt to convert numbers&#xa;            try:&#xa;                setattr(self, key, int(value))&#xa;            except ValueError:&#xa;                setattr(self, key, value)&#xa;        if self.count < 1:&#xa;            raise error.TestError(""Configuration for class %s count must ""&#xa;                                  ""be specified and greater than zero"")&#xa;        logging.info(""Setting up %d %s device(s)"", self.count, cls_name)&#xa;        # Setup each device_xml instance&#xa;        self._device_xml_list = [self.init_device(index)&#xa;                                 # test_params.dev_params() enforces count&#xa;                                 for index in xrange(0, self.count)]&#xa;&#xa;    def cleanup(self):&#xa;        """"""&#xa;        Remove any temporary files or processes created for testing&#xa;        """"""&#xa;        pass&#xa;&#xa;    @property&#xa;    def device_xmls(self):&#xa;        """"""&#xa;        Return list of device_xml instances&#xa;        """"""&#xa;        return self._device_xml_list&#xa;&#xa;    @property&#xa;    def operation_results(self):&#xa;        """"""&#xa;        Return a list of True/False lists for operation state per device&#xa;        """"""&#xa;        return [self.operate(index) for index in xrange(self.count)]&#xa;&#xa;    @property&#xa;    def function_results(self):&#xa;        """"""&#xa;        Return a list of True/False lists for functional state per device&#xa;        """"""&#xa;        return [self.function(index) for index in xrange(self.count)]&#xa;&#xa;    @staticmethod&#xa;    def good_results(results_list):&#xa;        """"""&#xa;        Return True if all member lists contain only True values&#xa;        """"""&#xa;        for outer in results_list:&#xa;            for inner in outer:&#xa;                if inner is False:&#xa;                    return False&#xa;        return True&#xa;&#xa;    @staticmethod&#xa;    def bad_results(results_list):&#xa;        """"""&#xa;        Return True if all member lists contain only False values&#xa;        """"""&#xa;        for outer in results_list:&#xa;            for inner in outer:&#xa;                if inner is True:&#xa;                    return False&#xa;        return True&#xa;&#xa;    # These should be overridden in subclasses&#xa;    def init_device(self, index):&#xa;        """"""&#xa;        Initialize and return instance of device xml for index&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;    def operate(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of operating on a device&#xa;        """"""&#xa;        # N/B: Take care of self.started&#xa;        raise NotImplementedError&#xa;&#xa;    def function(self, index):&#xa;        """"""&#xa;        Return True/False device functioning&#xa;        """"""&#xa;        # N/B: Take care of self.test_params.start_vm&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;def make_vadu_dargs(test_params, xml_filepath):&#xa;    """"""&#xa;    Return keyword argument dict for virsh attach, detach, update functions&#xa;&#xa;    @param: test_params: a TestParams object&#xa;    @param: xml_filepath: Full path to device XML file (may not exist)&#xa;    """"""&#xa;    dargs = {}&#xa;    # Params value for domain reference (i.e. specific name, number, etc).&#xa;    if test_params.dom_value is None:  # No specific value set&#xa;        if test_params.dom_ref == ""name"":  # reference by runtime name&#xa;            domain = test_params.main_vm.name&#xa;        elif test_params.dom_ref == ""id"":&#xa;            domain = test_params.main_vm.get_id()&#xa;        elif test_params.dom_ref == ""uuid"":&#xa;            domain = test_params.main_vm.get_uuid()&#xa;        elif test_params.dom_ref == ""bad_domain_hex"":&#xa;            domain = ""0x%x"" % int(test_params.main_vm.get_id())&#xa;        elif test_params.dom_ref == ""none"":&#xa;            domain = None&#xa;        else:&#xa;            raise error.TestError(""Parameter vadu_dom_ref or ""&#xa;                                  ""vadu_dom_value are required"")&#xa;    else:  # Config. specified a vadu_dom_value&#xa;        domain = test_params.dom_value&#xa;&#xa;    if test_params.file_ref == ""normal"":  # The default&#xa;        file_value = xml_filepath  # Use un-altered path&#xa;    elif test_params.file_ref == ""empty"":  # empty string&#xa;        file_value = """"  # Empty string argument will be passed!&#xa;    elif test_params.file_ref == ""missing"":&#xa;        file_value = os.path.join(""path"", ""does"", ""not"", ""exist"")&#xa;    elif test_params.file_ref == ""none"":&#xa;        file_value = None  # No file specified&#xa;    else:&#xa;        raise error.TestError(""Parameter vadu_file_ref is reuqired"")&#xa;&#xa;    if test_params.domain_positional:  # boolean&#xa;        dargs['domainarg'] = domain&#xa;    else:&#xa;        dargs['domain_opt'] = domain&#xa;&#xa;    if test_params.file_positional:&#xa;        dargs['filearg'] = file_value&#xa;    else:&#xa;        dargs['file_opt'] = file_value&#xa;&#xa;    if test_params.mmconfig:&#xa;        dargs['flagstr'] = ""--config""&#xa;    else:&#xa;        dargs['flagstr'] = """"&#xa;&#xa;    if test_params.extra is not None:&#xa;        dargs['flagstr'] += "" %s"" % test_params.extra&#xa;    return dargs&#xa;&#xa;&#xa;class AttachDeviceBase(TestDeviceBase):&#xa;&#xa;    """"""&#xa;    All operation behavior is same  all device types in this module&#xa;    """"""&#xa;&#xa;    def operate(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of operating on a device&#xa;        """"""&#xa;        vadu_dargs = make_vadu_dargs(self.test_params,&#xa;                                     self.device_xmls[index].xml)&#xa;        # Acts as a dict for it's own API params&#xa;        self.test_params.virsh['debug'] = True&#xa;        vadu_dargs.update(self.test_params.virsh)&#xa;        options = vadu_dargs.get('flagstr')&#xa;        if options:&#xa;            opt_list = options.split()&#xa;            for opt in opt_list:&#xa;                if not virsh.has_command_help_match(""attach-device"", opt) and\&#xa;                   not self.test_params.status_error:&#xa;                    raise error.TestNAError(""Current libvirt version doesn't ""&#xa;                                            ""support '%s' for attach-device""&#xa;                                            "" command"" % opt)&#xa;        cmdresult = self.test_params.virsh.attach_device(**vadu_dargs)&#xa;        self.test_params.virsh['debug'] = False&#xa;        # Command success is not enough, must also confirm activity worked&#xa;&#xa;        # output XML no matter attach pass or not&#xa;        logging.debug(""Attached XML:"")&#xa;        for line in str(self.device_xmls[index]).splitlines():&#xa;            logging.debug(""%s"", line)&#xa;&#xa;        if (cmdresult.exit_status == 0):&#xa;            if (cmdresult.stdout.count('attached successfully') or&#xa;                    cmdresult.stderr.count('attached successfully')):&#xa;                return True&#xa;        else:&#xa;            # See analyze_negative_results - expects return of true&#xa;            if self.test_params.status_error:&#xa;                return True&#xa;            else:&#xa;                return False&#xa;&#xa;    # Overridden in classes below&#xa;    def init_device(self, index):&#xa;        raise NotImplementedError&#xa;&#xa;    def function(self, index):&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;class SerialFile(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simplistic File-backed isa-serial device test helper&#xa;&#xa;    Consumes Cartesian object parameters:&#xa;        count - number of devices to make&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    type_name = ""file""&#xa;&#xa;    def make_filepath(self, index):&#xa;        """"""Return full path to unique filename per device index""""""&#xa;        # auto-cleaned at end of test&#xa;        return os.path.join(data_dir.get_tmp_dir(), 'serial_%s_%s-%d.log'&#xa;                            % (self.type_name, self.identifier, index))&#xa;&#xa;    @staticmethod&#xa;    def make_source(filepath):&#xa;        """"""Create filepath on disk""""""&#xa;        open(filepath, ""wb"")&#xa;&#xa;    def init_device(self, index):&#xa;        filepath = self.make_filepath(index)&#xa;        self.make_source(filepath)&#xa;        serialclass = self.test_params.vmxml.get_device_class('serial')&#xa;        serial_device = serialclass(type_name=self.type_name,&#xa;                                    virsh_instance=self.test_params.virsh)&#xa;        serial_device.add_source(path=filepath)&#xa;        # Assume default domain serial device on port 0 and index starts at 0&#xa;        serial_device.add_target(port=str(index + 1))&#xa;        return serial_device&#xa;&#xa;    def cleanup(self):&#xa;        for index in xrange(0, self.count):&#xa;            try:&#xa;                os.unlink(self.make_filepath(index))&#xa;            except OSError:&#xa;                pass  # Don't care if not there&#xa;&#xa;    def function(self, index):&#xa;        # TODO: Try to read/write some serial data&#xa;        # Just a stub for now&#xa;        logging.info(""STUB: Serial device functional test passed: %s"",&#xa;                     str(not self.test_params.status_error))&#xa;        # Return an error if an error is expected&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class SerialPipe(SerialFile):&#xa;&#xa;    """"""&#xa;    Simplistic pipe-backed isa-serial device&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    type_name = ""pipe""&#xa;&#xa;    @staticmethod&#xa;    def make_source(filepath):&#xa;        try:&#xa;            os.unlink(filepath)&#xa;        except OSError:&#xa;            pass&#xa;        os.mkfifo(filepath)&#xa;&#xa;    def init_device(self, index):&#xa;        return super(SerialPipe, self).init_device(index)  # stub for now&#xa;&#xa;&#xa;class Console(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple console device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        consoleclass = self.test_params.vmxml.get_device_class('console')&#xa;        console_device = consoleclass(type_name=self.type,&#xa;                                      virsh_instance=self.test_params.virsh)&#xa;        # Assume default domain console device on port 0 and index starts at 0&#xa;        console_device.add_target(type=self.targettype, port=str(index + 1))&#xa;        return console_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class Channel(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple channel device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        channelclass = self.test_params.vmxml.get_device_class('channel')&#xa;        channel_device = channelclass(type_name=self.type,&#xa;                                      virsh_instance=self.test_params.virsh)&#xa;        if hasattr(self, 'sourcemode') and hasattr(self, 'sourcepath'):&#xa;            channel_device.add_source(mode=self.sourcemode,&#xa;                                      path=self.sourcepath)&#xa;        if hasattr(self, 'targettype') and hasattr(self, 'targetname'):&#xa;            channel_device.add_target(type=self.targettype,&#xa;                                      name=self.targetname)&#xa;        return channel_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class Controller(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple controller device&#xa;    """"""&#xa;&#xa;    def init_device(self, index):&#xa;        controllerclass = self.test_params.vmxml.get_device_class('controller')&#xa;        controller_device = controllerclass(type_name=self.type,&#xa;                                            virsh_instance=self.test_params.virsh)&#xa;        controller_device.model = self.model&#xa;        return controller_device&#xa;&#xa;    def function(self, index):&#xa;        return not self.test_params.status_error&#xa;&#xa;&#xa;class VirtualDiskBasic(AttachDeviceBase):&#xa;&#xa;    """"""&#xa;    Simple File-backed virtio/usb/sata/scsi/ide raw disk device&#xa;    """"""&#xa;&#xa;    identifier = None&#xa;    count = 0  # number of devices to make&#xa;    meg = 0  # size of device in megabytes (1024**2)&#xa;    devidx = 1  # devnode name index to start at (0 = vda/sda, 1 = vdb/sdb, etc)&#xa;    devtype = 'file'&#xa;    targetbus = ""virtio""&#xa;&#xa;    @staticmethod&#xa;    def devname_suffix(index):&#xa;        """"""&#xa;        Return letter code for index position, a, b, c...aa, ab, ac...&#xa;        """"""&#xa;        # http://stackoverflow.com/questions/14381940/&#xa;        # python-pair-alphabets-after-loop-is-completed/14382997#14382997&#xa;        def multiletters():&#xa;            """"""Generator of count-by-letter strings""""""&#xa;            for num in itertools.count(1):&#xa;                for prod in itertools.product(ascii_lowercase, repeat=num):&#xa;                    yield ''.join(prod)&#xa;        return itertools.islice(multiletters(), index, index + 1).next()&#xa;&#xa;    def devname(self, index):&#xa;        """"""&#xa;        Return disk target name&#xa;        """"""&#xa;        if self.targetbus in ['usb', 'scsi', 'sata']:&#xa;            devname_prefix = ""sd""&#xa;        elif self.targetbus == ""virtio"":&#xa;            devname_prefix = ""vd""&#xa;        elif self.targetbus == ""ide"":&#xa;            devname_prefix = ""hd""&#xa;        else:&#xa;            raise error.TestNAError(""Unsupport bus '%s' in this test"" %&#xa;                                    self.targetbus)&#xa;        return devname_prefix + self.devname_suffix(self.devidx + index)&#xa;&#xa;    def make_image_file_path(self, index):&#xa;        """"""Create backing file for test disk device""""""&#xa;        return os.path.join(data_dir.get_tmp_dir(),&#xa;                            'disk_%s_%s_%d.raw'&#xa;                            % (self.__class__.__name__,&#xa;                               self.identifier,&#xa;                               index))&#xa;&#xa;    def make_image_file(self, index):&#xa;        """"""Create sparse backing file by writing it's full path at it's end""""""&#xa;        # Truncate file&#xa;        image_file_path = self.make_image_file_path(index)&#xa;        image_file = open(image_file_path, 'wb')&#xa;        byte_size = self.meg * 1024 * 1024&#xa;        # Make sparse file byte_size long (starting from 0)&#xa;        image_file.truncate(byte_size)&#xa;        # Write simple unique data to file before end&#xa;        image_file.seek(byte_size - len(image_file_path) - 1)&#xa;        # newline required by aexpect in function()&#xa;        image_file.write(image_file_path + '\n')&#xa;        image_file.close()&#xa;&#xa;    def init_device(self, index):&#xa;        """"""&#xa;        Initialize and return instance of device xml for index&#xa;        """"""&#xa;        self.make_image_file(index)&#xa;        disk_class = self.test_params.vmxml.get_device_class('disk')&#xa;        disk_device = disk_class(type_name=self.devtype,&#xa;                                 virsh_instance=self.test_params.virsh)&#xa;        disk_device.driver = {'name': 'qemu', 'type': 'raw'}&#xa;        # No source elements by default&#xa;        source_properties = {'attrs':&#xa;                             {'file': self.make_image_file_path(index)}}&#xa;        source = disk_device.new_disk_source(**source_properties)&#xa;        disk_device.source = source  # Modified copy, not original&#xa;        dev_name = self.devname(index)&#xa;        disk_device.target = {'dev': dev_name, 'bus': self.targetbus}&#xa;        # libvirt will automatically add <address> element&#xa;        return disk_device&#xa;&#xa;    def cleanup(self):&#xa;        for index in xrange(0, self.count):&#xa;            try:&#xa;                os.unlink(self.make_image_file_path(index))&#xa;            except OSError:&#xa;                pass  # Don't care if not there&#xa;&#xa;    def function(self, index):&#xa;        """"""&#xa;        Return True/False (good/bad) result of a device functioning&#xa;        """"""&#xa;        dev_name = '/dev/' + self.devname(index)&#xa;        # Host image path is static known value&#xa;        test_data = self.make_image_file_path(index)&#xa;        byte_size = self.meg * 1024 * 1024&#xa;        # Place test data at end of device to also confirm sizing&#xa;        offset = byte_size - len(test_data)&#xa;        logging.info('Trying to read test data, %dth device %s, '&#xa;                     'at offset %d.', index + 1, dev_name, offset)&#xa;        session = None&#xa;&#xa;        # Since we know we're going to fail, no sense waiting for the&#xa;        # default timeout to expire or login()'s code to get_address&#xa;        # (currently 300 seconds) or any other timeout code.  With the&#xa;        # guest not alive, just return failure. Not doing so caused a&#xa;        # a 96 minute pause per test with 16 devices all waiting for&#xa;        # the timeouts to occur and probably a days worth of log messages&#xa;        # waiting for something to happen that can't.&#xa;        if not self.test_params.main_vm.is_alive():&#xa;            logging.debug(""VirtualDiskBasic functional test skipping login ""&#xa;                          ""vm is not alive."")&#xa;            return False&#xa;&#xa;        try:&#xa;            session = self.test_params.main_vm.login()&#xa;&#xa;            # The device may not be ready on guest,&#xa;            # just wait at most 5 seconds here&#xa;            utils_misc.wait_for(lambda:&#xa;                                not session.cmd_status('ls %s' % dev_name), 5)&#xa;&#xa;            # aexpect combines stdout + stderr, throw away stderr&#xa;            output = session.cmd_output('tail -c %d %s'&#xa;                                        % (len(test_data) + 1, dev_name))&#xa;            session.close()&#xa;        except (virt_vm.VMAddressError, remote.LoginError,&#xa;                aexpect.ExpectError, aexpect.ShellError):&#xa;            try:&#xa;                session.close()&#xa;            except AttributeError:&#xa;                pass   # session == None&#xa;            logging.debug(""VirtualDiskBasic functional test raised an exception"")&#xa;            return False&#xa;        else:&#xa;            gotit = bool(output.count(test_data))&#xa;            logging.info(""Test data detected in device: %s"",&#xa;                         gotit)&#xa;            if not gotit:&#xa;                logging.debug(""Expecting: '%s'"", test_data)&#xa;                logging.debug(""Received: '%s'"", output)&#xa;            return gotit&#xa;&#xa;&#xa;def operational_action(test_params, test_devices, operational_results):&#xa;    """"""&#xa;    Call & store result list from operate() method on every device&#xa;    """"""&#xa;    if test_params.status_error:&#xa;        logging.info(""Running operational tests: Failure is expected!"")&#xa;    else:&#xa;        logging.info(""Running operational tests"")&#xa;    for device in test_devices:&#xa;        operational_results.append(device.operation_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        operational_results = [False] * device.count&#xa;    #    else:&#xa;    #        operational_results = [True] * device.count&#xa;&#xa;&#xa;def preboot_action(test_params, test_devices, preboot_results):&#xa;    """"""&#xa;    Call & store result of function() method on every device&#xa;    """"""&#xa;    if test_params.preboot_function_error:&#xa;        logging.info(""Running pre-reboot functional tests: Failure expected!"")&#xa;    else:&#xa;        logging.info(""Running pre-reboot functional tests"")&#xa;    for device in test_devices:&#xa;        preboot_results.append(device.function_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        preboot_results = [False] * device.count&#xa;    #    else:&#xa;    #        preboot_results = [True] * device.count&#xa;&#xa;&#xa;def postboot_action(test_params, test_devices, pstboot_results):&#xa;    """"""&#xa;    Call & store result of function() method on every device&#xa;    """"""&#xa;    if test_params.pstboot_function_error:&#xa;        logging.info(""Running post-reboot functional tests: Failure expected!"")&#xa;    else:&#xa;        logging.info(""Running post-reboot functional tests"")&#xa;    for device in test_devices:&#xa;        pstboot_results.append(device.function_results)  # list of bools&#xa;    # STUB:&#xa;    # for device in test_devices:&#xa;    #    if test_params.status_error:&#xa;    #        pstboot_results = [False] * device.count&#xa;    #    else:&#xa;    #        pstboot_results = [True] * device.count&#xa;&#xa;&#xa;# Save a little typing&#xa;all_true = TestDeviceBase.good_results&#xa;all_false = TestDeviceBase.bad_results&#xa;&#xa;&#xa;def analyze_negative_results(test_params, operational_results,&#xa;                             preboot_results, pstboot_results):&#xa;    """"""&#xa;    Analyze available results, return error message if fail&#xa;    """"""&#xa;    if operational_results:&#xa;        if not all_true(operational_results):&#xa;            return (""Negative testing operational test failed"")&#xa;    if preboot_results and test_params.start_vm:&#xa;        if not all_false(preboot_results):&#xa;            return (""Negative testing pre-boot functionality test passed"")&#xa;    if pstboot_results:&#xa;        if not all_false(pstboot_results):&#xa;            return (""Negative testing post-boot functionality ""&#xa;                    ""test passed"")&#xa;&#xa;&#xa;def analyze_positive_results(test_params, operational_results,&#xa;                             preboot_results, pstboot_results):&#xa;    """"""&#xa;    Analyze available results, return error message if fail&#xa;    """"""&#xa;    if operational_results:&#xa;        if not all_true(operational_results):&#xa;            return (""Positive operational test failed"")&#xa;    if preboot_results and test_params.start_vm:&#xa;        if not all_true(preboot_results):&#xa;            if not test_params.preboot_function_error:&#xa;                return (""Positive pre-boot functionality test failed"")&#xa;            # else: An error was expected&#xa;    if pstboot_results:&#xa;        if not all_true(pstboot_results):&#xa;            if not test_params.pstboot_function_error:&#xa;                return (""Positive post-boot functionality test failed"")&#xa;&#xa;&#xa;def analyze_results(test_params, operational_results=None,&#xa;                    preboot_results=None, pstboot_results=None):&#xa;    """"""&#xa;    Analyze available results, raise error message if fail&#xa;    """"""&#xa;    fail_msg = None  # Pass: None, Fail: failure reason&#xa;    if test_params.status_error:  # Negative testing&#xa;        fail_msg = analyze_negative_results(test_params, operational_results,&#xa;                                            preboot_results, pstboot_results)&#xa;    else:  # Positive testing&#xa;        fail_msg = analyze_positive_results(test_params, operational_results,&#xa;                                            preboot_results, pstboot_results)&#xa;    if fail_msg is not None:&#xa;        raise error.TestFail(fail_msg)&#xa;&#xa;&#xa;def run(test, params, env):&#xa;    """"""&#xa;    Test virsh {at|de}tach-device command.&#xa;&#xa;    1) Prepare test environment and its parameters&#xa;    2) Operate virsh on one or more devices&#xa;    3) Check functionality of each device&#xa;    4) Check functionality of mmconfig option&#xa;    5) Restore domain&#xa;    6) Handle results&#xa;    """"""&#xa;&#xa;    dev_obj = params.get(""vadu_dev_objs"")&#xa;    # Skip chardev hotplug on rhel6 host as it is not supported&#xa;    if ""Serial"" in dev_obj:&#xa;        if not libvirt_version.version_compare(1, 1, 0):&#xa;            raise error.TestNAError(""You libvirt version not supported""&#xa;                                    "" attach/detach Serial devices"")&#xa;&#xa;    logging.info(""Preparing initial VM state"")&#xa;    # Prepare test environment and its parameters&#xa;    test_params = TestParams(params, env, test)&#xa;    if test_params.start_vm:&#xa;        # Make sure VM is working&#xa;        test_params.main_vm.verify_alive()&#xa;        test_params.main_vm.wait_for_login().close()&#xa;    else:  # VM not suppose to be started&#xa;        if test_params.main_vm.is_alive():&#xa;            test_params.main_vm.destroy(gracefully=True)&#xa;    # Capture backup of original XML early in test&#xa;    test_params.vmxml = VMXML.new_from_inactive_dumpxml(&#xa;        test_params.main_vm.name)&#xa;    # All devices should share same access state&#xa;    test_params.virsh = virsh.Virsh(ignore_status=True)&#xa;    logging.info(""Creating %d test device instances"", len(test_params.devs))&#xa;    # Create test objects from cfg. class names via subclasses above&#xa;    test_devices = [globals()[class_name](test_params)  # instantiate&#xa;                    for class_name in test_params.devs]  # vadu_dev_objs&#xa;    operational_results = []&#xa;    preboot_results = []&#xa;    pstboot_results = []&#xa;    try:&#xa;        operational_action(test_params, test_devices, operational_results)&#xa;        # Fail early if attach-device return value is not expected&#xa;        analyze_results(test_params=test_params,&#xa;                        operational_results=operational_results)&#xa;&#xa;        #  Can't do functional testing with a cold VM, only test hot-attach&#xa;        preboot_action(test_params, test_devices, preboot_results)&#xa;&#xa;        logging.info(""Preparing test VM state for post-boot functional testing"")&#xa;        if test_params.start_vm:&#xa;            # Hard-reboot required&#xa;            test_params.main_vm.destroy(gracefully=True,&#xa;                                        free_mac_addresses=False)&#xa;        try:&#xa;            test_params.main_vm.start()&#xa;        except virt_vm.VMStartError:&#xa;            raise error.TestFail('VM Failed to start for some reason!')&#xa;        # Signal devices reboot is finished&#xa;        for test_device in test_devices:&#xa;            test_device.booted = True&#xa;        test_params.main_vm.wait_for_login().close()&#xa;        postboot_action(test_params, test_devices, pstboot_results)&#xa;        analyze_results(test_params=test_params,&#xa;                        preboot_results=preboot_results,&#xa;                        pstboot_results=pstboot_results)&#xa;    finally:&#xa;        logging.info(""Restoring VM from backup, then checking results"")&#xa;        test_params.main_vm.destroy(gracefully=False,&#xa;                                    free_mac_addresses=False)&#xa;        test_params.vmxml.undefine()&#xa;        test_params.vmxml.restore()  # Recover the original XML&#xa;        test_params.vmxml.define()&#xa;        if not test_params.start_vm:&#xa;            # Test began with not start_vm, shut it down.&#xa;            test_params.main_vm.destroy(gracefully=True)&#xa;        # Device cleanup can raise multiple exceptions, do it last:&#xa;        logging.info(""Cleaning up test devices"")&#xa;        test_params.cleanup(test_devices)&#xa;"
3876886|"# -*- coding: utf-8 -*-&#xa;##&#xa;## This file is part of Invenio.&#xa;## Copyright (C) 2008, 2009, 2010, 2011 CERN.&#xa;##&#xa;## Invenio is free software; you can redistribute it and/or&#xa;## modify it under the terms of the GNU General Public License as&#xa;## published by the Free Software Foundation; either version 2 of the&#xa;## License, or (at your option) any later version.&#xa;##&#xa;## Invenio is distributed in the hope that it will be useful, but&#xa;## WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;## General Public License for more details.&#xa;##&#xa;## You should have received a copy of the GNU General Public License&#xa;## along with Invenio; if not, write to the Free Software Foundation, Inc.,&#xa;## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.&#xa;&#xa;""""""&#xa;The shellutils module contains helper functions useful for interacting&#xa;with the operating system shell.&#xa;&#xa;The main API functions are:&#xa;   - run_shell_command()&#xa;""""""&#xa;&#xa;import os&#xa;import fcntl&#xa;import tempfile&#xa;import time&#xa;import signal&#xa;import select&#xa;from itertools import chain&#xa;from cStringIO import StringIO&#xa;import subprocess&#xa;&#xa;from invenio.config import CFG_MISCUTIL_DEFAULT_PROCESS_TIMEOUT&#xa;&#xa;__all__ = ['run_shell_command',&#xa;           'run_process_with_timeout',&#xa;           'Timeout',&#xa;           'split_cli_ids_arg']&#xa;&#xa;""""""&#xa;This module implements two functions:&#xa;    - L{run_shell_command}&#xa;    - L{run_process_with_timeout}&#xa;&#xa;L{run_shell_command} will run a command through a shell, capturing its&#xa;standard output and standard error.&#xa;&#xa;L{run_process_with_timeout} will run a process on its own allowing to&#xa;specify a input file, capturing the standard output and standard error and&#xa;killing the process after a given timeout.&#xa;""""""&#xa;&#xa;&#xa;class Timeout(Exception):&#xa;    """"""Exception raised by with_timeout() when the operation takes too long.&#xa;    """"""&#xa;    pass&#xa;&#xa;&#xa;def run_shell_command(cmd, args=None, filename_out=None, filename_err=None):&#xa;    """"""Run operating system command cmd with arguments from the args&#xa;    tuple in a sub-shell and return tuple (exit status code, stdout&#xa;    info, stderr info).&#xa;&#xa;    @param cmd: Command to execute in a shell; may contain %s&#xa;        placeholders for arguments that will be expanded from the args&#xa;        tuple. Example: cmd='echo %s', args = ('hello',).&#xa;    @type cmd: string&#xa;&#xa;    @param args: Arguments to be escaped and substituted for %s&#xa;        placeholders in cmd.&#xa;    @type args: tuple of strings&#xa;&#xa;    @param filename_out: Desired filename for stdout output&#xa;        (optional; see below).&#xa;    @type filename_out: string&#xa;&#xa;    @param filename_err: Desired filename for stderr output&#xa;        (optional; see below).&#xa;    @type filename_err: string&#xa;&#xa;    @return: Tuple (exit code, string containing stdout output buffer,&#xa;        string containing stderr output buffer).&#xa;&#xa;        However, if either filename_out or filename_err are defined,&#xa;        then the output buffers are not passed back but rather written&#xa;        into filename_out/filename_err pathnames.  This is useful for&#xa;        commands that produce big files, for which it is not practical&#xa;        to pass results back to the callers in a Python text buffer.&#xa;        Note that it is the client's responsibility to name these&#xa;        files in the proper fashion (e.g. to be unique) and to close&#xa;        these files after use.&#xa;    @rtype: (number, string, string)&#xa;&#xa;    @raise TypeError: if the number of args does not correspond to the&#xa;       number of placeholders in cmd.&#xa;&#xa;    @note: Uses temporary files to store out/err output, not pipes due&#xa;        to potential pipe race condition on some systems.  If either&#xa;        filename_out or filename_err are defined, then do not create&#xa;        temporary files, but store stdout or stderr output directly in&#xa;        these files instead, and do not delete them after execution.&#xa;    """"""&#xa;    # wash args value:&#xa;    if args:&#xa;        args = tuple(args)&#xa;    else:&#xa;        args = ()&#xa;    # construct command with argument substitution:&#xa;    try:&#xa;        cmd = cmd % tuple([escape_shell_arg(x) for x in args])&#xa;    except TypeError:&#xa;        # there were problems with %s and args substitution, so raise an error:&#xa;        raise&#xa;    cmd_out = ''&#xa;    cmd_err = ''&#xa;    # create files:&#xa;    if filename_out:&#xa;        cmd_out_fd = os.open(filename_out, os.O_CREAT, 0644)&#xa;        file_cmd_out = filename_out&#xa;    else:&#xa;        cmd_out_fd, file_cmd_out = \&#xa;                    tempfile.mkstemp(""invenio-shellutils-cmd-out"")&#xa;    if filename_err:&#xa;        cmd_err_fd = os.open(filename_err, os.O_CREAT, 0644)&#xa;        file_cmd_err = filename_err&#xa;    else:&#xa;        cmd_err_fd, file_cmd_err = \&#xa;                    tempfile.mkstemp(""invenio-shellutils-cmd-err"")&#xa;    # run command:&#xa;    cmd_exit_code = os.system(""%s > %s 2> %s"" % (cmd,&#xa;                                                 file_cmd_out,&#xa;                                                 file_cmd_err))&#xa;    # delete temporary files: (if applicable)&#xa;    if not filename_out:&#xa;        if os.path.exists(file_cmd_out):&#xa;            cmd_out_fo = open(file_cmd_out)&#xa;            cmd_out = cmd_out_fo.read()&#xa;            cmd_out_fo.close()&#xa;            os.remove(file_cmd_out)&#xa;    if not filename_err:&#xa;        if os.path.exists(file_cmd_err):&#xa;            cmd_err_fo = open(file_cmd_err)&#xa;            cmd_err = cmd_err_fo.read()&#xa;            cmd_err_fo.close()&#xa;            os.remove(file_cmd_err)&#xa;    os.close(cmd_out_fd)&#xa;    os.close(cmd_err_fd)&#xa;    # return results:&#xa;    return cmd_exit_code, cmd_out, cmd_err&#xa;&#xa;&#xa;def run_process_with_timeout(args, filename_in=None, filename_out=None, filename_err=None, cwd=None, timeout=CFG_MISCUTIL_DEFAULT_PROCESS_TIMEOUT, sudo=None):&#xa;    """"""Execute the specified process but within a certain timeout.&#xa;&#xa;    @param args: the actuall process. This should be a list of string as in:&#xa;         ['/usr/bin/foo', '--bar', 'baz']&#xa;    @type args: list of string&#xa;&#xa;    @param filename_in: the path to a file that should be provided as standard&#xa;        input to the process. If None this will default to /dev/null&#xa;    @type filename_in: string&#xa;&#xa;    @param filename_out: Desired filename for stdout output&#xa;        (optional; see below).&#xa;    @type filename_out: string&#xa;&#xa;    @param filename_err: Desired filename for stderr output&#xa;        (optional; see below).&#xa;    @type filename_err: string&#xa;&#xa;    @param cwd: the path from where to execute the process&#xa;    @type cwd: string&#xa;&#xa;    @param timeout: the timeout in seconds after which to consider the&#xa;        process execution as failed. a Timeout exception will be raised&#xa;    @type timeout: int&#xa;&#xa;    @param sudo: the optional name of the user under which to execute the&#xa;        process (by using sudo, without prompting for a password)&#xa;    @type sudo: string&#xa;&#xa;    @return: Tuple (exit code, string containing stdout output buffer,&#xa;        string containing stderr output buffer).&#xa;&#xa;        However, if either filename_out or filename_err are defined,&#xa;        then the output buffers are not passed back but rather written&#xa;        into filename_out/filename_err pathnames.  This is useful for&#xa;        commands that produce big files, for which it is not practical&#xa;        to pass results back to the callers in a Python text buffer.&#xa;        Note that it is the client's responsibility to name these&#xa;        files in the proper fashion (e.g. to be unique) and to close&#xa;        these files after use.&#xa;    @rtype: (number, string, string)&#xa;&#xa;    @raise Timeout: if the process does not terminate within the timeout&#xa;    """"""&#xa;    stdout = stderr = None&#xa;    if filename_in is not None:&#xa;        stdin = open(filename_in)&#xa;    else:&#xa;        ## FIXME: should use NUL on Windows&#xa;        stdin = open('/dev/null', 'r')&#xa;    if filename_out:&#xa;        stdout = open(filename_out, 'w')&#xa;    if filename_err:&#xa;        stderr = open(filename_err, 'w')&#xa;    tmp_stdout = StringIO()&#xa;    tmp_stderr = StringIO()&#xa;    if sudo is not None:&#xa;        args = ['sudo', '-u', sudo, '-S'] + list(args)&#xa;    ## See: <http://stackoverflow.com/questions/3876886/timeout-a-subprocess>&#xa;    process = subprocess.Popen(args, stdin=stdin, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, cwd=cwd, preexec_fn=os.setpgrp)&#xa;&#xa;    ## See: <http://stackoverflow.com/questions/375427/non-blocking-read-on-a-stream-in-python>&#xa;    fd = process.stdout.fileno()&#xa;    fl = fcntl.fcntl(fd, fcntl.F_GETFL)&#xa;    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)&#xa;    fd = process.stderr.fileno()&#xa;    fl = fcntl.fcntl(fd, fcntl.F_GETFL)&#xa;    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)&#xa;    fd_to_poll = [process.stdout, process.stderr]&#xa;    select_timeout = 0.5&#xa;    t1 = time.time()&#xa;    try:&#xa;        while process.poll() is None:&#xa;            if time.time() - t1 >= timeout:&#xa;                if process.stdin is not None:&#xa;                    process.stdin.close()&#xa;                time.sleep(1)&#xa;                if process.poll() is None:&#xa;                    ## See: <http://stackoverflow.com/questions/3876886/timeout-a-subprocess>&#xa;                    os.killpg(process.pid, signal.SIGTERM)&#xa;                    time.sleep(1)&#xa;                if process.poll() is None:&#xa;                    os.killpg(process.pid, signal.SIGKILL)&#xa;                try:&#xa;                    os.waitpid(process.pid, 0)&#xa;                except OSError:&#xa;                    pass&#xa;                raise Timeout()&#xa;            for fd in select.select(fd_to_poll, [], [], select_timeout)[0]:&#xa;                if fd == process.stdout:&#xa;                    buf = process.stdout.read(65536)&#xa;                    if stdout is None:&#xa;                        tmp_stdout.write(buf)&#xa;                    else:&#xa;                        stdout.write(buf)&#xa;                elif fd == process.stderr:&#xa;                    buf = process.stderr.read(65536)&#xa;                    if stderr is None:&#xa;                        tmp_stderr.write(buf)&#xa;                    else:&#xa;                        stderr.write(buf)&#xa;                else:&#xa;                    raise OSError(""fd %s is not a valid file descriptor"" % fd)&#xa;    finally:&#xa;        while True:&#xa;            ## Let's just read what is remaining to read.&#xa;            for fd in select.select(fd_to_poll, [], [], select_timeout)[0]:&#xa;                if fd == process.stdout:&#xa;                    buf = process.stdout.read(65536)&#xa;                    tmp_stdout.write(buf)&#xa;                    if stdout is not None:&#xa;                        stdout.write(buf)&#xa;                elif fd == process.stderr:&#xa;                    buf = process.stderr.read(65536)&#xa;                    tmp_stderr.write(buf)&#xa;                    if stderr is not None:&#xa;                        stderr.write(buf)&#xa;                else:&#xa;                    raise OSError(""fd %s is not a valid file descriptor"" % fd)&#xa;            else:&#xa;                break&#xa;    return process.poll(), tmp_stdout.getvalue(), tmp_stderr.getvalue()&#xa;&#xa;&#xa;def escape_shell_arg(shell_arg):&#xa;    """"""Escape shell argument shell_arg by placing it within&#xa;    single-quotes.  Any single quotes found within the shell argument&#xa;    string will be escaped.&#xa;&#xa;    @param shell_arg: The shell argument to be escaped.&#xa;    @type shell_arg: string&#xa;    @return: The single-quote-escaped value of the shell argument.&#xa;    @rtype: string&#xa;    @raise TypeError: if shell_arg is not a string.&#xa;    @see: U{http://mail.python.org/pipermail/python-list/2005-October/346957.html}&#xa;    """"""&#xa;    if type(shell_arg) is not str:&#xa;        msg = ""ERROR: escape_shell_arg() expected string argument but "" \&#xa;              ""got '%s' of type '%s'."" % (repr(shell_arg), type(shell_arg))&#xa;        raise TypeError(msg)&#xa;&#xa;    return ""'%s'"" % shell_arg.replace(""'"", r""'\''"")&#xa;&#xa;&#xa;def mymkdir(newdir, mode=0777):&#xa;    """"""works the way a good mkdir should :)&#xa;        - already exists, silently complete&#xa;        - regular file in the way, raise an exception&#xa;        - parent directory(ies) does not exist, make them as well&#xa;    """"""&#xa;    if os.path.isdir(newdir):&#xa;        pass&#xa;    elif os.path.isfile(newdir):&#xa;        raise OSError(""a file with the same name as the desired "" \&#xa;                      ""dir, '%s', already exists."" % newdir)&#xa;    else:&#xa;        head, tail = os.path.split(newdir)&#xa;        if head and not os.path.isdir(head):&#xa;            mymkdir(head, mode)&#xa;        if tail:&#xa;            os.umask(022)&#xa;            os.mkdir(newdir, mode)&#xa;&#xa;&#xa;def s(t):&#xa;    ## De-comment this to have lots of debugging information&#xa;    #print time.time(), t&#xa;    pass&#xa;&#xa;&#xa;def split_cli_ids_arg(value):&#xa;    """"""&#xa;    Split ids given in the command line&#xa;    Possible formats are:&#xa;    * 1&#xa;    * 1,2,3,4&#xa;    * 1-5,20,30,40&#xa;    Returns respectively&#xa;    * set([1])&#xa;    * set([1,2,3,4])&#xa;    * set([1,2,3,4,5,20,30,40])&#xa;    """"""&#xa;    def parse(el):&#xa;        el = el.strip()&#xa;        if not el:&#xa;            ret = []&#xa;        elif '-' in el:&#xa;            start, end = el.split('-', 1)&#xa;            ret = xrange(int(start), int(end) + 1)&#xa;        else:&#xa;            ret = [int(el)]&#xa;        return ret&#xa;    return set(chain(*(parse(c) for c in value.split(',') if c.strip())))&#xa;"
33175763|"#!/usr/bin/env python&#xa;# -*- coding: utf-8 -*-#&#xa;#&#xa;# Copyright (C) 2016, 2018 University of Zurich. All rights reserved.&#xa;#&#xa;#&#xa;# This program is free software; you can redistribute it and/or modify it&#xa;# under the terms of the GNU General Public License as published by the&#xa;# Free Software Foundation; either version 3 of the License, or (at your&#xa;# option) any later version.&#xa;#&#xa;# This program is distributed in the hope that it will be useful, but&#xa;# WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;# General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License along&#xa;# with this program; if not,&#xa;&#xa;__docformat__ = 'reStructuredText'&#xa;__author__ = 'Riccardo Murri <riccardo.murri@gmail.com>'&#xa;&#xa;# stdlib imports&#xa;from contextlib import contextmanager&#xa;import functools&#xa;import os&#xa;import re&#xa;import signal&#xa;import shutil&#xa;import sys&#xa;import tempfile&#xa;import time&#xa;import UserDict&#xa;&#xa;# 3rd party imports&#xa;import click&#xa;import netaddr&#xa;&#xa;&#xa;def confirm_or_abort(prompt, exitcode=os.EX_TEMPFAIL, msg=None, **extra_args):&#xa;    """"""&#xa;    Prompt user for confirmation and exit on negative reply.&#xa;&#xa;    Arguments `prompt` and `extra_args` will be passed unchanged to&#xa;    `click.confirm`:func: (which is used for actual prompting).&#xa;&#xa;    :param str prompt: Prompt string to display.&#xa;    :param int exitcode: Program exit code if negative reply given.&#xa;    :param str msg: Message to display before exiting.&#xa;    """"""&#xa;    if click.confirm(prompt, **extra_args):&#xa;        return True&#xa;    else:&#xa;        # abort&#xa;        if msg:&#xa;            sys.stderr.write(msg)&#xa;            sys.stderr.write('\n')&#xa;        sys.exit(exitcode)&#xa;&#xa;&#xa;@contextmanager&#xa;def environment(**kv):&#xa;    """"""&#xa;    Context manager to run Python code with a modified UNIX process environment.&#xa;&#xa;    All key/value pairs in the keyword arguments are added (or changed, if the&#xa;    key names an existing environmental variable) in the process environment&#xa;    upon entrance into the context. Changes are undone upon exit: added&#xa;    environmental variables are removed from the environment, and those whose&#xa;    value was changed are reset to their pristine value.&#xa;    """"""&#xa;    added = []&#xa;    changed = {}&#xa;    for key, value in kv.items():&#xa;        if key not in os.environ:&#xa;            added.append(key)&#xa;        else:&#xa;            changed[key] = os.environ[key]&#xa;        os.environ[key] = value&#xa;&#xa;    yield&#xa;&#xa;    # restore pristine process environment&#xa;    for key in added:&#xa;        del os.environ[key]&#xa;    for key in changed:&#xa;        os.environ[key] = changed[key]&#xa;&#xa;&#xa;def get_num_processors():&#xa;    """"""&#xa;    Return number of online processor cores.&#xa;    """"""&#xa;    # try different strategies and use first one that succeeeds&#xa;    try:&#xa;        return os.cpu_count()  # Py3 only&#xa;    except AttributeError:&#xa;        pass&#xa;    try:&#xa;        import multiprocessing&#xa;        return multiprocessing.cpu_count()&#xa;    except ImportError:  # no multiprocessing?&#xa;        pass&#xa;    except NotImplementedError:&#xa;        # multiprocessing cannot determine CPU count&#xa;        pass&#xa;    try:&#xa;        from subprocess32 import check_output&#xa;        ncpus = check_output('nproc')&#xa;        return int(ncpus)&#xa;    except CalledProcessError:  # no `/usr/bin/nproc`&#xa;        pass&#xa;    except (ValueError, TypeError):&#xa;        # unexpected output from `nproc`&#xa;        pass&#xa;    except ImportError:  # no subprocess32?&#xa;        pass&#xa;    try:&#xa;        from subprocess import check_output&#xa;        ncpus = check_output('nproc')&#xa;        return int(ncpus)&#xa;    except CalledProcessError:  # no `/usr/bin/nproc`&#xa;        pass&#xa;    except (ValueError, TypeError):&#xa;        # unexpected output from `nproc`&#xa;        pass&#xa;    except ImportError:  # no subprocess.check_call (Py 2.6)&#xa;        pass&#xa;    raise RuntimeError(""Cannot determine number of processors"")&#xa;&#xa;&#xa;def has_nested_keys(mapping, k1, *more):&#xa;    """"""&#xa;    Return ``True`` if `mapping[k1][k2]...[kN]` is valid.&#xa;&#xa;    Example::&#xa;&#xa;      >>> D = {&#xa;      ...   'a': {&#xa;      ...     'x':0,&#xa;      ...     'y':{&#xa;      ...       'z': 1,&#xa;      ...     },&#xa;      ...   },&#xa;      ...   'b': 3&#xa;      ... }&#xa;      >>> has_nested_keys(D, 'a', 'x')&#xa;      True&#xa;      >>> has_nested_keys(D, 'a', 'y', 'z')&#xa;      True&#xa;      >>> has_nested_keys(D, 'a', 'q')&#xa;      False&#xa;&#xa;    When a single key is passed, this is just another way of writing ``k1 in&#xa;    mapping``::&#xa;&#xa;      >>> has_nested_keys(D, 'b')&#xa;      True&#xa;    """"""&#xa;    if k1 in mapping:&#xa;        if more:&#xa;            return has_nested_keys(mapping[k1], *more)&#xa;        else:&#xa;            return True&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;class memoize(object):&#xa;    """"""&#xa;    Cache a function's return value each time it is called within a TTL.&#xa;&#xa;    If called within the TTL and the same arguments, the cached value is&#xa;    returned, If called outside the TTL or a different value, a fresh value is&#xa;    returned (and cached for future occurrences).&#xa;&#xa;    .. warning::&#xa;&#xa;      Only works on functions that take *no keyword arguments*.&#xa;&#xa;    Originally taken from: http://jonebird.com/2012/02/07/python-memoize-decorator-with-ttl-argument/&#xa;    """"""&#xa;    def __init__(self, ttl):&#xa;        self.cache = {}&#xa;        self.ttl = ttl&#xa;&#xa;    def __call__(self, f):&#xa;        @functools.wraps(f)&#xa;        def wrapped_f(*args):&#xa;            now = time.time()&#xa;            try:&#xa;                value, last_update = self.cache[args]&#xa;                if self.ttl > 0 and now - last_update > self.ttl:&#xa;                    raise AttributeError&#xa;                return value&#xa;            except (KeyError, AttributeError):&#xa;                value = f(*args)&#xa;                self.cache[args] = (value, now)&#xa;                return value&#xa;            except TypeError:&#xa;                # uncachable -- for instance, passing a list as an argument.&#xa;                # Better to not cache than to blow up entirely.&#xa;                return f(*args)&#xa;        return wrapped_f&#xa;&#xa;&#xa;# this is very liberal, in that it will accept malformed address&#xa;# strings like `0:::1` or '0::1::2', but we are going to do validation&#xa;# with `netaddr.IPAddress` later on so there is little advantage in&#xa;# being strict here&#xa;_IPV6_FRAG = r'[0-9a-z:]+'&#xa;&#xa;# likewise&#xa;_IPV4_FRAG = r'[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+'&#xa;&#xa;# should match a network interface name (for which there is no&#xa;# standard, so let's just assume it's alphanumeric)&#xa;_IFACE_FRAG = r'[a-z][0-9a-z]*'&#xa;&#xa;# XXX: order is important! IPv4 must come before IPv6 otherwise the&#xa;# _IPV6_FRAG will match a *part* of an IPv4 adress....&#xa;_IP_ADDRESS_RE = [&#xa;    # IPv4 literal, optionally with port&#xa;    re.compile(&#xa;        r'(?P<ip_addr>{0})(?P<port>:\d+)?'&#xa;        .format(_IPV4_FRAG), re.I),&#xa;&#xa;    # the kind of IPv6 literals returned by Azure, e.g., `[fe80::dead:beef%eth0]:2222`&#xa;    re.compile(&#xa;        r'\[(?P<ip_addr>{0})(?P<iface>%{1})?\](?P<port>:\d+)?'&#xa;        .format(_IPV6_FRAG, _IFACE_FRAG), re.I),&#xa;&#xa;    # IPv6 literal possibly with interface spec (note this cannot provide any port)&#xa;    re.compile(&#xa;        r'(?P<ip_addr>{0})(?P<iface>%{1})?'&#xa;        .format(_IPV6_FRAG, _IFACE_FRAG), re.I),&#xa;]&#xa;&#xa;def parse_ip_address_and_port(addr, default_port=22):&#xa;    """"""&#xa;    Return a pair (IP address, port) extracted from string `addr`.&#xa;&#xa;    Different formats are accepted for the address/port string:&#xa;&#xa;    * IPv6 literals in square brackets, with or without an optional&#xa;      port specification, as used in URLs::&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef]:1234')&#xa;        (IPAddress('fe80::dead:beef'), 1234)&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef]')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;    * IPv6 literals with a ""local interface"" specification::&#xa;&#xa;        >>> parse_ip_address_and_port('[fe80::dead:beef%eth0]')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef%eth0')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;    * bare IPv6 addresses::&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef')&#xa;        (IPAddress('fe80::dead:beef'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('2001:db8:5ca1:1f0:f816:3eff:fe05:f40f')&#xa;        (IPAddress('2001:db8:5ca1:1f0:f816:3eff:fe05:f40f'), 22)&#xa;&#xa;    * IPv4 addresses, with or without an additional port specification::&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123')&#xa;        (IPAddress('192.0.2.123'), 22)&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123:999')&#xa;        (IPAddress('192.0.2.123'), 999)&#xa;&#xa;    Note that the default port can be changed by passing an additional parameter::&#xa;&#xa;        >>> parse_ip_address_and_port('192.0.2.123', 987)&#xa;        (IPAddress('192.0.2.123'), 987)&#xa;&#xa;        >>> parse_ip_address_and_port('fe80::dead:beef', 987)&#xa;        (IPAddress('fe80::dead:beef'), 987)&#xa;&#xa;    :raise netaddr.AddrFormatError: Upon parse failure, e.g., syntactically incorrect IP address.&#xa;    """"""&#xa;    # we assume one and only one of the regexps will match&#xa;    for regexp in _IP_ADDRESS_RE:&#xa;        match = regexp.search(addr)&#xa;        if not match:&#xa;            continue&#xa;        # can raise netaddr.AddrFormatError&#xa;        ip_addr = netaddr.IPAddress(match.group('ip_addr'))&#xa;        try:&#xa;            port = match.group('port')&#xa;        except IndexError:&#xa;            port = None&#xa;        if port is not None:&#xa;            port = int(port[1:])  # skip leading `:`&#xa;        else:&#xa;            port = default_port&#xa;        return ip_addr, port&#xa;    # parse failed&#xa;    raise netaddr.AddrFormatError(&#xa;        ""Could not extract IP address and port from `{1}`""&#xa;        .format(addr))&#xa;&#xa;&#xa;# copied over from GC3Pie's `utils.py`&#xa;def string_to_boolean(word):&#xa;    """"""&#xa;    Convert `word` to a Python boolean value and return it.&#xa;    The strings `true`, `yes`, `on`, `1` (with any&#xa;    capitalization and any amount of leading and trailing&#xa;    spaces) are recognized as meaning Python `True`::&#xa;&#xa;      >>> string_to_boolean('yes')&#xa;      True&#xa;      >>> string_to_boolean('Yes')&#xa;      True&#xa;      >>> string_to_boolean('YES')&#xa;      True&#xa;      >>> string_to_boolean(' 1 ')&#xa;      True&#xa;      >>> string_to_boolean('True')&#xa;      True&#xa;      >>> string_to_boolean('on')&#xa;      True&#xa;&#xa;    Any other word is considered as boolean `False`::&#xa;&#xa;      >>> string_to_boolean('no')&#xa;      False&#xa;      >>> string_to_boolean('No')&#xa;      False&#xa;      >>> string_to_boolean('Nay!')&#xa;      False&#xa;      >>> string_to_boolean('woo-hoo')&#xa;      False&#xa;&#xa;    This includes also the empty string and whitespace-only::&#xa;&#xa;      >>> string_to_boolean('')&#xa;      False&#xa;      >>> string_to_boolean('  ')&#xa;      False&#xa;&#xa;    """"""&#xa;    if word.strip().lower() in ['true', 'yes', 'on', '1']:&#xa;        return True&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;class Struct(object, UserDict.DictMixin):&#xa;    """"""&#xa;    A `dict`-like object, whose keys can be accessed with the usual&#xa;    '[...]' lookup syntax, or with the '.' get attribute syntax.&#xa;&#xa;    Examples::&#xa;      >>> a = Struct()&#xa;      >>> a['x'] = 1&#xa;      >>> a.x&#xa;      1&#xa;      >>> a.y = 2&#xa;      >>> a['y']&#xa;      2&#xa;&#xa;    Values can also be initially set by specifying them as keyword&#xa;    arguments to the constructor::&#xa;&#xa;      >>> a = Struct(z=3)&#xa;      >>> a['z']&#xa;      3&#xa;      >>> a.z&#xa;      3&#xa;&#xa;    Like `dict` instances, `Struct`s have a `copy` method to get a&#xa;    shallow copy of the instance:&#xa;&#xa;      >>> b = a.copy()&#xa;      >>> b.z&#xa;      3&#xa;&#xa;    .. note::&#xa;&#xa;      This class is a clone of the `gc3libs.utils.Struct` class&#xa;      from the `GC3Pie package sources <https://github.com/uzh/gc3pie>`_&#xa;    """"""&#xa;    def __init__(self, initializer=None, **extra):&#xa;        if initializer is not None:&#xa;            try:&#xa;                # initializer is `dict`-like?&#xa;                for name, value in initializer.items():&#xa;                    self[name] = value&#xa;            except AttributeError:&#xa;                # initializer is a sequence of (name,value) pairs?&#xa;                for name, value in initializer:&#xa;                    self[name] = value&#xa;        for name, value in extra.items():&#xa;            self[name] = value&#xa;&#xa;    def copy(self):&#xa;        """"""Return a (shallow) copy of this `Struct` instance.""""""&#xa;        return Struct(self)&#xa;&#xa;    # the `DictMixin` class defines all std `dict` methods, provided&#xa;    # that `__getitem__`, `__setitem__` and `keys` are defined.&#xa;    def __setitem__(self, name, val):&#xa;        self.__dict__[name] = val&#xa;&#xa;    def __getitem__(self, name):&#xa;        return self.__dict__[name]&#xa;&#xa;    def keys(self):&#xa;        return self.__dict__.keys()&#xa;&#xa;&#xa;@contextmanager&#xa;def sighandler(signum, handler):&#xa;    """"""&#xa;    Context manager to run code with UNIX signal `signum` bound to `handler`.&#xa;&#xa;    The existing handler is saved upon entering the context and restored upon&#xa;    exit.&#xa;&#xa;    The `handler` argument may be anything that can be passed to Python's&#xa;    `signal.signal <https://docs.python.org/2/library/signal.html#signal.signal>`_&#xa;    standard library call.&#xa;    """"""&#xa;    prev_handler = signal.getsignal(signum)&#xa;    signal.signal(signum, handler)&#xa;    yield&#xa;    signal.signal(signum, prev_handler)&#xa;&#xa;&#xa;@contextmanager&#xa;def temporary_dir(delete=True, dir=None,&#xa;                  prefix='elasticluster.', suffix='.d'):&#xa;    """"""&#xa;    Make a temporary directory and make it current for the code in this context.&#xa;&#xa;    Delete temporary directory upon exit from the context, unless&#xa;    ``delete=False`` is passed in the arguments.&#xa;&#xa;    Arguments *suffix*, *prefix* and *dir* are exactly as in&#xa;    :func:`tempfile.mkdtemp()` (but have different defaults).&#xa;    """"""&#xa;    cwd = os.getcwd()&#xa;    tmpdir = tempfile.mkdtemp(suffix, prefix, dir)&#xa;    os.chdir(tmpdir)&#xa;    yield&#xa;    os.chdir(cwd)&#xa;    if delete:&#xa;        shutil.rmtree(tmpdir, ignore_errors=True)&#xa;&#xa;&#xa;@contextmanager&#xa;def timeout(delay, handler=None):&#xa;    """"""&#xa;    Context manager to run code and deliver a SIGALRM signal after `delay` seconds.&#xa;&#xa;    Note that `delay` must be a whole number; otherwise it is converted to an&#xa;    integer by Python's `int()` built-in function. For floating-point numbers,&#xa;    that means rounding off to the nearest integer from below.&#xa;&#xa;    If the optional argument `handler` is supplied, it must be a callable that&#xa;    is invoked if the alarm triggers while the code is still running. If no&#xa;    `handler` is provided (default), then a `RuntimeError` with message&#xa;    ``Timeout`` is raised.&#xa;    """"""&#xa;    delay = int(delay)&#xa;    if handler is None:&#xa;        def default_handler(signum, frame):&#xa;            raise RuntimeError(""{:d} seconds timeout expired"".format(delay))&#xa;        handler = default_handler&#xa;    prev_sigalrm_handler = signal.getsignal(signal.SIGALRM)&#xa;    signal.signal(signal.SIGALRM, handler)&#xa;    signal.alarm(delay)&#xa;    yield&#xa;    signal.alarm(0)&#xa;    signal.signal(signal.SIGALRM, prev_sigalrm_handler)&#xa;&#xa;&#xa;## Warnings redirection&#xa;#&#xa;# This is a modified version of the `logging.captureWarnings()` code from&#xa;# the Python 2.7 standard library:&#xa;#&#xa;# - backport the code to Python 2.6&#xa;# - make the logger configurable&#xa;#&#xa;# The original copyright notice is reproduced below:&#xa;#&#xa;#   Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.&#xa;#&#xa;#   Permission to use, copy, modify, and distribute this software and its&#xa;#   documentation for any purpose and without fee is hereby granted,&#xa;#   provided that the above copyright notice appear in all copies and that&#xa;#   both that copyright notice and this permission notice appear in&#xa;#   supporting documentation, and that the name of Vinay Sajip&#xa;#   not be used in advertising or publicity pertaining to distribution&#xa;#   of the software without specific, written prior permission.&#xa;#   VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING&#xa;#   ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL&#xa;#   VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR&#xa;#   ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER&#xa;#   IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT&#xa;#   OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.&#xa;#&#xa;&#xa;import logging&#xa;# ensure that `logging.NullHandler` is defined on Python 2.6 as well;&#xa;# see: http://stackoverflow.com/questions/33175763/how-to-use-logging-nullhandler-in-python-2-6&#xa;try:&#xa;    logging.NullHandler&#xa;except AttributeError:&#xa;    class _NullHandler(logging.Handler):&#xa;        def emit(self, record):&#xa;            pass&#xa;    logging.NullHandler = _NullHandler&#xa;&#xa;import warnings&#xa;&#xa;_warnings_showwarning = None&#xa;&#xa;&#xa;class _WarningsLogger(object):&#xa;    """"""&#xa;    Redirect warning messages to a chosen logger.&#xa;&#xa;    This is a callable object that implements a compatible interface&#xa;    to `warnings.showwarning` (which it is supposed to replace).&#xa;    """"""&#xa;&#xa;    def __init__(self, logger_name, format_warning=warnings.formatwarning):&#xa;        self._logger = logging.getLogger(logger_name)&#xa;        if not self._logger.handlers:&#xa;            self._logger.addHandler(logging.NullHandler())&#xa;        self._format_warning = format_warning&#xa;&#xa;    def __call__(self, message, category, filename, lineno, file=None, line=None):&#xa;        """"""&#xa;        Implementation of showwarnings which redirects to logging, which will first&#xa;        check to see if the file parameter is None. If a file is specified, it will&#xa;        delegate to the original warnings implementation of showwarning. Otherwise,&#xa;        it will call warnings.formatwarning and will log the resulting string to a&#xa;        warnings logger named ""py.warnings"" with level logging.WARNING.&#xa;        """"""&#xa;        if file is not None:&#xa;            assert _warnings_showwarning is not None&#xa;            _warnings_showwarning(message, category, filename, lineno, file, line)&#xa;        else:&#xa;            self._logger.warning(&#xa;                self._format_warning(message, category, filename, lineno))&#xa;&#xa;&#xa;def format_warning_oneline(message, category, filename, lineno,&#xa;                           file=None, line=None):&#xa;    """"""&#xa;    Format a warning for logging.&#xa;&#xa;    The returned value should be a single-line string, for better&#xa;    logging style (although this is not enforced by the code).&#xa;&#xa;    This methods' arguments have the same meaning of the&#xa;    like-named arguments from `warnings.formatwarning`.&#xa;    """"""&#xa;    # `warnings.formatwarning` produces multi-line output that does&#xa;    # not look good in a log file, so let us replace it with something&#xa;    # simpler...&#xa;    return ('{category}: {message}'&#xa;            .format(message=message, category=category.__name__))&#xa;&#xa;&#xa;def redirect_warnings(capture=True, logger='py.warnings'):&#xa;    """"""&#xa;    If capture is true, redirect all warnings to the logging package.&#xa;    If capture is False, ensure that warnings are not redirected to logging&#xa;    but to their original destinations.&#xa;    """"""&#xa;    global _warnings_showwarning&#xa;    if capture:&#xa;        assert _warnings_showwarning is None&#xa;        _warnings_showwarning = warnings.showwarning&#xa;        # `warnings.showwarning` must be a function, a generic&#xa;        # callable object is not accepted ...&#xa;        warnings.showwarning = _WarningsLogger(logger, format_warning_oneline).__call__&#xa;    else:&#xa;        assert _warnings_showwarning is not None&#xa;        warnings.showwarning = _warnings_showwarning&#xa;        _warnings_showwarning = None&#xa;"
5371992|"from six import BytesIO, u&#xa;from datetime import date&#xa;&#xa;import mock&#xa;from unittest2 import TestCase&#xa;from six.moves.urllib.parse import urlparse, parse_qsl, unquote_plus&#xa;&#xa;from authorize.apis.transaction import PROD_URL, TEST_URL, TransactionAPI&#xa;from authorize.data import Address, CreditCard&#xa;from authorize.exceptions import AuthorizeConnectionError, \&#xa;    AuthorizeResponseError&#xa;&#xa;class MockResponse(BytesIO):&#xa;    class Headers(dict):&#xa;        def getparam(self, *args, **kwargs):&#xa;            """"""Python 2 version""""""&#xa;            return None&#xa;        def get_content_charset(self, failobj=None, *args, **kwargs):&#xa;            """"""Python 3 version""""""&#xa;            return failobj&#xa;&#xa;    def __init__(self, *args, **kwargs):&#xa;        BytesIO.__init__(self, *args, **kwargs)&#xa;        self.headers = self.Headers()&#xa;&#xa;SUCCESS = MockResponse(&#xa;    b'1;1;1;This transaction has been approved.;IKRAGJ;Y;2171062816;;;20.00;CC'&#xa;    b';auth_only;;Jeffrey;Schenck;;45 Rose Ave;Venice;CA;90291;USA;;;;;;;;;;;;'&#xa;    b';;;;;375DD9293D7605E20DF0B437EE2A7B92;P;2;;;;;;;;;;;XXXX1111;Visa;;;;;;;'&#xa;    b';;;;;;;;;;Y')&#xa;PARSED_SUCCESS = {&#xa;    'cvv_response': 'P',&#xa;    'authorization_code': 'IKRAGJ',&#xa;    'response_code': '1',&#xa;    'amount': '20.00',&#xa;    'transaction_type': 'auth_only',&#xa;    'avs_response': 'Y',&#xa;    'response_reason_code': '1',&#xa;    'response_reason_text': 'This transaction has been approved.',&#xa;    'transaction_id': '2171062816',&#xa;}&#xa;ERROR = MockResponse(&#xa;    b'2;1;2;This transaction has been declined.;000000;N;2171062816;;;20.00;CC'&#xa;    b';auth_only;;Jeffrey;Schenck;;45 Rose Ave;Venice;CA;90291;USA;;;;;;;;;;;;'&#xa;    b';;;;;375DD9293D7605E20DF0B437EE2A7B92;N;1;;;;;;;;;;;XXXX1111;Visa;;;;;;;'&#xa;    b';;;;;;;;;;Y')&#xa;PARSED_ERROR = {&#xa;    'cvv_response': 'N',&#xa;    'authorization_code': '000000',&#xa;    'response_code': '2',&#xa;    'amount': '20.00',&#xa;    'transaction_type': 'auth_only',&#xa;    'avs_response': 'N',&#xa;    'response_reason_code': '2',&#xa;    'response_reason_text': 'This transaction has been declined.',&#xa;    'transaction_id': '2171062816',&#xa;}&#xa;&#xa;class URL(object):&#xa;    """"""&#xa;    a class to enable comparing of two urls regardless of order of parameters&#xa;    see http://stackoverflow.com/questions/5371992/comparing-two-urls-in-python &#xa;    """"""&#xa;    def __init__(self, url):&#xa;        parts = urlparse(url)&#xa;        _query = frozenset(parse_qsl(parts.query))&#xa;        _path = unquote_plus(parts.path)&#xa;        parts = parts._replace(query=_query, path=_path)&#xa;        self.parts = parts&#xa;&#xa;    def __eq__(self, other):&#xa;        return self.parts == other.parts&#xa;&#xa;    def __hash__(self, other):&#xa;        return hash(self.parts)&#xa;&#xa;class TransactionAPITests(TestCase):&#xa;    def setUp(self):&#xa;        self.api = TransactionAPI('123', '456')&#xa;        self.success = lambda *args, **kwargs: SUCCESS.seek(0) or SUCCESS&#xa;        self.error = lambda *args, **kwargs: ERROR.seek(0) or ERROR&#xa;        self.year = date.today().year + 10&#xa;        self.credit_card = CreditCard('4111111111111111', self.year, 1, '911')&#xa;        self.address = Address('45 Rose Ave', 'Venice', 'CA', '90291')&#xa;&#xa;    def test_basic_api(self):&#xa;        api = TransactionAPI('123', '456')&#xa;        self.assertEqual(api.url, TEST_URL)&#xa;        api = TransactionAPI('123', '456', debug=False)&#xa;        self.assertEqual(api.url, PROD_URL)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api._make_call({'a': '1', 'b': '2'})&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('{0}?a=1&b=2'.format(TEST_URL)))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_with_unicode(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api._make_call({u('\xe3'): '1', 'b': u('\xe3')})&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('{0}?%C3%A3=1&b=%C3%A3'.format(TEST_URL)))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_connection_error(self, urlopen):&#xa;        urlopen.side_effect = IOError('Borked')&#xa;        self.assertRaises(AuthorizeConnectionError, self.api._make_call,&#xa;            {'a': '1', 'b': '2'})&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_make_call_response_error(self, urlopen):&#xa;        urlopen.side_effect = self.error&#xa;        try:&#xa;            self.api._make_call({'a': '1', 'b': '2'})&#xa;        except AuthorizeResponseError as e:&#xa;            self.assertTrue(str(e).startswith('This transaction has been declined.'))&#xa;            self.assertEqual(e.full_response, PARSED_ERROR)&#xa;&#xa;    def test_add_params(self):&#xa;        self.assertEqual(self.api._add_params({}), {})&#xa;        params = self.api._add_params({}, credit_card=self.credit_card)&#xa;        self.assertEqual(params, {&#xa;            'x_card_num': '4111111111111111',&#xa;            'x_exp_date': '01-{0}'.format(self.year),&#xa;            'x_card_code': '911',&#xa;        })&#xa;        params = self.api._add_params({}, address=self.address)&#xa;        self.assertEqual(params, {&#xa;            'x_address': '45 Rose Ave',&#xa;            'x_city': 'Venice',&#xa;            'x_state': 'CA',&#xa;            'x_zip': '90291',&#xa;            'x_country': 'US',&#xa;        })&#xa;        params = self.api._add_params({},&#xa;            credit_card=self.credit_card, address=self.address)&#xa;        self.assertEqual(params, {&#xa;            'x_card_num': '4111111111111111',&#xa;            'x_exp_date': '01-{0}'.format(self.year),&#xa;            'x_card_code': '911',&#xa;            'x_address': '45 Rose Ave',&#xa;            'x_city': 'Venice',&#xa;            'x_state': 'CA',&#xa;            'x_zip': '90291',&#xa;            'x_country': 'US',&#xa;        })&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_auth(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.auth(20, self.credit_card, self.address)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_zip=90291&x_card_num=4111111111111111&x_amount=20.00'&#xa;            '&x_tran_key=456&x_city=Venice&x_country=US&x_version=3.1'&#xa;            '&x_state=CA&x_delim_char=%3B&x_address=45+Rose+Ave'&#xa;            '&x_exp_date=01-{0}&x_test_request=FALSE&x_card_code=911'&#xa;            '&x_type=AUTH_ONLY&x_delim_data=TRUE'.format(str(self.year))))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_capture(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.capture(20, self.credit_card, self.address)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_zip=90291&x_card_num=4111111111111111&x_amount=20.00'&#xa;            '&x_tran_key=456&x_city=Venice&x_country=US&x_version=3.1'&#xa;            '&x_state=CA&x_delim_char=%3B&x_address=45+Rose+Ave'&#xa;            '&x_exp_date=01-{0}&x_test_request=FALSE&x_card_code=911'&#xa;            '&x_type=AUTH_CAPTURE&x_delim_data=TRUE'.format(str(self.year))))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_settle(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;&#xa;        # Test without specified amount&#xa;        result = self.api.settle('123456')&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B'&#xa;            '&x_type=PRIOR_AUTH_CAPTURE&x_delim_data=TRUE&x_tran_key=456'&#xa;            '&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;        # Test with specified amount&#xa;        result = self.api.settle('123456', amount=10)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B'&#xa;            '&x_type=PRIOR_AUTH_CAPTURE&x_amount=10.00&x_delim_data=TRUE'&#xa;            '&x_tran_key=456&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_credit(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;&#xa;        # Test with transaction_id, amount&#xa;        result = self.api.credit('1111', '123456', 10)&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_amount=10.00&x_delim_char=%3B'&#xa;            '&x_type=CREDIT&x_card_num=1111&x_delim_data=TRUE&x_tran_key=456'&#xa;            '&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;&#xa;    @mock.patch('authorize.apis.transaction.urlopen')&#xa;    def test_void(self, urlopen):&#xa;        urlopen.side_effect = self.success&#xa;        result = self.api.void('123456')&#xa;        self.assertEqual(URL(urlopen.call_args[0][0]),&#xa;            URL('https://test.authorize.net/gateway/transact.dll?x_login=123'&#xa;            '&x_trans_id=123456&x_version=3.1&x_delim_char=%3B&x_type=VOID'&#xa;            '&x_delim_data=TRUE&x_tran_key=456&x_test_request=FALSE'))&#xa;        self.assertEqual(result, PARSED_SUCCESS)&#xa;"
653509|"from struct import pack, unpack&#xa;from struct import error as StructError&#xa;from math import log, floor, sqrt&#xa;from datetime import datetime&#xa;import os&#xa;import ntpath&#xa;import collections&#xa;import re&#xa;import copy&#xa;import sys&#xa;&#xa;from .stata_missing import (&#xa;    get_missing, MissingValue, MISSING, MISSING_VALS&#xa;)&#xa;from .stata_variable import StataVariable&#xa;&#xa;try:&#xa;    from stata import st_format&#xa;    IN_STATA = True&#xa;except ImportError:&#xa;    IN_STATA = False&#xa;&#xa;&#xa;__version__ = '0.2.0'&#xa;&#xa;__all__ = ['Dta', 'Dta115', 'Dta117', &#xa;           'display_diff', 'open_dta']&#xa;    &#xa;&#xa;VALID_NAME_RE = re.compile(r'^[_a-zA-Z][_a-zA-Z0-9]{0,31}$')&#xa;RESERVED = frozenset(('_all', '_b', 'byte', '_coef', '_cons', &#xa;            'double', 'float', 'if', 'in', 'int', 'long', '_n', '_N',&#xa;            '_pi', '_pred', '_rc', '_skip', 'using', 'with'))&#xa;# next re used with _fix_fmt, which enlarges fmts for displaying value labels&#xa;FMT_WIDTH_RE = re.compile(r'^\s*(%(-|~)?0?)([0-9]+)(\.[0-9]+)')&#xa;LARGEST_NONMISSING = 8.988465674311579e+307&#xa;SMALLEST_NONMISSING = -1.7976931348623157e+308&#xa;NUM_FMT_RE = re.compile(r'^%(-)?(0)?([0-9]+)(\.|\,)([0-9]+)(f|g|e)(c)?$')&#xa;STR_FMT_RE = re.compile(r'^%(-|~)?(0)?([0-9]+)s$')&#xa;HEX_RE = re.compile(r'^(-)?0x([0-9]+\.[0-9a-f]+)p(\+|-)([0-9]+)?$')&#xa;date_details = r'|'.join(d for d in &#xa;            ('CC', 'cc', 'YY', 'yy', 'JJJ', 'jjj', 'Month', 'Mon', 'month', &#xa;            'mon', 'NN', 'nn', 'DD', 'dd', 'DAYNAME', 'Dayname', 'Day', 'Da',&#xa;            'day', 'da', 'q', 'WW', 'ww', 'HH', 'Hh', 'hH', 'hh', 'h', 'MM', &#xa;            'mm', 'SS', 'ss', '.sss', '.ss', '.s', 'am', 'a.m.', 'AM', 'A.M.',&#xa;            '\.', ',', ':', '-', '\\\\', '_', '\+', '/', '!.'))&#xa;TIME_FMT_RE = re.compile(r'^%(-)?t(c|C|d|w|m|q|h|y|g)(' + date_details + ')*$')&#xa;TB_FMT_RE = re.compile(r'^%(-)?tb([^:]*)(:(' + date_details + ')*)?$')&#xa;MONTH_ABBREV = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', &#xa;                7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}&#xa;&#xa;                &#xa;# exception to raise when Dta file is mis-formatted&#xa;class DtaParseError(Exception):&#xa;    pass&#xa;    &#xa;    &#xa;class Dta():&#xa;    """"""A Python parent class for Stata datasets. &#xa;    Sub-classes implement methods for particular versions.&#xa;    &#xa;    """"""&#xa;    def __init__(self, *args, **kwargs):&#xa;        """"""Initialize Dta object.&#xa;        &#xa;        Dta objects can be created &#xa;        - from file, &#xa;        - from an iterable of values (as in a list of lists, with&#xa;        one sub-list per observation), &#xa;        - by subscripting an existing Dta (usually done with&#xa;        subscripting syntax like data[::2, (0,4,8)] ), or&#xa;        - by converting one sub-type of Dta to another (for example, &#xa;        converting from version 115 to 117 by converting a Dta115 &#xa;        instance to a Dta117 instance).&#xa;        &#xa;        &#xa;        New from file&#xa;        =============&#xa;        Parameters&#xa;        ----------&#xa;        address : str&#xa;            Address of dta file, including file name and "".dta"".&#xa;            &#xa;        Example&#xa;        -------&#xa;        >>> Dta115(""path/to/some_dta_v115.dta"")&#xa;        >>> Dta117(""path/to/some_dta_v117.dta"")&#xa;        # if uncertain of version, open_dta() can open 114, 115, or 117&#xa;        >>> open_dta(""recent_version.dta"")&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        If the data set has a label, the label will be printed.&#xa;        &#xa;        &#xa;        New from iterable&#xa;        =================&#xa;        Parameters&#xa;        ----------&#xa;        varvals : iterable&#xa;            Values to &#xa;        compress : bool (or coercible to bool), optional&#xa;            This sets the default type to attempt to assign to a&#xa;            data variable as byte if compress=True, or float if &#xa;            compress=False. Default type is overridden as necessary.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        single_row : bool (or coercible to bool), optional&#xa;            The code tries to be helpful with inputs. The correct way to&#xa;            specify a single row data set is with a non-string iterable&#xa;            within another iterable, as in ((0,1,2,3)) or [[0,1,2,3]].&#xa;            With single_row=True, a single row can be specified as,&#xa;            for example, just (0,1,2,3) or [0,1,2,3].&#xa;            (Don't combine ((0,1,2,3)) with single_row=True.)&#xa;            Default value is False.&#xa;            &#xa;        Example&#xa;        -------&#xa;        >>> v = [[0.0, 0.1, 0.2],[1.0, 1.1, 1.2],[2.0,2.1,2.2]]&#xa;        >>> Dta117(v)&#xa;                &#xa;        &#xa;        Subscripting a Dta instance (usually used indirectly)&#xa;        =====================================================&#xa;        Parameters&#xa;        ----------&#xa;        old_dta : Dta instance&#xa;        sel_rows : iterable of int, optional&#xa;            The rows (observations) to be selected from the `old_dta`.&#xa;            Defaults to all observations.&#xa;        sel_cols : iterable of int, optional&#xa;            The cols (data variables) to be selected from the `old_dta`.&#xa;            Defaults to all variables.&#xa;            &#xa;        Example&#xa;        -------&#xa;        # take even-numbered observations and variables 0, 4, 8&#xa;        >>> smaller_dta = some_dta[::2, (0,4,8)]&#xa;            &#xa;        &#xa;        Converting a Dta instance&#xa;        =========================&#xa;        Parameters&#xa;        ----------&#xa;        old_dta : Dta instance&#xa;        sel_rows : iterable of int, optional&#xa;            The rows (observations) to be selected from the `old_dta`.&#xa;            Defaults to all observations.&#xa;        sel_cols : iterable of int, optional&#xa;            The cols (data variables) to be selected from the `old_dta`.&#xa;            Defaults to all variables.&#xa;        &#xa;        Example&#xa;        -------&#xa;        # open a version 115 file&#xa;        >>> dta115 = open_dta(""some_dta_v115.dta"")&#xa;        # convert to version 117&#xa;        >>> dta117 = Dta117(dta115)&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Data may be changed or variables dropped if converting from a &#xa;        more permissive format to a more restrictive one.&#xa;        &#xa;        &#xa;        Returns&#xa;        =======&#xa;        All examples above return an instance of a Dta sub-class.&#xa;        &#xa;        &#xa;        Side effects&#xa;        ============&#xa;        Initializes Dta object.&#xa;        &#xa;        """"""&#xa;        nargs = len(args) + len(kwargs)&#xa;        if nargs == 0:&#xa;            raise TypeError(""one or more arguments required (0 given)"")&#xa;        &#xa;        first_arg = args[0]&#xa;        if isinstance(first_arg, str):&#xa;            if nargs > 2 or (nargs > 1 and ""quiet"" not in kwargs):&#xa;                raise TypeError(&#xa;                    ""incorrect arguments for creating Dta from file""&#xa;                )&#xa;            self._new_from_file(*args, **kwargs)&#xa;        elif isinstance(first_arg, Dta):&#xa;            if nargs > 3:&#xa;                raise TypeError(&#xa;                    ""too many arguments to create Dta from existing Dta""&#xa;                )&#xa;            self._new_from_dta(*args, **kwargs)&#xa;        elif isinstance(first_arg, collections.Iterable):&#xa;            self._new_from_iter(*args, **kwargs)&#xa;        else:&#xa;            raise TypeError(""Dta cannot be created from these arguments:"")&#xa;        &#xa;    def _new_from_dta(self, old_dta, sel_rows=None, sel_cols=None):&#xa;        """"""create data object by subscripting another data object""""""&#xa;        sel_rows = sel_rows if sel_rows is not None else range(old_dta._nobs)&#xa;        sel_cols = sel_cols if sel_cols is not None else range(old_dta._nvar)&#xa;        &#xa;        self._quiet = old_dta._quiet&#xa;        &#xa;        #header&#xa;        self._ds_format  = old_dta._ds_format&#xa;        self._byteorder  = old_dta._byteorder&#xa;        self._nvar       = len(sel_cols)&#xa;        self._nobs       = len(sel_rows)&#xa;        self._data_label = old_dta._data_label&#xa;        self._set_timestamp()&#xa;        &#xa;        #descriptors&#xa;        self._typlist = [old_dta._typlist[i] for i in sel_cols]&#xa;        self._varlist = [old_dta._varlist[i] for i in sel_cols]&#xa;        # can't copy srtlist because rows could appear out of order in sel_rows&#xa;        self._srtlist = [None for i in sel_cols]&#xa;        self._fmtlist = [old_dta._fmtlist[i] for i in sel_cols]&#xa;        self._lbllist = [old_dta._lbllist[i] for i in sel_cols]&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [old_dta._vlblist[i] for i in sel_cols]&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {k:v for k,v in old_dta._chrdict.items() &#xa;                         if k == '_dta' or k in self._varlist}&#xa;        &#xa;        # data&#xa;        self._varvals = [[old_dta._varvals[i][j] for j in sel_cols] &#xa;                         for i in sel_rows]&#xa;        &#xa;        # value labels&#xa;        self._vallabs = copy.deepcopy(old_dta._vallabs)  # copy&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # convert type if old_dta was a different version of .dta&#xa;        old_type = old_dta.__class__&#xa;        if not isinstance(self, old_type):&#xa;            self._convert_dta(old_type)&#xa;    &#xa;    def _new_from_file(self, address, quiet=False):&#xa;        """"""get data object from file""""""&#xa;        address = self._get_fullpath(address)&#xa;        &#xa;        version = self._dta_format(address)&#xa;        &#xa;        if version in (114, 115):&#xa;            self._file_to_Dta115(address)&#xa;            if not isinstance(self, Dta115):&#xa;                if not quiet:&#xa;                    msg = ""file format is {}, converting to 117""&#xa;                    print(msg.format(version))&#xa;                self._convert_dta(Dta115)&#xa;        else:&#xa;            self._file_to_Dta117(address)&#xa;            if not isinstance(self, Dta117):&#xa;                if not quiet:&#xa;                    msg = ""file format is {}, converting to 115""&#xa;                    print(msg.format(version))&#xa;                self._convert_dta(Dta117)&#xa;                &#xa;        # set self's path and filename&#xa;        self._set_path(address)&#xa;        &#xa;        # set changed to False, since dataset comes directly from file&#xa;        self._changed = False&#xa;        &#xa;        # display data label if in Stata&#xa;        if not quiet and IN_STATA and self._data_label.strip() != """":&#xa;            print(""{txt}("" + self._data_label + ""){txt}"")&#xa;            &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;            &#xa;    def __getattr__(self, name):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Raises AttributeError if name does not end with ""_"".&#xa;        Tries to find variable and return StataVariable otherwise.&#xa;        """"""&#xa;        if not name.endswith(""_""):&#xa;            msg = ""'{}' object has no attribute '{}'""&#xa;            raise AttributeError(msg.format(self.__class__.__name__, name))&#xa;            &#xa;        varname = self._find_vars((name[:-1],))[0]&#xa;        &#xa;        return StataVariable(self, varname)&#xa;        &#xa;    def __setattr__(self, name, value):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Creates or replaces variable if name ends with ""_"".&#xa;        Creates or replaces regular attribute otherwise.&#xa;        """"""&#xa;        if not name.endswith(""_""):&#xa;            self.__dict__[name] = value&#xa;        else:&#xa;            varname = name[:-1]&#xa;            if varname in self._varlist:&#xa;                self[:, self._varlist.index(varname)] = value&#xa;            else:&#xa;                self.append_var(varname, value)&#xa;        &#xa;    def __delattr__(self, name):&#xa;        """"""Provides shortcut to Dta variables by appending ""_"".&#xa;        Raises AttributeError if name does not end with ""_"".&#xa;        Otherwise, tries to find variable and drop it.&#xa;        """"""&#xa;        if name.endswith(""_""):&#xa;            self.drop_var(name[:-1])&#xa;        else:&#xa;            if name not in self.__dict__:&#xa;                msg = ""'{}' object has no attribute '{}'""&#xa;                raise AttributeError(msg.format(self.__class__.__name__, name))&#xa;            del self.__dict__[name]&#xa;        &#xa;    def save(self, address=None, replace=False):&#xa;        """"""Save current Dta object as dta file.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        address : str&#xa;            Address of file to save to.&#xa;            Optional if Dta object was created from file&#xa;            or has been saved already, otherwise required.&#xa;        replace : bool, optional&#xa;            Default value is False.&#xa;            True is required to write over existing file.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        creates or replaces dta file&#xa;        &#xa;        """"""&#xa;        if address is None:&#xa;            if not hasattr(self, ""_fullpath""):&#xa;                raise ValueError(""address or filename needed"")&#xa;            address = self._fullpath&#xa;        elif not isinstance(address, str):&#xa;            raise TypeError(""given address or filename should be str"")&#xa;        else:&#xa;            address = self._get_fullpath(address)&#xa;            self._set_path(address)&#xa;            &#xa;        if os.path.isfile(address) and not replace:&#xa;            msg = (""file exists; use replace option to overwrite"")&#xa;            raise IOError(msg)&#xa;            &#xa;        self._dta_obj_to_file(address)&#xa;        self._changed = False&#xa;        &#xa;    def _get_fullpath(self, address):&#xa;        """"""convert address to full path of dta file, &#xa;        adding "".dta"" if necessary&#xa;        &#xa;        """"""&#xa;        address = os.path.abspath(address)&#xa;        if len(address) < 4 or address[-4:] != "".dta"":&#xa;            address = address + "".dta""&#xa;        return address&#xa;        &#xa;    def _set_path(self, address):&#xa;        """"""set self's _fullpath and _filename from address""""""&#xa;        self._fullpath = address&#xa;        # http://stackoverflow.com/questions/8384737&#xa;        split_path = ntpath.split(address)&#xa;        self._filename = split_path[1] or ntpath.basename(split_path[0])    &#xa;&#xa;    def _set_timestamp(self):&#xa;        """"""make new time stamp""""""&#xa;        d = datetime.now()&#xa;        self._time_stamp = ""{:>2} {} {} {:>2}:{:>02}"".format(&#xa;                        d.day, MONTH_ABBREV[d.month], d.year, d.hour, d.minute)&#xa;        &#xa;    def ismissing(self, item):&#xa;        """"""Determine if item qualifies as a numeric missing value.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        item : None, int, float, or MissingValue instance&#xa;        &#xa;        Returns&#xa;        -------&#xa;        bool&#xa;        &#xa;        Notes&#xa;        ------&#xa;        This function is not meant to be used with non-numeric or &#xa;        non-real values, and will raise an error when given such.&#xa;        &#xa;        """"""&#xa;        return (item is None or isinstance(item, MissingValue) &#xa;                or not (SMALLEST_NONMISSING <= item <= LARGEST_NONMISSING))&#xa;                &#xa;    def quiet(self, q=True):&#xa;        """"""Allow or disallow output messages.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        q : bool or coercible to bool&#xa;            Default value is True, meaning disallow messages.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Notes&#xa;        -----&#xa;        Methods that are primarily designed for output--like `describe`,&#xa;        `list`, and `summary`--will show output regardless. Setting this&#xa;        function affects warnings or other 'unexpected' messages.&#xa;        &#xa;        """"""&#xa;        self._quiet = bool(q)&#xa;        &#xa;    def __iter__(self):&#xa;        for row in self._varvals:&#xa;            yield row&#xa;        &#xa;    def to_list(self):&#xa;        """"""Return list of data observations.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        list&#xa;            List of observations, each of which is also a list.&#xa;        &#xa;        """"""&#xa;        return copy.deepcopy(self._varvals)&#xa;        &#xa;    def _find_vars(self, varnames, unique=False, evars=False, all_ok=False, &#xa;                    empty_ok=False, single=False):&#xa;        """"""Take tuple of string abbreviations to variable names,&#xa;        and return list of lists of all matches within varlist.&#xa;        Raises error if no match or ambiguous abbreviation.&#xa;        &#xa;        Strip out duplicates if unique==True. Allow '_dta' if evars==True.&#xa;        &#xa;        If all_ok==True, using '_all' returns entire varlist if evars==False&#xa;        or entire varlist + '_dta' if evars==True. If all_ok==True and '_all' &#xa;        is present, there should be no other varnames present.&#xa;        &#xa;        """"""&#xa;        if isinstance(varnames, str):&#xa;            varnames = (varnames,)&#xa;        elif not isinstance(varnames, collections.Iterable):&#xa;            raise TypeError(""variable names should be str or iterable of str"")&#xa;        &#xa;        # first split into list of single abbrevs per str&#xa;        split_names = []&#xa;        for name in varnames:&#xa;            if not isinstance(name, str):&#xa;                raise TypeError(""must specify variables as string(s)"")&#xa;            split_names += name.split()&#xa;        nnames = len(split_names)&#xa;        &#xa;        # check for _all, check for proper usage, and return copy of varlist&#xa;        # if evars==False or ['_dta'] + varlist if evars==True&#xa;        all_specified = False&#xa;        if '_all' in split_names:&#xa;            if not all_ok:&#xa;                raise ValueError(""\""_all\"" not allowed in this context"")&#xa;            elif not nnames == 1:&#xa;                raise ValueError(&#xa;                    ""\""_all\"" may not be combined with other names"")&#xa;            all_specified = True&#xa;            all_names = (['_dta'] if evars else []) + list(self._varlist)&#xa;            nnames = len(all_names)&#xa;            &#xa;        # check that more than 0 names specified if empty_ok==False, and&#xa;        # ignore extras (with message) if single==True&#xa;        if not empty_ok and nnames == 0:&#xa;            raise ValueError(""no variables specified"")&#xa;        if single and nnames > 1:&#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                msg = smcl + ""only one {}varname allowed; ignoring the rest""&#xa;                print(msg.format('e' if evars else ''))&#xa;            split_names = split_names[:1]&#xa;    &#xa;        # if all_specified, return aleady-constructed all_names&#xa;        if all_specified:&#xa;            return all_names&#xa;    &#xa;        # Create match list of [abbrev, match1, match2, ...].&#xa;        # The loops below identify when exact varname given, but that varname&#xa;        # happens to be abbreviation of other varnames.&#xa;        varlist = self._varlist&#xa;        matches = []&#xa;        append = matches.append&#xa;        if evars:&#xa;            for name in split_names:&#xa;                if name == ""_dta"":&#xa;                    append([name, name])&#xa;                else:&#xa;                    match = [var for var in varlist if var.startswith(name)]&#xa;                    append([name, name] if name in match else [name] + match)&#xa;        else:&#xa;            for name in split_names:&#xa;                match = [var for var in varlist if var.startswith(name)]&#xa;                append([name, name] if name in match else [name] + match)&#xa;                  &#xa;        # abbreviation was a good, unambiguous abbreviation if exactly&#xa;        # one match found, i.e. if the corresponding entry in -matches- &#xa;        # is [abbrev, match1]&#xa;        if not all(len(m) == 2 for m in matches):&#xa;            # there were unmatched or ambiguous abbreviations&#xa;            zeros = "" "".join([m[0] for m in matches if len(m) == 1])&#xa;            twos  = "" "".join([m[0] for m in matches if len(m) >= 3])&#xa;            if zeros != """" and twos != """":&#xa;                msg = ""no variables found for {}; multiple found for {}""&#xa;                raise ValueError(msg.format(zeros, twos))&#xa;            if zeros != """":&#xa;                raise ValueError(&#xa;                    ""no variables found for {}"".format(zeros, twos))&#xa;            # if getting here, twos != """" and zeros == """"&#xa;            raise ValueError(""multiple variables found for '{}'"".format(twos))&#xa;            &#xa;        if not unique:&#xa;            return [m[1] for m in matches]&#xa;        seen = set()&#xa;        # if name has not been encountered, add to list and set of encountered&#xa;        return [m[1] for m in matches &#xa;                if m[1] not in seen and not seen.add(m[1])]&#xa;    &#xa;    def describe(self, simple=False, short=False, &#xa;                 fullnames=False, numbers=False, varlist=False):&#xa;        """"""Display a description of the data set.&#xa;        &#xa;        Paramters&#xa;        ---------&#xa;        simple : bool (or coercible to bool), optional&#xa;            Specify that only a list of variable names be displayed.&#xa;            No other options may be combined with `simple`.&#xa;            Default value is False.&#xa;        short : bool (or coercible to bool), optional&#xa;            Specify that only data set information be displayed.&#xa;            No information about individual variables will be displayed.&#xa;            Default value is False.&#xa;        fullnames : bool (or coercible to bool), optional&#xa;            Specify that full variable names be used.&#xa;            Default value is False.&#xa;        numbers : bool (or coercible to bool), optional&#xa;            Specify that variables' numbers be included.&#xa;            Default value is False.&#xa;        varlist : bool (or coercible to bool), optional&#xa;            Specify that sortlist and varlist information be saved&#xa;            (accessible via the `return_list` method).&#xa;            Default value is False.&#xa;            &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays description of data set. Saves associated values,&#xa;        which can be displayed with `return_list` method.&#xa;        &#xa;        """"""&#xa;        &#xa;        varlist_opt = varlist  # ""varlist"" below will mean self._varlist&#xa;        &#xa;        squish = self._squish_name&#xa;        varlist = self._varlist&#xa;        srtlist = self._srtlist&#xa;        nvar = self._nvar&#xa;        nobs = self._nobs&#xa;        width = self.width&#xa;        txt, res = (""{txt}"", ""{res}"") if IN_STATA else ("""", """")&#xa;        &#xa;        # basic return values are the same for all descriptions&#xa;        self._return_values = {&#xa;            'changed': self._changed,&#xa;            'width': width,&#xa;            'k': nvar,&#xa;            'N': nobs,&#xa;            'key_order': ('changed', 'width', 'k', 'N')&#xa;        }&#xa;        &#xa;        if simple:&#xa;            if any((short, fullnames, numbers, varlist_opt)):&#xa;                msg = ""simple may not be combined with other options""&#xa;                raise ValueError(msg)&#xa;            &#xa;            print(txt, end="""")&#xa;            for i in range(nvar):&#xa;                print(""{:<14}"".format(varlist[i]), end="""" if (i+1)%5 else ""\n"")&#xa;            print(""\n"" if nvar%5 else """")  # to add line before next prompt&#xa;            return&#xa;        &#xa;        size = self.width * self._nobs&#xa;        sort_names = "" "".join(varlist[j] for j in srtlist if j is not None)&#xa;        &#xa;        # if varlist_opt, add srtlist and varlist info to return_values&#xa;        if varlist_opt:&#xa;            self._return_values.update({&#xa;                'sortlist': sort_names,&#xa;                'varlist': "" "".join(varlist),&#xa;                'key_order': (&#xa;                    'changed',&#xa;                    'width',&#xa;                    'k',&#xa;                    'N',&#xa;                    'sortlist',&#xa;                    'varlist'&#xa;                )&#xa;            })&#xa;        &#xa;        # short and non-short descriptions share header and footer&#xa;        print(&#xa;            ""\n{}  obs:{} {:>13}"".format(txt, res, self._nobs),&#xa;            "" "" * 25,&#xa;            end=""""&#xa;        )&#xa;        i = 34&#xa;        label = self._data_label&#xa;        lablength = len(label)&#xa;        print(label[:i])&#xa;        while i < lablength:&#xa;            i += 32&#xa;            print("" "" * 47, label[:i])&#xa;        &#xa;        print(&#xa;            ""{} vars:{} {:>13}"".format(txt, res, self._nvar),&#xa;            "" "" * 24,&#xa;            self._time_stamp&#xa;        )&#xa;        &#xa;        if '_dta' in self._chrdict and 'note0' in self._chrdict['_dta']:&#xa;            note_text = "" "" * 25 + ""(_dta has notes)""&#xa;        else:&#xa;            note_text = """"&#xa;        print(""{} size:{} {:>13}"".format(txt, res, size), note_text)&#xa;        &#xa;        # add the stuff that's not in short version&#xa;        if not short:&#xa;            typlist = self._typlist&#xa;            fmtlist = self._fmtlist&#xa;            get_type_name = self._get_type_name&#xa;            lbllist = self._lbllist&#xa;            vlblist = self._vlblist&#xa;            &#xa;            hline = ""{txt}{hline}"" if IN_STATA else ""-"" * 80&#xa;            print(hline)&#xa;            print(""              storage   display    value"")&#xa;            print(""variable name   type    format     label      variable label"")&#xa;            print(hline)&#xa;            name_size = 8 if numbers else 15&#xa;            for i in range(nvar):&#xa;                if fullnames:&#xa;                    name = varlist[i]&#xa;                    if len(name) > name_size:&#xa;                        name += ""\n"" + "" "" * 15&#xa;                else:&#xa;                    name = squish(varlist[i], name_size)&#xa;                if len(name) < name_size:&#xa;                    name += "" "" * (name_size - len(name))&#xa;                num = ""{}{:>5}. "".format(txt, i) if numbers else """"&#xa;                fmt = fmtlist[i]&#xa;                lbl = lbllist[i]&#xa;                vlb = vlblist[i]&#xa;                print(&#xa;                    num,&#xa;                    ""{}{} "".format(res, name),&#xa;                    ""{}{:<7} {:<10} {:<10} {}{}"".format(&#xa;                        txt,&#xa;                        get_type_name(typlist[i]), &#xa;                        fmt if len(fmt) <= 10 else fmt[:8] + "".."",&#xa;                        lbl if len(lbl) <= 10 else lbl[:8] + "".."",&#xa;                        res,&#xa;                        vlb if len(vlb) <= 34 else vlb[:32] + ""..""&#xa;                    ),&#xa;                    sep=''&#xa;                )&#xa;            print(hline)&#xa;        &#xa;        # footer&#xa;        print(""{}Sorted by:  {}{}"".format(txt, res, sort_names))&#xa;        if self._changed:&#xa;            print(""     Note:  dataset has changed since last saved"")&#xa;        print("""")&#xa;                &#xa;    def return_list(self):&#xa;        """"""Display any saved results for this dta object.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays contents of saved results, if any.&#xa;        &#xa;        """"""&#xa;        if (not hasattr(self, '_return_values') or not self._return_values or &#xa;                not isinstance(self._return_values, dict)):&#xa;            print("""")&#xa;            return&#xa;        rv = self._return_values&#xa;        keys = rv.keys() if 'key_order' not in rv else rv['key_order']&#xa;        tplt = ""{{txt}}{:>22} = {{res}}{}"" if IN_STATA else ""{:>22} = {}""&#xa;        &#xa;        print("""")&#xa;        for key in keys:&#xa;            value = str(rv[key])&#xa;            if len(value) > 55: value = value[:53] + ""..""&#xa;            print(tplt.format(key, value))&#xa;        if not IN_STATA: print("""")&#xa;    &#xa;    def index(self, varname):&#xa;        """"""Get index of given data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varname : str&#xa;            Single varname (abbreviation allowed if unambiguous).&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Index of variable in data set.&#xa;        &#xa;        """"""&#xa;        if not isinstance(varname, str):&#xa;            raise TypeError(""argument must be str"")&#xa;        varname = self._find_vars(varname, empty_ok=False, single=True)[0]&#xa;        return self._varlist.index(varname)&#xa;        &#xa;    def variable(self, id):&#xa;        """"""Get a list of all values of a data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        id : int or str&#xa;            Single variable index (int) or name (str).&#xa;            For str, an abbreviation is allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        list&#xa;            List of values of the specified data variable.&#xa;        &#xa;        """"""&#xa;        if isinstance(id, str):&#xa;            varname = self._find_vars(id, empty_ok=False, single=True)[0]&#xa;            col = self._varlist.index(varname)&#xa;        elif isinstance(id, int):&#xa;            if not -self._nvar <= id < self._nvar:&#xa;                raise ValueError(""data variable index out of range"")&#xa;            col = id if id >= 0 else self._nvar + id&#xa;        else:&#xa;            raise TypeError(""argument must be str name or int column index"")&#xa;        &#xa;        varvals = self._varvals&#xa;        return [row[col] for row in varvals]&#xa;        &#xa;    def _squish_name(self, name, space):&#xa;        """"""Shorten name to fit in given space.&#xa;        Characters from middle of name replaced with '~'.&#xa;        &#xa;        """"""&#xa;        if len(name) <= space:&#xa;            return name&#xa;        if space < 3:&#xa;            raise ValueError(""too much squishing!"")&#xa;        return name[:space - 2] + ""~"" + name[-1]&#xa;        &#xa;    def rename(self, oldname, newname):&#xa;        """"""Replace old variable name with new.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        oldname : str&#xa;            Single variable name (abbreviation allowed if unambiguous).&#xa;        newname : str&#xa;            New variable name.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Replace old data variable name with new.&#xa;        &#xa;        """"""&#xa;        if not isinstance(oldname, str) or not isinstance(newname, str):&#xa;            raise TypeError(""old and new variable names should be str"")&#xa;        # unabbreviate oldname&#xa;        oldname = self._find_vars(oldname, empty_ok=False)[0] &#xa;        if oldname == newname:&#xa;            return&#xa;        newname = newname.strip()&#xa;       &#xa;        if not self._is_valid_varname(newname):&#xa;            raise ValueError(newname + "" is not a valid Stata name"")&#xa;        if newname in self._varlist:&#xa;            raise ValueError(newname + "" already exists"")&#xa; &#xa;        index = self._varlist.index(oldname)&#xa;        self._varlist[index] = newname&#xa;        &#xa;        # if oldname in chrdict, change to newname&#xa;        chrdict = self._chrdict&#xa;        if oldname in chrdict:&#xa;            chrdict[newname] = chrdict[oldname]&#xa;            del chrdict[oldname]&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    def set_obs(self, num_obs):&#xa;        """"""Increase number of observations in data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        num_obs : int&#xa;            Number of observations to increase to.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Changes number of observations. Appends observations with&#xa;        MissingValue instance for numeric variables, """" for string.&#xa;        Marks data as unsorted.&#xa;        &#xa;        """"""&#xa;        curr_obs = self._nobs&#xa;        if num_obs < curr_obs:&#xa;            raise ValueError(""num_obs must be >= "" + str(curr_obs))&#xa;        if num_obs == curr_obs:&#xa;            return&#xa;        isstrvar = self._isstrvar&#xa;        empty_row = ['' if isstrvar(i) else MISSING for i in range(self._nvar)]&#xa;        self._varvals += [copy.copy(empty_row) &#xa;                          for _ in range(num_obs - curr_obs)]&#xa;        self._nobs = num_obs&#xa;        self._changed = True&#xa;        # Need to clear srtlist. If there are string variables, there &#xa;        # might now be empty strings after non-empty string. If there &#xa;        # are numerical variables with extended missing, there will now &#xa;        # be ""."" missing after extended missing. Issue pointed out at&#xa;        # http://www.stata.com/statalist/archive/2013-08/msg00576.html&#xa;        self._srtlist = [None]*self._nvar&#xa;        &#xa;    def drop_obs(self, in_ = None, if_ = None, all_obs = False):&#xa;        """"""Drop observations from the data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to drop.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to drop.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;        all_obs : bool or coercible to Boolean, optional&#xa;            Option to drop all observations. Default value is False.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the dropped observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes specified observations.&#xa;        &#xa;        """"""&#xa;        if self._nobs == 0:&#xa;            return&#xa;        if all_obs and (in_ is not None or if_ is not None):&#xa;            raise ValueError(""all_obs cannot be combined with in_ or if_"")&#xa;        if not all_obs and in_ is None and if_ is None:&#xa;            raise ValueError(""must specify one of in_, if_, or all_obs"")&#xa;        &#xa;        if all_obs:&#xa;            self._varvals = []&#xa;            self._nobs = 0&#xa;        else:&#xa;            varvals = self._varvals&#xa;            if if_ is None:&#xa;                to_drop = [i for i in in_]&#xa;            else:&#xa;                if in_ is None: in_ = range(self._nobs)&#xa;                to_drop = [i for i in in_ if if_(i)]&#xa;            to_drop.reverse()&#xa;            for i in to_drop:&#xa;                del varvals[i]&#xa;            self._nobs = len(self._varvals)&#xa;        self._changed = True&#xa;            &#xa;    def keep_obs(self, in_ = None, if_ = None):&#xa;        """"""Keep specified observations, remove all others.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to keep.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to keep.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the kept observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes all observations except those specified.&#xa;        &#xa;        """"""&#xa;        if self._nobs == 0:&#xa;            return&#xa;        if in_ is None and if_ is None:&#xa;            raise ValueError(""must specify one of in_ or if_"")&#xa;            &#xa;        if if_ is None:&#xa;            self._varvals = [self._varvals[i] for i in in_]&#xa;        else:&#xa;            if in_ is None: in_ = range(self._nobs)&#xa;            self._varvals = [self._varvals[i] for i in in_ if if_(i)]&#xa;        self._nobs = len(self._varvals)&#xa;        self._changed = True&#xa;        &#xa;    def drop_var(self, varnames):&#xa;        """"""Delete specified variable(s).&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes specified data variables.&#xa;        &#xa;        """"""&#xa;        drop_vars = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        &#xa;        find_index = self._varlist.index&#xa;        drop_indexes = set(find_index(varname) for varname in drop_vars)&#xa;        keep_indexes = sorted(set(range(self._nvar)) - drop_indexes)&#xa;        &#xa;        # temporarily shorten srtlist to relevant values&#xa;        srtlist = self._srtlist&#xa;        if None in srtlist:&#xa;            srtlist = srtlist[:srtlist.index(None)]&#xa;        &#xa;        for var in drop_vars:&#xa;            ind = find_index(var)&#xa;                      &#xa;            del self._typlist[ind]&#xa;            del self._varlist[ind]&#xa;            del self._fmtlist[ind]&#xa;            del self._lbllist[ind]&#xa;            del self._vlblist[ind]&#xa;            &#xa;            if var in self._chrdict: del self._chrdict[var]&#xa;            &#xa;            # if ind was in srtlist, &#xa;            #    1) drop entry, and drop any entries to the right&#xa;            #    2) for entries to the left, decrease by 1 if greater than ind&#xa;            if ind in srtlist:&#xa;                srt_ind = srtlist.index(ind)&#xa;                srtlist = [(s - 1 if s > ind else s) &#xa;                           for s in srtlist[:srt_ind]]&#xa;            else:&#xa;                srtlist = [(s - 1 if s > ind else s) for s in srtlist]&#xa;        &#xa;        # fill out srtlist with Nones&#xa;        self._srtlist = srtlist + [None]*(len(keep_indexes) - len(srtlist))&#xa;            &#xa;        # remove data values&#xa;        self._varvals = [[row[v] for v in keep_indexes]&#xa;                         for row in self._varvals]&#xa;            &#xa;        self._nvar = len(keep_indexes)&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    drop_vars = drop_var&#xa;        &#xa;    def keep_var(self, varnames):&#xa;        """"""Keep specified variable(s), delete all others.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes data variables other than those specified.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=False)&#xa;        vars_to_drop = set(self._varlist) - set(varnames)&#xa;        if len(vars_to_drop) > 0:&#xa;            self.drop_var(vars_to_drop)&#xa;        &#xa;    keep_vars = keep_var&#xa;        &#xa;    def _summ_stats_meanonly(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;     &#xa;        varvals = self._varvals&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                sum_v += x&#xa;                n += 1&#xa;                mean = mean + (x - mean) / n&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w += w&#xa;                mean += (x - mean) * w / sum_w&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;     &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'N': sum_w if w_type == 'f' else n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;        stats['key_order'] = ('N', 'sum_w', 'sum', 'mean', 'min', 'max')&#xa;        return stats&#xa;        &#xa;    def _summ_stats_detail(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        M2 = 0&#xa;        M3 = 0&#xa;        M4 = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;        sum_w2 = 0&#xa;        varvals = self._varvals&#xa;        values = []&#xa;        append = values.append&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                sum_v += x&#xa;                n1 = n&#xa;                n += 1&#xa;                delta = x - mean&#xa;                delta_n = delta / n&#xa;                delta_n2 = delta_n * delta_n&#xa;                term1 = delta * delta_n * n1&#xa;                mean = mean + delta_n&#xa;                M4 += (term1 * delta_n2 * (n*n - 3*n + 3) +  &#xa;                       6 * delta_n2 * M2 - 4 * delta_n * M3)&#xa;                M3 += term1 * delta_n * (n - 2) - 3 * delta_n * M2&#xa;                M2 += term1&#xa;                &#xa;                append(x)&#xa;            &#xa;            # percentiles&#xa;            values.sort()&#xa;            pospc = [(pc * n / 100, int(pc * n / 100), pc) &#xa;                     for pc in (1, 5, 10, 25, 50, 75, 90, 95, 99)]&#xa;            stats = {'p' + str(p[2]):((values[p[1]-1] + values[p[1]]) / 2 &#xa;                     if p[0] == p[1] else values[floor(p[0])]) for p in pospc}&#xa;                        &#xa;            # largest and smallest values, with .'s added if n < 4&#xa;            prt_vals = ([""{:>9g}"".format(v) for v in values[:4]] + &#xa;                        [""{:>9}"".format(""."") for i in range(4 - n)]*2 +&#xa;                        [""{:>9g}"".format(v) for v in values[-4:]])&#xa;            # this kurtosis matches Stata, but wikipedia's is this minus 3&#xa;            stats.update({&#xa;                'kurtosis': (n*M4) / (M2*M2), &#xa;                'skewness': sqrt(n) * M3 / M2**(3/2),&#xa;                'mean': mean,&#xa;                'Var': M2 / n1,&#xa;                'sd': sqrt(M2 / n1),&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': values[0],&#xa;                'max': values[-1]&#xa;            })&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w_1 = sum_w&#xa;                sum_w += w&#xa;                sum_w2 += w*w&#xa;                delta = x - mean&#xa;                delta_W = delta / sum_w&#xa;                delta_w = delta * w / sum_w&#xa;                term1 = delta * delta_w * sum_w_1&#xa;                mean += delta_w&#xa;                M4 += (sum_w_1 * delta_w * (delta_W)**3 * (sum_w_1**3 + w**3) +&#xa;                       6 * delta_w * delta_w * M2 - 4 * delta_w * M3)&#xa;                M3 += term1 * delta_W * (sum_w_1 - w) - 3 * delta_w * M2&#xa;                M2 += term1&#xa;                &#xa;                append((x, w))&#xa;                &#xa;            values.sort()&#xa;            min_val = values[0][0]&#xa;            max_val = values[-1][0]&#xa;                    &#xa;            # Assign stats that are the same for all weight types &#xa;            # (except percentiles, handled next). This kurtosis&#xa;            # matches Stata, but wikipedia's is this minus 3.&#xa;            stats = {&#xa;                'kurtosis': (sum_w * M4) / (M2 * M2),&#xa;                'skewness': sqrt(sum_w) * M3 / M2**(3/2),&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;            # get percentiles&#xa;            pcsum = 0&#xa;            pospc = [(pc * n / 100, pc) &#xa;                     for pc in (1, 5, 10, 25, 50, 75, 90, 95, 99)]&#xa;            pos, pc = pospc[0]&#xa;            for i, (x, w) in zip(range(n), values):&#xa;                pcsum += w * n / sum_w&#xa;                while pcsum >= pos:&#xa;                    if pcsum == pos:&#xa;                        # not needed for these pcs, but if code is reused&#xa;                        # elsewhere, the i+1 below should be min((n, i+1))&#xa;                        stats['p' + str(pc)] = (x + values[i+1][0]) / 2 &#xa;                    else:&#xa;                        stats['p' + str(pc)] = x&#xa;                    del pospc[0]&#xa;                    if pospc == []:&#xa;                        break&#xa;                    pos, pc = pospc[0]&#xa;                else:           # these next three lines exit the for loop&#xa;                    continue    # if -break- encountered in the while loop&#xa;                break           # i.e. if there are no more pcs to assign&#xa;                &#xa;            # in case there are any percentiles that haven't been assigned:&#xa;            for pos, pc in pospc:&#xa;                stats['p' + str(pc)] = values[-1][0]&#xa;            &#xa;            # assign stats that depend on weight type&#xa;            if w_type == 'f':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = sum_w&#xa;            else: # just aweight ; iweight not allowed&#xa;                adj = sum_w / (sum_w * sum_w - sum_w2)&#xa;                &#xa;                stats['Var'] = M2 * adj&#xa;                stats['sd']  = sqrt(M2 * adj)&#xa;                stats['N']   = n&#xa;            &#xa;            # largest and smallest values, with .'s added if n < 4&#xa;            prt_vals = ([""{:>9g}"".format(v[0]) for v in values[:4]] + &#xa;                        [""{:>9}"".format(""."") for i in range(4 - n)]*2 +&#xa;                        [""{:>9g}"".format(v[0]) for v in values[-4:]])&#xa;        &#xa;        stats['key_order'] = ('N', 'sum_w', 'mean', 'Var', 'sd', 'skewness', &#xa;                              'kurtosis', 'sum', 'min', 'max', 'p1', 'p5', &#xa;                              'p10', 'p25', 'p50', 'p75', 'p90', 'p95', 'p99')&#xa;        return stats, prt_vals&#xa;        &#xa;    def _summ_stats_default(self, v_index, w_index, w_type, obs_nums):&#xa;        n = 0&#xa;        mean = 0&#xa;        M2 = 0&#xa;        min_val = float('inf')&#xa;        max_val = float('-inf')&#xa;        sum_v = 0&#xa;        sum_w = 0&#xa;        sum_w2 = 0&#xa;     &#xa;        varvals = self._varvals&#xa;        ismissing = self.ismissing&#xa;        &#xa;        if w_index is None:&#xa;            for i in obs_nums:&#xa;                x = varvals[i][v_index]&#xa;                if ismissing(x):&#xa;                    continue&#xa;                n1 = n&#xa;                n += 1&#xa;                sum_v += x&#xa;                delta = x - mean&#xa;                delta_n = delta / n&#xa;                mean = mean + delta_n&#xa;                M2 += delta * delta_n * n1&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'Var': M2 / n1,&#xa;                'sd': sqrt(M2 / n1),&#xa;                'sum_w': n,&#xa;                'sum': sum_v,&#xa;                'N': n,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;        else:&#xa;            for i in obs_nums:&#xa;                row = varvals[i]&#xa;                x, w = row[v_index], row[w_index]&#xa;                if ismissing(x) or w == 0 or ismissing(w):&#xa;                    continue&#xa;                n += 1&#xa;                sum_v += x * w&#xa;                sum_w_1 = sum_w&#xa;                sum_w += w&#xa;                sum_w2 += w*w&#xa;                delta = x - mean&#xa;                delta_w = delta * w / sum_w&#xa;                mean += delta_w&#xa;                M2 += delta * delta_w * sum_w_1&#xa;                &#xa;                min_val = min((min_val, x))&#xa;                max_val = max((max_val, x))&#xa;                &#xa;            stats = {&#xa;                'mean': mean,&#xa;                'sum_w': sum_w,&#xa;                'sum': sum_v,&#xa;                'min': min_val,&#xa;                'max': max_val&#xa;            }&#xa;            &#xa;            if w_type == 'f':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = sum_w&#xa;            elif w_type == 'i':&#xa;                stats['Var'] = M2 / (sum_w - 1)&#xa;                stats['sd']  = sqrt(M2 / (sum_w - 1))&#xa;                stats['N']   = n&#xa;            else:&#xa;                adj = sum_w / (sum_w * sum_w - sum_w2)&#xa;                &#xa;                stats['Var'] = M2 * adj&#xa;                stats['sd']  = sqrt(M2 * adj)&#xa;                stats['N']   = n&#xa;                &#xa;        stats['key_order'] = ('N', 'sum_w', 'mean', 'Var', &#xa;                              'sd', 'min', 'max', 'sum')&#xa;        return stats&#xa;                    &#xa;    def _pctiles_from_sorted_v2(self, values, pcs):&#xa;        """"""get percentiles from given sorted iterable of values""""""&#xa;        if not all(0 <= pc <= 100 for pc in pcs):&#xa;            raise ValueError(""pctiles must be between 0 and 100"")&#xa;        nvals = len(values)&#xa;        pctiles = []&#xa;        for pc in pcs:&#xa;            if pc == 0:&#xa;                new_pct = values[0]&#xa;            elif pc == 100:&#xa;                new_pct = values[nvals-1]&#xa;            else:&#xa;                loc = nvals * pc / 100&#xa;                loc_flr = floor(loc)&#xa;                t = loc - loc_flr&#xa;                new_pct = (1 - t) * values[loc_flr - 1] + t * values[loc_flr]&#xa;            pctiles.append(new_pct)&#xa;        return pctiles&#xa;    &#xa;    def _pctiles_from_sorted(self, values, pcs):&#xa;        """"""get percentiles from given sorted iterable of values""""""&#xa;        if not all(0 <= pc <= 100 for pc in pcs):&#xa;            raise ValueError(""pctiles must be between 0 and 100"")&#xa;        nvals = len(values)&#xa;        pctiles = []&#xa;        for pc in pcs:&#xa;            if pc == 0:&#xa;                new_pct = values[0]&#xa;            elif pc == 100:&#xa;                new_pct = values[nvals-1]&#xa;            else:&#xa;                n = pc * nvals / 100&#xa;                if n == int(n):&#xa;                    new_pct = (values[int(n)-1] + values[int(n)]) / 2&#xa;                else:&#xa;                    new_pct = values[floor(n)]&#xa;            pctiles.append(new_pct)&#xa;        return pctiles&#xa;    &#xa;    def _obs_from_in_if(self, in_=None, if_=None):&#xa;        """"""helper for any method that takes in_ and if_ observation args""""""&#xa;        &#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (isinstance(in_, str) or &#xa;                    not isinstance(in_, collections.Iterable)):&#xa;                raise TypeError(""in_ option should be int or iterable of int"")&#xa;            else:&#xa;                in_ = tuple(in_)&#xa;                if not all(isinstance(i, int) for i in in_):&#xa;                    raise TypeError(""in_ should be int or iterable of int"")&#xa;        else:&#xa;            in_ = range(self._nobs)&#xa;            &#xa;        if if_ is not None:&#xa;            if not hasattr(if_, ""__call__""):&#xa;                raise TypeError(""if_ option should be callable"")&#xa;            obs = tuple(i for i in in_ if if_(i))&#xa;        else:&#xa;            obs = tuple(i for i in in_)&#xa;        &#xa;        return obs&#xa;    &#xa;    def _summ_template(self, w_index=None, w_type=None, detail=False):&#xa;        """"""helper for summarize()""""""&#xa;        if IN_STATA:&#xa;            if detail:&#xa;                header = ""{{txt}}{}\n{{hline 61}}""&#xa;                var_tplt = """".join(&#xa;                   (""{{txt}}      Percentiles      Smallest\n"",&#xa;                    ""{{txt}} 1%    {{res}}{:>9g}      {}\n"",&#xa;                    ""{{txt}} 5%    {{res}}{:>9g}      {}\n"", &#xa;                    ""{{txt}}10%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Obs          {{res}}{:>9d}\n"",&#xa;                    ""{{txt}}25%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Sum of Wgt.  {{res}}{:>9g}\n"",&#xa;                    ""\n"",&#xa;                    ""{{txt}}50%    {{res}}{:>9g}        "",&#xa;                        ""              {{txt}}Mean         {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}                        "",&#xa;                        ""Largest       Std. Dev.    {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}75%    {{res}}{:>9g}      {}\n"",&#xa;                    ""{{txt}}90%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Variance     {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}95%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Skewness     {{res}}{:>9g}\n"",&#xa;                    ""{{txt}}99%    {{res}}{:>9g}      {}"",&#xa;                        ""       {{txt}}Kurtosis     {{res}}{:>9g}""))&#xa;                    &#xa;                tplt = (header, var_tplt)&#xa;            elif w_index is None or w_type == 'f':&#xa;                header = """".join((""\n{txt}    Variable {c |}       "",&#xa;                    ""Obs        Mean    Std. Dev.       Min        Max""))&#xa;                sepline = ""{txt}{hline 13}{c +}{hline 56}""&#xa;                row = """".join((""{{txt}}{:>12} {{c |}} {{res}}{N:>9g} "", &#xa;                               ""{mean:>11g} {sd:>11g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{{txt}}{:>12} {{c |}} {{res}}        0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;            else:&#xa;                header = """".join((""\n{txt}    Variable {c |}     Obs      "",&#xa;                      ""Weight        Mean   Std. Dev.       Min        Max""))&#xa;                sepline = ""{txt}{hline 13}{c +}{hline 65}""&#xa;                row = """".join((""{{txt}}{:>12} {{c |}} {{res}}"",&#xa;                               ""{N:>7g} {sum_w:>11g} {mean:>11g} "", &#xa;                               ""{sd:>10g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{{txt}}{:>12} {{c |}} {{res}}      0           0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;        else:&#xa;            if detail:&#xa;                header = """".join((""{}\n"", ""-"" * 61))&#xa;                var_tplt = """".join(&#xa;                   (""      Percentiles      Smallest\n"",&#xa;                    "" 1%    {:>9g}      {}\n"",&#xa;                    "" 5%    {:>9g}      {}\n"", &#xa;                    ""10%    {:>9g}      {}       Obs          {:>9d}\n"",&#xa;                    ""25%    {:>9g}      {}       Sum of Wgt.  {:>9g}\n"",&#xa;                    ""\n"",&#xa;                    ""50%    {:>9g}"", "" "" * 22, ""Mean         {:>9g}\n"",&#xa;                    "" "" * 24, ""Largest       Std. Dev.    {:>9g}\n"",&#xa;                    ""75%    {:>9g}      {}\n"",&#xa;                    ""90%    {:>9g}      {}       Variance     {:>9g}\n"",&#xa;                    ""95%    {:>9g}      {}       Skewness     {:>9g}\n"",&#xa;                    ""99%    {:>9g}      {}       Kurtosis     {:>9g}""))&#xa;                    &#xa;                tplt = (header, var_tplt)&#xa;            elif w_index is None or w_type == 'f':&#xa;                header = """".join((""\n    Variable |       "",&#xa;                    ""Obs        Mean    Std. Dev.       Min        Max""))&#xa;                sepline = """".join((""-"" * 13, ""+"", ""-"" * 56))&#xa;                row = """".join((""{:>12} | {N:>9g} {mean:>11g} "", &#xa;                               ""{sd:>11g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{:>12} |         0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;            else:&#xa;                header = """".join((""\n    Variable |     Obs      "",&#xa;                      ""Weight        Mean   Std. Dev.       Min        Max""))&#xa;                sepline = """".join((""-"" * 13, ""+"", ""-"" * 65))&#xa;                row = """".join((""{:>12} | {N:>7g} {sum_w:>11g} {mean:>11g} "", &#xa;                               ""{sd:>10g} {min:>10g} {max:>10g}""))&#xa;                zero_row = ""{:>12} |       0           0""&#xa;                &#xa;                tplt = (header, sepline, row, zero_row)&#xa;                            &#xa;        return tplt&#xa;        &#xa;    def _summ_meanonly(self, wt_index, wt_type, obs, varnames, indexes):&#xa;        """"""do summary if meanonly""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        index = indexes[-1]&#xa;        &#xa;        if self._isnumvar(index):&#xa;            info = self._summ_stats_meanonly(index, wt_index, wt_type, obs)&#xa;        else:&#xa;            info = zero_info&#xa;            &#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;        &#xa;    def _summ_detail(self, wt_index, wt_type, obs, varnames, indexes):&#xa;        """"""do summary if detail""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        isnumvar = self._isnumvar&#xa;        summ_stats = self._summ_stats_detail&#xa;        vlblist = self._vlblist&#xa;        &#xa;        header, var_tplt = self._summ_template(detail=True)&#xa;        print("""")&#xa;        for i, (name, index) in enumerate(zip(varnames, indexes)):&#xa;            if isnumvar(index):&#xa;                info, vals = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = zero_info&#xa;            &#xa;            label = vlblist[index]&#xa;            label = label[:60] if label != """" else name&#xa;            label = """".join(("" "" * (30 - floor(len(label)/2)), label))&#xa;            print(header.format(label))&#xa;            if info[""N""] != 0:&#xa;                print(&#xa;                    var_tplt.format(&#xa;                        info['p1'], vals[0], &#xa;                        info['p5'], vals[1], &#xa;                        info['p10'], vals[2], info['N'], &#xa;                        info['p25'], vals[3], info['sum_w'], &#xa;                        info['p50'], info['mean'], &#xa;                        info['sd'], &#xa;                        info['p75'], vals[-4], &#xa;                        info['p90'], vals[-3], info['Var'], &#xa;                        info['p95'], vals[-2], info['skewness'], &#xa;                        info['p99'], vals[-1], info['kurtosis']&#xa;                    )&#xa;                )&#xa;            else:&#xa;                print(""no observations"")&#xa;           &#xa;            print("""")&#xa;        &#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;    &#xa;    def _summ_default(self, wt_index, wt_type, obs, &#xa;                      varnames, indexes, separator):&#xa;        """"""do summary if not detail and not meanonly""""""&#xa;        zero_info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                     'key_order': ('N', 'sum_w', 'sum')}&#xa;        isnumvar = self._isnumvar&#xa;        summ_stats = self._summ_stats_default&#xa;        squish_name = self._squish_name&#xa;        &#xa;        tplt = self._summ_template(wt_index, wt_type)&#xa;        header, sepline, row_tplt, zero_row = tplt&#xa;        print(header)&#xa;        for i, (name, index) in enumerate(zip(varnames, indexes)):&#xa;            if i % separator == 0: print(sepline)&#xa;            &#xa;            if isnumvar(index):&#xa;                info = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = zero_info&#xa;            &#xa;            small_name = squish_name(name, 12)&#xa;            &#xa;            if info[""N""] != 0:&#xa;                print(row_tplt.format(small_name, **info))&#xa;            else:&#xa;                print(zero_row.format(small_name))&#xa;        &#xa;        print("""")&#xa;        self._return_values = info if info[""N""] != 0 else zero_info&#xa;        &#xa;    def _check_summ_args(self, detail=False, meanonly=False, separator=5, &#xa;                         quietly=False, weight=None, fweight=None, &#xa;                         aweight=None, iweight=None, in_=None, if_=None):&#xa;        """"""helper for summarize()""""""&#xa;        obs = self._obs_from_in_if(in_, if_)&#xa;        &#xa;        # weight stuff&#xa;            # check that all non-None weights are string&#xa;        if any(w is not None and not isinstance(w, str) &#xa;               for w in [weight, fweight, aweight, iweight]):&#xa;            raise TypeError(""weight options must be None or string"")&#xa;            &#xa;            # count weights that are not None and not empty string&#xa;        nweights = len([w for w in [weight, fweight, aweight, iweight] &#xa;                            if w is not None and w.strip() != """"])&#xa;        &#xa;        if nweights > 1:&#xa;            raise ValueError(""weight options cannot be combined"")&#xa;        elif nweights == 1:&#xa;            wt_type, wt_name = (x for x in (('a', weight), ('f', fweight),&#xa;                                            ('a', aweight), ('i', iweight))&#xa;                                if x[1] is not None).__next__()&#xa;            wt_vars = self._find_vars(wt_name)&#xa;            if len(wt_vars) > 1:&#xa;                raise ValueError(""only one weight variable allowed"")&#xa;            wt_index = self._varlist.index(wt_name)&#xa;                &#xa;            if self._isstrvar(wt_index):&#xa;                raise TypeError(""strings cannot be used as weights"")&#xa;                    &#xa;            if wt_type == 'i' and detail:&#xa;                msg = ""iweight may not be combined with detail option""&#xa;                raise ValueError(msg)&#xa;                &#xa;            if wt_type == 'f' and not self._isintvar(wt_index):&#xa;                raise TypeError(""frequency weights must be integer"")        &#xa;                &#xa;            if wt_type == 'a' and weight is not None and not self._quiet:&#xa;                if IN_STATA: print(""{txt}(analytic weights assumed)"")&#xa;                else: print(""(analytic weights assumed)"")&#xa;        else:&#xa;            wt_type, wt_index = ('a', None)&#xa;        &#xa;        # misc.&#xa;        if detail and meanonly:&#xa;            raise ValueError(""options meanonly and detail cannot be combined"")&#xa;        if separator != 5:&#xa;            if not isinstance(separator, int):&#xa;                raise TypeError(""separator option should be an integer"")&#xa;            if separator < 0:&#xa;                separator = 5&#xa;            &#xa;        return obs, (wt_type, wt_index), detail, meanonly, quietly, separator&#xa;        &#xa;    def summarize(self, varnames="""", *args, **kwargs):&#xa;        """"""Summarize data variables.&#xa;        &#xa;        Summarize specified variable(s), or, if no variables specified,&#xa;        summarize all variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str, optional&#xa;            Default is none specified (i.e. summarize all).&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        detail : bool (or coercible to bool)&#xa;            May not be combined with `meanonly`.&#xa;        meanonly : bool (or coercible to bool)&#xa;            May not be combined with `detail`.&#xa;        separator : int&#xa;            Number of summaries to group together with dividing line.&#xa;            Has no effect&#xa;        quietly : bool (or coercible to bool)&#xa;            Create summary, but do not display. Useful if only wanting&#xa;            to save summary results, to be displayed with `return_list`.&#xa;        weight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;        aweight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;        fweight : str &#xa;            Single varname (or abbreviation), of an integer variable. &#xa;            May not be combined with other weights.&#xa;        iweight : str &#xa;            Single varname (or abbreviation). &#xa;            May not be combined with other weights.&#xa;            May not be combined with `detail` option.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to include in summary.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to include in summary.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        Above parameters can be accessed by name.&#xa;        Otherwise, the parameters appear in the above order.&#xa;        If both `in_` and `if_` are used, the summarized observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays summary of specified variable(s). Saves the values&#xa;        from the last summary, which can be displayed with `return_list`.&#xa;        &#xa;        """"""&#xa;        (obs, (wt_type, wt_index), detail,&#xa;         meanonly, quietly, separator) = self._check_summ_args(*args, **kwargs)&#xa;         &#xa;        # get variables and their indices&#xa;        varnames = self._find_vars(varnames, empty_ok=True)&#xa;        nvarnames = len(varnames)&#xa;        if nvarnames == 0:&#xa;            varnames = self._varlist&#xa;            indexes = list(range(self._nvar))&#xa;        else:&#xa;            indexes = list(map(self._varlist.index, varnames))&#xa;        &#xa;        # do the summ&#xa;        if meanonly:&#xa;            self._summ_meanonly(wt_index, wt_type, obs, varnames, indexes)&#xa;        elif quietly:&#xa;            summ_stats = (self._summ_stats_detail &#xa;                          if detail &#xa;                          else self._summ_stats_default)&#xa;            index = indexes[-1]&#xa;            &#xa;            if self._isnumvar(index):&#xa;                info = summ_stats(index, wt_index, wt_type, obs)&#xa;            else:&#xa;                info = {'N': 0, 'sum_w': 0, 'sum': 0, &#xa;                        'key_order': ('N', 'sum_w', 'sum')}&#xa;            &#xa;            self._return_values = info&#xa;        elif detail:&#xa;            self._summ_detail(wt_index, wt_type, obs, varnames, indexes)&#xa;        else:&#xa;            self._summ_default(wt_index, wt_type, obs, &#xa;                               varnames, indexes, separator)&#xa;            &#xa;    summ = summarize&#xa;        &#xa;    def sort(self, varnames):&#xa;        """"""Sort data values according to given variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Sorts observations of the data set.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        var_ind_list = list(map(self._varlist.index, varnames))&#xa;        new_srtlist = var_ind_list + [None]*(self._nvar - len(varnames))&#xa;        if self._srtlist == new_srtlist:&#xa;            return&#xa;        sort_key = lambda row: [row[i] for i in var_ind_list]&#xa;        self._varvals.sort(key = sort_key)&#xa;        self._srtlist = new_srtlist&#xa;        self._changed = True&#xa;    &#xa;    def _convert_hex(self, hex_value):&#xa;        """"""convert Python's hex representation to Stata's""""""&#xa;        if not isinstance(hex_value, str):&#xa;            raise TypeError(""given hex value must be str"")&#xa;        m = HEX_RE.match(hex_value)&#xa;        if m is None:&#xa;            raise ValueError(""given string does not seem to be Python hex"")&#xa;        sign_char, base, exp_sign, exp = [m.group(i) for i in range(1,5)]&#xa;        new_sign = ""+"" if sign_char is None else sign_char&#xa;        # Line below converts exp to hex value. The ""0x"" prefix is removed &#xa;        # with [2:]. The exponent is padded with (too many) zeros (Stata &#xa;        # requires 3 digits), and reduced to last 3 digits with [-3:].&#xa;        new_exp = (""000"" + hex(int(exp))[2:])[-3:]&#xa;        return """".join((new_sign, base, 'X', exp_sign, new_exp))&#xa;        &#xa;    def _stata_hex_format(self, value):&#xa;        """"""convert numeric value to string in Stata hex format""""""&#xa;        return self._convert_hex(float(value).hex())&#xa;        &#xa;    def _stata_HL_format(self, fmt, value):&#xa;        """"""convert numeric value to string in one of Stata's H or L formats""""""&#xa;        if fmt == '%16H':&#xa;            packed_value = pack('>d', value)&#xa;        elif fmt == '%8H':&#xa;            packed_value = pack('>f', value)&#xa;        elif fmt == '%16L':&#xa;            packed_value = pack('<d', value)&#xa;        elif fmt == '%8L':&#xa;            packed_value = pack('<f', value)&#xa;        else:&#xa;            raise ValueError(""{} is not a recognized hilo format"".format(fmt))&#xa;        &#xa;        return """".join(hex(x)[2:].zfill(2) for x in packed_value)&#xa;    &#xa;    def _translate_fmts(self):&#xa;        """"""Translate Stata formats to Python. Bad formats&#xa;        are replaced by default format for given type.&#xa;        &#xa;        """"""&#xa;        fmt_info = []&#xa;        fmt_append = fmt_info.append&#xa;        &#xa;        isvalid = self._is_valid_fmt&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        default_fmts = self._default_fmts&#xa;        &#xa;        for i, fmt in enumerate(self._fmtlist):&#xa;            fmt = fmt.strip()&#xa;            &#xa;            iscalendar = (fmt[1] == 't' or fmt[1:3] == '-t')&#xa;            &#xa;            if iscalendar or not isvalid(fmt):&#xa;                if isstrvar(i):&#xa;                    wid = min(typlist[i], 10)&#xa;                    fmt_append(('s', ""{{:>{}s}}"".format(wid), wid))&#xa;                    continue&#xa;                else:&#xa;                    fmt = default_fmts[typlist[i]]&#xa;            &#xa;            last_char = fmt[-1]&#xa;            if last_char == 's': # string&#xa;                m = STR_FMT_RE.match(fmt)&#xa;                align, _, wid = m.group(1), m.group(2), m.group(3)&#xa;                new_align = (""<"" if align == ""-"" &#xa;                                 else ""^"" if align == ""~"" else "">"")&#xa;                new = """".join((""{:"", new_align, wid, ""s}""))&#xa;                fmt_append(('s', new, int(wid)))&#xa;            elif last_char == 'H' or last_char == 'L': # binary&#xa;                fmt_append((last_char, fmt, int(fmt[1:-1])))&#xa;            elif last_char == 'x': # hexadecimal&#xa;                fmt_append(('x', fmt, 21))&#xa;            elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;                m = NUM_FMT_RE.match(fmt)&#xa;                align, _, wid, delim, prec, type, com = (m.group(1), m.group(2), &#xa;                                                         m.group(3), m.group(4),&#xa;                                                         m.group(5), m.group(6),&#xa;                                                         m.group(7))&#xa;                aln = ""<"" if align == ""-"" else "">""&#xa;                sep = "","" if com is not None else """"&#xa;                if type == ""g"" and int(prec) == 0:&#xa;                    new = """".join((""{:"", aln, wid, sep, type, ""}""))&#xa;                else:&#xa;                    new = """".join((""{:"", aln, wid, sep, ""."", prec, type, ""}""))&#xa;                fmt_append((type, new, int(wid), delim, com))&#xa;                &#xa;        return fmt_info&#xa;    &#xa;    def _list_format_withstata(self, fmt, val):&#xa;        """"""helper for list()""""""&#xa;        if isinstance(val, float) or isinstance(val, int):&#xa;            return st_format(fmt, val)&#xa;        elif isinstance(val, MissingValue):&#xa;            return st_format(fmt, val.value)&#xa;        else: # str, presumably&#xa;            width = fmt[1:-1]&#xa;            return ((""{:>"" + width).replace("">-"", ""<"") + ""}"").format(val)&#xa;    &#xa;    def _list_format_nostata(self, fmt_info, val):&#xa;        """"""helper for list()""""""&#xa;        if isinstance(val, MissingValue):&#xa;            aln = fmt_info[1][2]&#xa;            wid = str(fmt_info[2])&#xa;            return """".join((""{:"", aln, wid, ""s}"")).format(str(val))&#xa;        &#xa;        fmt_type = fmt_info[0]&#xa;        decimal_comma = fmt_type in ('f', 'g', 'e') and fmt_info[3] is not None&#xa;        if fmt_type == 's':  # ie, no comma needed&#xa;            # use fmt_info[2], the intended width, to chop off, just in case&#xa;            return fmt_info[1].format(val)[:fmt_info[2]]&#xa;        if fmt_type in ('f', 'g', 'e'):&#xa;            val_str = fmt_info[1].format(val)&#xa;            if fmt_info[3] == "","":  # decimal comma&#xa;                if fmt_info[4] is None:  # no thousands separator&#xa;                    return val_str.replace(""."", "","")&#xa;                else:&#xa;                    return ""."".join(v.replace(""."", "","") &#xa;                                    for v in val_str.split("",""))&#xa;            else:&#xa;                return val_str&#xa;        elif fmt_type == 'x':&#xa;            return self._stata_hex_format(val)&#xa;        elif fmt_type in ('H', 'L'):&#xa;            return self._stata_HL_format(fmt_info[1], val)&#xa;        else:&#xa;            raise ValueError(""internal error; contact package author"")&#xa;            &#xa;    def _check_list_args(self, separator=5, in_=None, if_=None):&#xa;        """"""helper for list()""""""&#xa;        &#xa;        obs = self._obs_from_in_if(in_, if_)&#xa;        &#xa;        if separator != 5:&#xa;            if not isinstance(separator, int):&#xa;                raise TypeError(""separator option should be an integer"")&#xa;            if separator < 0:&#xa;                separator = 5&#xa;            &#xa;        return obs, separator&#xa;    &#xa;    def list(self, varnames="""", **kwargs):&#xa;        """"""Print table of data values.&#xa;        &#xa;        Print table of values for specified variable(s), or all&#xa;        variables if none specified. &#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str, optional&#xa;            Default is none specified (i.e. list all).&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations to list.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations to list.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the listed observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays table of values.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=True)&#xa;        if len(varnames) == 0:&#xa;            varnames = self._varlist&#xa;        ncols = len(varnames)&#xa;        varvals = self._varvals&#xa;        &#xa;        find_index = self._varlist.index&#xa;        indexes = [find_index(name) for name in varnames]&#xa;        &#xa;        if IN_STATA:&#xa;            list_format = self._list_format_withstata&#xa;            fmts = self._fmtlist&#xa;            widths = [len(list_format(fmts[i], varvals[0][i])) &#xa;                      for i in indexes]&#xa;        else:&#xa;            list_format = self._list_format_nostata&#xa;            fmts = self._translate_fmts()  # formats plus other info&#xa;            widths = [fmts[i][2] for i in indexes]&#xa;        &#xa;        obs, separator = self._check_list_args(**kwargs)&#xa;        &#xa;        &#xa;        ndigits = (1 if len(obs) == 0 or obs[-1] <= 1 &#xa;                   else floor(log(obs[-1] - 1, 10)) + 1)&#xa;        rownum_tplt = "" {{:>{}}}. "".format(ndigits)&#xa;        colnum_tplt = [""{:"" + (""<"" if self._fmtlist[i][1] == ""-"" else "">"") + &#xa;                       ""{}}}"".format(w) for i, w in zip(indexes, widths)]&#xa;        spacer = "" ""*(ndigits + 3)&#xa;        &#xa;        # table boundaries&#xa;        inner_width = 2*ncols + sum(widths)&#xa;        if IN_STATA:&#xa;            hline = ""{hline "" + str(inner_width) + ""}""&#xa;            top_line = spacer + ""{c TLC}"" + hline + ""{c TRC}""&#xa;            mid_line = spacer + ""{c LT}"" + hline + ""{c RT}""&#xa;            bot_line = spacer + ""{c BLC}"" + hline + ""{c BRC}""&#xa;            row_tplt = ""{}{{c |}} {{res}}{} {{txt}}{{c |}}""&#xa;        else:&#xa;            hline = ""-"" * inner_width&#xa;            top_line = mid_line = bot_line = spacer + ""+"" + hline + ""+""&#xa;            row_tplt = ""{}| {} |""&#xa;        &#xa;        # variable names&#xa;        if IN_STATA: print(""{txt}"")&#xa;        print(top_line)&#xa;        squish = self._squish_name&#xa;        row_info = ""  "".join(tplt.format(squish(n, w)) &#xa;            for tplt, n, w in zip(colnum_tplt, varnames, widths))&#xa;        print(row_tplt.format(spacer, row_info))&#xa;        &#xa;        # values&#xa;        for obs_count, i in enumerate(obs):&#xa;            if obs_count % separator == 0:&#xa;                print(mid_line)&#xa;            row = varvals[i]&#xa;            row_info = ""  "".join(list_format(fmts[j], row[j]) for j in indexes)&#xa;            row_info = row_tplt.format(rownum_tplt.format(i), row_info)&#xa;            try:&#xa;                print(row_info)&#xa;            except UnicodeEncodeError:&#xa;                print(row_info.encode('ascii', 'replace').decode())&#xa;        &#xa;        print(bot_line)&#xa;    &#xa;    def order(self, varnames, last=False, &#xa;              before=None, after=None, alpha=False):&#xa;        """"""Change order of varlist.&#xa;        &#xa;        Any duplicates in varnames will be ignored.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        last : bool (or coercible to bool), optional&#xa;            Signal that specified variables should come last in the&#xa;            varlist instead of first. Default is False.&#xa;            May not be combined with `before` or `after`.&#xa;        before : str, optional&#xa;            Name of variable to put the specified variables before.&#xa;            An abbreviation is allowed if unambiguous.&#xa;            By default this option is turned off.&#xa;            May not be combined with `last' or `after'&#xa;        after : str, optional&#xa;            Name of variable to put the specified variables after.&#xa;            An abbreviation is allowed if unambiguous.&#xa;            By default this option is turned off.&#xa;            May not be combined with `last' or `before'&#xa;        alpha : bool (or coercible to bool), optional&#xa;            Signal that varlist should be sorted alphabetically &#xa;            before rearranging. Default is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Reorders varlist.&#xa;        &#xa;        """"""&#xa;        # check for bad combinations of options&#xa;        if before and after:&#xa;            raise ValueError(""options before and after cannot be combined"")&#xa;        if last and (before or after):&#xa;            msg = ""options last and {} cannot be combined""&#xa;            raise ValueError(msg.format(""before"" if before else ""after""))&#xa;        &#xa;        varnames = self._find_vars(varnames, unique=True, &#xa;                                  all_ok=True, empty_ok=False)&#xa;        &#xa;        # put in alphabetic order, if requested&#xa;        if alpha:&#xa;            varnames.sort()&#xa;        &#xa;        # find sort order &#xa;        var_index = self._varlist.index&#xa;        new_order = [var_index(name) for name in varnames]&#xa;        used_vars = set(new_order)&#xa;        &#xa;        if last:&#xa;            new_order = ([i for i in range(self._nvar) if i not in used_vars] +&#xa;                         new_order)&#xa;        elif before:&#xa;            before = self._find_vars(before, single=True)[0]&#xa;            if before in varnames:&#xa;                msg = ""varname in -before- option may not be in varlist""&#xa;                raise ValueError(msg)&#xa;                &#xa;            before_index = var_index(before)&#xa;            new_order = ([i for i in range(before_index)&#xa;                            if i not in used_vars] + &#xa;                         new_order + &#xa;                         [i for i in range(before_index, self._nvar) &#xa;                            if i not in used_vars])&#xa;        elif after:&#xa;            after = self._find_vars(after, single=True)[0]&#xa;            if after in varnames:&#xa;                msg = ""varname in -after- option may not be in varlist""&#xa;                raise ValueError(msg)&#xa;                &#xa;            after_index = var_index(after)&#xa;            new_order = ([i for i in range(after_index+1) &#xa;                            if i not in used_vars] +&#xa;                         new_order + &#xa;                         [i for i in range(after_index+1, self._nvar) &#xa;                            if i not in used_vars])&#xa;        else:&#xa;            new_order += [i for i in range(self._nvar) if i not in used_vars]&#xa;          &#xa;        # if new_order same as old order, abort&#xa;        if new_order == list(range(self._nvar)):&#xa;            return&#xa;        &#xa;        # do reordering&#xa;        new_order_index = new_order.index&#xa;        &#xa;        self._typlist = [self._typlist[i] for i in new_order]&#xa;        self._varlist = [self._varlist[i] for i in new_order]&#xa;        # renumber sort entries&#xa;        self._srtlist = [new_order_index(srt) &#xa;                         if srt is not None else None for srt in self._srtlist]&#xa;        self._fmtlist = [self._fmtlist[i] for i in new_order]&#xa;        self._lbllist = [self._lbllist[i] for i in new_order]&#xa;        self._vlblist = [self._vlblist[i] for i in new_order]&#xa;        &#xa;        varvals = self._varvals&#xa;        self._varvals = [[row[i] for i in new_order] for row in varvals]&#xa;        &#xa;        self._changed = True&#xa;        &#xa;    def clonevar(self, oldname, newname):&#xa;        """"""Create a data variable into a new variable.&#xa;        &#xa;        New data variable will have the same data values, display format,&#xa;        labels, value labels, notes, and characteristics.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        oldname : str&#xa;            Single variable name (abbreviation allowed if unambiguous).&#xa;        newname : str&#xa;            New variable name.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates new variable. Copies data values, display format, &#xa;        labels, value labels, notes, and characteristics.&#xa;        &#xa;        """"""&#xa;        if not isinstance(oldname, str) or not isinstance(newname, str):&#xa;            raise TypeError(""old and new variable names should be str"")&#xa;        # unabbreviate oldname&#xa;        oldname = self._find_vars(oldname, empty_ok=False)[0] &#xa;&#xa;        if oldname == newname:&#xa;            return&#xa;        newname = newname.strip()&#xa;&#xa;        if not self._is_valid_varname(newname):&#xa;            raise ValueError(newname + "" is not a valid Stata name"")&#xa;        if newname in self._varlist:&#xa;            raise ValueError(newname + "" already exists"")&#xa;                          &#xa;        #Make new var and index it&#xa;        self._varlist.append(newname) &#xa;                       &#xa;        #Find old and make a new var with old data                   &#xa;        index_old = self._varlist.index(oldname)&#xa;    &#xa;        for row in self._varvals:&#xa;            row.append(row[index_old])&#xa;&#xa;        #Copy Srt Lst   &#xa;        self._srtlist.append(None) &#xa;        &#xa;        #Copy Type information&#xa;        nlst = self._typlist&#xa;        num = nlst[index_old]        &#xa;        self._typlist.append(num)&#xa;       &#xa;        #Copy Display Format of New Variable from Old&#xa;        distype = self._fmtlist[index_old]&#xa;        self._fmtlist.append(distype)&#xa;&#xa;        #Copy Label List&#xa;        labellist = self._lbllist[index_old]&#xa;        self._lbllist.append(labellist)&#xa;&#xa;        #Copy variable labels&#xa;        varlab = self._vlblist[index_old]&#xa;        self._vlblist.append(varlab)&#xa;        &#xa;        #Copy characeristics&#xa;        if oldname in self._chrdict:&#xa;            chars = self._chrdict[oldname].copy()&#xa;            self._chrdict[newname] = chars&#xa;&#xa;        # increment self._nvar by 1&#xa;        self._nvar = self._nvar + 1 &#xa;    &#xa;        self._changed = True&#xa;        &#xa;    def append_obs(self, value):&#xa;        """"""Append observations to the end of the dataset.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        value : iterable&#xa;            Should be an iterable of iterables, one sub-iterable&#xa;            per observation. The observations should contain as&#xa;            many values as there are variables in the data set,&#xa;            and the values should have correct type.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates new observations in the data set, and inputs the given&#xa;        values into those observations.&#xa;        &#xa;        """"""&#xa;        &#xa;        # Create index for columns&#xa;        ncols = self._nvar&#xa;        col_nums = list(range(ncols))&#xa;              &#xa;        # Put value list of lists into the valvars form&#xa;        value = self._standardize_input(value)&#xa;       &#xa;        # Make sure value is in correct form&#xa;        if (ncols != 1 and len(value) == ncols and &#xa;                all(len(v) == 1 for v in value)):&#xa;            value = (tuple(v[0] for v in value),)&#xa;       &#xa;        # Create index for rows&#xa;        nrows = len(value)&#xa;        row_nums = list(range(self._nobs, self._nobs + nrows))&#xa;&#xa;        # Check that input is in correct shape&#xa;        if nrows == 0:&#xa;            raise ValueError(""value is empty"")&#xa;        &#xa;        if not all(len(row)==self._nvar for row in value):&#xa;            msg = ""new row length does not match number of variables""&#xa;            raise ValueError(msg)&#xa;&#xa;        # Append observation(s)&#xa;        self.set_obs(nrows + self._nobs) &#xa;        self._set_values(row_nums, col_nums, value)&#xa;       &#xa;        # self._changed set to True in set_obs&#xa;        &#xa;    def xpose(self, clear=False, varname=False):&#xa;        """"""Transpose data. &#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        clear : Boolean , required&#xa;            The purpose of this parameter is to remind the user&#xa;            that this method replaces the data in the dataset.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Note&#xa;        ----&#xa;        This method does not yet support the -promote- option in Stata.&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Can change almost everything in the data set.&#xa;        Replaces string values with missing.&#xa;        Transposes numeric values in the data, and thus changing number&#xa;        of observations, number of variables, variable names, etc.&#xa;        Removes or replaces existing variable labels, characteristics, &#xa;        display formats, sort info.&#xa;        &#xa;        """"""   &#xa;        if not clear:&#xa;            raise ValueError(""must specify clear=True to use xpose"")&#xa;        &#xa;        # Without the -promote- option, any values outside the float range&#xa;        # will be converted to MISSING.&#xa;        convert = lambda x: (&#xa;            x if (isinstance(x, MissingValue) or &#xa;                  -1.7014117331926443e+38 <= x <= 1.7014117331926443e+38) &#xa;            else MISSING&#xa;        )&#xa;        &#xa;        # If varname=True, save old varnames to be added in later&#xa;        if varname:&#xa;            old_varnames = [v for v in self._varlist]&#xa;        &#xa;        # Change string values to missing values&#xa;        nobs = range(self._nobs)&#xa;        columns = [i for i in range(self._nvar) if self._isstrvar(i)]&#xa;        varvals = self._varvals&#xa;        for i in nobs:&#xa;            for j in columns:&#xa;                varvals[i][j] = MISSING&#xa;            &#xa;        # Transpose&#xa;        self._varvals = [[convert(x) for x in row] &#xa;                         for row in zip(*self._varvals)]&#xa;        &#xa;        # Resize matrix nXm to mXn&#xa;        self._nobs, self._nvar = self._nvar, self._nobs&#xa;        new_nvar = self._nvar&#xa;    &#xa;        # Change format&#xa;        self._fmtlist = ['%9.0g'] * new_nvar&#xa;    &#xa;        # Change type&#xa;        new_type = self._default_new_type&#xa;        self._typlist = [new_type] * new_nvar&#xa;    &#xa;        # Change names of Variabls&#xa;        self._varlist = ['v' + str(i) for i in range(new_nvar)]&#xa;    &#xa;        # Change sort list to all Nones&#xa;        self._srtlist = [None] * new_nvar&#xa;    &#xa;        # Change label list to all empties&#xa;        self._lbllist = [''] * new_nvar&#xa;    &#xa;        # Change var label list to all empties&#xa;        self._vlblist = [''] * new_nvar&#xa;    &#xa;        # Empty Character Dict&#xa;        self._chrdict = {}&#xa;        &#xa;        # If varname=True , append old variable names in a new variable&#xa;        # called _varname&#xa;        if varname:&#xa;            self.append_var(""_varname"", old_varnames)&#xa;    &#xa;        # Set changed to True&#xa;        self._changed = True&#xa;        &#xa;    def replace(self, id, values, in_=None, if_=None):&#xa;        """"""Replace values in given data variable.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        id : int or str&#xa;            Single variable index (int) or name (str).&#xa;            For str, an abbreviation is allowed if unambiguous.&#xa;        values : iterable&#xa;            Can be a flat iterable like [1, 5, 9, ...] or iterable of&#xa;            rows, like [[1], [5], [9], ...].&#xa;            Should have the same number of values as current number of&#xa;            observations or as implied by `in_` and `if_`.&#xa;        in_ : iterable, optional&#xa;            Used to specify observations replace.&#xa;            Should be an iterable of int.&#xa;            Default is all observations.&#xa;        if_ : function, optional&#xa;            Used to specify observations replace.&#xa;            Should be a function taking int and &#xa;            returning Boolean (or coercible to Boolean).&#xa;            Default is True for all obs.&#xa;            &#xa;        Parameters note&#xa;        ---------------&#xa;        If both `in_` and `if_` are used, the replaced observations&#xa;        are the numbers in `in_` that satisfy `if_`.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Replaces values in given data variable.&#xa;        &#xa;        """"""&#xa;        # argument checking will be done with __setitem__&#xa;        &#xa;        if in_ is None:&#xa;            in_ = range(self._nobs)&#xa;        &#xa;        if if_ is None:&#xa;            rows = tuple(in_)&#xa;        else:&#xa;            rows = tuple(i for i in in_ if if_(i))&#xa;        &#xa;        # type and size checking happens in __setitem__&#xa;        self.__setitem__((rows, id), values)&#xa;        &#xa;        # __setitem__ will set self._changed = True if appropriate&#xa;&#xa;    def note_add(self, evarname, note, replace=False, in_=None):&#xa;        """"""Add given note for varname or '_dta', &#xa;        or replacing existing note if specified.&#xa;        &#xa;        Note will be truncated to 67,784 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        note : str&#xa;            Note should be ascii ('iso-8859-1'). &#xa;            Otherwise, the note will not be saved as intended.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Specify that existing note should be replaced.&#xa;            If `replace` is True, `in_` must be specified as well.&#xa;            Otherwise, `replace` will be ignored.&#xa;            Default value is False.&#xa;        in_ : int, optional&#xa;            Note number to replace (>= 1).&#xa;            Only used if `replace` is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Inserts text as new note or replacement for old note.&#xa;        &#xa;        """"""&#xa;        if not isinstance(note, str):&#xa;            raise TypeError(""note should be a string"")&#xa;        names = self._find_vars(evarname, evars=True,&#xa;                                empty_ok=False, single=True)&#xa;        evarname = names[0]&#xa;        replace = replace and (in_ is not None)&#xa;        if replace:&#xa;            if not isinstance(in_, int):&#xa;                raise TypeError(""in_ should be int or None"")&#xa;            if in_ <= 0:&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;            if (evarname not in self._chrdict &#xa;                  or 'note0' not in self._chrdict[evarname] &#xa;                  or 'note' + str(in_) not in self._chrdict[evarname]):&#xa;                raise ValueError(""note not found; could not be replaced"")&#xa;                &#xa;        if evarname not in self._chrdict:&#xa;            self._chrdict[evarname] = {}&#xa;        evar_chars = self._chrdict[evarname]&#xa;&#xa;        # In Stata, number of notes is limited to 9999. Limit here &#xa;        # is set at 10000, assuming one of the notes is 'note0'.&#xa;        nnotes = len([1 for k, v in evar_chars.items() &#xa;                      if re.match(r'^note[0-9]+$', k)])&#xa;        if nnotes > 10000 or (nnotes >= 10000 and not replace):&#xa;            raise ValueError(evarname + "" already has 10000 notes"")&#xa;            &#xa;        if 'note0' not in evar_chars:&#xa;            evar_chars['note0'] = '1'&#xa;            note_num = 1&#xa;        elif not replace:&#xa;            note_num = int(evar_chars['note0']) + 1&#xa;            evar_chars['note0'] = str(note_num)&#xa;        else:&#xa;            note_num = in_&#xa;        &#xa;        evar_chars['note' + str(note_num)] = note[:67784]&#xa;        self._changed = True&#xa;        &#xa;    notes_add = note_add&#xa;        &#xa;    def note_replace(self, evarname, note, in_):&#xa;        """"""Replace existing note for varname or '_dta'.&#xa;        &#xa;        Note will be truncated to 67,784 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        note : str&#xa;            Note should be ascii ('iso-8859-1'). &#xa;            Otherwise, the note will not be saved as intended.&#xa;        in_ : int&#xa;            Note number to replace (>= 1).&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Inserts text as replacement for old note.&#xa;        &#xa;        """"""&#xa;        self.note_add(evarname, note, replace=True, in_=in_)&#xa;        &#xa;    notes_replace = note_replace&#xa;        &#xa;    def note_renumber(self, evarname):&#xa;        """"""Remove gaps in note numbers.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarname : str&#xa;            Name of data variable or '_dta'.&#xa;            An abbreviation of a variable name is allowed if unambiguous.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Renumbers notes if necessary.&#xa;        &#xa;        """"""&#xa;        names = self._find_vars(evarname, evars=True,&#xa;                                empty_ok=False, single=True)&#xa;        evarname = names[0]&#xa;        if (evarname not in self._chrdict or &#xa;                'note0' not in self._chrdict[evarname]):&#xa;            return&#xa;        evar_chars = self._chrdict[evarname]&#xa;        last_seen_old = 0&#xa;        last_seen_new = 0&#xa;        nnotes = int(evar_chars['note0'])&#xa;        for new_num in range(1, nnotes+1):&#xa;            for old_num in range(last_seen_old+1, nnotes+1):&#xa;                old_name = 'note' + str(old_num)&#xa;                if old_name in evar_chars:&#xa;                    last_seen_old = old_num&#xa;                    if old_num == new_num: break&#xa;                    evar_chars['note' + str(new_num)] = evar_chars[old_name]&#xa;                    last_seen_new = new_num&#xa;                    del evar_chars[old_name]&#xa;                    self._changed = True&#xa;                    break&#xa;        if last_seen_new == 0: # probably shouldn't occur during normal usage&#xa;            del evar_chars['note0']&#xa;        else:&#xa;            evar_chars['note0'] = str(last_seen_new)&#xa;        &#xa;    notes_renumber = note_renumber&#xa;        &#xa;    def note_drop(self, evarnames, in_=None):&#xa;        """"""Drop notes in given numbers for given evarnames.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarnames : str or iterable of str&#xa;            Names of data variable(s) or '_dta'.&#xa;            Abbreviations of variable names are allowed if unambiguous.&#xa;        in_ : int or iterable of int, optional&#xa;            Note number(s) to drop (>= 1).&#xa;            If `in_' not specified or is None, all notes will be dropped&#xa;            for given evarnames.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Deletes notes.&#xa;        &#xa;        """"""&#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (not isinstance(in_, collections.Iterable)&#xa;                    or not all(isinstance(n, int) for n in in_)):&#xa;                raise TypeError(""in_ should be int or iterable of int"")&#xa;            if any(n <= 0 for n in in_):&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;        else:&#xa;            in_ = ()&#xa;            &#xa;        in_intersect = set(in_).intersection&#xa;        &#xa;        evarnames = self._find_vars(evarnames, evars=True, empty_ok=False)&#xa;        chrdict = self._chrdict&#xa;        for name in evarnames:&#xa;            if name not in chrdict: continue&#xa;            chars = chrdict[name]&#xa;            &#xa;            if 'note0' not in chars: continue&#xa;            &#xa;            note_nums = {int(k[4:]) for k in chars if k.startswith(""note"")}&#xa;            drop_nums = in_intersect(note_nums) if in_ else note_nums&#xa;            &#xa;            if len(drop_nums) == 0: continue            &#xa;            &#xa;            keep_nums = note_nums - drop_nums&#xa;            &#xa;            drop_all = False&#xa;            &#xa;            if keep_nums == set() or keep_nums == {0,}:&#xa;                drop_all = True&#xa;                drop_nums.add(0)&#xa;                &#xa;            if drop_all and len(drop_nums) == len(chars):&#xa;                del chrdict[name]&#xa;            else:&#xa;                for num in drop_nums:&#xa;                    del chars['note' + str(num)]&#xa;                &#xa;                if not drop_all:&#xa;                    chars['note0'] = str(max(keep_nums))&#xa;            &#xa;            self._changed = True&#xa;        &#xa;    notes_drop = note_drop&#xa;        &#xa;    def note_list(self, evarnames="""", in_=None):&#xa;        """"""List notes in given numbers for given evarnames.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        evarnames : str or iterable of str&#xa;            Names of data variable(s) or '_dta'.&#xa;            Abbreviations of variable names are allowed if unambiguous.&#xa;        in_ : int or iterable of int, optional&#xa;            Note number(s) to drop (>= 1).&#xa;            If `in_' not specified or is None, all notes will be dropped&#xa;            for given evarnames.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays notes.&#xa;        &#xa;        """"""&#xa;        evarnames = self._find_vars(evarnames, evars=True, &#xa;                                    unique=True, empty_ok=True)&#xa;        if len(evarnames) == 0:&#xa;            evarnames = ['_dta'] + self._varlist&#xa;        if in_ is not None:&#xa;            if isinstance(in_, int):&#xa;                in_ = (in_,)&#xa;            elif (not isinstance(in_, collections.Iterable)&#xa;                    or not all(isinstance(n, int) for n in in_)):&#xa;                raise TypeError(""in_ should be int or iterable of int"")&#xa;            if any(n <= 0 for n in in_):&#xa;                raise ValueError(""note numbers must be >= 1"")&#xa;        for name in evarnames:&#xa;            if name not in self._chrdict: continue&#xa;            chars = self._chrdict[name]&#xa;            if 'note0' not in chars: continue&#xa;            nnotes = int(chars['note0'])&#xa;            in_range = in_ if in_ is not None else range(1, nnotes + 1)&#xa;            note_info = []&#xa;            for note_num in in_range:&#xa;                note_name = 'note' + str(note_num)&#xa;                if note_name in chars:&#xa;                    note_info.append(note_num)&#xa;            if note_info != []:&#xa;                note_info.insert(0, name)&#xa;                self._display_notes(note_info)&#xa;        &#xa;    notes_list = note_list&#xa;        &#xa;    def _search_in_notes(self, evarname, text):&#xa;        """"""convenience function for self.note_search""""""&#xa;        matches = []&#xa;        if evarname in self._chrdict and ""note0"" in self._chrdict[evarname]:&#xa;            chars = self._chrdict[evarname]&#xa;            nnotes = int(chars['note0'])&#xa;            for note_num in range(1, nnotes + 1):&#xa;                note_name = 'note' + str(note_num)&#xa;                if note_name in chars and text in chars[note_name]:&#xa;                    matches.append(note_num)&#xa;            if matches != []:&#xa;                matches.insert(0, evarname)&#xa;        return matches&#xa;        &#xa;    def _display_notes(self, note_info):&#xa;        """"""convenience function for self.note_search""""""&#xa;        if note_info == []: return&#xa;        evarname = note_info[0]&#xa;        chars = self._chrdict[evarname]&#xa;        &#xa;        if IN_STATA:&#xa;            tplt = ""{{text}}{:>3}. {}""&#xa;            print(""\n{res}"" + evarname)&#xa;        else:&#xa;            tplt = ""{:>3}. {}""&#xa;            print(""\n"" + evarname)&#xa;        &#xa;        for num in note_info[1:]:&#xa;            print(tplt.format(num, chars['note' + str(num)]))&#xa;        &#xa;    def note_search(self, text):&#xa;        """"""Search in notes for exact matches of given text.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        text : str&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays notes matching text.&#xa;        &#xa;        """"""&#xa;        if not isinstance(text, str):&#xa;            raise TypeError(""search argument should be str"")&#xa;        search_in_notes = self._search_in_notes&#xa;        display_notes = self._display_notes&#xa;        varlist = self._varlist&#xa;        display_notes(search_in_notes('_dta', text))&#xa;        for evarname in varlist:&#xa;            display_notes(search_in_notes(evarname, text))&#xa;        &#xa;    notes_search = note_search&#xa;        &#xa;    def label_data(self, label):&#xa;        """"""Add given label to data. &#xa;        &#xa;        Label will be truncated to 80 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        label : str&#xa;            Label should be ascii ('iso-8859-1'). &#xa;            Otherwise, the label will not be saved as intended.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds label to data.&#xa;        &#xa;        """"""&#xa;        if not isinstance(label, str):&#xa;            raise TypeError(""data label should be a string"")&#xa;        if len(label) > 80:&#xa;            if not self._quiet:&#xa;                if IN_STATA:&#xa;                    print(""{err}truncating label to 80 characters"")&#xa;                else:&#xa;                    print(""truncating label to 80 characters"")&#xa;            label = label[:80]&#xa;        if self._data_label == label:&#xa;            return&#xa;        self._data_label = label&#xa;        self._changed = True&#xa;        &#xa;    def label_variable(self, varname, label):&#xa;        """"""Add given label to variable.&#xa;        &#xa;        Label will be truncated to 80 characters if necessary.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varname : str&#xa;            Single varname (abbreviation allowed if unambiguous).&#xa;        label : str&#xa;            Label should be ascii ('iso-8859-1'). &#xa;            Otherwise, the label will not be saved as intended.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds label to variable.&#xa;        &#xa;        """"""&#xa;        if not isinstance(label, str):&#xa;            raise TypeError(""variable label should be a string"")&#xa;        names = self._find_vars(varname, empty_ok=False, single=True)&#xa;        index = self._varlist.index(names[0])&#xa;        label = label[:80]&#xa;        if self._vlblist[index] == label:&#xa;            return&#xa;        self._vlblist[index] = label&#xa;        self._changed = True&#xa;                &#xa;    def _fix_fmts(self, labname, mapping):&#xa;        """"""For use in labeling functions. This function modifies &#xa;        fmts if needed to accomodate values in labeling dict.&#xa;        &#xa;        """"""&#xa;        default_fmt_widths = self._default_fmt_widths&#xa;        &#xa;        indexes = [i for i in range(self._nvar) if self._lbllist[i] == labname]&#xa;        if indexes == []: return&#xa;        lab_size = max([len(v) for k, v in mapping.items()])&#xa;        fmtlist = self._fmtlist&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        for i in indexes:&#xa;            if isstrvar(i):&#xa;                continue # string values should not be labeled&#xa;            old_fmt = fmtlist[i]&#xa;            # check match agains numerical format&#xa;            match = NUM_FMT_RE.match(old_fmt)&#xa;            if match:&#xa;                fmt_width = int(match.group(3))&#xa;                if fmt_width < lab_size:&#xa;                    prefix = ('%' + (match.group(1) or '') + &#xa;                              (match.group(2) or ''))&#xa;                    suffix = (match.group(4) + match.group(5) + &#xa;                              match.group(6) + (match.group(7) or ''))&#xa;                    new_fmt = prefix + str(lab_size) + suffix&#xa;                    fmtlist[i] = new_fmt&#xa;                    self._changed = True&#xa;            elif TIME_FMT_RE.match(old_fmt) or TB_FMT_RE.match(old_fmt):&#xa;                continue&#xa;            else: &#xa;                # Here, some garbled format must have been entered. &#xa;                # More effort could be made to identify intended format, &#xa;                # but instead we'll just paint over it.&#xa;                fmt_width = default_fmt_widths[typlist[i]]&#xa;                fmtlist[i] = '%' + str(max((lab_size,fmt_width))) + '.0g'&#xa;                self._changed = True&#xa;        &#xa;    def label_define(self, name, mapping, &#xa;            add=False, replace=False, modify=False, fix=True):&#xa;        """"""Define a VALUE label, a mapping from numeric values to string.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of the value label, i.e. name of the mapping.&#xa;            If a mapping with this name already exists, `add`, &#xa;            `replace`, or `modify` should be set to True.&#xa;        mapping : dict&#xa;            Keys should be int or float, dict values should be str.&#xa;        add : bool (or coercible to bool), optional&#xa;            Whether mapping should be added to existing mapping.&#xa;            An error will be raised if old and new mapping share keys.&#xa;            Default value is False.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Whether mapping should replace existing mapping entirely.&#xa;            Default value is False.&#xa;        modify : bool (or coercible to bool), optional&#xa;            Whether mapping should update existing mapping, i.e.&#xa;            replace when there is an overlap in keys, add otherwise.&#xa;            Default value is False.&#xa;        fix : bool (or coercible to bool), optional&#xa;            When replacing or modifying an existing mapping, this&#xa;            determines whether display formats on any data variables &#xa;            that use this mapping should be expanded to accommodate &#xa;            the new labels, if necessary.&#xa;            Default value is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Stores value -> label map in dataset (does not apply it).&#xa;        &#xa;        """"""&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""label name should be str"")&#xa;        if not isinstance(mapping, dict):&#xa;            raise TypeError(""value, label mapping should be dict"")&#xa;        if modify: add = True&#xa;        if add and replace:&#xa;            raise ValueError(""replace option may not be combined with add"")&#xa;        if name in self._vallabs:&#xa;            if not (add or replace or modify):&#xa;                raise ValueError(""label exists; use add, replace, or modify"")&#xa;            elif add and not modify:&#xa;                # check for conflicts between new and old mapping&#xa;                old_map = self._vallabs[name]&#xa;                for k in mapping:&#xa;                    if k in old_map:&#xa;                        raise ValueError(""conflict with existing labels"")&#xa;        &#xa;        # test that keys are int and labels are str&#xa;        if not all(isinstance(k, int) and isinstance(v, str) &#xa;                   for k,v in mapping.items()):&#xa;            raise TypeError(""value, label mapping should be from int to str"")&#xa;        &#xa;        # make copy of mapping&#xa;        mapping = {k:v for k,v in mapping.items()}&#xa;            &#xa;        if not add or name not in self._vallabs:&#xa;            # also if replace=True (after passing checks above)&#xa;            self._vallabs[name] = mapping&#xa;        else:&#xa;            # update old value label map with new map&#xa;            self._vallabs[name].update(mapping)&#xa;        &#xa;        # if any variables already use this label,&#xa;        # check and possibly change fmt&#xa;        if fix:&#xa;            self._fix_fmts(name, mapping)&#xa;        &#xa;        # it would be a little complicated here to check if anything actually &#xa;        # changes (only in doubt with replace and modify), so assume changed&#xa;        self._changed = True&#xa;    &#xa;    def label_copy(self, orig_name, copy_name, replace=False):&#xa;        """"""Make a copy of mapping `orig_name` with name `copy_name`&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        orig_name : str&#xa;            Name of the existing value label mapping.&#xa;        copy_name : str&#xa;            Name to be given to the copy.&#xa;        replace : bool (or coercible to bool), optional&#xa;            Whether the copy should replace an existing mapping.&#xa;            Required if a mapping with name `copy_name` already exists.&#xa;            Default value is False.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Stores or replaces a copy of a value -> label map in the data set.&#xa;        &#xa;        """"""&#xa;        if not isinstance(orig_name, str) or not isinstance(copy_name, str):&#xa;            raise TypeError(""label names should be str"")&#xa;        if orig_name not in self._vallabs:&#xa;            raise KeyError(orig_name + "" is not an existing label name"")&#xa;        if copy_name in self._vallabs and not replace:&#xa;            msg = copy_name + "" label exists; use replace option to replace""&#xa;            raise ValueError(msg)&#xa;        self._vallabs[copy_name] = self._vallabs[orig_name].copy()&#xa;        # assume something has changed (only in doubt with replace)&#xa;        self._changed = True&#xa;        &#xa;    def label_dir(self):&#xa;        """"""Display names of defined value -> label maps&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays names of existing value -> label maps.&#xa;        &#xa;        """"""&#xa;        for lblname in self._vallabs:&#xa;            print(lblname)&#xa;        &#xa;    def label_list(self, labnames=None):&#xa;        """"""Show value, label pairs for given maps,&#xa;        or for all such maps if none specified.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        labnames : str or iterable of str, optional&#xa;            One or more names of existing value -> label maps to display.&#xa;            Default is to show all defined maps.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Displays value -> label maps.&#xa;        &#xa;        """"""&#xa;        vallabs = self._vallabs&#xa;        if labnames is None:&#xa;            labnames = vallabs.keys()&#xa;        else:&#xa;            if isinstance(labnames, str):&#xa;                labnames = (labnames,)&#xa;            elif (not isinstance(labnames, collections.Iterable)&#xa;                    or not all(isinstance(value, str) for value in labnames)):&#xa;                raise TypeError(""labnames should be str or iterable of str"")  &#xa;            labnames = set(name for value in labnames&#xa;                                for name in value.split())&#xa;            if not labnames.issubset(vallabs.keys()):&#xa;                bad_names = "", "".join(str(lbl) for lbl in &#xa;                                     labnames.difference(vallabs.keys()))&#xa;                raise KeyError(bad_names + "" are not defined labels"")&#xa;        for name in labnames:&#xa;            print(name + "":"")&#xa;            lbldict = vallabs[name]&#xa;            for value in lbldict:&#xa;                print(""{:>12} {}"".format(value, lbldict[value]))&#xa;        &#xa;    def label_drop(self, labnames=None, drop_all=False):&#xa;        """"""Delete value -> label maps from data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        labnames : str or iterable of str, optional&#xa;            One or more names of existing value -> label maps to display.&#xa;        drop_all : bool (or coercible to bool), optional&#xa;            Whether all value -> label maps should be dropped.&#xa;            Default value is False.&#xa;            &#xa;        Note&#xa;        ----&#xa;        Nothing will be done if neither `labnames` nor `drop_all` are&#xa;        specified. If both are specified, `drop_all' will be ignored.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Removes value -> label maps from data set. Association between&#xa;        data variables and maps names are not removed.&#xa;        &#xa;        """"""&#xa;        vallabs = self._vallabs&#xa;        if labnames is None:&#xa;            if drop_all:&#xa;                # Create copy of keys. Otherwise, set of keys changes.&#xa;                labnames = set(vallabs.keys()) &#xa;            else:&#xa;                msg = ""must specify label name(s) or drop_all==True""&#xa;                raise ValueError(msg)&#xa;        else:&#xa;            if isinstance(labnames, str):&#xa;                labnames = (labnames,)&#xa;            elif (not isinstance(labnames, collections.Iterable)&#xa;                    or not all(isinstance(value, str) for value in labnames)):&#xa;                raise TypeError(""labnames should be str or iterable of str"") &#xa;            labnames = set(name for value in labnames&#xa;                                for name in value.split())&#xa;            if not labnames.issubset(vallabs.keys()):&#xa;                bad_names = "", "".join(str(lbl) for lbl in &#xa;                                     labnames.difference(vallabs.keys()))&#xa;                raise KeyError(bad_names + "" are not defined labels"")&#xa;        for name in labnames:&#xa;            del vallabs[name]&#xa;        self._changed = True&#xa;        &#xa;    def label_values(self, varnames, labname, fix=True):&#xa;        """"""Associate (possibly non-existent) value -> label map with&#xa;        given data variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str or iterable of str&#xa;            One or more names of data variables.&#xa;            Abbreviations are allowed if unambiguous.&#xa;        labname : str&#xa;            Name of value -> label mapping to use with given variables.&#xa;            `labname` does not need to be the name of an existing map.&#xa;        fix : bool (or coercible to bool), optional&#xa;            Whether the variables' display formats should be expanded &#xa;            to accommodate the labels, if necessary.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Associates (possibly non-existent) value -> label map with&#xa;        given data variables.&#xa;        &#xa;        """"""&#xa;        if labname is None: labname = """"&#xa;        if not isinstance(labname, str):&#xa;            raise TypeError(""label name should be str"")&#xa;        varnames = self._find_vars(varnames, unique=True, empty_ok=False)&#xa;        index_func = self._varlist.index&#xa;        indexes = [index_func(name) for name in varnames]&#xa;        lbllist = self._lbllist&#xa;        typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        for i in indexes:&#xa;            if isstrvar(i):&#xa;                raise TypeError(""may not label strings"")&#xa;            lbllist[i] = labname&#xa;        if fix and labname in self._vallabs:&#xa;            self._fix_fmts(labname, self._vallabs[labname])&#xa;        # assume there are actual changes&#xa;        self._changed = True&#xa;        &#xa;    def _label_language_list_smcl(self, nlangs, langs, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        print(""{txt}{title:Language for variable and value labels}\n"")&#xa;        &#xa;        if nlangs <= 1:&#xa;            print(""    {txt}In this dataset, value and variable labels have"", &#xa;                  ""been defined in only one language: {res}"",&#xa;                  curr_lang)&#xa;        else:&#xa;            print(""    {txt}Available languages:"")&#xa;            for lang in langs:&#xa;                print(""            {res}"" + lang)&#xa;            print(""\n    {txt}Currently set is:{col 37}{res}"",&#xa;                  ""label_language(\""{}\"")\n"".format(curr_lang),&#xa;                  ""\n    {txt}To select different language:{col 37}{res}"", &#xa;                  ""<self>.label_language(<name>)"")&#xa;        &#xa;        print(""\n    {txt}To create new language:{col 37}{res}"",&#xa;              ""<self>.label_language(<name>, new=True)"",&#xa;              ""\n    {txt}To rename current language:{col 37}{res}"",&#xa;              ""<self>.label_language(<name>, rename=True)"")&#xa;        &#xa;    def _label_language_list_nosmcl(self, nlangs, langs, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        print(""Language for variable and value labels\n"")&#xa;        &#xa;        if nlangs <= 1:&#xa;            print(""    In this dataset, value and variable labels"",&#xa;                  ""have been defined in only one language: "",&#xa;                  curr_lang)&#xa;        else:&#xa;            print(""    Available languages:"")&#xa;            for lang in langs:&#xa;                print(""            {}"".format(lang))&#xa;            print(""\n    Currently set is:              "",  &#xa;                  ""label_language(\""{}\"")\n"".format(curr_lang),&#xa;                  ""\n    To select different language:  "", &#xa;                  ""<self>.label_language(<name>)"")&#xa;        &#xa;        print(""\n    To create new language:        "",&#xa;              ""<self>.label_language(<name>, new=True)"",&#xa;              ""\n    To rename current language:    "",&#xa;              ""<self>.label_language(<name>, rename=True)"")&#xa;        &#xa;    def _label_language_delete(self, languagename, langs,&#xa;                               curr_lang, name_exists):&#xa;        """"""helper function for label_language()""""""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        &#xa;        # shorten language list&#xa;        langs = [lang for lang in langs if lang != languagename]&#xa;        &#xa;        if languagename == curr_lang:&#xa;            vlblist = self._vlblist&#xa;            lbllist = self._lbllist&#xa;            &#xa;            curr_lang = langs[0]&#xa;            if not self._quiet:&#xa;                msg = ""{}(language {} now current language)""&#xa;                print(msg.format(""{txt}"" if IN_STATA else """", curr_lang))&#xa;            &#xa;            varlab_key = ""_lang_v_"" + curr_lang&#xa;            vallab_key = ""_lang_l_"" + curr_lang&#xa;            &#xa;            # replace data label, _lang_list, and _lang_c&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_c""] = curr_lang&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            if varlab_key in dta_dict:&#xa;                self._data_label = dta_dict.pop(varlab_key)&#xa;            &#xa;            # Stata does not drop value label&#xa;            &#xa;            # Replace variable and value labels, &#xa;            # and pop these entries from chrdict.&#xa;            # If this leaves a chrdict[varname] empty, delete it.&#xa;            for varname, i in zip(varlist, range(self._nvar)):&#xa;                lbllist[i] = '' &#xa;                # Next line probably not necessary. &#xa;                # There should be a var label in chrdict, &#xa;                # even if it's empty str.&#xa;                vlblist[i] = ''&#xa;                if varname in chrdict:&#xa;                    var_dict = chrdict[varname]&#xa;                    if varlab_key in var_dict:&#xa;                        vlblist[i] = var_dict.pop(varlab_key)&#xa;                    if vallab_key in var_dict:&#xa;                        lbllist[i] = var_dict.pop(vallab_key)&#xa;                    if len(var_dict) == 0:&#xa;                        del chrdict[varname]&#xa;                        &#xa;        # if deleted language is not the current language, &#xa;        # delete entries from chrdict&#xa;        else:&#xa;            varlab_key = ""_lang_v_"" + languagename&#xa;            vallab_key = ""_lang_l_"" + languagename&#xa;            &#xa;            # delete data label (if necessary) and replace _lang_list&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            if varlab_key in dta_dict:&#xa;                del dta_dict[varlab_key]&#xa;            &#xa;            # Stata does not drop value label&#xa;            &#xa;            # Delete variable and value label entries from chrdict.&#xa;            # If this leaves the sub-dictionary empty, delete it.&#xa;            for varname, i in zip(varlist, range(self._nvar)):&#xa;                if varname in chrdict:&#xa;                    var_dict = chrdict[varname]&#xa;                    if varlab_key in var_dict:&#xa;                        del var_dict[varlab_key]&#xa;                    if vallab_key in var_dict:&#xa;                        del var_dict[vallab_key]&#xa;                    if len(var_dict) == 0:&#xa;                        del chrdict[varname]&#xa;                        &#xa;    def _label_language_swap(self, languagename, curr_lang):&#xa;        """"""helper function for label_language()""""""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        vlblist = self._vlblist&#xa;        lbllist = self._lbllist&#xa;            &#xa;        old_varlab_key = ""_lang_v_"" + curr_lang&#xa;        old_vallab_key = ""_lang_l_"" + curr_lang&#xa;        &#xa;        new_varlab_key = ""_lang_v_"" + languagename&#xa;        new_vallab_key = ""_lang_l_"" + languagename&#xa;        &#xa;        # Replace data label and _lang_c. No need to set _lang_list: &#xa;        # can only swap between two defined languages.&#xa;        dta_dict = chrdict[""_dta""]&#xa;        dta_dict[""_lang_c""] = languagename&#xa;        if self._data_label != '':&#xa;            dta_dict[old_varlab_key] = self._data_label&#xa;        self._data_label = (dta_dict.pop(new_varlab_key) &#xa;                            if new_varlab_key in dta_dict else '')&#xa;        &#xa;        # put current variable and value labels in chrdict &#xa;        # and replace with languagename's&#xa;        for varname, i in zip(varlist, range(self._nvar)):&#xa;            varlab = vlblist[i]&#xa;            vallab = lbllist[i]&#xa;            &#xa;            if varname not in chrdict: # then nothing to retreive&#xa;                if varlab == '' and vallab == '': # then nothing to store&#xa;                    continue&#xa;                chrdict[varname] = {}&#xa;                &#xa;            var_dict = chrdict[varname]&#xa;            &#xa;            # store current if non-empty&#xa;            if varlab != '': var_dict[old_varlab_key] = varlab&#xa;            if vallab != '': var_dict[old_vallab_key] = vallab&#xa;            &#xa;            # set languagename's labels as current&#xa;            vlblist[i] = (var_dict.pop(new_varlab_key) &#xa;                          if new_varlab_key in var_dict else '')&#xa;            lbllist[i] = (var_dict.pop(new_vallab_key) &#xa;                          if new_vallab_key in var_dict else '')&#xa;            &#xa;            # delete sub-dict from chrdict if empty&#xa;            if len(var_dict) == 0:&#xa;                del chrdict[varname]&#xa;                &#xa;    def _put_labels_in_chr(self, languagename, langs, curr_lang):&#xa;        """"""Helper function for label_language(). Should only be called &#xa;        with -new- option. The languagename is the language that will &#xa;        be current _after_ calling this function. The curr_lang is the &#xa;        language that was current _before_ calling this function.&#xa;        &#xa;        """"""&#xa;        chrdict = self._chrdict&#xa;        varlist = self._varlist&#xa;        vlblist = self._vlblist&#xa;        lbllist = self._lbllist&#xa;            &#xa;        old_varlab_key = ""_lang_v_"" + curr_lang&#xa;        old_vallab_key = ""_lang_l_"" + curr_lang&#xa;        &#xa;        # change _lang_c and _lang_list, &#xa;        # and put data_label in chrdict if non-empty&#xa;        if ""_dta"" not in chrdict:&#xa;            chrdict[""_dta""] = {}&#xa;        dta_dict = chrdict[""_dta""]&#xa;        dta_dict[""_lang_c""] = languagename&#xa;        dta_dict[""_lang_list""] = "" "".join(langs) + "" "" + languagename&#xa;        if self._data_label != '':&#xa;            dta_dict[old_varlab_key] = self._data_label&#xa;        &#xa;        # put current variable and value labels in chrdict&#xa;        for varname, i in zip(varlist, range(self._nvar)):&#xa;            varlab = vlblist[i]&#xa;            vallab = lbllist[i]&#xa;                &#xa;            if varlab == '' and vallab == '': # then nothing to store&#xa;                continue&#xa;            &#xa;            if varname not in chrdict:&#xa;                chrdict[varname] = {}&#xa;                &#xa;            var_dict = chrdict[varname]&#xa;            &#xa;            # store current if non-empty&#xa;            if varlab != '': var_dict[old_varlab_key] = varlab&#xa;            if vallab != '': var_dict[old_vallab_key] = vallab&#xa;            &#xa;    def _get_language_info(self):&#xa;        """"""helper function for label_language()""""""&#xa;        # If the following does not find _lang_list, then it assumes &#xa;        # there are no defined languages. If it finds _lang_list and &#xa;        # _lang_c, and _lang_c is listed in _lang_list then it assumes &#xa;        # everything is correct. It only does further checking if &#xa;        # _lang_list is there AND either _lang_c is missing or _lang_c &#xa;        # is not in _lang_list.&#xa;        &#xa;        chrdict = self._chrdict&#xa;    &#xa;        if ""_dta"" not in chrdict or ""_lang_list"" not in chrdict[""_dta""]:&#xa;            nlangs = 1&#xa;            curr_lang = ""default""&#xa;            langs = [curr_lang,]&#xa;        else:&#xa;            dta_dict = chrdict[""_dta""]&#xa;            langs = dta_dict[""_lang_list""].split()&#xa;            nlangs = len(langs)&#xa;            has_lang_c = (""_lang_c"" in dta_dict)&#xa;            curr_lang = dta_dict['_lang_c'] if has_lang_c else 'default'&#xa;            # Safety in case of malformed chrdict. &#xa;            # Also guards against empty lang list.&#xa;            if curr_lang not in langs or not has_lang_c:&#xa;                if IN_STATA:&#xa;                    print("""".join(&#xa;                        (""{err}"",&#xa;                        ""odd values in characteristics; "",&#xa;                        ""trying to recover"")))&#xa;                else:&#xa;                    print(""odd values in characteristics; trying to recover"")&#xa;                &#xa;                # make sure curr_lang is not one of the stored languages&#xa;                &#xa;                # get stored languages&#xa;                stored_langs = set()&#xa;                for sub_dict in chrdict.values():&#xa;                    for key in sub_dict.keys():&#xa;                        if (key.startswith('_lang_l_') or &#xa;                                key.startswith('_lang_v_')):&#xa;                            stored_langs.add(key[8:])&#xa;                &#xa;                # if curr_lang in stored_langs, change curr_lang until it isn't&#xa;                count = 1&#xa;                while curr_lang in stored_langs:&#xa;                    if curr_lang[:7] == 'default':&#xa;                        count += 1&#xa;                        curr_lang = 'default' + str(count)&#xa;                    else:&#xa;                        curr_lang = 'default'&#xa;                    &#xa;                # make new langs and nlangs&#xa;                langs = list(stored_langs.union({curr_lang,}))&#xa;                nlangs = len(langs)&#xa;                &#xa;        return curr_lang, langs, nlangs&#xa;        &#xa;    def label_language(self, languagename=None, &#xa;                      new=False, copy=False, rename=False, delete=False):&#xa;        """"""Various functionality for manipulating groups of variable&#xa;        and data labels.&#xa;        &#xa;        Users may, for example, make the same data and variable labels&#xa;        multiple times in different languages, with each language a&#xa;        separate group. This function creates, deletes, renames and &#xa;        swaps groups.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        languagename : str, optional&#xa;            Name of label language/group.&#xa;            Required if using any of the other parameters.&#xa;            If used without other parameters, the action is to&#xa;            set specified language/group as ""current"".&#xa;        new : bool (or coercible to bool), optional&#xa;            Used to create new language and set as ""current"".&#xa;            New labels are empty when `copy` is not specified.&#xa;            Default value is False.&#xa;        copy : bool (or coercible to bool), optional&#xa;            Used to copy the ""current"" group of labels.&#xa;            Cannot be used without setting `new'=True.&#xa;            Hence, also sets the copy as ""current"".&#xa;            Default value is False.&#xa;        rename : bool (or coercible to bool), optional&#xa;            Rename current group of labels, or set the name of the &#xa;            current set, if no name has been applied.&#xa;            Default value is False.&#xa;        delete : bool (or coercible to bool), optional&#xa;            Delete given label language/group. Not allowed if there&#xa;            is only one defined label language/group.&#xa;            Default value is False.&#xa;            &#xa;        Note&#xa;        ----&#xa;        If no parameters are specified, the action is to list all label&#xa;        languages/groups. Of `new`, `copy`, `rename`, and `delete`, only&#xa;        `new` and `copy` may be used together. A `languagename` is&#xa;        restricted to 24 characters and will be truncated if necessary.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Creates, deletes, renames, or swaps label languages/groups.&#xa;        &#xa;        """"""&#xa;        global IN_STATA&#xa;        &#xa;        curr_lang, langs, nlangs = self._get_language_info()&#xa;        &#xa;        noptions = sum((new, copy, rename, delete))&#xa;        &#xa;        # list language info&#xa;        if languagename is None:&#xa;            if noptions != 0:&#xa;                msg = ""options cannot be used without language name""&#xa;                raise ValueError(msg)&#xa;            if IN_STATA:&#xa;                self._label_language_list_smcl(nlangs, langs, curr_lang)&#xa;            else:&#xa;                self._label_language_list_nosmcl(nlangs, langs, curr_lang)&#xa;            return&#xa;        &#xa;        # more error checking&#xa;        # only options that can be combined are -new- and -copy-&#xa;        if noptions > 1 and not (noptions == 2 and new and copy):&#xa;            raise ValueError(""only options -copy- and -new- can be combined"")&#xa;        if copy and not new:&#xa;            msg = ""option -copy- may only be specified with option -new-""&#xa;            raise ValueError(msg)&#xa;        &#xa;        if not isinstance(languagename, str):&#xa;            raise TypeError(""given language name must be str"")&#xa;        if len(languagename) > 24:&#xa;            if not self._quiet:&#xa;                if IN_STATA:&#xa;                    print(""{err}shortening language name to 24 characters"")&#xa;                else:&#xa;                    print(""shortening language name to 24 characters"")&#xa;            languagename = languagename[:24]&#xa;        &#xa;        name_exists = languagename in langs&#xa;        &#xa;        # switch languages&#xa;        if noptions == 0:&#xa;            if not name_exists:&#xa;                msg = ""language {} not defined"".format(languagename)&#xa;                raise ValueError(msg)&#xa;            if languagename == curr_lang:&#xa;                if not self._quiet:&#xa;                    msg = ""{}({} already current language)""&#xa;                    print(msg.format(""{txt}"" if IN_STATA else """", curr_lang))&#xa;            else:&#xa;                self._label_language_swap(languagename, curr_lang)&#xa;            return&#xa;        &#xa;        # delete language&#xa;        if delete:&#xa;            if not name_exists:&#xa;                msg = ""language {} not defined"".format(languagename)&#xa;                raise ValueError(msg)&#xa;            if nlangs == 1:&#xa;                msg = (""language {} is the only language defined; "" + &#xa;                       ""it may not be deleted"")&#xa;                raise ValueError(msg.format(languagename))&#xa;            self._label_language_delete(languagename, langs,&#xa;                                        curr_lang, name_exists)&#xa;            return&#xa;        &#xa;        # From this point, rename == True or new == True.&#xa;        # Both require given languagename not already exist.&#xa;        if name_exists:&#xa;            raise ValueError(""language {} already exists"".format(languagename))&#xa;        &#xa;        # rename current language&#xa;        if rename:&#xa;            chrdict = self._chrdict&#xa;            # only need to change _lang_c and _lang_list&#xa;            if '_dta' not in chrdict:&#xa;                chrdict['_dta'] = {}&#xa;            dta_dict = chrdict[""_dta""]&#xa;            dta_dict[""_lang_c""] = languagename&#xa;            curr_lang_index = langs.index(curr_lang)&#xa;            langs[curr_lang_index] = languagename&#xa;            dta_dict[""_lang_list""] = "" "".join(langs)&#xa;            return&#xa;            &#xa;        # only option left is -new-&#xa;        # push current labels to chrdict&#xa;        if nlangs >= 100:&#xa;            raise ValueError(""100 label languages exist; limit reached"")&#xa;        self._put_labels_in_chr(languagename, langs, curr_lang)&#xa;        if copy:&#xa;            # use current labels&#xa;            if not self._quiet:&#xa;                msg = ""{}(language {} now current language)""&#xa;                print(msg.format(""{txt}"" if IN_STATA else """", languagename))&#xa;        else:&#xa;            # empty current labels&#xa;            nvar = self._nvar&#xa;            self._data_label = ''&#xa;            self._vlblist = [''] * nvar&#xa;            self._lbllist = [''] * nvar&#xa;    &#xa;    def _check_index(self, prior_len, index):&#xa;        """"""To be used with __getitem__ and __setitem__ . Checks that &#xa;        index is well-formed and puts it in consistent form.&#xa;        &#xa;        """"""&#xa;        if index is None: return range(prior_len)&#xa;        if isinstance(index, slice):&#xa;            start, stop, step = index.indices(prior_len)&#xa;            return range(start, stop, step)&#xa;        if isinstance(index, collections.Iterable):&#xa;            if not hasattr(index, ""__len__""):&#xa;                index = tuple(index)&#xa;            if not all(isinstance(i, int) for i in index):&#xa;                raise TypeError(""individual indices must be int"")&#xa;            # Integers assumed to be within proper range.&#xa;            # Later use will raise error otherwise, so no need to check here.&#xa;            return index&#xa;        # if next_index not slice and not iterable, assume it's an int&#xa;        if not isinstance(index, int):&#xa;            raise TypeError(""index must be slice, iterable (of int), or int"")&#xa;        if not -prior_len <= index < prior_len:&#xa;            raise IndexError(""index out of range"")&#xa;        return (index,)&#xa;        &#xa;    def _convert_col_index(self, index):&#xa;        """"""To be used with __getitem__ and __setitem__ . Checks that &#xa;        column index is well-formed and puts it in consistent form.&#xa;        &#xa;        """"""&#xa;        if index is None or isinstance(index, int): return index&#xa;        if isinstance(index, str):&#xa;            find_index = self._varlist.index&#xa;            return [find_index(v) for v in self._find_vars(index)]&#xa;        if isinstance(index, collections.Iterable):&#xa;            new_index = []&#xa;            append = new_index.append&#xa;            find_vars = self._find_vars&#xa;            find_index = self._varlist.index&#xa;            for i in index:&#xa;                if isinstance(i, str):&#xa;                    new_index += [find_index(i) for i in find_vars(i)]&#xa;                elif isinstance(i, int):&#xa;                    append(i)&#xa;                else:&#xa;                    msg = ""column iterable should contain only int or str""&#xa;                    raise TypeError(msg)&#xa;            if len(new_index) != len(set(new_index)):&#xa;                msg = ""columns cannot be repeated; use -clonevar- to copy""&#xa;                raise ValueError(msg)&#xa;            return new_index&#xa;        if isinstance(index, slice):&#xa;            start, stop, step = index.start, index.stop, index.step&#xa;            if not isinstance(start, int) and start is not None:&#xa;                if isinstance(start, str):&#xa;                    start = self._varlist.index(self._find_vars(start)[0])&#xa;                else:&#xa;                    raise TypeError(""column slice values must be str or int"")&#xa;            if not isinstance(stop, int) and stop is not None:&#xa;                if isinstance(stop, str):&#xa;                    stop = self._varlist.index(self._find_vars(stop)[0])&#xa;                else:&#xa;                    raise TypeError(""column slice values must be str or int"")&#xa;            return slice(start, stop, step)&#xa;        msg = ""column should be index (int), name (str), slice, or iterable""&#xa;        raise TypeError(msg)&#xa;        &#xa;    def get(self, row, col):&#xa;        """"""Get single data value.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        row : integer&#xa;            Row (observation) number of data value.&#xa;        col : integer&#xa;            Column (data variable) number of data value.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        string, numeric, missing value, or binary data value&#xa;        &#xa;        """"""&#xa;        return self._varvals[row][col]&#xa;            &#xa;    def __getitem__(self, index):&#xa;        """"""return Dta object containing obs &#xa;        and vars specified by index tuple&#xa;        &#xa;        """"""&#xa;        if not isinstance(index, tuple) or not 1 <= len(index) <= 2:&#xa;            msg = ""data subscripting must be [rows,cols] or [rows,]""&#xa;            raise ValueError(msg)&#xa;        sel_rows = self._check_index(self._nobs, index[0])&#xa;        sel_cols = (self._convert_col_index(index[1]) &#xa;                    if len(index) == 2 else None)&#xa;        sel_cols = self._check_index(self._nvar, sel_cols)&#xa;        # call instance constructor&#xa;        return self.__class__(self, sel_rows, sel_cols)&#xa;        &#xa;    def _standardize_input(self, value):&#xa;        """"""helper for functions like __setitem__ &#xa;        that put new values into the dataset&#xa;        &#xa;        """"""&#xa;        tuple_maker = lambda x: ((x,) &#xa;            if (any(isinstance(x, t) for t in (str, bytes, bytearray)) &#xa;                or not isinstance(x, collections.Iterable))&#xa;            else (x if hasattr(x, ""__len__"") else tuple(x)))&#xa;            &#xa;        if isinstance(value, Dta):&#xa;            value = value._varvals&#xa;        else: # force input into 2d structure&#xa;            if (any(isinstance(value, t) for t in (str,bytes,bytearray))&#xa;                    or not isinstance(value, collections.Iterable)):&#xa;                value = ((value,),)&#xa;            else:&#xa;                value = tuple(tuple_maker(v) for v in value)&#xa;            &#xa;        return value&#xa;        &#xa;    def __setitem__(self, index, value):&#xa;        """"""Replace values in specified obs and vars of -index- tuple.&#xa;        The shape of -value- should match the shape implied by -index-,&#xa;        and sub-values should be consistent with existing Stata types&#xa;        (e.g. non-string values cannot be added to string columns).&#xa;        &#xa;        """"""&#xa;        if not isinstance(index, tuple) or len(index) > 2:&#xa;            msg = ""data subscripting must be [rows,cols] or [rows,]""&#xa;            raise ValueError(msg)&#xa;        sel_rows = self._check_index(self._nobs, index[0])&#xa;        sel_cols = (self._convert_col_index(index[1])&#xa;                    if len(index) == 2 else None)&#xa;        sel_cols = self._check_index(self._nvar, sel_cols)&#xa;            &#xa;        nrows, ncols = len(sel_rows), len(sel_cols)&#xa;            &#xa;        value = self._standardize_input(value)&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if (nrows == 1 and ncols != 1 and &#xa;                len(value) == ncols and all(len(v) == 1 for v in value)):&#xa;            value = (tuple(v[0] for v in value),)&#xa;        else: # check that value dimensions match expected&#xa;            if not len(value) == nrows:&#xa;                raise ValueError(""length of value does not match # of rows"")&#xa;            if not all(len(v) == ncols for v in value):&#xa;                raise ValueError(""inner dimensions do not match # of columns"")&#xa;        &#xa;        # If no rows or no cols, nothing to do.&#xa;        # Could put this above the call to _standardize_input, &#xa;        # but then input of any shape allowed.&#xa;        if nrows == 0 or ncols == 0:&#xa;            return&#xa;        &#xa;        self._set_values(sel_rows, sel_cols, value)&#xa;        &#xa;        # Modify srtlist if necessary. If col_num is in srtlist, drop it&#xa;        # and any to the right. Ideally, would only make this change if &#xa;        # values were truly changed, by comparing new value with old.&#xa;        srtlist = self._srtlist&#xa;        nvar = self._nvar&#xa;        for col_num in sel_cols:&#xa;            if col_num in srtlist:&#xa;                srt_pos = srtlist.index(col_num)&#xa;                srtlist = srtlist[:srt_pos] + [None]*(nvar - srt_pos)&#xa;        self._srtlist = srtlist&#xa;        &#xa;        self._changed = True&#xa;                    &#xa;    def __len__(self):&#xa;        """"""return number of observations""""""&#xa;        return len(self._varvals)&#xa;        &#xa;    def format(self, varnames, fmts):&#xa;        """"""Set the Stata display format of given data variables.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        varnames : str, or iterable of str&#xa;            Can be a str containing one varname (e.g. ""mpg""),&#xa;            a str with multiple varnames (e.g. ""make price mpg""),&#xa;            or an iterable of such str&#xa;            (e.g. (""make"", ""price"", ""mpg"") or (""make"", ""price mpg"")).&#xa;            Abbreviations are allowed if unambiguous.&#xa;        fmts : str, or iterable of str&#xa;            Can be a str containing one format (e.g. ""%9.2f""),&#xa;            a str with multiple formats (e.g. ""%9.2f %12s %9.0g""),&#xa;            or an iterable of such str&#xa;            (e.g. (""%9.2f"", ""%12s"", ""%9.0g"") or (""%9.2f"", ""%12s %9.0g"")).&#xa;        &#xa;        Notes&#xa;        -----&#xa;        If there are fewer formats than varnames, the last format will&#xa;        be repeated. Any extraneous formats will be ignored.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Sets the display format of the given data variables.&#xa;        &#xa;        """"""&#xa;        varnames = self._find_vars(varnames, empty_ok=False)&#xa;        indexes = list(map(self._varlist.index, varnames))&#xa;        &#xa;        # check that fmts are specified properly&#xa;        if isinstance(fmts, str):&#xa;            fmts = fmts.split()&#xa;        else:&#xa;            if ( not isinstance(fmts, collections.Iterable) &#xa;                    or not all(isinstance(f, str) for f in fmts) ):&#xa;                raise TypeError(""given fmts must be str or iterable of str"")&#xa;            fmts = [x for s in fmts for x in s.split()]&#xa;        if len(fmts) == 0:&#xa;            raise ValueError(""no formats specified"")&#xa;        &#xa;        # check fmts for validity&#xa;        is_valid = self._is_valid_fmt&#xa;        if not all(is_valid(fmt) for fmt in fmts):&#xa;            bad_fmts = "" "".join(fmt for fmt in fmts if not is_valid(fmt))&#xa;            raise ValueError(""invalid formats: "" + bad_fmts)&#xa;            &#xa;        # pad fmts if necessary &#xa;        nvarnames = len(varnames)&#xa;        nfmts = len(fmts)&#xa;        if nfmts < nvarnames:&#xa;            fmts = list(fmts) + [fmts[-1]]*(nvarnames - nfmts)&#xa;            &#xa;        # check that formats match Stata types&#xa;        #typlist = self._typlist&#xa;        isstrvar = self._isstrvar&#xa;        if not all(isstrvar(i) == bool(STR_FMT_RE.match(fmt))&#xa;                for i, fmt in zip(indexes, fmts)):&#xa;            raise ValueError(""format does not match Stata variable type"")&#xa;        &#xa;        # replace fmts (extras, if any, don't get used)&#xa;        for i, fmt in zip(indexes, fmts):&#xa;            self._fmtlist[i] = fmt&#xa;        &#xa;        # assume there are changes&#xa;        self._changed = True&#xa;        &#xa;    def copy(self):&#xa;        """"""Create a copy of the current Dta instance.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        Dta instance&#xa;        &#xa;        """"""&#xa;        c = self.__class__(self) # using self's constructor on self&#xa;        c._srtlist = list(self._srtlist)  # copy, srtlist not copied in init&#xa;        return c&#xa;        &#xa;    def __eq__(self, other):&#xa;        """"""Compare two datasets for equality. &#xa;        &#xa;        Does not test data label, time stamp, or file name.&#xa;        &#xa;        Tests&#xa;        - number of data variables,&#xa;        - number of observations,&#xa;        - dta version,&#xa;        - data types,&#xa;        - data variable names,&#xa;        - 'sorted by' information,&#xa;        - display formats,&#xa;        - value -> label mappings applied to data variables,&#xa;        - data variable labels,&#xa;        - defined characteristics,&#xa;        - data values, and&#xa;        - defined value -> label mappings.&#xa;        &#xa;        If wanting information about differences between two&#xa;        Dta objects, use the function `stata_dta.display_diff`.&#xa;        &#xa;        """"""&#xa;        # check that is Dta and is same version&#xa;        if not self.__class__ == other.__class__: return False&#xa;        &#xa;        # pertinent header info&#xa;        if not self._nvar == other._nvar: return False&#xa;        if not self._nobs == other._nobs: return False&#xa;        if not self._ds_format == other._ds_format: return False&#xa;        #if not self._data_label == other._data_label: return False # keep ?&#xa;        &#xa;        #descriptors&#xa;        if not self._typlist == other._typlist: return False&#xa;        if not self._varlist == other._varlist: return False&#xa;        # Remove check on srtlist? With this here, data[:, :] != data.&#xa;        if not self._srtlist == other._srtlist: return False&#xa;        if not self._fmtlist == other._fmtlist: return False&#xa;        if not self._lbllist == other._lbllist: return False&#xa;        &#xa;        # variable labels&#xa;        if not self._vlblist == other._vlblist: return False&#xa;        &#xa;        # expansion fields&#xa;        if not self._chrdict == other._chrdict: return False&#xa;        &#xa;        # data&#xa;        if not self._varvals == other._varvals: return False&#xa;        &#xa;        # value labels&#xa;        if not self._vallabs == other._vallabs: return False&#xa;        &#xa;        return True&#xa;        &#xa;    def check(self, version=None):&#xa;        """"""Determine whether saved data set would conform to limits&#xa;        of given *Stata* version. (Not .dta version.)&#xa;        &#xa;        See -help limits- in Stata for more info.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        version : int, optional&#xa;            Specify a Stata version to check against.&#xa;            Default is to check against Stata version 13.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Display summary of limits violations, if any.&#xa;        &#xa;        """"""&#xa;        &#xa;        if version is None:&#xa;            print(""assuming Stata version 13"")&#xa;            version = 13&#xa;        if version not in (11, 12, 13):&#xa;            raise ValueError(""allowed versions are 11 through 13"")&#xa;        &#xa;        width = self.width&#xa;        nvar = self._nvar&#xa;        nobs = self._nobs&#xa;        &#xa;        chrdict = self._chrdict&#xa;        &#xa;        char_len = max([0] + [len(char) for evar, evardict in chrdict.items()&#xa;                                 for char in evardict.values()])&#xa;        num_val_encodings = max([0] + [len(mapping) &#xa;                                       for mapping in self._vallabs.values()])&#xa;        max_note_size = max([0] + [len(note) for d in chrdict.values()&#xa;                                   for name, note in d.items()&#xa;                                   if re.match(r'^note[0-9]+$', name)])&#xa;        max_num_notes = max([0] + [len([1 for name,note in d.items() &#xa;                                        if re.match(r'^note[0-9]+$', name)])&#xa;                                   for d in chrdict.values()])&#xa;                &#xa;        if '_dta' in chrdict and '_lang_list' in chrdict['_dta']:&#xa;            n_label_langs = len(chrdict['_dta'][""_lang_list""].split())&#xa;        else:&#xa;            n_label_langs = 0&#xa;                                    &#xa;        small_good = medium_good = large_good = True&#xa;        general_good = format_good = True&#xa;        &#xa;        print(""\nformat problems"")&#xa;        if self._ds_format == 117 and version <= 12:&#xa;            format_good = False&#xa;            print(""    format 117 cannot be opened by Stata version "" + &#xa;                  str(version))&#xa;        if version < 12 and any(TB_FMT_RE.match(fmt) for fmt in self._fmtlist):&#xa;            format_good = False&#xa;            print(""    Stata version "" + str(version) + &#xa;                  "" cannot understand tb format"")&#xa;        if format_good:&#xa;            print(""    none"")&#xa;        &#xa;        print(""\ngeneral size problems"")&#xa;        if len(self._data_label) > 80:&#xa;            general_good = False&#xa;            print(""    data label length > 80"")&#xa;        if any(len(name) > 32 for name in self._varlist):&#xa;            general_good = False&#xa;            print(""    variable name length > 32"")&#xa;        if any(len(v) > 80 for v in self._vlblist):&#xa;            general_good = False&#xa;            print(""    variable label length > 80"")&#xa;        if any(len(name) > 32 for name in self._vallabs.keys()):&#xa;            general_good = False&#xa;            print(""    value label name length > 32"")&#xa;        if any(len(valstr) > 32000 &#xa;               for mapping in self._vallabs.values()&#xa;               for valstr in mapping.values()):&#xa;            general_good = False&#xa;            print(""    value label string length > 32,000"")&#xa;        if max_num_notes > 10000:&#xa;            # limit here is set at 10000, assuming one of the notes is 'note0'&#xa;            general_good = False&#xa;            print(""    number of notes for single variable or _dta > 9,999"")&#xa;        if n_label_langs > 100:&#xa;            general_good = False&#xa;            print(""    number of label languages > 100"")&#xa;        if general_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata small problems"")&#xa;        if width > 800:&#xa;            small_good = False&#xa;            print(""    data set width > 800"")&#xa;        if nvar > 99:&#xa;            small_good = False&#xa;            print(""    numbar of variables > 99"")&#xa;        if nobs > 1200:&#xa;            small_good = False&#xa;            print(""    number of observations > 1,200"")&#xa;        if num_val_encodings > 1000:&#xa;            small_good = False&#xa;            print(""    number of encodings within single value label > 1,000"")&#xa;        if version == 13:&#xa;            if max_note_size > 13400:&#xa;                small_good = False&#xa;                print(""    note size > 13,400"")&#xa;            if char_len > 13400:&#xa;                small_good = False&#xa;                print(""    char length > 13,400"")&#xa;        else:&#xa;            if max_note_size > 8681:&#xa;                small_good = False&#xa;                print(""    note size > 8,681"")&#xa;            if char_len > 8681:&#xa;                small_good = False&#xa;                print(""    char length > 8,681"")&#xa;        if small_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata IC problems"")&#xa;        if width > 24564:&#xa;            medium_good = False&#xa;            print(""    data set width > 24,564"")&#xa;        if nvar > 2047:&#xa;            medium_good = False&#xa;            print(""    numbar of variables > 2,047"")&#xa;        if nobs > 2147483647:&#xa;            medium_good = False&#xa;            print(""    number of observations > 2,147,483,647"")&#xa;        if num_val_encodings > 65536:&#xa;            medium_good = False&#xa;            print(""    number of encodings within single value label > 65,536"")&#xa;        if max_note_size > 67784:&#xa;            medium_good = False&#xa;            print(""    note size > 67,784"")&#xa;        if char_len > 67784:&#xa;            medium_good = False&#xa;            print(""    char length > 67,784"")&#xa;        if medium_good:&#xa;            print(""    none"")&#xa;&#xa;        print(""\nStata MP & SE problems"")&#xa;        if width > 393192:&#xa;            large_good = False&#xa;            print(""    data set width > 393,192"")&#xa;        if nvar > 32767:&#xa;            large_good = False&#xa;            print(""    numbar of variables > 32,767"")&#xa;        if nobs > 2147483647:&#xa;            large_good = False&#xa;            print(""    number of observations > 2,147,483,647"")&#xa;        if num_val_encodings > 65536:&#xa;            large_good = False&#xa;            print(""    number of encodings within single value label > 65,536"")&#xa;        if max_note_size > 67784:&#xa;            large_good = False&#xa;            print(""    note size > 67,784"")&#xa;        if char_len > 67784:&#xa;            large_good = False&#xa;            print(""    char length > 67,784"")&#xa;        if large_good:&#xa;            print(""    none"")&#xa;            &#xa;    def _dta_format(self, address):&#xa;        """"""find version number of any recent version dta file""""""&#xa;        with open(address, 'rb') as dta_file:&#xa;            first_bytes = dta_file.read(11)&#xa;        ds_format = first_bytes[0]&#xa;        if isinstance(ds_format, str):  # happens in Python 2.7&#xa;            ds_format = ord(ds_format)&#xa;        # If format is 117, then first_bytes[0] is ""<"", which == 60.&#xa;        if ds_format == 114 or ds_format == 115:&#xa;            return ds_format&#xa;        elif first_bytes.decode('iso-8859-1') == ""<stata_dta>"":&#xa;            return 117&#xa;        else:&#xa;            raise ValueError(""file seems to have an unsupported format"")&#xa;        &#xa;    def _get_srtlist(self, sfile):&#xa;        """"""helper function for reading dta files""""""&#xa;        srtlist = list(unpack(self._byteorder + 'H'*(self._nvar+1), &#xa;                              sfile.read(2*(self._nvar+1))))&#xa;        zero_pos = srtlist.index(0)&#xa;        srtlist = [srt - 1 for srt in srtlist]&#xa;        # srtlist contains a terminating zero int, which need not be kept&#xa;        if zero_pos == -1: return srtlist[:-1]&#xa;        return srtlist[:zero_pos] + [None]*(len(srtlist) - 1 - zero_pos)&#xa;        &#xa;    def _parse_value_label_table(self, sfile):&#xa;        """"""helper function for reading dta files""""""&#xa;        byteorder = self._byteorder&#xa;        &#xa;        nentries = unpack(byteorder + 'l', sfile.read(4))[0]&#xa;        txtlen = unpack(byteorder + 'l', sfile.read(4))[0]&#xa;        off = []&#xa;        val = []&#xa;        txt = []&#xa;        for i in range(nentries):&#xa;            off.append(unpack(byteorder+'l',sfile.read(4))[0])&#xa;        for i in range(nentries):&#xa;            val.append(unpack(byteorder+'l',sfile.read(4))[0])&#xa;        &#xa;        txt_block = unpack(str(txtlen) + ""s"", sfile.read(txtlen))&#xa;        txt = [t.decode('iso-8859-1') &#xa;               for b in txt_block for t in b.split(b'\0')]&#xa;        &#xa;        # put (off, val) pairs in same order as txt&#xa;        sorter = list(zip(off, val))&#xa;        sorter.sort()&#xa;        &#xa;        # dict of val[i]:txt[i]&#xa;        table = {sorter[i][1]: txt[i] for i in range(len(sorter))}&#xa;        &#xa;        return table&#xa;            &#xa;    def _file_to_Dta115(self, address):&#xa;        """"""populate fields of dta object with values from disk""""""&#xa;        missing_above = {251: 100, 252: 32740, 253: 2147483620, &#xa;                        254: float.fromhex('0x1.fffffep+126'), &#xa;                        255: float.fromhex('0x1.fffffffffffffp+1022')}&#xa;        # decimal numbers given in -help dta- for float and double &#xa;        # are approximations: 'f': 1.701e38, 'd': 8.988e307&#xa;        type_dict = {251: ['b',1], 252: ['h',2], 253: ['l',4], &#xa;                    254: ['f',4], 255: ['d',8]}&#xa;                    &#xa;        def get_byte_str(str_len):&#xa;            s = unpack(str(str_len) + 's', sfile.read(str_len))[0]&#xa;            return s.partition(b'\0')[0].decode('iso-8859-1')&#xa;            &#xa;        def missing_object(miss_val, st_type):&#xa;            if st_type == 251: # byte&#xa;                value = MISSING_VALS[miss_val - 101]&#xa;            elif st_type == 252: # int&#xa;                value = MISSING_VALS[miss_val - 32741]&#xa;            elif st_type == 253: # long&#xa;                value = MISSING_VALS[miss_val - 2147483621]&#xa;            elif st_type == 254: # float&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            elif st_type == 255: # double&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            return value&#xa;            &#xa;        def get_var_val(st_type):&#xa;            if st_type <= 244:&#xa;                return get_byte_str(st_type)&#xa;            else:&#xa;                fmt, nbytes = type_dict[st_type]&#xa;                val = unpack(byteorder+fmt, sfile.read(nbytes))[0]&#xa;                return (val if val <= missing_above[st_type] &#xa;                        else missing_object(val, st_type))&#xa;        &#xa;        with open(address, 'rb') as sfile:&#xa;            # header info&#xa;            self._ds_format = unpack('b', sfile.read(1))[0]&#xa;            byteorder = '>' if unpack('b', sfile.read(1))[0] == 1 else '<'&#xa;            self._byteorder = byteorder&#xa;            sfile.seek(1,1) # filetype&#xa;            sfile.seek(1,1) # padding&#xa;            self._nvar = nvar = unpack(byteorder + 'h', sfile.read(2))[0]&#xa;            self._nobs = nobs = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            self._data_label = get_byte_str(81)&#xa;            self._time_stamp = get_byte_str(18)&#xa;            &#xa;            # descriptors&#xa;            self._typlist = [ord(sfile.read(1)) for i in range(nvar)]&#xa;            self._varlist = [get_byte_str(33) for i in range(nvar)]&#xa;            self._srtlist = self._get_srtlist(sfile)&#xa;            self._fmtlist = [get_byte_str(49) for i in range(nvar)]&#xa;            self._lbllist = [get_byte_str(33) for i in range(nvar)]&#xa;            &#xa;            # variable labels&#xa;            self._vlblist = [get_byte_str(81) for i in range(nvar)]&#xa;            &#xa;            # expansion fields&#xa;            data_type = unpack(byteorder + 'b', sfile.read(1))[0]&#xa;            data_len = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            chrdict = {}&#xa;            while not (data_type == 0 and data_len == 0):&#xa;                s = unpack(str(data_len) + 's', sfile.read(data_len))[0]&#xa;                varname = s[:33].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charname = s[33:66].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charstring = s[66:].partition(b'\0')[0].decode('iso-8859-1')&#xa;                if varname not in chrdict:&#xa;                    chrdict[varname] = {}&#xa;                chrdict[varname][charname] = charstring&#xa;                data_type = unpack(byteorder + 'b', sfile.read(1))[0]&#xa;                data_len = unpack(byteorder + 'i', sfile.read(4))[0]&#xa;            self._chrdict = chrdict&#xa;            &#xa;            # data&#xa;            varvals = []&#xa;            append = varvals.append&#xa;            typlist = self._typlist&#xa;            for _ in range(nobs):&#xa;                new_row = [get_var_val(typlist[i]) for i in range(nvar)]&#xa;                append(new_row)&#xa;            self._varvals = varvals&#xa;            &#xa;            # value labels&#xa;            value_labels = {}&#xa;            parse_value_label_table = self._parse_value_label_table&#xa;            while True:&#xa;                try:&#xa;                    sfile.seek(4,1) # table length&#xa;                    labname = get_byte_str(33)&#xa;                    sfile.seek(3,1) # padding&#xa;                    vl_table = parse_value_label_table(sfile)&#xa;                    value_labels[labname] = vl_table&#xa;                except StructError:&#xa;                    break&#xa;            self._vallabs = value_labels&#xa;        &#xa;    def _file_to_Dta117(self, address):&#xa;        """"""populate fields of dta object with values from disk""""""&#xa;        missing_above = {65530: 100, 65529: 32740, 65528: 2147483620, &#xa;                        65527: float.fromhex('0x1.fffffep+126'), &#xa;                        65526: float.fromhex('0x1.fffffffffffffp+1022')}&#xa;        # decimal numbers given in -help dta- for float and double &#xa;        # are approximations: 'f': 1.701e38, 'd': 8.988e307&#xa;        type_dict = {65530: ['b',1], 65529: ['h',2], 65528: ['l',4], &#xa;                    65527: ['f',4], 65526: ['d',8]}&#xa;            &#xa;        get_str = lambda n: (&#xa;            unpack(&#xa;                str(n) + 's',&#xa;                sfile.read(n)&#xa;            )[0].decode('iso-8859-1')&#xa;        )&#xa;        &#xa;        get_term_str = lambda n: (&#xa;            unpack(&#xa;                str(n) + 's', &#xa;                sfile.read(n)&#xa;            )[0].partition(b'\0')[0].decode('iso-8859-1')&#xa;        )&#xa;&#xa;        def missing_object(miss_val, st_type):&#xa;            if st_type == 65530: # byte&#xa;                value = MISSING_VALS[miss_val - 101]&#xa;            elif st_type == 65529: # int&#xa;                value = MISSING_VALS[miss_val - 32741]&#xa;            elif st_type == 65528: # long&#xa;                value = MISSING_VALS[miss_val - 2147483621]&#xa;            elif st_type == 65527: # float&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            elif st_type == 65526: # double&#xa;                value = MISSING_VALS[int(miss_val.hex()[5:7], 16)]&#xa;            return value&#xa;        &#xa;        with open(address, 'rb') as sfile:&#xa;            if get_str(11) != ""<stata_dta>"":&#xa;                raise DtaParseError(""expected '<stata_dta>'"")&#xa;        &#xa;            # header info&#xa;            if get_str(8) != ""<header>"":&#xa;                raise DtaParseError(""expected '<header>'"")&#xa;            &#xa;            if get_str(9) != ""<release>"": &#xa;                raise DtaParseError(""expected '<release>'"")&#xa;            self._ds_format = int(get_str(3))&#xa;            if self._ds_format != 117: &#xa;                raise DtaParseError(""expected release 117"")&#xa;            if get_str(10) != ""</release>"": &#xa;                raise DtaParseError(""expected '</release>'"")&#xa;            &#xa;            if get_str(11) != ""<byteorder>"": &#xa;                raise DtaParseError(""expected '<byteorder>'"")&#xa;            self._byteorder = byteorder = ('>' if get_str(3) == ""MSF"" else '<')&#xa;            if get_str(12) != ""</byteorder>"": &#xa;                raise DtaParseError(""expected '</byteorder>'"")&#xa;            &#xa;            if get_str(3) != ""<K>"": &#xa;                raise DtaParseError(""expected '<K>'"")&#xa;            self._nvar = nvar = unpack(byteorder + 'H', sfile.read(2))[0]&#xa;            if get_str(4) != ""</K>"": &#xa;                raise DtaParseError(""expected '</K>'"")&#xa;            &#xa;            if get_str(3) != ""<N>"": &#xa;                raise DtaParseError(""expected '<N>'"")&#xa;            self._nobs = nobs = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;            if get_str(4) != ""</N>"": &#xa;                raise DtaParseError(""expected '</N>'"")&#xa;            &#xa;            if get_str(7) != ""<label>"": &#xa;                raise DtaParseError(""expected '<label>'"")&#xa;            label_length = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            self._data_label = get_str(label_length)&#xa;            if get_str(8) != ""</label>"": &#xa;                raise DtaParseError(""expected '</label>'"")&#xa;            &#xa;            if get_str(11) != ""<timestamp>"":&#xa;                raise DtaParseError(""expected '<timestamp>'"")&#xa;            stamp_length = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            self._time_stamp = get_str(stamp_length)&#xa;            # -help dta- seems to indicate there's an optional binary zero here&#xa;            next = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;            if (not (next == b'\0' and get_str(12) == ""</timestamp>"") and&#xa;                    not (next == 60 and get_str(11) == ""/timestamp>"")):&#xa;                raise DtaParseError(""'</timestamp>'"")&#xa;            # 60 is int of '<' with iso-8859-1 encoding&#xa;            &#xa;            if get_str(9) != ""</header>"": &#xa;                raise DtaParseError(""expected '</header>'"")&#xa;            &#xa;            # map&#xa;            if get_str(5) != ""<map>"": &#xa;                raise DtaParseError(""expected '<map>'"")&#xa;            locs = unpack(byteorder + 'Q'*14, sfile.read(14 * 8))&#xa;            if get_str(6) != ""</map>"": &#xa;                raise DtaParseError(""expected '</map>'"")&#xa;            &#xa;            # variable types&#xa;            if sfile.tell() != locs[2] or get_str(16) != ""<variable_types>"":&#xa;                raise DtaParseError(""expected '<variable_types>'"")&#xa;            self._typlist = [unpack(byteorder + 'H', sfile.read(2))[0] &#xa;                             for i in range(nvar)]&#xa;            if get_str(17) != ""</variable_types>"": &#xa;                raise DtaParseError(""expected '</variable_types>'"")&#xa;            &#xa;            # varnames&#xa;            if sfile.tell() != locs[3] or get_str(10) != ""<varnames>"":&#xa;                raise DtaParseError(""expected '<varnames>'"")&#xa;            self._varlist = [get_term_str(33) for i in range(nvar)]&#xa;            if get_str(11) != ""</varnames>"": &#xa;                raise DtaParseError(""expected '</varnames>'"")&#xa;            &#xa;            # sortlist&#xa;            if sfile.tell() != locs[4] or get_str(10) != ""<sortlist>"":&#xa;                raise DtaParseError(""expected '<sortlist>'"")&#xa;            self._srtlist = self._get_srtlist(sfile)&#xa;            if get_str(11) != ""</sortlist>"": &#xa;                raise DtaParseError(""expected '</sortlist>'"")&#xa;            &#xa;            # formats&#xa;            if sfile.tell() != locs[5] or get_str(9) != ""<formats>"":&#xa;                raise DtaParseError(""expected '<formats>'"")&#xa;            self._fmtlist = [get_term_str(49) for i in range(nvar)]&#xa;            if get_str(10) != ""</formats>"": &#xa;                raise DtaParseError(""expected '</formats>'"")&#xa;            &#xa;            # value label names&#xa;            if sfile.tell() != locs[6] or get_str(19) != ""<value_label_names>"":&#xa;                raise DtaParseError(""expected '<value_label_names>'"")&#xa;            self._lbllist = [get_term_str(33) for i in range(nvar)]&#xa;            if get_str(20) != ""</value_label_names>"": &#xa;                raise DtaParseError(""expected '</value_label_names>'"")&#xa;            &#xa;            # variable labels&#xa;            # Before the 02jul2013 update, Stata put a zero in location&#xa;            # map for ""<variable_labels>"". Allow zero for files created&#xa;            # before that update.&#xa;            if (not (locs[7] == 0 or sfile.tell() == locs[7]) or &#xa;                    get_str(17) != ""<variable_labels>""):&#xa;                raise DtaParseError(""expected '<variable_labels>'"")&#xa;            self._vlblist = [get_term_str(81) for i in range(nvar)]&#xa;            if get_str(18) != ""</variable_labels>"": &#xa;                raise DtaParseError(""expected '</variable_labels>'"")&#xa;            &#xa;            # characteristics&#xa;            if sfile.tell() != locs[8] or get_str(17) != ""<characteristics>"":&#xa;                raise DtaParseError(""expected '<characteristics>'"")&#xa;            chrdict = {}&#xa;            next_four = get_term_str(4)&#xa;            while next_four == ""<ch>"":&#xa;                char_len = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;                s = unpack(str(char_len) + 's', sfile.read(char_len))[0]&#xa;                varname = s[:33].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charname = s[33:66].partition(b'\0')[0].decode('iso-8859-1')&#xa;                charstring = s[66:].partition(b'\0')[0].decode('iso-8859-1')&#xa;                if varname not in chrdict:&#xa;                    chrdict[varname] = {}&#xa;                chrdict[varname][charname] = charstring&#xa;                if get_str(5) != ""</ch>"": &#xa;                    raise DtaParseError(""expected '</ch>'"")&#xa;                next_four = get_term_str(4)&#xa;            self._chrdict = chrdict&#xa;            if next_four != ""</ch"" or get_str(14) != ""aracteristics>"":&#xa;                raise DtaParseError(""expected '</characteristics>'"")&#xa;            &#xa;            # data&#xa;            if sfile.tell() != locs[9] or get_str(6) != ""<data>"":&#xa;                raise DtaParseError(""expected '<data>'"")&#xa;            varvals = []&#xa;            data_append = varvals.append&#xa;            typlist = self._typlist&#xa;            for _ in range(nobs):&#xa;                new_row = []&#xa;                append = new_row.append&#xa;                for st_type in typlist:&#xa;                    if st_type <= 2045: # str&#xa;                        new_val = get_term_str(st_type)&#xa;                    elif st_type == 32768: # strl&#xa;                        # new_val == (v,o). Later replace (v,o) -> strl value.&#xa;                        new_val = unpack(byteorder + 'II', sfile.read(8))   &#xa;                    else:&#xa;                        fmt, nbytes = type_dict[st_type]&#xa;                        new_val = unpack(&#xa;                            byteorder + fmt, sfile.read(nbytes))[0]&#xa;                        if new_val > missing_above[st_type]:&#xa;                            new_val = missing_object(new_val, st_type)&#xa;                    append(new_val)&#xa;                data_append(new_row)&#xa;            self._varvals = varvals&#xa;            if get_str(7) != ""</data>"": &#xa;                raise DtaParseError(""expected '</data>'"")&#xa;            &#xa;            # strls&#xa;            if sfile.tell() != locs[10] or get_str(7) != ""<strls>"": &#xa;                raise DtaParseError(""expected '<strls>'"")&#xa;            strls = {(0,0): """"}&#xa;            next_three = get_str(3)&#xa;            while next_three == ""GSO"":&#xa;                vo = unpack(byteorder + ""II"", sfile.read(8))&#xa;                t = unpack(byteorder + 'B', sfile.read(1))[0]&#xa;                str_len = unpack(byteorder + 'I', sfile.read(4))[0]&#xa;                if t == 130:&#xa;                    new_str = (&#xa;                        unpack(str(str_len) + 's', sfile.read(str_len))[0]&#xa;                    )[:-1].decode('iso-8859-1')&#xa;                else:&#xa;                    new_str = sfile.read(str_len)&#xa;                strls.update({vo: new_str})&#xa;                next_three = get_str(3)&#xa;            if next_three != ""</s"" or get_str(5) != ""trls>"":&#xa;                raise DtaParseError(""expected '</strls>'"")&#xa;                        &#xa;            # put strls in data&#xa;            for st_type, var_num in zip(typlist, range(nvar)):&#xa;                if st_type == 32768:&#xa;                    for obs_num in range(nobs):&#xa;                        v, o = varvals[obs_num][var_num]&#xa;                        # raise error if having 'forward reference'?&#xa;                        # if not (o < j or (o == j and v < i)):&#xa;                        #    raise ...&#xa;                        varvals[obs_num][var_num] = strls[(v,o)]&#xa;            &#xa;            # value labels&#xa;            if sfile.tell() != locs[11] or get_str(14) != ""<value_labels>"":&#xa;                raise DtaParseError(""expected '<value_labels>'"")&#xa;            value_labels = {}&#xa;            parse_value_label_table = self._parse_value_label_table&#xa;            next_five = get_str(5)&#xa;            while next_five == ""<lbl>"":&#xa;                sfile.seek(4, 1) # table length&#xa;                label_name = get_term_str(33)&#xa;                sfile.seek(3, 1) # padding&#xa;                label_table = parse_value_label_table(sfile)&#xa;                value_labels[label_name] = label_table&#xa;                if get_str(6) != ""</lbl>"": &#xa;                    raise DtaParseError(""expected '</lbl>'"")&#xa;                next_five = get_str(5)&#xa;            self._vallabs = value_labels&#xa;            if next_five != ""</val"" or get_str(10) != ""ue_labels>"":&#xa;                raise DtaParseError(""expected '</value_labels>'"")&#xa;                &#xa;            &#xa;            # end tag&#xa;            if sfile.tell() != locs[12] or get_str(12) != ""</stata_dta>"":&#xa;                raise DtaParseError(""expected '</stata_dta>'"")&#xa;            &#xa;            # end of file&#xa;            if sfile.tell() != locs[13]:&#xa;                raise DtaParseError(""expected end of file"")&#xa;        &#xa;    def _get_type_name(self, st_type):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _isintvar(self, index):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        raise NotImplementedError&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        raise NotImplementedError&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        raise NotImplementedError&#xa;    &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        raise NotImplementedError&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        raise NotImplementedError&#xa;            &#xa;    def _dta_obj_to_file(self, address, replace=False):&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;class Dta115(Dta):&#xa;    """"""A Python class for Stata dataset version 115. It provides methods &#xa;    for creating, opening, manipulating, and saving Stata datasets.&#xa;    &#xa;    """"""  &#xa;    &#xa;    _default_fmt_widths = {251: 8, 252: 8, 253: 12, 254: 9, 255: 10}&#xa;    _default_fmts = {&#xa;        251: '%8.0g',&#xa;        252: '%8.0g', &#xa;        253: '%12.0g',&#xa;        254: '%9.0g',&#xa;        255: '%10.0g'&#xa;    }&#xa;    _default_new_type = 254&#xa;    &#xa;    _type_names = {&#xa;        251: 'byte',&#xa;        252: 'int',&#xa;        253: 'long',&#xa;        254: 'float',&#xa;        255: 'double'&#xa;    }&#xa;    &#xa;    def _get_type_name(self, st_type):&#xa;        """"""convert Stata type number to name""""""&#xa;        if st_type <= 244: return 'str' + str(st_type)&#xa;        return self._type_names[st_type]&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        """"""determine whether Stata variable is string""""""&#xa;        return self._typlist[index] <= 244&#xa;    &#xa;    def _isintvar(self, index):&#xa;        """"""determine whether Stata variable is integer""""""&#xa;        return 251 <= self._typlist[index] <= 253&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        return 251 <= self._typlist[index] <= 255&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        """"""convert other Dta version to 115""""""&#xa;        if old_type not in (Dta117,):&#xa;            msg = """".join(&#xa;                (""conversion from {} "".format(old_type.__name__),&#xa;                ""to Dta115 not supported""))&#xa;            raise TypeError(msg)&#xa;        &#xa;        self._ds_format = 115&#xa;        &#xa;        if old_type == Dta117:&#xa;            typlist = self._typlist&#xa;            fmtlist = self._fmtlist&#xa;            varlist = self._varlist&#xa;            varvals = self._varvals&#xa;            nobs = self._nobs&#xa;            nvar = self._nvar&#xa;            seen_strl = False&#xa;            seen_long_str = False&#xa;            seen_strange = False&#xa;            for j, st_type, varname in zip(range(nvar), typlist, varlist):&#xa;                if st_type == 32768:&#xa;                    if not seen_strl:&#xa;                        seen_strl = True&#xa;                        if not self._quiet:&#xa;                            msg = ""warning: strLs converted to strfs""&#xa;                            print((""{err}"" if IN_STATA else """") + msg)&#xa;                    str_len = 0&#xa;                    for i in range(nobs):&#xa;                        new_val = str(varvals[i][j])&#xa;                        val_len = len(new_val)&#xa;                        if val_len > 244:&#xa;                            if not seen_long_str:&#xa;                                seen_long_str = True&#xa;                                if not self._quiet:&#xa;                                    msg = ""warning: long strings truncated""&#xa;                                    print((""{err}"" if IN_STATA else """") + msg)&#xa;                            new_val = new_val[:244]&#xa;                            val_len = 244&#xa;                        varvals[i][j] = new_val&#xa;                        str_len = max(str_len, val_len)&#xa;                    typlist[j] = str_len&#xa;                    m = STR_FMT_RE.match(fmtlist[j])&#xa;                    if not m or m.group(0) == '%9s' or int(m.group(3)) > 244:&#xa;                        align = m.group(1) if m.group(1) else ''&#xa;                        fmtlist[j] = '%' + align + str(str_len) + 's'&#xa;                elif 0 < st_type < 245:&#xa;                    pass&#xa;                elif 245 < st_type <= 2045:&#xa;                    if not seen_long_str:&#xa;                        seen_long_str = True&#xa;                        if not self._quiet:&#xa;                            msg = ""warning: long strings truncated""&#xa;                            print((""{err}"" if IN_STATA else """") + msg)&#xa;                    str_len = 0&#xa;                    for i in range(nobs):&#xa;                        new_val = varvals[i][j]&#xa;                        val_len = len(new_val)&#xa;                        # it is possible that st_type > actual string lengths&#xa;                        if val_len > 244: &#xa;                            new_val = new_val[:244]&#xa;                            val_len = 244&#xa;                        varvals[i][j] = new_val&#xa;                        str_len = max(str_len, val_len)&#xa;                    typlist[j] = str_len&#xa;                    m = STR_FMT_RE.match(fmtlist[j])&#xa;                    if not m or m.group(0) == '%9s' or int(m.group(3)) > 244:&#xa;                        align = m.group(1) if m.group(1) else ''&#xa;                        fmtlist[j] = '%' + align + str(str_len) + 's'&#xa;                elif 65526 <= st_type <= 65530:&#xa;                    typlist[j] = 251 + (65530 - st_type)&#xa;                elif not seen_strange:&#xa;                    # just a safety; not needed in normal usage&#xa;                    seen_strange = True&#xa;                    if not self._quiet:&#xa;                        msg = ""strange Stata types encountered; ignoring""&#xa;                        print((""{err}"" if IN_STATA else """") + msg)&#xa;        &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        """"""create dataset from iterable of values""""""&#xa;        global get_missing&#xa;        &#xa;        # first, list-alize like tuplize in __setitem__&#xa;        def make_list(x):&#xa;            if isinstance(x, str) or not isinstance(x, collections.Iterable):&#xa;                return [x]&#xa;            return list(x)&#xa;        &#xa;        # force input into 2d structure&#xa;        varvals = [make_list(v) for v in varvals]&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if single_row:&#xa;            varvals = [[v[0] for v in varvals]]              &#xa;    &#xa;        # Get correct type and homogenize rows by setting same length &#xa;        # (append missing values of correct type when necessary) and &#xa;        # by coercing when necessary to make each column same type.&#xa;        ismissing = self.ismissing&#xa;        &#xa;        typlist = []&#xa;        &#xa;        str_clipped = False&#xa;        alt_missing = False&#xa;        &#xa;        curr_nvars = 0&#xa;        nrows = len(varvals)&#xa;        &#xa;        for i in range(nrows):&#xa;            row = varvals[i]&#xa;            row_len = len(row)&#xa;            if row_len < curr_nvars:&#xa;                row += ['' if st_type <= 244 else MISSING &#xa;                        for st_type in typlist[row_len:]]&#xa;            elif row_len > curr_nvars:&#xa;                # add to typelist&#xa;                diff = row_len - curr_nvars&#xa;                typlist += [251 if compress else 254]*diff &#xa;                # extend all previous rows with missing values&#xa;                padding = [MISSING]*diff&#xa;                for j in range(i):&#xa;                    varvals[j] += padding&#xa;                # update curr_nvars&#xa;                curr_nvars = row_len&#xa;            # verify type of values and convert or &#xa;            # make retroactive changes when necessary&#xa;            for k in range(curr_nvars):&#xa;                value = row[k]&#xa;                st_type = typlist[k]&#xa;                if st_type <= 244:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        typlist[k] = max(st_type, val_len)&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or &#xa;                            value > 8.988465674311579e+307):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        value = str(value)&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        typlist[k] = max(st_type, val_len)&#xa;                    row[k] = value&#xa;                else:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        if val_len > 244:&#xa;                            value = value[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        row[k] = value&#xa;                        st_type = val_len&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(new_val)&#xa;                                val_len = len(new_val)&#xa;                                if val_len > 244:&#xa;                                    new_val = new_val[:244]&#xa;                                    val_len = 244&#xa;                                    str_clipped = True&#xa;                                varvals[j][k] = new_val&#xa;                                st_type = max(st_type, val_len)&#xa;                        typlist[k] = st_type&#xa;                    elif value is None:&#xa;                        row[k] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(value, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(value, float) and&#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = get_missing(value)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (value != int(value) or -2147483647 > value or&#xa;                                value > 2147483620): &#xa;                            # val is not int or is outside of bounds&#xa;                            typlist[k] = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= value <= 32740):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            typlist[k] = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= value <= 100): &#xa;                            # st_type byte, but val is outside of bounds&#xa;                            typlist[k] = 252 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 254 and &#xa;                                (-1.7014117331926443e+38 <= value or&#xa;                                 value > 1.7014117331926443e+38)):&#xa;                            # st_type float, but val is outside of bounds&#xa;                            typlist[k] = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;        &#xa;        if not quiet:&#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if str_clipped:&#xa;                msg = ""warning: some strings were shortened to 244 characters""&#xa;                print(smcl + msg)&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        # header&#xa;        self._ds_format  = 115&#xa;        self._byteorder  = "">"" if sys.byteorder == ""big"" else ""<""&#xa;        self._nvar       = curr_nvars&#xa;        self._nobs       = nrows&#xa;        self._data_label = """"&#xa;        self._set_timestamp()&#xa;           &#xa;        # descriptors&#xa;        formats = self._default_fmts&#xa;        &#xa;        self._typlist = typlist&#xa;        self._varlist = [""var"" + str(i) for i in range(curr_nvars)]&#xa;        self._srtlist = [None for i in range(curr_nvars)]&#xa;        self._fmtlist = ['%' + str(max(9,st_type)) + 's' if st_type <= 244 &#xa;                         else formats[st_type] for st_type in typlist]&#xa;        self._lbllist = [""""]*curr_nvars&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [""""]*curr_nvars&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {}&#xa;        &#xa;        # data&#xa;        self._varvals = varvals&#xa;        &#xa;        # value labels&#xa;        self._vallabs = {}&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        """"""Add new variable to data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of variable to be created.&#xa;        values : iterable&#xa;            Should be a flat iterable like [1, 5, 9, ...] or&#xa;            (1, 5, 9, ...). Not like [[1], [5], [9], ...].&#xa;        st_type : int or str, optional&#xa;            Examples: 212 or ""str212"", 254 or ""float"".&#xa;            Intended Stata type of the data variable. &#xa;            The intended type will be overridden when necessary.&#xa;            Default value depends on the given values.&#xa;        compress : bool (or coercible to bool), optional&#xa;            If st_type is None, this sets st_type to byte if &#xa;            compress=True, or float if compress=False.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds new variable to data set.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        if (isinstance(values, str) or &#xa;                not isinstance(values, collections.Iterable)):&#xa;            if self._nobs <= 1:&#xa;                values = [values]&#xa;            else:&#xa;                raise TypeError(""values to add must be in an iterable"")&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""variable name must be str"")&#xa;        &#xa;        name = name.strip()&#xa;        if name == """":&#xa;            raise ValueError(""variable name required"")&#xa;        &#xa;        if name in self._varlist:&#xa;            raise ValueError(""variable name already exists"")&#xa;        elif not self._is_valid_varname(name):&#xa;            raise ValueError(name + "" is not a valid Stata name"")&#xa;            &#xa;        type_names = (""byte"", ""int"", ""long"", ""float"", ""double"")&#xa;          &#xa;        init_st_type = st_type&#xa;        if st_type is None:&#xa;            st_type = 251 if compress else 254&#xa;        elif isinstance(st_type, str):&#xa;            if re.match(r'^str[0-9]+$', st_type):&#xa;                st_type = int(st_type[3:])&#xa;                if st_type > 244:&#xa;                    msg = ""given string type too large; shortening to 244""&#xa;                    print((""{err}"" if IN_STATA else """") + msg)&#xa;                    st_type = 244&#xa;                    init_st_type = st_type&#xa;            elif st_type in type_names:&#xa;                st_type = 251 + type_names.index(st_type)&#xa;                init_st_type = st_type&#xa;            else:&#xa;                raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        elif (st_type not in (251, 252, 253, 254, 255) &#xa;                and not (isinstance(st_type, int) and 1 <= st_type <= 244)):&#xa;            raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        &#xa;        # Given iterable could be generator. Ensure it is in static form.&#xa;        values = [v for v in values]&#xa;        nvals = len(values)&#xa;        &#xa;        varvals = self._varvals&#xa;        &#xa;        if nvals == 0:&#xa;            this_missing = '' if st_type <= 244 else MISSING&#xa;            for row in varvals:&#xa;                row.append(this_missing)&#xa;        else:&#xa;            str_clipped = False&#xa;            alt_missing = False&#xa;            &#xa;            ismissing = self.ismissing&#xa;        &#xa;            for val, i in zip(values, range(nvals)):&#xa;                if st_type <= 244:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            values[i] = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        st_type = max(st_type, val_len)&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif not (isinstance(val, int) or isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        val = str(val)&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            val = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        values[i] = val&#xa;                        st_type = max(st_type, val_len)&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            values[i] = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        st_type = val_len&#xa;                        for j in range(i):&#xa;                            valj = values[j]&#xa;                            if ismissing(valj): &#xa;                                # If encountering a missing value here,  &#xa;                                # should be instance of MissingValue. &#xa;                                # Could just check for that.&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val_j = str(values[j])&#xa;                                val_len = len(new_val_j)&#xa;                                if val_len > 244:&#xa;                                    new_val_j = new_val_j[:244]&#xa;                                    val_len = 244&#xa;                                    str_clipped = True&#xa;                                values[j] = new_val_j&#xa;                                st_type = max(st_type, val_len)&#xa;                    elif val is None:&#xa;                        values[i] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif not (isinstance(val, float) or isinstance(val, int)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (val != int(val) or &#xa;                                not (-2147483647 <= val <= 2147483620)):&#xa;                            # val is not int or is outside of bounds of long&#xa;                            st_type = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            st_type = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= val <= 100):&#xa;                            # st_type byte, but val is outside of bounds&#xa;                            st_type = 252 # int&#xa;                    else: # was float and will continue to be&#xa;                        if st_type == 254 and (-1.7014117331926443e+38 > val or&#xa;                                val > 1.7014117331926443e+38):&#xa;                            # st_type float, but val is outisde of bounds&#xa;                            st_type = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                &#xa;            if nvals < self._nobs:&#xa;                this_missing = '' if st_type <= 244 else MISSING&#xa;                values += [this_missing]*(self._nobs - nvals)&#xa;            elif nvals > self._nobs:&#xa;                self.set_obs(nvals)&#xa;            &#xa;            for row, new_val in zip(varvals, values):&#xa;                row.append(new_val)&#xa;            &#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                if init_st_type is not None and init_st_type != st_type:&#xa;                    st_type_name = self._get_type_name(st_type)&#xa;                    msg = (smcl + ""warning: some values were incompatible with "" + &#xa;                           ""specified type;\n    type changed to "" + st_type_name)&#xa;                    print(msg)&#xa;                if str_clipped:&#xa;                    print(smcl + ""warning: some strings were "" + &#xa;                          ""shortened to 244 characters"")&#xa;                if alt_missing:&#xa;                    print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        &#xa;        self._typlist.append(st_type)&#xa;        self._varlist.append(name)&#xa;        self._srtlist.append(None)&#xa;        self._fmtlist.append('%' + str(max(9,st_type)) + 's' if st_type <= 244&#xa;                             else self._default_fmts[st_type])&#xa;        self._lbllist.append('')&#xa;        self._vlblist.append('')&#xa;        &#xa;        self._nvar += 1&#xa;        self._changed = True&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        """"""Check to see if given str is a valid Stata name.&#xa;        Be sure to strip spaces before calling this function.&#xa;        &#xa;        """"""&#xa;        if name in RESERVED or re.match(r'^str[0-9]+$', name): return False&#xa;        return True if VALID_NAME_RE.match(name) else False&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        """"""check that given str fmt is a valid Stata format""""""&#xa;        # make sure there is no leading or trailing whitespace&#xa;        fmt = fmt.strip()&#xa;        &#xa;        if fmt[0] != '%':&#xa;            return False&#xa;        &#xa;        # Handle business calendars first.&#xa;        # This does not check the calendar name.&#xa;        if fmt[1:3] == ""tb"" or fmt[1:4] == ""-tb"":&#xa;            return True if TB_FMT_RE.match(fmt) else False&#xa;            &#xa;        # date formats&#xa;        if fmt[1] == 't' or fmt[1:3] == '-t':&#xa;            return True if TIME_FMT_RE.match(fmt) else False&#xa;        &#xa;        # categorize using last character&#xa;        last_char = fmt[-1]&#xa;        if last_char == 's': # string&#xa;            m = STR_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width > 244: return False&#xa;            return True&#xa;        elif last_char == 'H' or last_char == 'L': # binary&#xa;            # Valid binary formats are ^%(8|16)(H|L)$. Stata doesn't raise &#xa;            # error with -8 or -16, but the results are perhaps unexpected.&#xa;            return True if fmt[1:-1] in ('8', '16', '-8', '-16') else False&#xa;        elif last_char == 'x': # hexadecimal&#xa;            return True if fmt == '%21x' or fmt == '%-12x' else False&#xa;        elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;            m = NUM_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width <= int(m.group(5)) or width > 244: &#xa;                return False&#xa;            return True&#xa;            &#xa;        return False&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        """"""Width of an observation as saved.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Width of a single observation in bytes.&#xa;        &#xa;        """"""&#xa;        widths = { 251: 1, 252: 2, 253: 4, 254: 4, 255: 8 }&#xa;        return sum([0] + [t if t < 245 else widths[t] for t in self._typlist])&#xa;            &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        """"""Helper for functions like __setitem__ that put &#xa;        new values into the dataset. This function does the &#xa;        job of inserting the values.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        varvals = self._varvals&#xa;        typlist = self._typlist&#xa;        varlist = self._varlist&#xa;        old_typlist = [typlist[i] for i in sel_cols]&#xa;        &#xa;        str_clipped = False&#xa;        alt_missing = False&#xa;        &#xa;        for row_num, i in zip(sel_rows, range(len(sel_rows))):&#xa;            row = value[i]&#xa;            for col_num, k in zip(sel_cols, range(len(sel_cols))):&#xa;                val = row[k]&#xa;                st_type = typlist[col_num]&#xa;                if st_type <= 244:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        if val_len > 244:&#xa;                            val = val[:244]&#xa;                            val_len = 244&#xa;                            str_clipped = True&#xa;                        if val_len > st_type:&#xa;                            typlist[col_num] = val_len&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        val = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot "" + &#xa;                               ""take non-string values"")&#xa;                        raise TypeError(msg)&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot take "" + &#xa;                               ""string values; has Stata type "" + &#xa;                               self._get_type_name(st_type))&#xa;                        raise TypeError(msg)&#xa;                    elif val is None:&#xa;                        val = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif not (isinstance(val, float) or isinstance(val, int)):&#xa;                        msg = (""value in right-hand position "" + &#xa;                               ""{},{} is not of recognized type"".format(i, k))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        val = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type <= 253: # int types&#xa;                        if (val != int(val) or -2147483647 > val or&#xa;                                val > 2147483620):&#xa;                            typlist[col_num] = 255 # double&#xa;                        elif st_type <= 252 and not (-32767 <= val <= 32740):&#xa;                            typlist[col_num] = 253 # long&#xa;                        elif st_type == 251 and not (-127 <= val <= 100):&#xa;                            typlist[col_num] = 252 # int&#xa;                    else: # was float and will continue to be&#xa;                        if (st_type == 254 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)):&#xa;                            typlist[col_num] = 255 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                varvals[row_num][col_num] = val&#xa;        &#xa;        if not self._quiet:&#xa;            seen_cols = set() # same column can appear multiple times&#xa;            smcl = ""{txt}"" if IN_STATA else """"&#xa;            msg = smcl + ""Stata type for {} was {}, now {}""&#xa;            for old_type,c in zip(old_typlist, sel_cols):&#xa;                new_type = typlist[c]&#xa;                if old_type != new_type and c not in seen_cols:&#xa;                    old_name = self._get_type_name(old_type)&#xa;                    new_name = self._get_type_name(new_type)&#xa;                    print(msg.format(varlist[c], old_name, new_name))&#xa;                seen_cols.add(c)&#xa;            &#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if str_clipped:&#xa;                msg = ""warning: some strings were shortened to 244 characters""&#xa;                print(smcl + msg)&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        """"""helper function for writing dta files""""""&#xa;        n = miss_val.index&#xa;        &#xa;        if st_type == 251: # byte&#xa;            value = n + 101&#xa;        elif st_type == 252: # int&#xa;            value = n + 32741&#xa;        elif st_type == 253: # long&#xa;            value = n + 2147483621&#xa;        elif st_type == 254: # float&#xa;            value = float.fromhex('0x1.0' + hex(n)[2:].zfill(2) + 'p+127')&#xa;        elif st_type == 255: # double&#xa;            value = float.fromhex('0x1.0' + hex(n)[2:].zfill(2) + 'p+1023')&#xa;        return value&#xa;            &#xa;    def _dta_obj_to_file(self, address):&#xa;        """"""save dta object to disk""""""&#xa;        global get_missing&#xa;        &#xa;        type_dict = {&#xa;            251: ['b',1],&#xa;            252: ['h',2], &#xa;            253: ['l',4],&#xa;            254: ['f',4],&#xa;            255: ['d',8]&#xa;        }&#xa;        first_missing = {&#xa;            251: 101,&#xa;            252: 32741,&#xa;            253: 2147483620, &#xa;            254: float.fromhex('0x1.0p+127'),&#xa;            255: float.fromhex('0x1.0p+1023')&#xa;        }&#xa;        typlist = self._typlist&#xa;        nvar = self._nvar&#xa;        &#xa;        missing_save_val = self._missing_save_val&#xa;                &#xa;        def write_value_label_table(labname, table):&#xa;            # Stata limits are a bit confusing. Total length of text &#xa;            # (including null terminators) must be <= 32000? Total &#xa;            # number of vals must be <= 65536? But the limit on text &#xa;            # length forces no. of vals <= 16000 since each label must &#xa;            # occupy at least two bytes (including null terminator).&#xa;            &#xa;            labname = labname[:32]&#xa;            &#xa;            val = sorted(table.keys())&#xa;            # each value may be up to 81 chars including null&#xa;            txt = [table[v][:80] for v in val] &#xa;            &#xa;            nval = len(val)&#xa;            if nval > 65536: # max number of values allowed&#xa;                val = val[:65536]&#xa;                txt = txt[:65536]&#xa;                nval = 65536&#xa;            &#xa;            off = [0]&#xa;            for i in range(nval - 1):&#xa;                # in next line, ""+ 1"" to leave room for \0&#xa;                offset = off[i] + len(txt[i]) + 1&#xa;                if offset > 32000: # if too much text&#xa;                    off = off[:i] # cut off at before the ith one&#xa;                    val = val[:i]&#xa;                    txt = txt[:i]&#xa;                    nval = i&#xa;                    break&#xa;                off.append(offset)&#xa;            txt_len = off[-1] + len(txt[-1]) + 1&#xa;            &#xa;            table_len = 4 + 4 + 4*nval + 4*nval + txt_len&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", table_len))&#xa;            dta.write(bytearray(labname, 'iso-8859-1') +&#xa;                      b'\0'*(33-len(labname)))&#xa;            dta.write(b'\x00\x00\x00')&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", nval))&#xa;            dta.write(pack(byteorder + ""l"", txt_len))&#xa;            for o in off: dta.write(pack(byteorder + ""l"", o))&#xa;            for v in val: dta.write(pack(byteorder + ""l"", v))&#xa;            #for t in txt: write_byte_str((t,), len(t) + 1)&#xa;            for t in txt: dta.write(bytearray(t, 'iso-8859-1') + b'\0')&#xa;        &#xa;        with open(address, 'wb') as dta:&#xa;            # header&#xa;            dta.write(pack('b', 115)) # ds_format&#xa;            byteorder = self._byteorder&#xa;            dta.write(pack('b', 1 if byteorder == '>' else 2)) # byteorder&#xa;            dta.write(pack('b', 1)) # filetype&#xa;            dta.write(pack('b', 0)) # padding&#xa;            dta.write(pack(byteorder + 'h', self._nvar))&#xa;            dta.write(pack(byteorder + 'i', self._nobs))&#xa;            data_label = self._data_label[:80]&#xa;            dta.write(bytearray(data_label, 'iso-8859-1') +&#xa;                      b'\0'*(81-len(data_label)))&#xa;            self._set_timestamp() # new time_stamp&#xa;            time_stamp = self._time_stamp[:17]&#xa;            dta.write(bytearray(time_stamp, 'iso-8859-1') +&#xa;                      b'\0'*(18-len(time_stamp)))&#xa;            &#xa;            # descriptors&#xa;            dta.write(bytes(self._typlist))&#xa;            for name in self._varlist:&#xa;                name = name[:32]&#xa;                dta.write(bytearray(name, 'iso-8859-1') + b'\0'*(33-len(name)))&#xa;            # In srtlist, Nones are replaced with zeroes and &#xa;            # a terminating zero is appended (the file needs &#xa;            # nvar + 1 ints including terminating zero).&#xa;            srtlist = self._srtlist + [None]&#xa;            srtlist = [srt + 1 if srt is not None else 0 for srt in srtlist]&#xa;            dta.write(pack(byteorder + 'h'*(nvar + 1), *srtlist))&#xa;            for fmt in self._fmtlist:&#xa;                fmt = fmt[:48]&#xa;                dta.write(bytearray(fmt, 'iso-8859-1') + b'\0'*(49-len(fmt)))&#xa;            for lab in self._lbllist:&#xa;                lab = lab[:32]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(33-len(lab)))&#xa;            &#xa;            # variable labels&#xa;            for lab in self._vlblist:&#xa;                lab = lab[:80]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(81-len(lab)))&#xa;            &#xa;            # characteristics&#xa;            chrdict = self._chrdict&#xa;            for varname in chrdict:&#xa;                varname = varname[:32]&#xa;                vardict = chrdict[varname]&#xa;                for charname in vardict:&#xa;                    charname = charname[:32]&#xa;                    char = vardict[charname][:67784]  # or 8681 for Small Stata&#xa;                    data_len = 66 + len(char) + 1 # +1 for null termination&#xa;                    dta.write(b'\x01') # data_type&#xa;                    dta.write(pack(byteorder + 'i', data_len))&#xa;                    dta.write(bytearray(varname, 'iso-8859-1') + &#xa;                              b'\0'*(33 - len(varname)))&#xa;                    dta.write(bytearray(charname, 'iso-8859-1') + &#xa;                              b'\0'*(33 - len(charname)))&#xa;                    dta.write(bytearray(char, 'iso-8859-1') + b'\0')&#xa;            dta.write(b'\x00\x00\x00\x00\x00')&#xa;            &#xa;            # data&#xa;            for row in self._varvals:&#xa;                for value, st_type in zip(row, typlist):&#xa;                    if st_type <= 244:&#xa;                        dta.write(bytearray(value, 'iso-8859-1') + &#xa;                                  b'\0'*(st_type - len(value)))&#xa;                    else:&#xa;                        fmt, nbytes = type_dict[st_type]&#xa;                        # Get correct dta value if missing. As a safety, check&#xa;                        # for non-standard missing (None and large values).&#xa;                        if value is None:&#xa;                            value = first_missing[st_type]&#xa;                        elif isinstance(value, MissingValue):&#xa;                            value = missing_save_val(value, st_type)&#xa;                        elif (value > 8.988465674311579e+307 or &#xa;                                value < -1.7976931348623157e+308):&#xa;                            # is this the right way to handle this ?&#xa;                            value = missing_save_val(&#xa;                                get_missing(value), st_type) &#xa;                        dta.write(pack(byteorder + fmt, value))&#xa;                &#xa;            # value labels&#xa;            value_labels = self._vallabs&#xa;            for labname in value_labels.keys():&#xa;                write_value_label_table(labname, value_labels[labname])&#xa;&#xa;&#xa;class Dta117(Dta):&#xa;    """"""A Python class for Stata dataset version 117. It provides methods&#xa;    for creating, opening, manipulating, and saving Stata datasets.&#xa;    &#xa;    """"""&#xa;    _default_fmt_widths = {65530: 8, 65529: 8, 65528: 12, 65527: 9, 65526: 10}&#xa;    _default_fmts = {&#xa;        65530: '%8.0g',&#xa;        65529: '%8.0g', &#xa;        65528: '%12.0g',&#xa;        65527: '%9.0g',&#xa;        65526: '%10.0g'&#xa;    }&#xa;    _default_new_type = 65527&#xa;    &#xa;    _type_names = {&#xa;        65530: 'byte',&#xa;        65529: 'int',&#xa;        65528: 'long',&#xa;        65527: 'float',&#xa;        65526: 'double',&#xa;        32768: 'strL'&#xa;    }&#xa;    &#xa;    def _get_type_name(self, st_type):&#xa;        """"""convert Stata type number to name""""""&#xa;        if st_type <= 2045: return 'str' + str(st_type)&#xa;        return self._type_names[st_type]&#xa;    &#xa;    def _isstrvar(self, index):&#xa;        """"""determine if Stata variable is string""""""&#xa;        return self._typlist[index] <= 32768&#xa;        &#xa;    def _isintvar(self, index):&#xa;        """"""determine if Stata variable is integer""""""&#xa;        return 65528 <= self._typlist[index] <= 65530&#xa;        &#xa;    def _isnumvar(self, index):&#xa;        """"""determine if Stata variable is numeric""""""&#xa;        return 65526 <= self._typlist[index] <= 65530&#xa;    &#xa;    def _convert_dta(self, old_type):&#xa;        """"""convert other Dta version to 117""""""&#xa;        if old_type not in (Dta115,):&#xa;            msg = """".join(&#xa;                (""conversion from {} "".format(old_type.__name__),&#xa;                ""to Dta117 not supported""))&#xa;            raise TypeError(msg)&#xa;        self._ds_format = 117&#xa;        self._typlist = [i if i <= 244 else 65530 + (251 - i) &#xa;                         for i in self._typlist]&#xa;        &#xa;    def _new_from_iter(self, varvals, compress=True,&#xa;                       single_row=False, quiet=False):&#xa;        """"""create dataset from iterable of values""""""&#xa;        global get_missing&#xa;        &#xa;        # first, list-alize like tuplize in __setitem__&#xa;        def make_list(x):&#xa;            if (any(isinstance(x, t) for t in (str,bytes,bytearray))&#xa;                    or not isinstance(x, collections.Iterable)):&#xa;                return [x]&#xa;            return list(x)&#xa;        &#xa;        # force input into 2d structure&#xa;        varvals = [make_list(v) for v in varvals]&#xa;            &#xa;        # Reformation above is wrong for a single-row assignment, where&#xa;        # values [val1, val2, ...] should be interpreted as  &#xa;        # single row: [[val1, val2, ...]]. Procedure above makes it   &#xa;        # into [[val1], [val2], ...] (the correct assumption otherwise).&#xa;        if single_row:&#xa;            varvals = [[v[0] for v in varvals]]              &#xa;    &#xa;        # Get correct type and homogenize rows by setting same length &#xa;        # (append missing values of correct type when necessary) and &#xa;        # by coercing when necessary to make each column same type.&#xa;        ismissing = self.ismissing&#xa;        &#xa;        typlist = []&#xa;        &#xa;        alt_missing = False&#xa;        &#xa;        curr_nvars = 0&#xa;        nrows = len(varvals)&#xa;        &#xa;        for i in range(nrows):&#xa;            row = varvals[i]&#xa;            row_len = len(row)&#xa;            if row_len < curr_nvars:&#xa;                row += ['' if st_type <= 32768 else MISSING &#xa;                        for st_type in typlist[row_len:]]&#xa;            elif row_len > curr_nvars:&#xa;                # add to typelist&#xa;                diff = row_len - curr_nvars&#xa;                # Default type is byte or float. Stata doesn't convert &#xa;                # from float, but we should be able to here.&#xa;                typlist += [65530 if compress else 65527]*diff&#xa;                # extend all previous rows with missing values&#xa;                padding = [MISSING]*diff&#xa;                for j in range(i):&#xa;                    varvals[j] += padding&#xa;                # update curr_nvars&#xa;                curr_nvars = row_len&#xa;            # verify type of values and convert or &#xa;            # make retroactive changes when necessary&#xa;            for k in range(curr_nvars):&#xa;                value = row[k]&#xa;                st_type = typlist[k]&#xa;                if st_type == 32768:&#xa;                    if any(isinstance(value, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        pass&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        row[k] = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(value, int) and&#xa;                            not isinstance(value, float)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        row[k] = str(value)&#xa;                elif st_type < 32768:&#xa;                    if isinstance(value, str):&#xa;                        val_len = len(value)&#xa;                        typlist[k] = (32768 if val_len > 2045 &#xa;                                            else max(st_type, val_len))&#xa;                    elif value is None or isinstance(value, MissingValue):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    elif (isinstance(value, bytes) or &#xa;                            isinstance(value, bytearray)):&#xa;                        typlist[k] = 32768&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        value = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        value = str(value)&#xa;                        val_len = len(value)&#xa;                        typlist[k] = (32768 if val_len > 2045 &#xa;                                            else max(st_type, val_len))&#xa;                    row[k] = value&#xa;                else:&#xa;                    if isinstance(value, str):&#xa;                        max_len = len(value)&#xa;                        row[k] = value&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered  &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(new_val)&#xa;                                max_len = max(max_len, len(new_val))&#xa;                                varvals[j][k] = new_val&#xa;                        typlist[k] = 32768 if max_len > 2045 else max_len&#xa;                    elif (isinstance(value, bytes) or &#xa;                            isinstance(value, bytearray)):&#xa;                        for j in range(i):&#xa;                            new_val = varvals[j][k]&#xa;                            if ismissing(new_val):&#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                varvals[j][k] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                varvals[j][k] = str(new_val)&#xa;                        typlist[k] = 32768&#xa;                    elif value is None:&#xa;                        row[k] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(value, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(value, float) and &#xa;                            not isinstance(value, int)):&#xa;                        msg = (""value {},{} has invalid type {}""&#xa;                                ).format(i, k, value.__class__.__name__)&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > value or&#xa;                            value > 8.988465674311579e+307):&#xa;                        row[k] = get_missing(value)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (value != int(value) or -2147483647 > value or&#xa;                                value > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            typlist[k] = 65526 # double&#xa;                        elif (st_type >= 65529 and &#xa;                                not (-32767 <= value <= 32740)):&#xa;                            # st_type int, but val is outside of bounds&#xa;                            typlist[k] = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= value <= 100): &#xa;                            # st_type byte, but val is outside of bounds&#xa;                            typlist[k] = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > value or&#xa;                                 value > 1.7014117331926443e+38)): &#xa;                            # st_type float, but val is outside of bounds&#xa;                            typlist[k] = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;        &#xa;        if not quiet:&#xa;            if alt_missing:&#xa;                smcl = ""{err}"" if IN_STATA else """" &#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        # header&#xa;        self._ds_format  = 117&#xa;        self._byteorder  = "">"" if sys.byteorder == ""big"" else ""<""&#xa;        self._nvar       = curr_nvars&#xa;        self._nobs       = nrows&#xa;        self._data_label = """"&#xa;        self._set_timestamp()&#xa;           &#xa;        # descriptors&#xa;        formats = self._default_fmts&#xa;        &#xa;        self._typlist = typlist&#xa;        self._varlist = [""var"" + str(i) for i in range(curr_nvars)]&#xa;        self._srtlist = [None for i in range(curr_nvars)]&#xa;        self._fmtlist = [&#xa;            '%' + str(max(9,st_type) if st_type <= 2045 else 9) + 's' &#xa;            if st_type <= 32768 else formats[st_type] for st_type in typlist]&#xa;        self._lbllist = [""""]*curr_nvars&#xa;        &#xa;        # variable labels&#xa;        self._vlblist = [""""]*curr_nvars&#xa;        &#xa;        # expansion fields&#xa;        self._chrdict = {}&#xa;        &#xa;        # data&#xa;        self._varvals = varvals&#xa;        &#xa;        # value labels&#xa;        self._vallabs = {}&#xa;        &#xa;        # set changed to True, since new dataset has not been saved&#xa;        self._changed = True&#xa;        &#xa;        # set quiet on or off&#xa;        self._quiet = bool(quiet)&#xa;        &#xa;    def append_var(self, name, values, st_type=None, compress=True):&#xa;        """"""Add new variable to data set.&#xa;        &#xa;        Parameters&#xa;        ----------&#xa;        name : str&#xa;            Name of variable to be created.&#xa;        values : iterable&#xa;            Should be a flat iterable like [1, 5, 9, ...] or&#xa;            (1, 5, 9, ...). Not like [[1], [5], [9], ...].&#xa;        st_type : int or str, optional&#xa;            Examples: 212 or ""str212"", 65527 or ""float"".&#xa;            Intended Stata type of the data variable. &#xa;            The intended type will be overridden when necessary.&#xa;            Default value depends on the given values.&#xa;        compress : bool (or coercible to bool), optional&#xa;            If st_type is None, this sets st_type to byte if &#xa;            compress=True, or float if compress=False.&#xa;            Using compress=True can result in smaller files.&#xa;            Default value is True.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        None&#xa;        &#xa;        Side effects&#xa;        ------------&#xa;        Adds new variable to data set.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        if (any(isinstance(values, t) for t in (str,bytes,bytearray))&#xa;                or not isinstance(values, collections.Iterable)):&#xa;            if self._nobs <= 1:&#xa;                values = [values]&#xa;            else:&#xa;                raise TypeError(""values to add must be in an iterable"")&#xa;        if not isinstance(name, str):&#xa;            raise TypeError(""variable name must be str"")&#xa;        &#xa;        name = name.strip()&#xa;        if name == """":&#xa;            raise ValueError(""variable name required"")&#xa;        &#xa;        if name in self._varlist:&#xa;            raise ValueError(""variable name already exists"")&#xa;        elif not self._is_valid_varname(name):&#xa;            raise ValueError(name + "" is not a valid Stata name"")&#xa;            &#xa;        type_names = (""byte"", ""int"", ""long"", ""float"", ""double"")&#xa;          &#xa;        init_st_type = st_type&#xa;        if st_type is None:&#xa;            st_type = 65530 if compress else 65527&#xa;        elif isinstance(st_type, str):&#xa;            m = re.match(r'^str([0-9]+|L)$', st_type)&#xa;            if m:&#xa;                if m.group(1) == ""L"":&#xa;                    st_type = 32768&#xa;                else:&#xa;                    st_type = int(m.group(1)) &#xa;                    if st_type > 2045:&#xa;                        if not self._quiet:&#xa;                            print(""string type > 2045; appending as strL"")&#xa;                        st_type = 32768&#xa;                init_st_type = st_type&#xa;            elif st_type in type_names:&#xa;                st_type = 65530 - type_names.index(st_type)&#xa;                init_st_type = st_type&#xa;            else:&#xa;                raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        elif (st_type not in (65530, 65529, 65528, 65527, 65526, 32768) &#xa;                and not (isinstance(st_type, int) and 1 <= st_type <= 2045)):&#xa;            raise TypeError(str(st_type) + "" is not a valid Stata type"")&#xa;        &#xa;        # Given iterable could be generator. Ensure it is in static form.&#xa;        values = [v for v in values]&#xa;        nvals = len(values)&#xa;        &#xa;        varvals = self._varvals&#xa;        &#xa;        if nvals == 0:&#xa;            this_missing = '' if st_type <= 32768 else MISSING&#xa;            for row in varvals:&#xa;                row.append(this_missing)&#xa;        else:&#xa;            alt_missing = False&#xa;            &#xa;            ismissing = self.ismissing&#xa;        &#xa;            for val, i in zip(values, range(nvals)):&#xa;                if st_type == 32768:&#xa;                    if any(isinstance(val, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        pass&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif (not isinstance(val, int) and &#xa;                            not isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        values[i] = str(val)&#xa;                elif st_type <= 2045:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        st_type = (32768 if val_len > 2045 &#xa;                                        else max(st_type, val_len))&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        st_type = 32768&#xa;                    elif (not isinstance(val, int) and &#xa;                            not isinstance(val, float)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = ''&#xa;                        alt_missing = True&#xa;                    else:&#xa;                        val = str(val)&#xa;                        val_len = len(val)&#xa;                        values[i] = val&#xa;                        st_type = (32768 if val_len > 2045 &#xa;                                        else max(st_type, val_len))&#xa;                else:&#xa;                    if isinstance(val, str):&#xa;                        max_len = len(val)&#xa;                        for j in range(i):&#xa;                            valj = values[j]&#xa;                            if ismissing(valj): &#xa;                                # If encountering a missing value here, &#xa;                                # should be instance of MissingValue.&#xa;                                # Could just check for that.&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                new_val = str(valj)&#xa;                                max_len = max(max_len, len(new_val))&#xa;                                values[j] = new_val&#xa;                        st_type = 32768 if max_len > 2045 else max_len&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        for j in range(i):&#xa;                            new_val = values[j]&#xa;                            if ismissing(new_val): &#xa;                                # all missing values already encountered &#xa;                                # should be instances of MissingValue, &#xa;                                # so could just check that&#xa;                                values[j] = ''&#xa;                                alt_missing = True&#xa;                            else:&#xa;                                values[j] = str(new_val)&#xa;                        st_type = 32768&#xa;                    elif val is None:&#xa;                        values[i] = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(val, float) and &#xa;                            not isinstance(val, int)):&#xa;                        msg = (""value in position {} has invalid "".format(i) +&#xa;                               ""type {}"".format(val.__class__.__name__))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        values[i] = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (val != int(val) or -2147483647 > val &#xa;                                or val > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            st_type = 65526 # double&#xa;                        elif st_type <= 65529 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val outside of bounds&#xa;                            st_type = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= val <= 100): &#xa;                            # st_type byte, but val outside of bounds&#xa;                            st_type = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)): &#xa;                            # st_type float, but outside of bounds&#xa;                            st_type = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                            &#xa;            if nvals < self._nobs:&#xa;                this_missing = '' if st_type <= 32768 else MISSING&#xa;                values += [this_missing]*(self._nobs - nvals)&#xa;            elif nvals > self._nobs:&#xa;                self.set_obs(nvals)&#xa;            &#xa;            for row, new_val in zip(varvals, values):&#xa;                row.append(new_val)&#xa;            &#xa;            if not self._quiet:&#xa;                smcl = ""{err}"" if IN_STATA else """"&#xa;                if init_st_type is not None and init_st_type != st_type:&#xa;                    st_type_name = self._get_type_name(st_type)&#xa;                    msg = (""warning: some values were incompatible with "" + &#xa;                           ""specified type;\n    type changed to "" + st_type_name)&#xa;                    print(smcl + msg)&#xa;                if alt_missing:&#xa;                    print(smcl + ""warning: some missing values inserted"")&#xa;            &#xa;        &#xa;        self._typlist.append(st_type)&#xa;        self._varlist.append(name)&#xa;        self._srtlist.append(None)&#xa;        self._fmtlist.append(&#xa;            '%' + str(max(9,st_type) if st_type <= 2045 else 9) + 's'&#xa;            if st_type <= 32768 else self._default_fmts[st_type])&#xa;        self._lbllist.append('')&#xa;        self._vlblist.append('')&#xa;        &#xa;        self._nvar += 1&#xa;        self._changed = True&#xa;        &#xa;    def _is_valid_varname(self, name):&#xa;        """"""Check to see if given str is a valid Stata name.&#xa;        Be sure to strip spaces before calling this function.&#xa;        &#xa;        """"""&#xa;        if name in RESERVED or re.match(r'^str([0-9]+|L)$', name): return False&#xa;        return True if VALID_NAME_RE.match(name) else False&#xa;        &#xa;    def _is_valid_fmt(self, fmt):&#xa;        """"""check that given str fmt is a valid Stata format""""""&#xa;        # make sure there is no leading or trailing whitespace&#xa;        fmt = fmt.strip()&#xa;        &#xa;        if fmt[0] != '%':&#xa;            return False&#xa;        &#xa;        # Handle business calendars first.&#xa;        # This does not check the calendar name.&#xa;        if fmt[1:3] == ""tb"" or fmt[1:4] == ""-tb"":&#xa;            return True if TB_FMT_RE.match(fmt) else False&#xa;            &#xa;        # date formats&#xa;        if fmt[1] == 't' or fmt[1:3] == '-t':&#xa;            return True if TIME_FMT_RE.match(fmt) else False&#xa;        &#xa;        # categorize using last character&#xa;        last_char = fmt[-1]&#xa;        if last_char == 's': # string&#xa;            m = STR_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width > 2045: return False&#xa;            return True&#xa;        elif last_char == 'H' or last_char == 'L': # binary&#xa;            # Valid binary formats are ^%(8|16)(H|L)$. Stata doesn't raise &#xa;            # error with -8 or -16, but the results are perhaps unexpected.&#xa;            return True if fmt[1:-1] in ('8', '16', '-8', '-16') else False&#xa;        elif last_char == 'x': # hexadecimal&#xa;            return True if fmt == '%21x' or fmt == '%-12x' else False&#xa;        elif last_char in {'f', 'g', 'e', 'c'}: # numeric&#xa;            m = NUM_FMT_RE.match(fmt)&#xa;            if not m: return False&#xa;            width = int(m.group(3))&#xa;            if width == 0 or width <= int(m.group(5)) or width > 2045: &#xa;                return False&#xa;            return True&#xa;            &#xa;        return False&#xa;        &#xa;    @property&#xa;    def width(self):&#xa;        """"""Width of an observation as saved.&#xa;        &#xa;        Returns&#xa;        -------&#xa;        int&#xa;            Width of a single observation in bytes.&#xa;        &#xa;        """"""&#xa;        widths = {65530: 1, 65529: 2, 65528: 4, 65527: 4, 65526: 8, 32768: 8}&#xa;        return sum([0] + [t if t <= 2045 else widths[t] &#xa;                   for t in self._typlist])&#xa;    &#xa;    def _set_values(self, sel_rows, sel_cols, value):&#xa;        """"""Replace the values in the obs and vars of the -index- tuple.&#xa;        The shape of -value- should match the shape implied by -index-, &#xa;        and sub-values should be consistent with the existing Stata &#xa;        types in the columns.&#xa;        &#xa;        """"""&#xa;        global get_missing&#xa;        &#xa;        varvals = self._varvals&#xa;        typlist = self._typlist&#xa;        varlist = self._varlist&#xa;        # copy typlist to check changes against later&#xa;        old_typlist = [typlist[i] for i in sel_cols] &#xa;        &#xa;        alt_missing = False&#xa;        &#xa;        for row_num, i in zip(sel_rows, range(len(sel_rows))):&#xa;            row = value[i]&#xa;            for col_num, k in zip(sel_cols, range(len(sel_cols))):&#xa;                val = row[k]&#xa;                st_type = typlist[col_num]&#xa;                if st_type == 32768:&#xa;                    if all(not isinstance(val, t) &#xa;                            for t in (str, bytes, bytearray)):&#xa;                        msg = (""values in \"""" + varlist[col_num] + &#xa;                               ""\"" must be str or bytes"")&#xa;                        raise TypeError(msg)&#xa;                elif st_type <= 2045:&#xa;                    if isinstance(val, str):&#xa;                        val_len = len(val)&#xa;                        typlist[col_num] = (32768 if val_len > 2045 &#xa;                                                 else max(st_type, val_len))&#xa;                    elif val is None or isinstance(val, MissingValue):&#xa;                        val = ''&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, bytes) or isinstance(val, bytearray):&#xa;                        typlist[col_num] = 32768&#xa;                    else:&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot "" + &#xa;                               ""take non-string values"")&#xa;                        raise TypeError(msg)&#xa;                else:&#xa;                    if any(isinstance(val, t) &#xa;                           for t in (str, bytes, bytearray)):&#xa;                        msg = (""\"""" + varlist[col_num] + ""\"" cannot take "" + &#xa;                               ""string or bytes values; has Stata type "" + &#xa;                               self._get_type_name(st_type))&#xa;                        raise TypeError(msg)&#xa;                    elif val is None:&#xa;                        val = MISSING&#xa;                        alt_missing = True&#xa;                    elif isinstance(val, MissingValue):&#xa;                        pass&#xa;                    elif (not isinstance(val, float) and &#xa;                            not isinstance(val, int)):&#xa;                        msg = (""value in right-hand position "" + &#xa;                               ""{},{} is not of recognized type"".format(i, k))&#xa;                        raise TypeError(msg)&#xa;                    elif (-1.7976931348623157e+308 > val or&#xa;                            val > 8.988465674311579e+307):&#xa;                        val = get_missing(val)&#xa;                        alt_missing = True&#xa;                    elif st_type >= 65528: # int types&#xa;                        if (val != int(val) or -2147483647 > val or &#xa;                                val > 2147483620): &#xa;                            # val is not int or is outside of bounds of long&#xa;                            typlist[col_num] = 65526 # double&#xa;                        elif st_type >= 65529 and not (-32767 <= val <= 32740):&#xa;                            # st_type int, but val outside of bounds&#xa;                            typlist[col_num] = 65528 # long&#xa;                        elif st_type == 65530 and not (-127 <= val <= 100): &#xa;                            # st_type byte, but val outside of bounds&#xa;                            typlist[col_num] = 65529 # int&#xa;                    else: # was float or double and will continue to be&#xa;                        if (st_type == 65527 and &#xa;                                (-1.7014117331926443e+38 > val or&#xa;                                 val > 1.7014117331926443e+38)): &#xa;                            # st_type float, but outisde of bounds&#xa;                            typlist[col_num] = 65526 # double&#xa;                            # This should maybe just set value to missing?&#xa;                            # Stata sets value to missing, &#xa;                            # does not promote float to double.&#xa;                            &#xa;                varvals[row_num][col_num] = val&#xa;            &#xa;        if not self._quiet: &#xa;            # Record seen columns. &#xa;            # Use a set because same column can appear multiple times.&#xa;            seen_cols = set()&#xa;            smcl = ""{txt}"" if IN_STATA else """"&#xa;            msg = smcl + ""Stata type for {} was {}, now {}""&#xa;            for old_type,c in zip(old_typlist, sel_cols):&#xa;                new_type = typlist[c]&#xa;                if old_type != new_type and c not in seen_cols:&#xa;                    old_name = self._get_type_name(old_type)&#xa;                    new_name = self._get_type_name(new_type)&#xa;                    print(msg.format(varlist[c], old_name, new_name))&#xa;                seen_cols.add(c)&#xa;            &#xa;            smcl = ""{err}"" if IN_STATA else """"&#xa;            if alt_missing:&#xa;                print(smcl + ""warning: some missing values inserted"")&#xa;        &#xa;    def _missing_save_val(self, miss_val, st_type):&#xa;        """"""helper function for writing dta files""""""&#xa;        diff = miss_val.index&#xa;        &#xa;        if st_type == 65530: # byte&#xa;            value = diff + 101&#xa;        elif st_type == 65529: # int&#xa;            value = diff + 32741&#xa;        elif st_type == 65528: # long&#xa;            value = diff + 2147483621&#xa;        elif st_type == 65527: # float&#xa;            value = float.fromhex('0x1.0' + hex(diff)[2:].zfill(2) + 'p+127')&#xa;        elif st_type == 65526: # double&#xa;            value = float.fromhex('0x1.0' + hex(diff)[2:].zfill(2) + 'p+1023')&#xa;        return value&#xa;            &#xa;    def _dta_obj_to_file(self, address):&#xa;        """"""save dta object to disk""""""&#xa;        global get_missing&#xa;        &#xa;        type_dict = {&#xa;            65530: ['b',1],&#xa;            65529: ['h',2],&#xa;            65528: ['l',4], &#xa;            65527: ['f',4],&#xa;            65526: ['d',8]&#xa;        }&#xa;        first_missing = {&#xa;            65530: 101,&#xa;            65529: 32741,&#xa;            65528: 2147483620,&#xa;            65527: float.fromhex('0x1.0p+127'),&#xa;            65526: float.fromhex('0x1.0p+1023')&#xa;        }&#xa;        typlist = self._typlist&#xa;        byteorder = self._byteorder&#xa;        nvar = self._nvar&#xa;                &#xa;        def write_value_label_table(labname, table):&#xa;            # Stata limits are a bit confusing.&#xa;            # Total length of text (incl. null terminators) must be <= 32000 ?&#xa;            # Total number of vals must be <= 65536 ?&#xa;            # But the limit on text length forces no. of vals <= 16000 since&#xa;            # each label must occupy at least two bytes &#xa;            # (including null terminator).&#xa;            labname = labname[:32]&#xa;            &#xa;            val = sorted(table.keys())&#xa;            # each value may be up to 81 chars including null&#xa;            txt = [table[v][:80] for v in val] &#xa;            &#xa;            nval = len(val)&#xa;            if nval > 65536: # max number of values allowed&#xa;                val = val[:65536]&#xa;                txt = txt[:65536]&#xa;                nval = 65536&#xa;            &#xa;            off = [0]&#xa;            for i in range(nval - 1):&#xa;                # in next line, ""+ 1"" to leave room for \0&#xa;                offset = off[i] + len(txt[i]) + 1&#xa;                if offset > 32000: # if too much text&#xa;                    off = off[:i] # cut off at before the ith one&#xa;                    val = val[:i]&#xa;                    txt = txt[:i]&#xa;                    nval = i&#xa;                    break&#xa;                off.append(offset)&#xa;            txt_len = off[-1] + len(txt[-1]) + 1&#xa;            &#xa;            table_len = 4 + 4 + 4*nval + 4*nval + txt_len&#xa;            &#xa;            dta.write(bytearray('<lbl>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + ""l"", table_len))&#xa;            dta.write(bytearray(labname, 'iso-8859-1') + &#xa;                      b'\0'*(33-len(labname)))&#xa;            dta.write(b'\x00\x00\x00')&#xa;            &#xa;            dta.write(pack(byteorder + ""l"", nval))&#xa;            dta.write(pack(byteorder + ""l"", txt_len))&#xa;            for o in off: dta.write(pack(byteorder + ""l"", o))&#xa;            for v in val: dta.write(pack(byteorder + ""l"", v))&#xa;            for t in txt: dta.write(bytearray(t, 'iso-8859-1') + b'\0')&#xa;            dta.write(bytearray('</lbl>', 'iso-8859-1'))&#xa;        &#xa;        with open(address, 'wb') as dta:&#xa;            dta.write(bytearray('<stata_dta>', 'iso-8859-1'))&#xa;            &#xa;            # header&#xa;            dta.write(bytearray('<header>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<release>', 'iso-8859-1'))&#xa;            dta.write(bytearray('117', 'iso-8859-1'))&#xa;            dta.write(bytearray('</release>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<byteorder>', 'iso-8859-1'))&#xa;            dta.write(&#xa;                bytearray('MSF' if byteorder == '>' else 'LSF', 'iso-8859-1'))&#xa;            dta.write(bytearray('</byteorder>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<K>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'H', self._nvar))&#xa;            dta.write(bytearray('</K>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<N>', 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'I', self._nobs))&#xa;            dta.write(bytearray('</N>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<label>', 'iso-8859-1'))&#xa;            label = self._data_label&#xa;            label_length = len(label)&#xa;            dta.write(pack(byteorder + 'B', label_length))&#xa;            dta.write(bytearray(label, 'iso-8859-1'))&#xa;            dta.write(bytearray('</label>', 'iso-8859-1'))&#xa;            dta.write(bytearray('<timestamp>', 'iso-8859-1'))&#xa;            stamp = self._time_stamp&#xa;            m = re.match(&#xa;                '^([ 0-3][0-9]) ' + &#xa;                '(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) ' + &#xa;                '[0-9]{4} ([ 0-2][0-9]):([0-9]{2})$', &#xa;                stamp)&#xa;            if (m and &#xa;                    1 <= int(m.group(1)) <= 31 and &#xa;                    0 <= int(m.group(3)) <= 24 and&#xa;                    0 <= int(m.group(4)) < 60):&#xa;                dta.write(pack(byteorder + 'B', 17))&#xa;                # next line includes optional binary zero&#xa;                dta.write(bytearray(stamp, 'iso-8859-1'))&#xa;            else: # there's something wrong with the time stamp, just skip it&#xa;                dta.write(pack(byteorder + 'B', 0))&#xa;            dta.write(bytearray('</timestamp>', 'iso-8859-1'))&#xa;            dta.write(bytearray('</header>', 'iso-8859-1'))&#xa;            &#xa;            # map&#xa;            offset_map = [0, dta.tell()]&#xa;            dta.write(bytearray(""<map>"", 'iso-8859-1'))&#xa;            for i in range(14):&#xa;                dta.write(pack(byteorder + 'Q', 0))&#xa;            dta.write(bytearray(""</map>"", ""iso-8859-1""))&#xa;            &#xa;            # variable types&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<variable_types>"", 'iso-8859-1'))&#xa;            dta.write(pack(byteorder + 'H'*nvar, *typlist))&#xa;            dta.write(bytearray(""</variable_types>"", 'iso-8859-1'))&#xa;            &#xa;            # variable names&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<varnames>"", 'iso-8859-1'))&#xa;            for name in self._varlist:&#xa;                name = name[:32]&#xa;                dta.write(bytearray(name, 'iso-8859-1') + b'\0'*(33-len(name)))&#xa;            dta.write(bytearray(""</varnames>"", 'iso-8859-1'))&#xa;            &#xa;            # sort order&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<sortlist>"", 'iso-8859-1'))&#xa;            srtlist = self._srtlist + [None]&#xa;            srtlist = [srt + 1 if srt is not None else 0 for srt in srtlist]&#xa;            dta.write(pack(byteorder + 'H'*(nvar + 1), *srtlist))&#xa;            dta.write(bytearray(""</sortlist>"", 'iso-8859-1'))&#xa;            &#xa;            # formats&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<formats>"", 'iso-8859-1'))&#xa;            for fmt in self._fmtlist:&#xa;                fmt = fmt[:48]&#xa;                dta.write(bytearray(fmt, 'iso-8859-1') + b'\0'*(49-len(fmt)))&#xa;            dta.write(bytearray(""</formats>"", 'iso-8859-1'))&#xa;            &#xa;            # value-label names&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<value_label_names>"", 'iso-8859-1'))&#xa;            for lab in self._lbllist:&#xa;                lab = lab[:32]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(33-len(lab)))&#xa;            dta.write(bytearray(""</value_label_names>"", 'iso-8859-1'))&#xa;            &#xa;            # variable labels&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<variable_labels>"", 'iso-8859-1'))&#xa;            for lab in self._vlblist:&#xa;                lab = lab[:80]&#xa;                dta.write(bytearray(lab, 'iso-8859-1') + b'\0'*(81-len(lab)))&#xa;            dta.write(bytearray(""</variable_labels>"", 'iso-8859-1'))&#xa;            &#xa;            # characteristics&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<characteristics>"", 'iso-8859-1'))&#xa;            chrdict = self._chrdict&#xa;            for varname in chrdict:&#xa;                varname = varname[:32]&#xa;                var_dict = chrdict[varname]&#xa;                for charname in var_dict:&#xa;                    charname = charname[:32]&#xa;                    char = var_dict[charname][:67784] # or 8681 for Small Stata&#xa;                    full_length = 66 + len(char) + 1 # +1 for null termination&#xa;                    &#xa;                    dta.write(bytearray('<ch>', 'iso-8859-1'))&#xa;                    dta.write(pack(byteorder + 'I', full_length))&#xa;                    dta.write(bytearray(varname, 'iso-8859-1') + &#xa;                              b'\0'*(33-len(varname)))&#xa;                    dta.write(bytearray(charname, 'iso-8859-1') + &#xa;                              b'\0'*(33-len(charname)))&#xa;                    dta.write(bytearray(char, 'iso-8859-1') + b'\0')&#xa;                    dta.write(bytearray('</ch>', 'iso-8859-1'))&#xa;            dta.write(bytearray(""</characteristics>"", 'iso-8859-1'))&#xa;            &#xa;            # data&#xa;            offset_map.append(dta.tell())&#xa;            strls = {}&#xa;            dta.write(bytearray(""<data>"", 'iso-8859-1'))&#xa;            varvals = self._varvals&#xa;            nvar, nobs = self._nvar, self._nobs&#xa;            missing_save_val = self._missing_save_val&#xa;            for i in range(nobs):&#xa;                row = varvals[i]&#xa;                for j in range(nvar):&#xa;                    value, st_type = row[j], typlist[j]&#xa;                    if st_type <= 2045:&#xa;                        value = value[:st_type]&#xa;                        dta.write(bytearray(value, 'iso-8859-1') + &#xa;                                  b'\0'*(st_type - len(value)))&#xa;                    elif st_type == 32768:&#xa;                        if value == """":&#xa;                            o,v = 0,0&#xa;                        elif value in strls:&#xa;                            o,v = strls[value]&#xa;                        else:&#xa;                            strls[value] = o,v = (i+1,j+1)&#xa;                        dta.write(pack(byteorder + 'II', v, o))&#xa;                    else:&#xa;                        fmt = 'bhlfd'[65530 - st_type]&#xa;                        if value is None:&#xa;                            value = first_missing[st_type]&#xa;                        elif isinstance(value, MissingValue):&#xa;                            value = missing_save_val(value, st_type)&#xa;                        elif (value > 8.988465674311579e+307 or &#xa;                                value < -1.7976931348623157e+308):&#xa;                            # is this the right way to handle this ?&#xa;                            value = missing_save_val(&#xa;                                get_missing(value), st_type)&#xa;                        dta.write(pack(byteorder + fmt, value))&#xa;            dta.write(bytearray(""</data>"", 'iso-8859-1'))&#xa;                &#xa;            # strls&#xa;            offset_map.append(dta.tell())&#xa;            strls = [(val, key) for key,val in strls.items()]&#xa;            strls.sort()&#xa;            dta.write(bytearray(""<strls>"", 'iso-8859-1'))&#xa;            for (o,v), value in strls:&#xa;                dta.write(bytearray('GSO', 'iso-8859-1'))&#xa;                dta.write(pack(byteorder + 'II', v, o))&#xa;                if isinstance(value, str):&#xa;                    try:&#xa;                        # expect error in next line if anywhere&#xa;                        value = bytes(value, 'iso-8859-1') + b'\x00'&#xa;                        dta.write(pack('B', 130))&#xa;                    except UnicodeEncodeError:&#xa;                        value = bytes(value, 'utf-8')&#xa;                        dta.write(pack('B', 129))&#xa;                elif (not isinstance(value, bytes) and &#xa;                        not isinstance(value, bytearray)):&#xa;                    msg = ""only bytes or str object allowed in Stata strl""&#xa;                    raise TypeError(msg)&#xa;                else:&#xa;                    dta.write(pack('B', 129))&#xa;                val_len = len(value)&#xa;                dta.write(pack(byteorder + 'I', val_len))&#xa;                num_vals = unpack(str(val_len) + 'b', value)&#xa;                dta.write(value)&#xa;            dta.write(bytearray(""</strls>"", 'iso-8859-1'))&#xa;            &#xa;            # value labels&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""<value_labels>"", 'iso-8859-1'))&#xa;            for name, table in self._vallabs.items():&#xa;                write_value_label_table(name, table)&#xa;            dta.write(bytearray(""</value_labels>"", 'iso-8859-1'))&#xa;            &#xa;            # end file&#xa;            offset_map.append(dta.tell())&#xa;            dta.write(bytearray(""</stata_dta>"", 'iso-8859-1'))&#xa;            &#xa;            offset_map.append(dta.tell())&#xa;            &#xa;            # write map&#xa;            dta.seek(offset_map[1] + 5)&#xa;            for offset in offset_map:&#xa;                dta.write(pack(byteorder + 'Q', offset))&#xa;&#xa;&#xa;def display_diff(dta1, dta2, all_data=False):&#xa;    """"""Display summary of differences between two Dta objects.&#xa;    &#xa;    Parameters&#xa;    ----------&#xa;    dta1 : Dta instance&#xa;    dta2 : Dta instance&#xa;    all_data : bool (or coercible to bool), optional&#xa;        Specify that all data values should be checked for&#xa;        equality, rather than stopping at first inequality. &#xa;        Default value is False.&#xa;    &#xa;    Returns&#xa;    -------&#xa;    None&#xa;    &#xa;    Side effects&#xa;    ------------&#xa;    Displays summary of differences.&#xa;    &#xa;    """"""&#xa;    if not isinstance(dta1, Dta) or not isinstance(dta2, Dta):&#xa;        raise TypeError(""objects to be compared must be Dta"")&#xa;    &#xa;    typlist_converters = {&#xa;        'Dta115': {&#xa;            'Dta117': lambda i: i if i <= 244 else 65530 + (251 - i)&#xa;        }&#xa;    }&#xa;    &#xa;    different = False&#xa;    &#xa;    # Python class types <-> dta version&#xa;    # ----------------------------------&#xa;    dta1_type, dta2_type = dta1.__class__.__name__, dta2.__class__.__name__&#xa;    if not dta1_type == dta2_type:&#xa;        different = True&#xa;        print(""    class types differ:"")&#xa;        print(""        {} vs {}"".format(dta1_type, dta2_type))&#xa;    &#xa;    # data set descriptors&#xa;    # --------------------&#xa;    if not dta1._ds_format == dta2._ds_format:&#xa;        different = True&#xa;        print(""    formats differ:"")&#xa;        print(""        {} vs {}"".format(dta1._ds_format, dta2._ds_format))&#xa;    &#xa;    if not dta1._data_label == dta2._data_label:&#xa;        different = True&#xa;        print(""    data labels differ:"")&#xa;        print(""        {} vs {}"".format(dta1._data_label, dta2._data_label))&#xa;    &#xa;    # time stamp&#xa;    # ----------&#xa;    stamp1 = dta1._time_stamp.split()&#xa;    stamp2 = dta2._time_stamp.split()&#xa;    stamp1[0] = int(stamp1[0]) #day&#xa;    stamp2[0] = int(stamp2[0])&#xa;    stamp1[2] = int(stamp1[2]) #year&#xa;    stamp2[2] = int(stamp2[2])&#xa;    stamp1 = stamp1[:-1] + [int(x) for x in stamp1[-1].split(':')]  # hr & min&#xa;    stamp2 = stamp2[:-1] + [int(x) for x in stamp2[-1].split(':')]&#xa;    if not stamp1 == stamp2:&#xa;        different = True&#xa;        print(""    time stamps differ:"")&#xa;        print(""        {} vs {}"".format(dta1._time_stamp, dta2._time_stamp))&#xa;    &#xa;    # number of variables and observations&#xa;    # ------------------------------------&#xa;    if not dta1._nvar == dta2._nvar:&#xa;        different = True&#xa;        print(""    # of vars differs:"")&#xa;        print(""        {} vs {}"".format(dta1._nvar, dta2._nvar))&#xa;        print(""   > comparison now limited to vars 0 .. min(nvar1, nvar2)"")&#xa;    &#xa;    if not dta1._nobs == dta2._nobs:&#xa;        different = True&#xa;        print(""    # of obs differs:"")&#xa;        print(""        {} vs {}"".format(dta1._nobs, dta2._nobs))&#xa;        print(""   > comparison now limited to obs 0 .. min(nobs1, nobs2)"")&#xa;        &#xa;    nvar = min(dta1._nvar, dta2._nvar)&#xa;    nobs = min(dta1._nobs, dta2._nobs)&#xa;    &#xa;    # descriptors&#xa;    # -----------&#xa;    &#xa;    # typlist&#xa;    # If dta versions are the same, can make direct comparison. If versions&#xa;    # are different, a direct comparison doesn't mean much if data types&#xa;    # are encoded differently, so convert one before comparing.&#xa;    if dta1_type == dta2_type:&#xa;        diff = [i for i in range(nvar) if dta1._typlist[i] != dta2._typlist[i]]&#xa;    else:&#xa;        s = sorted(((dta1_type, dta1), (dta2_type, dta2)))&#xa;        (older_type, older_dta), (newer_type, newer_dta) = s&#xa;        converter = typlist_converters[older_type][newer_type]&#xa;        diff = [i for i in range(nvar) &#xa;                if converter(older_dta._typlist[i]) != newer_dta._typlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    Stata data types differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;    &#xa;    # varlist&#xa;    diff = [i for i in range(nvar) if dta1._varlist[i] != dta2._varlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    variable names differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # srtlist&#xa;    diff = [i for i in range(nvar) if dta1._srtlist[i] != dta2._srtlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    sort lists differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;    &#xa;    # fmtlist&#xa;    diff = [i for i in range(nvar) if dta1._fmtlist[i] != dta2._fmtlist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    display formats differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # lbllist&#xa;    diff = [i for i in range(nvar) if dta1._lbllist[i] != dta2._lbllist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        msg = ""    attached value labels differ in {} places"".format(len(diff))&#xa;        print(msg)&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;        &#xa;    # vlblist&#xa;    diff = [i for i in range(nvar) if dta1._vlblist[i] != dta2._vlblist[i]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    variable labels differ in {} places"".format(len(diff)))&#xa;        print(""        first difference in position {}"".format(diff[0]))&#xa;      &#xa;    # characteristics&#xa;    # ---------------&#xa;    keys1 = set(dta1._chrdict.keys())&#xa;    keys2 = set(dta2._chrdict.keys())&#xa;    diff = keys1 - keys2&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    charataristic keys in #1 but not in #2:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = keys2 - keys1&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    charataristic keys in #2 but not in #1:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = [k for k in keys1.intersection(keys2) &#xa;                if dta1._chrdict[k] != dta2._chrdict[k]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    charataristic keys with different value:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    # defined value labels&#xa;    # --------------------&#xa;    keys1 = set(dta1._vallabs.keys())&#xa;    keys2 = set(dta2._vallabs.keys())&#xa;    diff = keys1 - keys2&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    value labels defined in #1 but not in #2:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = keys2 - keys1&#xa;    if diff != set():&#xa;        different = True&#xa;        print(""    value labels defined in #2 but not in #1:"")&#xa;        print(""       "", str(diff))&#xa;        &#xa;    diff = [k for k in keys1.intersection(keys2)&#xa;                if dta1._vallabs[k] != dta2._vallabs[k]]&#xa;    if diff != []:&#xa;        different = True&#xa;        print(""    value labels with same name but different mapping:"")&#xa;        print(""       "", str(diff))&#xa;    &#xa;    # data values&#xa;    # -----------&#xa;    if all_data:&#xa;        diff = sum([0] + [1 for i in range(nobs) for j in range(nvar)&#xa;                    if dta1._varvals[i][j] != dta2._varvals[i][j]])&#xa;        if diff != 0:&#xa;            different = True&#xa;            print(""    data values differ in "" + str(diff) + "" places"")&#xa;    else:&#xa;        for i in range(nobs):&#xa;            for j in range(nvar):&#xa;                if dta1._varvals[i][j] != dta2._varvals[i][j]:&#xa;                    different = True&#xa;                    print("""".join(&#xa;                        (""    data values differ\n        "",&#xa;                        ""first difference in position {},{}"".format(i,j))))&#xa;                    break&#xa;            else:&#xa;                continue  # executed if the loop ended normally (no break)&#xa;            break  # executed if 'continue' was skipped (break)&#xa;            # trick from http://stackoverflow.com/questions/653509 &#xa;            # to exit from nested for loops&#xa;&#xa;    if not different:&#xa;        print(""    no difference found"")&#xa;&#xa;def open_dta(address):&#xa;    """"""Open any recent version dta file (versions 114, 115, 117) .&#xa;    &#xa;    Parameters&#xa;    ----------&#xa;    Address of file, including file name and "".dta"".&#xa;    &#xa;    Returns&#xa;    -------&#xa;    Instance of sub-class of Dta, depending on dta file.&#xa;    &#xa;    """"""&#xa;    with open(address, 'rb') as dta_file:&#xa;        first_bytes = dta_file.read(11)&#xa;    ds_format = first_bytes[0]&#xa;    if isinstance(ds_format, str):  # happens in Python 2.7&#xa;        ds_format = ord(ds_format)&#xa;    # If format is 117, then first_bytes[0] is ""<"", which gets unpacked as 60.&#xa;    if ds_format == 114 or ds_format == 115:&#xa;        return Dta115(address)&#xa;    elif first_bytes.decode('iso-8859-1') == ""<stata_dta>"":&#xa;        return Dta117(address)&#xa;    else:&#xa;        raise ValueError(""only dta formats 117, 115, and 114 are supported"")&#xa;    &#xa;    "
568271|"""""""&#xa;    ***&#xa;    Modified generic daemon class&#xa;    ***&#xa;&#xa;    Author:     http://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/&#xa;                www.boxedice.com&#xa;                www.datadoghq.com&#xa;&#xa;    License:    http://creativecommons.org/licenses/by-sa/3.0/&#xa;""""""&#xa;&#xa;# Core modules&#xa;import atexit&#xa;import errno&#xa;import logging&#xa;import os&#xa;import signal&#xa;import sys&#xa;import time&#xa;&#xa;# 3p&#xa;from psutil import pid_exists&#xa;&#xa;log = logging.getLogger(__name__)&#xa;&#xa;class AgentSupervisor(object):&#xa;    ''' A simple supervisor to keep a restart a child on expected auto-restarts&#xa;    '''&#xa;    RESTART_EXIT_STATUS = 5&#xa;&#xa;    @classmethod&#xa;    def start(cls, parent_func, child_func=None):&#xa;        ''' `parent_func` is a function that's called every time the child&#xa;            process dies.&#xa;            `child_func` is a function that should be run by the forked child&#xa;            that will auto-restart with the RESTART_EXIT_STATUS.&#xa;        '''&#xa;        exit_code = cls.RESTART_EXIT_STATUS&#xa;&#xa;        # Allow the child process to die on SIGTERM&#xa;        signal.signal(signal.SIGTERM, cls._handle_sigterm)&#xa;&#xa;        cls.need_stop = False&#xa;&#xa;        while True:&#xa;            try:&#xa;                if hasattr(cls, 'child_pid'):&#xa;                    delattr(cls, 'child_pid')&#xa;                pid = os.fork()&#xa;                if pid > 0:&#xa;                    # The parent waits on the child.&#xa;                    cls.child_pid = pid&#xa;                    while not cls.need_stop:&#xa;                        cpid, status = os.waitpid(pid, os.WNOHANG)&#xa;                        if (cpid, status) != (0, 0):&#xa;                            break&#xa;                        time.sleep(1)&#xa;                    exit_code = status >> 8&#xa;                    if parent_func is not None:&#xa;                        parent_func()&#xa;&#xa;                    if cls.need_stop:&#xa;                        break&#xa;                else:&#xa;                    # The child will call our given function&#xa;                    if child_func is not None:&#xa;                        child_func()&#xa;                    else:&#xa;                        break&#xa;            except OSError, e:&#xa;                msg = ""Agent fork failed: %d (%s)"" % (e.errno, e.strerror)&#xa;                logging.error(msg)&#xa;                sys.stderr.write(msg + ""\n"")&#xa;                sys.exit(1)&#xa;&#xa;        # Exit from the parent cleanly&#xa;        if pid > 0:&#xa;            sys.exit(0)&#xa;&#xa;    @classmethod&#xa;    def _handle_sigterm(cls, signum, frame):&#xa;        # in the parent&#xa;        if hasattr(cls, 'child_pid'):&#xa;            os.kill(cls.child_pid, signal.SIGTERM)&#xa;            cls.need_stop = True&#xa;        # in the child&#xa;        else:&#xa;            sys.exit(0)&#xa;&#xa;&#xa;class Daemon(object):&#xa;    """"""&#xa;    A generic daemon class.&#xa;&#xa;    Usage: subclass the Daemon class and override the run() method&#xa;    """"""&#xa;    def __init__(self, pidfile, stdin=os.devnull, stdout=os.devnull, stderr=os.devnull, autorestart=False):&#xa;        self.autorestart = autorestart&#xa;        self.stdin = stdin&#xa;        self.stdout = stdout&#xa;        self.stderr = stderr&#xa;        self.pidfile = pidfile&#xa;&#xa;    def daemonize(self):&#xa;        """"""&#xa;        Do the UNIX double-fork magic, see Stevens' ""Advanced&#xa;        Programming in the UNIX Environment"" for details (ISBN 0201563177)&#xa;        http://www.erlenstar.demon.co.uk/unix/faq_2.html#SEC16&#xa;        """"""&#xa;        try:&#xa;            pid = os.fork()&#xa;            if pid > 0:&#xa;                # Exit first parent&#xa;                sys.exit(0)&#xa;        except OSError, e:&#xa;            msg = ""fork #1 failed: %d (%s)"" % (e.errno, e.strerror)&#xa;            log.error(msg)&#xa;            sys.stderr.write(msg + ""\n"")&#xa;            sys.exit(1)&#xa;&#xa;        log.debug(""Fork 1 ok"")&#xa;&#xa;        # Decouple from parent environment&#xa;        os.chdir(""/"")&#xa;        os.setsid()&#xa;&#xa;        if self.autorestart:&#xa;            # Set up the supervisor callbacks and put a fork in it.&#xa;            logging.info('Running with auto-restart ON')&#xa;            AgentSupervisor.start(parent_func=None, child_func=None)&#xa;        else:&#xa;            # Do second fork&#xa;            try:&#xa;                pid = os.fork()&#xa;                if pid > 0:&#xa;                    # Exit from second parent&#xa;                    sys.exit(0)&#xa;            except OSError, e:&#xa;                msg = ""fork #2 failed: %d (%s)"" % (e.errno, e.strerror)&#xa;                logging.error(msg)&#xa;                sys.stderr.write(msg + ""\n"")&#xa;                sys.exit(1)&#xa;&#xa;        if sys.platform != 'darwin': # This block breaks on OS X&#xa;            # Redirect standard file descriptors&#xa;            sys.stdout.flush()&#xa;            sys.stderr.flush()&#xa;            si = file(self.stdin, 'r')&#xa;            so = file(self.stdout, 'a+')&#xa;            se = file(self.stderr, 'a+', 0)&#xa;            os.dup2(si.fileno(), sys.stdin.fileno())&#xa;            os.dup2(so.fileno(), sys.stdout.fileno())&#xa;            os.dup2(se.fileno(), sys.stderr.fileno())&#xa;&#xa;        log.info(""Daemon started"")&#xa;&#xa;&#xa;    def start(self, foreground=False):&#xa;        log.info(""Starting"")&#xa;        pid = self.pid()&#xa;&#xa;        if pid:&#xa;            # Check if the pid in the pidfile corresponds to a running process&#xa;            if pid_exists(pid):&#xa;                log.error(""Not starting, another instance is already running""&#xa;                          "" (using pidfile {0})"".format(self.pidfile))&#xa;                sys.exit(1)&#xa;            else:&#xa;                log.warn('pidfile contains the pid of a stopped process.'&#xa;                         ' Starting normally')&#xa;&#xa;        log.info(""Pidfile: %s"" % self.pidfile)&#xa;        if not foreground:&#xa;            self.daemonize()&#xa;        self.write_pidfile()&#xa;        self.run()&#xa;&#xa;&#xa;    def stop(self):&#xa;        log.info(""Stopping daemon"")&#xa;        pid = self.pid()&#xa;&#xa;        # Clear the pid file&#xa;        if os.path.exists(self.pidfile):&#xa;            os.remove(self.pidfile)&#xa;&#xa;        if pid > 1:&#xa;            try:&#xa;                if self.autorestart:&#xa;                    # Try killing the supervising process&#xa;                    try:&#xa;                        os.kill(os.getpgid(pid), signal.SIGTERM)&#xa;                    except OSError:&#xa;                        log.warn(""Couldn't not kill parent pid %s. Killing pid."" % os.getpgid(pid))&#xa;                        os.kill(pid, signal.SIGTERM)&#xa;                else:&#xa;                    # No supervising process present&#xa;                    os.kill(pid, signal.SIGTERM)&#xa;                log.info(""Daemon is stopped"")&#xa;            except OSError, err:&#xa;                if str(err).find(""No such process"") <= 0:&#xa;                    log.exception(""Cannot kill Agent daemon at pid %s"" % pid)&#xa;                    sys.stderr.write(str(err) + ""\n"")&#xa;        else:&#xa;            message = ""Pidfile %s does not exist. Not running?\n"" % self.pidfile&#xa;            log.info(message)&#xa;            sys.stderr.write(message)&#xa;&#xa;            # A ValueError might occur if the PID file is empty but does actually exist&#xa;            if os.path.exists(self.pidfile):&#xa;                os.remove(self.pidfile)&#xa;&#xa;            return # Not an error in a restart&#xa;&#xa;&#xa;    def restart(self):&#xa;        ""Restart the daemon""&#xa;        self.stop()&#xa;        self.start()&#xa;&#xa;&#xa;    def run(self):&#xa;        """"""&#xa;        You should override this method when you subclass Daemon. It will be called after the process has been&#xa;        daemonized by start() or restart().&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;    @classmethod&#xa;    def info(cls):&#xa;        """"""&#xa;        You should override this method when you subclass Daemon. It will be&#xa;        called to provide information about the status of the process&#xa;        """"""&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;    def status(self):&#xa;        """"""&#xa;        Get the status of the daemon. Exits with 0 if running, 1 if not.&#xa;        """"""&#xa;        pid = self.pid()&#xa;&#xa;        if pid < 0:&#xa;            message = '%s is not running' % self.__class__.__name__&#xa;            exit_code = 1&#xa;        else:&#xa;            # Check for the existence of a process with the pid&#xa;            try:&#xa;                # os.kill(pid, 0) will raise an OSError exception if the process&#xa;                # does not exist, or if access to the process is denied (access denied will be an EPERM error).&#xa;                # If we get an OSError that isn't an EPERM error, the process&#xa;                # does not exist.&#xa;                # (from http://stackoverflow.com/questions/568271/check-if-pid-is-not-in-use-in-python,&#xa;                #  Giampaolo's answer)&#xa;                os.kill(pid, 0)&#xa;            except OSError, e:&#xa;                if e.errno != errno.EPERM:&#xa;                    message = '%s pidfile contains pid %s, but no running process could be found' % (self.__class__.__name__, pid)&#xa;                else:&#xa;                    message = 'You do not have sufficient permissions'&#xa;                exit_code = 1&#xa;&#xa;            else:&#xa;                message = '%s is running with pid %s' % (self.__class__.__name__, pid)&#xa;                exit_code = 0&#xa;&#xa;        log.info(message)&#xa;        sys.stdout.write(message + ""\n"")&#xa;        sys.exit(exit_code)&#xa;&#xa;&#xa;    def pid(self):&#xa;        # Get the pid from the pidfile&#xa;        try:&#xa;            pf = file(self.pidfile, 'r')&#xa;            pid = int(pf.read().strip())&#xa;            pf.close()&#xa;            return pid&#xa;        except IOError:&#xa;            return None&#xa;        except ValueError:&#xa;            return None&#xa;&#xa;&#xa;    def write_pidfile(self):&#xa;        # Write pidfile&#xa;        atexit.register(self.delpid) # Make sure pid file is removed if we quit&#xa;        pid = str(os.getpid())&#xa;        try:&#xa;            fp = open(self.pidfile, 'w+')&#xa;            fp.write(str(pid))&#xa;            fp.close()&#xa;            os.chmod(self.pidfile, 0644)&#xa;        except Exception, e:&#xa;            msg = ""Unable to write pidfile: %s"" % self.pidfile&#xa;            log.exception(msg)&#xa;            sys.stderr.write(msg + ""\n"")&#xa;            sys.exit(1)&#xa;&#xa;&#xa;    def delpid(self):&#xa;        try:&#xa;            os.remove(self.pidfile)&#xa;        except OSError:&#xa;            pass&#xa;"
267399|"# module pyparsing.py&#xa;#&#xa;# Copyright (c) 2003-2016  Paul T. McGuire&#xa;#&#xa;# Permission is hereby granted, free of charge, to any person obtaining&#xa;# a copy of this software and associated documentation files (the&#xa;# ""Software""), to deal in the Software without restriction, including&#xa;# without limitation the rights to use, copy, modify, merge, publish,&#xa;# distribute, sublicense, and/or sell copies of the Software, and to&#xa;# permit persons to whom the Software is furnished to do so, subject to&#xa;# the following conditions:&#xa;#&#xa;# The above copyright notice and this permission notice shall be&#xa;# included in all copies or substantial portions of the Software.&#xa;#&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,&#xa;# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF&#xa;# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.&#xa;# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY&#xa;# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,&#xa;# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE&#xa;# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;#&#xa;&#xa;__doc__ = \&#xa;""""""&#xa;pyparsing module - Classes and methods to define and execute parsing grammars&#xa;&#xa;The pyparsing module is an alternative approach to creating and executing simple grammars,&#xa;vs. the traditional lex/yacc approach, or the use of regular expressions.  With pyparsing, you&#xa;don't need to learn a new syntax for defining grammars or matching expressions - the parsing module&#xa;provides a library of classes that you use to construct the grammar directly in Python.&#xa;&#xa;Here is a program to parse ""Hello, World!"" (or any greeting of the form &#xa;C{""<salutation>, <addressee>!""}), built up using L{Word}, L{Literal}, and L{And} elements &#xa;(L{'+'<ParserElement.__add__>} operator gives L{And} expressions, strings are auto-converted to&#xa;L{Literal} expressions)::&#xa;&#xa;    from pyparsing import Word, alphas&#xa;&#xa;    # define grammar of a greeting&#xa;    greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;&#xa;    hello = ""Hello, World!""&#xa;    print (hello, ""->"", greet.parseString(hello))&#xa;&#xa;The program outputs the following::&#xa;&#xa;    Hello, World! -> ['Hello', ',', 'World', '!']&#xa;&#xa;The Python representation of the grammar is quite readable, owing to the self-explanatory&#xa;class names, and the use of '+', '|' and '^' operators.&#xa;&#xa;The L{ParseResults} object returned from L{ParserElement.parseString<ParserElement.parseString>} can be accessed as a nested list, a dictionary, or an&#xa;object with named attributes.&#xa;&#xa;The pyparsing module handles some of the problems that are typically vexing when writing text parsers:&#xa; - extra or missing whitespace (the above program will also handle ""Hello,World!"", ""Hello  ,  World  !"", etc.)&#xa; - quoted strings&#xa; - embedded comments&#xa;""""""&#xa;&#xa;__version__ = ""2.1.10""&#xa;__versionTime__ = ""07 Oct 2016 01:31 UTC""&#xa;__author__ = ""Paul McGuire <ptmcg@users.sourceforge.net>""&#xa;&#xa;import string&#xa;from weakref import ref as wkref&#xa;import copy&#xa;import sys&#xa;import warnings&#xa;import re&#xa;import sre_constants&#xa;import collections&#xa;import pprint&#xa;import traceback&#xa;import types&#xa;from datetime import datetime&#xa;&#xa;try:&#xa;    from _thread import RLock&#xa;except ImportError:&#xa;    from threading import RLock&#xa;&#xa;try:&#xa;    from collections import OrderedDict as _OrderedDict&#xa;except ImportError:&#xa;    try:&#xa;        from ordereddict import OrderedDict as _OrderedDict&#xa;    except ImportError:&#xa;        _OrderedDict = None&#xa;&#xa;#~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )&#xa;&#xa;__all__ = [&#xa;'And', 'CaselessKeyword', 'CaselessLiteral', 'CharsNotIn', 'Combine', 'Dict', 'Each', 'Empty',&#xa;'FollowedBy', 'Forward', 'GoToColumn', 'Group', 'Keyword', 'LineEnd', 'LineStart', 'Literal',&#xa;'MatchFirst', 'NoMatch', 'NotAny', 'OneOrMore', 'OnlyOnce', 'Optional', 'Or',&#xa;'ParseBaseException', 'ParseElementEnhance', 'ParseException', 'ParseExpression', 'ParseFatalException',&#xa;'ParseResults', 'ParseSyntaxException', 'ParserElement', 'QuotedString', 'RecursiveGrammarException',&#xa;'Regex', 'SkipTo', 'StringEnd', 'StringStart', 'Suppress', 'Token', 'TokenConverter', &#xa;'White', 'Word', 'WordEnd', 'WordStart', 'ZeroOrMore',&#xa;'alphanums', 'alphas', 'alphas8bit', 'anyCloseTag', 'anyOpenTag', 'cStyleComment', 'col',&#xa;'commaSeparatedList', 'commonHTMLEntity', 'countedArray', 'cppStyleComment', 'dblQuotedString',&#xa;'dblSlashComment', 'delimitedList', 'dictOf', 'downcaseTokens', 'empty', 'hexnums',&#xa;'htmlComment', 'javaStyleComment', 'line', 'lineEnd', 'lineStart', 'lineno',&#xa;'makeHTMLTags', 'makeXMLTags', 'matchOnlyAtCol', 'matchPreviousExpr', 'matchPreviousLiteral',&#xa;'nestedExpr', 'nullDebugAction', 'nums', 'oneOf', 'opAssoc', 'operatorPrecedence', 'printables',&#xa;'punc8bit', 'pythonStyleComment', 'quotedString', 'removeQuotes', 'replaceHTMLEntity', &#xa;'replaceWith', 'restOfLine', 'sglQuotedString', 'srange', 'stringEnd',&#xa;'stringStart', 'traceParseAction', 'unicodeString', 'upcaseTokens', 'withAttribute',&#xa;'indentedBlock', 'originalTextFor', 'ungroup', 'infixNotation','locatedExpr', 'withClass',&#xa;'CloseMatch', 'tokenMap', 'pyparsing_common',&#xa;]&#xa;&#xa;system_version = tuple(sys.version_info)[:3]&#xa;PY_3 = system_version[0] == 3&#xa;if PY_3:&#xa;    _MAX_INT = sys.maxsize&#xa;    basestring = str&#xa;    unichr = chr&#xa;    _ustr = str&#xa;&#xa;    # build list of single arg builtins, that can be used as parse actions&#xa;    singleArgBuiltins = [sum, len, sorted, reversed, list, tuple, set, any, all, min, max]&#xa;&#xa;else:&#xa;    _MAX_INT = sys.maxint&#xa;    range = xrange&#xa;&#xa;    def _ustr(obj):&#xa;        """"""Drop-in replacement for str(obj) that tries to be Unicode friendly. It first tries&#xa;           str(obj). If that fails with a UnicodeEncodeError, then it tries unicode(obj). It&#xa;           then < returns the unicode object | encodes it with the default encoding | ... >.&#xa;        """"""&#xa;        if isinstance(obj,unicode):&#xa;            return obj&#xa;&#xa;        try:&#xa;            # If this works, then _ustr(obj) has the same behaviour as str(obj), so&#xa;            # it won't break any existing code.&#xa;            return str(obj)&#xa;&#xa;        except UnicodeEncodeError:&#xa;            # Else encode it&#xa;            ret = unicode(obj).encode(sys.getdefaultencoding(), 'xmlcharrefreplace')&#xa;            xmlcharref = Regex('&#\d+;')&#xa;            xmlcharref.setParseAction(lambda t: '\\u' + hex(int(t[0][2:-1]))[2:])&#xa;            return xmlcharref.transformString(ret)&#xa;&#xa;    # build list of single arg builtins, tolerant of Python version, that can be used as parse actions&#xa;    singleArgBuiltins = []&#xa;    import __builtin__&#xa;    for fname in ""sum len sorted reversed list tuple set any all min max"".split():&#xa;        try:&#xa;            singleArgBuiltins.append(getattr(__builtin__,fname))&#xa;        except AttributeError:&#xa;            continue&#xa;            &#xa;_generatorType = type((y for y in range(1)))&#xa; &#xa;def _xml_escape(data):&#xa;    """"""Escape &, <, >, "", ', etc. in a string of data.""""""&#xa;&#xa;    # ampersand must be replaced first&#xa;    from_symbols = '&><""\''&#xa;    to_symbols = ('&'+s+';' for s in ""amp gt lt quot apos"".split())&#xa;    for from_,to_ in zip(from_symbols, to_symbols):&#xa;        data = data.replace(from_, to_)&#xa;    return data&#xa;&#xa;class _Constants(object):&#xa;    pass&#xa;&#xa;alphas     = string.ascii_uppercase + string.ascii_lowercase&#xa;nums       = ""0123456789""&#xa;hexnums    = nums + ""ABCDEFabcdef""&#xa;alphanums  = alphas + nums&#xa;_bslash    = chr(92)&#xa;printables = """".join(c for c in string.printable if c not in string.whitespace)&#xa;&#xa;class ParseBaseException(Exception):&#xa;    """"""base exception class for all parsing runtime exceptions""""""&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, pstr, loc=0, msg=None, elem=None ):&#xa;        self.loc = loc&#xa;        if msg is None:&#xa;            self.msg = pstr&#xa;            self.pstr = """"&#xa;        else:&#xa;            self.msg = msg&#xa;            self.pstr = pstr&#xa;        self.parserElement = elem&#xa;        self.args = (pstr, loc, msg)&#xa;&#xa;    @classmethod&#xa;    def _from_exception(cls, pe):&#xa;        """"""&#xa;        internal factory method to simplify creating one type of ParseException &#xa;        from another - avoids having __init__ signature conflicts among subclasses&#xa;        """"""&#xa;        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)&#xa;&#xa;    def __getattr__( self, aname ):&#xa;        """"""supported attributes by name are:&#xa;            - lineno - returns the line number of the exception text&#xa;            - col - returns the column number of the exception text&#xa;            - line - returns the line containing the exception text&#xa;        """"""&#xa;        if( aname == ""lineno"" ):&#xa;            return lineno( self.loc, self.pstr )&#xa;        elif( aname in (""col"", ""column"") ):&#xa;            return col( self.loc, self.pstr )&#xa;        elif( aname == ""line"" ):&#xa;            return line( self.loc, self.pstr )&#xa;        else:&#xa;            raise AttributeError(aname)&#xa;&#xa;    def __str__( self ):&#xa;        return ""%s (at char %d), (line:%d, col:%d)"" % \&#xa;                ( self.msg, self.loc, self.lineno, self.column )&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;    def markInputline( self, markerString = "">!<"" ):&#xa;        """"""Extracts the exception line from the input string, and marks&#xa;           the location of the exception with a special symbol.&#xa;        """"""&#xa;        line_str = self.line&#xa;        line_column = self.column - 1&#xa;        if markerString:&#xa;            line_str = """".join((line_str[:line_column],&#xa;                                markerString, line_str[line_column:]))&#xa;        return line_str.strip()&#xa;    def __dir__(self):&#xa;        return ""lineno col line"".split() + dir(type(self))&#xa;&#xa;class ParseException(ParseBaseException):&#xa;    """"""&#xa;    Exception thrown when parse expressions don't match class;&#xa;    supported attributes by name are:&#xa;     - lineno - returns the line number of the exception text&#xa;     - col - returns the column number of the exception text&#xa;     - line - returns the line containing the exception text&#xa;        &#xa;    Example::&#xa;        try:&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")&#xa;        except ParseException as pe:&#xa;            print(pe)&#xa;            print(""column: {}"".format(pe.col))&#xa;            &#xa;    prints::&#xa;       Expected integer (at char 0), (line:1, col:1)&#xa;        column: 1&#xa;    """"""&#xa;    pass&#xa;&#xa;class ParseFatalException(ParseBaseException):&#xa;    """"""user-throwable exception thrown when inconsistent parse content&#xa;       is found; stops all parsing immediately""""""&#xa;    pass&#xa;&#xa;class ParseSyntaxException(ParseFatalException):&#xa;    """"""just like L{ParseFatalException}, but thrown internally when an&#xa;       L{ErrorStop<And._ErrorStop>} ('-' operator) indicates that parsing is to stop &#xa;       immediately because an unbacktrackable syntax error has been found""""""&#xa;    pass&#xa;&#xa;#~ class ReparseException(ParseBaseException):&#xa;    #~ """"""Experimental class - parse actions can raise this exception to cause&#xa;       #~ pyparsing to reparse the input string:&#xa;        #~ - with a modified input string, and/or&#xa;        #~ - with a modified start location&#xa;       #~ Set the values of the ReparseException in the constructor, and raise the&#xa;       #~ exception in a parse action to cause pyparsing to use the new string/location.&#xa;       #~ Setting the values as None causes no change to be made.&#xa;       #~ """"""&#xa;    #~ def __init_( self, newstring, restartLoc ):&#xa;        #~ self.newParseText = newstring&#xa;        #~ self.reparseLoc = restartLoc&#xa;&#xa;class RecursiveGrammarException(Exception):&#xa;    """"""exception thrown by L{ParserElement.validate} if the grammar could be improperly recursive""""""&#xa;    def __init__( self, parseElementList ):&#xa;        self.parseElementTrace = parseElementList&#xa;&#xa;    def __str__( self ):&#xa;        return ""RecursiveGrammarException: %s"" % self.parseElementTrace&#xa;&#xa;class _ParseResultsWithOffset(object):&#xa;    def __init__(self,p1,p2):&#xa;        self.tup = (p1,p2)&#xa;    def __getitem__(self,i):&#xa;        return self.tup[i]&#xa;    def __repr__(self):&#xa;        return repr(self.tup[0])&#xa;    def setOffset(self,i):&#xa;        self.tup = (self.tup[0],i)&#xa;&#xa;class ParseResults(object):&#xa;    """"""&#xa;    Structured parse results, to provide multiple means of access to the parsed data:&#xa;       - as a list (C{len(results)})&#xa;       - by list index (C{results[0], results[1]}, etc.)&#xa;       - by attribute (C{results.<resultsName>} - see L{ParserElement.setResultsName})&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;        # equivalent form:&#xa;        # date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;&#xa;        # parseString returns a ParseResults object&#xa;        result = date_str.parseString(""1999/12/31"")&#xa;&#xa;        def test(s, fn=repr):&#xa;            print(""%s -> %s"" % (s, fn(eval(s))))&#xa;        test(""list(result)"")&#xa;        test(""result[0]"")&#xa;        test(""result['month']"")&#xa;        test(""result.day"")&#xa;        test(""'month' in result"")&#xa;        test(""'minutes' in result"")&#xa;        test(""result.dump()"", str)&#xa;    prints::&#xa;        list(result) -> ['1999', '/', '12', '/', '31']&#xa;        result[0] -> '1999'&#xa;        result['month'] -> '12'&#xa;        result.day -> '31'&#xa;        'month' in result -> True&#xa;        'minutes' in result -> False&#xa;        result.dump() -> ['1999', '/', '12', '/', '31']&#xa;        - day: 31&#xa;        - month: 12&#xa;        - year: 1999&#xa;    """"""&#xa;    def __new__(cls, toklist=None, name=None, asList=True, modal=True ):&#xa;        if isinstance(toklist, cls):&#xa;            return toklist&#xa;        retobj = object.__new__(cls)&#xa;        retobj.__doinit = True&#xa;        return retobj&#xa;&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance ):&#xa;        if self.__doinit:&#xa;            self.__doinit = False&#xa;            self.__name = None&#xa;            self.__parent = None&#xa;            self.__accumNames = {}&#xa;            self.__asList = asList&#xa;            self.__modal = modal&#xa;            if toklist is None:&#xa;                toklist = []&#xa;            if isinstance(toklist, list):&#xa;                self.__toklist = toklist[:]&#xa;            elif isinstance(toklist, _generatorType):&#xa;                self.__toklist = list(toklist)&#xa;            else:&#xa;                self.__toklist = [toklist]&#xa;            self.__tokdict = dict()&#xa;&#xa;        if name is not None and name:&#xa;            if not modal:&#xa;                self.__accumNames[name] = 0&#xa;            if isinstance(name,int):&#xa;                name = _ustr(name) # will always return a str, but use _ustr for consistency&#xa;            self.__name = name&#xa;            if not (isinstance(toklist, (type(None), basestring, list)) and toklist in (None,'',[])):&#xa;                if isinstance(toklist,basestring):&#xa;                    toklist = [ toklist ]&#xa;                if asList:&#xa;                    if isinstance(toklist,ParseResults):&#xa;                        self[name] = _ParseResultsWithOffset(toklist.copy(),0)&#xa;                    else:&#xa;                        self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]),0)&#xa;                    self[name].__name = name&#xa;                else:&#xa;                    try:&#xa;                        self[name] = toklist[0]&#xa;                    except (KeyError,TypeError,IndexError):&#xa;                        self[name] = toklist&#xa;&#xa;    def __getitem__( self, i ):&#xa;        if isinstance( i, (int,slice) ):&#xa;            return self.__toklist[i]&#xa;        else:&#xa;            if i not in self.__accumNames:&#xa;                return self.__tokdict[i][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[i] ])&#xa;&#xa;    def __setitem__( self, k, v, isinstance=isinstance ):&#xa;        if isinstance(v,_ParseResultsWithOffset):&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]&#xa;            sub = v[0]&#xa;        elif isinstance(k,(int,slice)):&#xa;            self.__toklist[k] = v&#xa;            sub = v&#xa;        else:&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [_ParseResultsWithOffset(v,0)]&#xa;            sub = v&#xa;        if isinstance(sub,ParseResults):&#xa;            sub.__parent = wkref(self)&#xa;&#xa;    def __delitem__( self, i ):&#xa;        if isinstance(i,(int,slice)):&#xa;            mylen = len( self.__toklist )&#xa;            del self.__toklist[i]&#xa;&#xa;            # convert int to slice&#xa;            if isinstance(i, int):&#xa;                if i < 0:&#xa;                    i += mylen&#xa;                i = slice(i, i+1)&#xa;            # get removed indices&#xa;            removed = list(range(*i.indices(mylen)))&#xa;            removed.reverse()&#xa;            # fixup indices in token dictionary&#xa;            for name,occurrences in self.__tokdict.items():&#xa;                for j in removed:&#xa;                    for k, (value, position) in enumerate(occurrences):&#xa;                        occurrences[k] = _ParseResultsWithOffset(value, position - (position > j))&#xa;        else:&#xa;            del self.__tokdict[i]&#xa;&#xa;    def __contains__( self, k ):&#xa;        return k in self.__tokdict&#xa;&#xa;    def __len__( self ): return len( self.__toklist )&#xa;    def __bool__(self): return ( not not self.__toklist )&#xa;    __nonzero__ = __bool__&#xa;    def __iter__( self ): return iter( self.__toklist )&#xa;    def __reversed__( self ): return iter( self.__toklist[::-1] )&#xa;    def _iterkeys( self ):&#xa;        if hasattr(self.__tokdict, ""iterkeys""):&#xa;            return self.__tokdict.iterkeys()&#xa;        else:&#xa;            return iter(self.__tokdict)&#xa;&#xa;    def _itervalues( self ):&#xa;        return (self[k] for k in self._iterkeys())&#xa;            &#xa;    def _iteritems( self ):&#xa;        return ((k, self[k]) for k in self._iterkeys())&#xa;&#xa;    if PY_3:&#xa;        keys = _iterkeys       &#xa;        """"""Returns an iterator of all named result keys (Python 3.x only).""""""&#xa;&#xa;        values = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 3.x only).""""""&#xa;&#xa;        items = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 3.x only).""""""&#xa;&#xa;    else:&#xa;        iterkeys = _iterkeys&#xa;        """"""Returns an iterator of all named result keys (Python 2.x only).""""""&#xa;&#xa;        itervalues = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 2.x only).""""""&#xa;&#xa;        iteritems = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 2.x only).""""""&#xa;&#xa;        def keys( self ):&#xa;            """"""Returns all named result keys (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iterkeys())&#xa;&#xa;        def values( self ):&#xa;            """"""Returns all named result values (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.itervalues())&#xa;                &#xa;        def items( self ):&#xa;            """"""Returns all named result key-values (as a list of tuples in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iteritems())&#xa;&#xa;    def haskeys( self ):&#xa;        """"""Since keys() returns an iterator, this method is helpful in bypassing&#xa;           code that looks for the existence of any defined results names.""""""&#xa;        return bool(self.__tokdict)&#xa;        &#xa;    def pop( self, *args, **kwargs):&#xa;        """"""&#xa;        Removes and returns item at specified index (default=C{last}).&#xa;        Supports both C{list} and C{dict} semantics for C{pop()}. If passed no&#xa;        argument or an integer argument, it will use C{list} semantics&#xa;        and pop tokens from the list of parsed tokens. If passed a &#xa;        non-integer argument (most likely a string), it will use C{dict}&#xa;        semantics and pop the corresponding value from any defined &#xa;        results names. A second default return value argument is &#xa;        supported, just as in C{dict.pop()}.&#xa;&#xa;        Example::&#xa;            def remove_first(tokens):&#xa;                tokens.pop(0)&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(""0 123 321"")) # -> ['123', '321']&#xa;&#xa;            label = Word(alphas)&#xa;            patt = label(""LABEL"") + OneOrMore(Word(nums))&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;&#xa;            # Use pop() in a parse action to remove named result (note that corresponding value is not&#xa;            # removed from list form of results)&#xa;            def remove_LABEL(tokens):&#xa;                tokens.pop(""LABEL"")&#xa;                return tokens&#xa;            patt.addParseAction(remove_LABEL)&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;        prints::&#xa;            ['AAB', '123', '321']&#xa;            - LABEL: AAB&#xa;&#xa;            ['AAB', '123', '321']&#xa;        """"""&#xa;        if not args:&#xa;            args = [-1]&#xa;        for k,v in kwargs.items():&#xa;            if k == 'default':&#xa;                args = (args[0], v)&#xa;            else:&#xa;                raise TypeError(""pop() got an unexpected keyword argument '%s'"" % k)&#xa;        if (isinstance(args[0], int) or &#xa;                        len(args) == 1 or &#xa;                        args[0] in self):&#xa;            index = args[0]&#xa;            ret = self[index]&#xa;            del self[index]&#xa;            return ret&#xa;        else:&#xa;            defaultvalue = args[1]&#xa;            return defaultvalue&#xa;&#xa;    def get(self, key, defaultValue=None):&#xa;        """"""&#xa;        Returns named result matching the given key, or if there is no&#xa;        such name, then returns the given C{defaultValue} or C{None} if no&#xa;        C{defaultValue} is specified.&#xa;&#xa;        Similar to C{dict.get()}.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            result = date_str.parseString(""1999/12/31"")&#xa;            print(result.get(""year"")) # -> '1999'&#xa;            print(result.get(""hour"", ""not specified"")) # -> 'not specified'&#xa;            print(result.get(""hour"")) # -> None&#xa;        """"""&#xa;        if key in self:&#xa;            return self[key]&#xa;        else:&#xa;            return defaultValue&#xa;&#xa;    def insert( self, index, insStr ):&#xa;        """"""&#xa;        Inserts new element at location index in the list of parsed tokens.&#xa;        &#xa;        Similar to C{list.insert()}.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;&#xa;            # use a parse action to insert the parse location in the front of the parsed results&#xa;            def insert_locn(locn, tokens):&#xa;                tokens.insert(0, locn)&#xa;            print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString(""0 123 321"")) # -> [0, '0', '123', '321']&#xa;        """"""&#xa;        self.__toklist.insert(index, insStr)&#xa;        # fixup indices in token dictionary&#xa;        for name,occurrences in self.__tokdict.items():&#xa;            for k, (value, position) in enumerate(occurrences):&#xa;                occurrences[k] = _ParseResultsWithOffset(value, position + (position > index))&#xa;&#xa;    def append( self, item ):&#xa;        """"""&#xa;        Add single element to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            &#xa;            # use a parse action to compute the sum of the parsed integers, and add it to the end&#xa;            def append_sum(tokens):&#xa;                tokens.append(sum(map(int, tokens)))&#xa;            print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString(""0 123 321"")) # -> ['0', '123', '321', 444]&#xa;        """"""&#xa;        self.__toklist.append(item)&#xa;&#xa;    def extend( self, itemseq ):&#xa;        """"""&#xa;        Add sequence of elements to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            &#xa;            # use a parse action to append the reverse of the matched strings, to make a palindrome&#xa;            def make_palindrome(tokens):&#xa;                tokens.extend(reversed([t[::-1] for t in tokens]))&#xa;                return ''.join(tokens)&#xa;            print(patt.addParseAction(make_palindrome).parseString(""lskdj sdlkjf lksd"")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'&#xa;        """"""&#xa;        if isinstance(itemseq, ParseResults):&#xa;            self += itemseq&#xa;        else:&#xa;            self.__toklist.extend(itemseq)&#xa;&#xa;    def clear( self ):&#xa;        """"""&#xa;        Clear all elements and results names.&#xa;        """"""&#xa;        del self.__toklist[:]&#xa;        self.__tokdict.clear()&#xa;&#xa;    def __getattr__( self, name ):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            return """"&#xa;            &#xa;        if name in self.__tokdict:&#xa;            if name not in self.__accumNames:&#xa;                return self.__tokdict[name][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[name] ])&#xa;        else:&#xa;            return """"&#xa;&#xa;    def __add__( self, other ):&#xa;        ret = self.copy()&#xa;        ret += other&#xa;        return ret&#xa;&#xa;    def __iadd__( self, other ):&#xa;        if other.__tokdict:&#xa;            offset = len(self.__toklist)&#xa;            addoffset = lambda a: offset if a<0 else a+offset&#xa;            otheritems = other.__tokdict.items()&#xa;            otherdictitems = [(k, _ParseResultsWithOffset(v[0],addoffset(v[1])) )&#xa;                                for (k,vlist) in otheritems for v in vlist]&#xa;            for k,v in otherdictitems:&#xa;                self[k] = v&#xa;                if isinstance(v[0],ParseResults):&#xa;                    v[0].__parent = wkref(self)&#xa;            &#xa;        self.__toklist += other.__toklist&#xa;        self.__accumNames.update( other.__accumNames )&#xa;        return self&#xa;&#xa;    def __radd__(self, other):&#xa;        if isinstance(other,int) and other == 0:&#xa;            # useful for merging many ParseResults using sum() builtin&#xa;            return self.copy()&#xa;        else:&#xa;            # this may raise a TypeError - so be it&#xa;            return other + self&#xa;        &#xa;    def __repr__( self ):&#xa;        return ""(%s, %s)"" % ( repr( self.__toklist ), repr( self.__tokdict ) )&#xa;&#xa;    def __str__( self ):&#xa;        return '[' + ', '.join(_ustr(i) if isinstance(i, ParseResults) else repr(i) for i in self.__toklist) + ']'&#xa;&#xa;    def _asStringList( self, sep='' ):&#xa;        out = []&#xa;        for item in self.__toklist:&#xa;            if out and sep:&#xa;                out.append(sep)&#xa;            if isinstance( item, ParseResults ):&#xa;                out += item._asStringList()&#xa;            else:&#xa;                out.append( _ustr(item) )&#xa;        return out&#xa;&#xa;    def asList( self ):&#xa;        """"""&#xa;        Returns the parse results as a nested list of matching tokens, all converted to strings.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            result = patt.parseString(""sldkj lsdkj sldkj"")&#xa;            # even though the result prints in string-like form, it is actually a pyparsing ParseResults&#xa;            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']&#xa;            &#xa;            # Use asList() to create an actual list&#xa;            result_list = result.asList()&#xa;            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']&#xa;        """"""&#xa;        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]&#xa;&#xa;    def asDict( self ):&#xa;        """"""&#xa;        Returns the named parse results as a nested dictionary.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})&#xa;            &#xa;            result_dict = result.asDict()&#xa;            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}&#xa;&#xa;            # even though a ParseResults supports dict-like access, sometime you just need to have a dict&#xa;            import json&#xa;            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable&#xa;            print(json.dumps(result.asDict())) # -> {""month"": ""31"", ""day"": ""1999"", ""year"": ""12""}&#xa;        """"""&#xa;        if PY_3:&#xa;            item_fn = self.items&#xa;        else:&#xa;            item_fn = self.iteritems&#xa;            &#xa;        def toItem(obj):&#xa;            if isinstance(obj, ParseResults):&#xa;                if obj.haskeys():&#xa;                    return obj.asDict()&#xa;                else:&#xa;                    return [toItem(v) for v in obj]&#xa;            else:&#xa;                return obj&#xa;                &#xa;        return dict((k,toItem(v)) for k,v in item_fn())&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Returns a new copy of a C{ParseResults} object.&#xa;        """"""&#xa;        ret = ParseResults( self.__toklist )&#xa;        ret.__tokdict = self.__tokdict.copy()&#xa;        ret.__parent = self.__parent&#xa;        ret.__accumNames.update( self.__accumNames )&#xa;        ret.__name = self.__name&#xa;        return ret&#xa;&#xa;    def asXML( self, doctag=None, namedItemsOnly=False, indent="""", formatted=True ):&#xa;        """"""&#xa;        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.&#xa;        """"""&#xa;        nl = ""\n""&#xa;        out = []&#xa;        namedItems = dict((v[1],k) for (k,vlist) in self.__tokdict.items()&#xa;                                                            for v in vlist)&#xa;        nextLevelIndent = indent + ""  ""&#xa;&#xa;        # collapse out indents if formatting is not desired&#xa;        if not formatted:&#xa;            indent = """"&#xa;            nextLevelIndent = """"&#xa;            nl = """"&#xa;&#xa;        selfTag = None&#xa;        if doctag is not None:&#xa;            selfTag = doctag&#xa;        else:&#xa;            if self.__name:&#xa;                selfTag = self.__name&#xa;&#xa;        if not selfTag:&#xa;            if namedItemsOnly:&#xa;                return """"&#xa;            else:&#xa;                selfTag = ""ITEM""&#xa;&#xa;        out += [ nl, indent, ""<"", selfTag, "">"" ]&#xa;&#xa;        for i,res in enumerate(self.__toklist):&#xa;            if isinstance(res,ParseResults):&#xa;                if i in namedItems:&#xa;                    out += [ res.asXML(namedItems[i],&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;                else:&#xa;                    out += [ res.asXML(None,&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;            else:&#xa;                # individual token, see if there is a name for it&#xa;                resTag = None&#xa;                if i in namedItems:&#xa;                    resTag = namedItems[i]&#xa;                if not resTag:&#xa;                    if namedItemsOnly:&#xa;                        continue&#xa;                    else:&#xa;                        resTag = ""ITEM""&#xa;                xmlBodyText = _xml_escape(_ustr(res))&#xa;                out += [ nl, nextLevelIndent, ""<"", resTag, "">"",&#xa;                                                xmlBodyText,&#xa;                                                ""</"", resTag, "">"" ]&#xa;&#xa;        out += [ nl, indent, ""</"", selfTag, "">"" ]&#xa;        return """".join(out)&#xa;&#xa;    def __lookup(self,sub):&#xa;        for k,vlist in self.__tokdict.items():&#xa;            for v,loc in vlist:&#xa;                if sub is v:&#xa;                    return k&#xa;        return None&#xa;&#xa;    def getName(self):&#xa;        """"""&#xa;        Returns the results name for this token expression. Useful when several &#xa;        different expressions might match at a particular location.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            ssn_expr = Regex(r""\d\d\d-\d\d-\d\d\d\d"")&#xa;            house_number_expr = Suppress('#') + Word(nums, alphanums)&#xa;            user_data = (Group(house_number_expr)(""house_number"") &#xa;                        | Group(ssn_expr)(""ssn"")&#xa;                        | Group(integer)(""age""))&#xa;            user_info = OneOrMore(user_data)&#xa;            &#xa;            result = user_info.parseString(""22 111-22-3333 #221B"")&#xa;            for item in result:&#xa;                print(item.getName(), ':', item[0])&#xa;        prints::&#xa;            age : 22&#xa;            ssn : 111-22-3333&#xa;            house_number : 221B&#xa;        """"""&#xa;        if self.__name:&#xa;            return self.__name&#xa;        elif self.__parent:&#xa;            par = self.__parent()&#xa;            if par:&#xa;                return par.__lookup(self)&#xa;            else:&#xa;                return None&#xa;        elif (len(self) == 1 and&#xa;               len(self.__tokdict) == 1 and&#xa;               next(iter(self.__tokdict.values()))[0][1] in (0,-1)):&#xa;            return next(iter(self.__tokdict.keys()))&#xa;        else:&#xa;            return None&#xa;&#xa;    def dump(self, indent='', depth=0, full=True):&#xa;        """"""&#xa;        Diagnostic method for listing out the contents of a C{ParseResults}.&#xa;        Accepts an optional C{indent} argument so that this string can be embedded&#xa;        in a nested display of other data.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(result.dump())&#xa;        prints::&#xa;            ['12', '/', '31', '/', '1999']&#xa;            - day: 1999&#xa;            - month: 31&#xa;            - year: 12&#xa;        """"""&#xa;        out = []&#xa;        NL = '\n'&#xa;        out.append( indent+_ustr(self.asList()) )&#xa;        if full:&#xa;            if self.haskeys():&#xa;                items = sorted((str(k), v) for k,v in self.items())&#xa;                for k,v in items:&#xa;                    if out:&#xa;                        out.append(NL)&#xa;                    out.append( ""%s%s- %s: "" % (indent,('  '*depth), k) )&#xa;                    if isinstance(v,ParseResults):&#xa;                        if v:&#xa;                            out.append( v.dump(indent,depth+1) )&#xa;                        else:&#xa;                            out.append(_ustr(v))&#xa;                    else:&#xa;                        out.append(repr(v))&#xa;            elif any(isinstance(vv,ParseResults) for vv in self):&#xa;                v = self&#xa;                for i,vv in enumerate(v):&#xa;                    if isinstance(vv,ParseResults):&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),vv.dump(indent,depth+1) ))&#xa;                    else:&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),_ustr(vv)))&#xa;            &#xa;        return """".join(out)&#xa;&#xa;    def pprint(self, *args, **kwargs):&#xa;        """"""&#xa;        Pretty-printer for parsed results as a list, using the C{pprint} module.&#xa;        Accepts additional positional or keyword args as defined for the &#xa;        C{pprint.pprint} method. (U{http://docs.python.org/3/library/pprint.html#pprint.pprint})&#xa;&#xa;        Example::&#xa;            ident = Word(alphas, alphanums)&#xa;            num = Word(nums)&#xa;            func = Forward()&#xa;            term = ident | num | Group('(' + func + ')')&#xa;            func <<= ident + Group(Optional(delimitedList(term)))&#xa;            result = func.parseString(""fna a,b,(fnb c,d,200),100"")&#xa;            result.pprint(width=40)&#xa;        prints::&#xa;            ['fna',&#xa;             ['a',&#xa;              'b',&#xa;              ['(', 'fnb', ['c', 'd', '200'], ')'],&#xa;              '100']]&#xa;        """"""&#xa;        pprint.pprint(self.asList(), *args, **kwargs)&#xa;&#xa;    # add support for pickle protocol&#xa;    def __getstate__(self):&#xa;        return ( self.__toklist,&#xa;                 ( self.__tokdict.copy(),&#xa;                   self.__parent is not None and self.__parent() or None,&#xa;                   self.__accumNames,&#xa;                   self.__name ) )&#xa;&#xa;    def __setstate__(self,state):&#xa;        self.__toklist = state[0]&#xa;        (self.__tokdict,&#xa;         par,&#xa;         inAccumNames,&#xa;         self.__name) = state[1]&#xa;        self.__accumNames = {}&#xa;        self.__accumNames.update(inAccumNames)&#xa;        if par is not None:&#xa;            self.__parent = wkref(par)&#xa;        else:&#xa;            self.__parent = None&#xa;&#xa;    def __getnewargs__(self):&#xa;        return self.__toklist, self.__name, self.__asList, self.__modal&#xa;&#xa;    def __dir__(self):&#xa;        return (dir(type(self)) + list(self.keys()))&#xa;&#xa;collections.MutableMapping.register(ParseResults)&#xa;&#xa;def col (loc,strg):&#xa;    """"""Returns current column within a string, counting newlines as line separators.&#xa;   The first column is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    s = strg&#xa;    return 1 if 0<loc<len(s) and s[loc-1] == '\n' else loc - s.rfind(""\n"", 0, loc)&#xa;&#xa;def lineno(loc,strg):&#xa;    """"""Returns current line number within a string, counting newlines as line separators.&#xa;   The first line is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    return strg.count(""\n"",0,loc) + 1&#xa;&#xa;def line( loc, strg ):&#xa;    """"""Returns the line of text containing loc within a string, counting newlines as line separators.&#xa;       """"""&#xa;    lastCR = strg.rfind(""\n"", 0, loc)&#xa;    nextCR = strg.find(""\n"", loc)&#xa;    if nextCR >= 0:&#xa;        return strg[lastCR+1:nextCR]&#xa;    else:&#xa;        return strg[lastCR+1:]&#xa;&#xa;def _defaultStartDebugAction( instring, loc, expr ):&#xa;    print ((""Match "" + _ustr(expr) + "" at loc "" + _ustr(loc) + ""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) )))&#xa;&#xa;def _defaultSuccessDebugAction( instring, startloc, endloc, expr, toks ):&#xa;    print (""Matched "" + _ustr(expr) + "" -> "" + str(toks.asList()))&#xa;&#xa;def _defaultExceptionDebugAction( instring, loc, expr, exc ):&#xa;    print (""Exception raised:"" + _ustr(exc))&#xa;&#xa;def nullDebugAction(*args):&#xa;    """"""'Do-nothing' debug action, to suppress debugging output during parsing.""""""&#xa;    pass&#xa;&#xa;# Only works on Python 3.x - nonlocal is toxic to Python 2 installs&#xa;#~ 'decorator to trim function calls to match the arity of the target'&#xa;#~ def _trim_arity(func, maxargs=3):&#xa;    #~ if func in singleArgBuiltins:&#xa;        #~ return lambda s,l,t: func(t)&#xa;    #~ limit = 0&#xa;    #~ foundArity = False&#xa;    #~ def wrapper(*args):&#xa;        #~ nonlocal limit,foundArity&#xa;        #~ while 1:&#xa;            #~ try:&#xa;                #~ ret = func(*args[limit:])&#xa;                #~ foundArity = True&#xa;                #~ return ret&#xa;            #~ except TypeError:&#xa;                #~ if limit == maxargs or foundArity:&#xa;                    #~ raise&#xa;                #~ limit += 1&#xa;                #~ continue&#xa;    #~ return wrapper&#xa;&#xa;# this version is Python 2.x-3.x cross-compatible&#xa;'decorator to trim function calls to match the arity of the target'&#xa;def _trim_arity(func, maxargs=2):&#xa;    if func in singleArgBuiltins:&#xa;        return lambda s,l,t: func(t)&#xa;    limit = [0]&#xa;    foundArity = [False]&#xa;    &#xa;    # traceback return data structure changed in Py3.5 - normalize back to plain tuples&#xa;    if system_version[:2] >= (3,5):&#xa;        def extract_stack(limit=0):&#xa;            # special handling for Python 3.5.0 - extra deep call stack by 1&#xa;            offset = -3 if system_version == (3,5,0) else -2&#xa;            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;        def extract_tb(tb, limit=0):&#xa;            frames = traceback.extract_tb(tb, limit=limit)&#xa;            frame_summary = frames[-1]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;    else:&#xa;        extract_stack = traceback.extract_stack&#xa;        extract_tb = traceback.extract_tb&#xa;    &#xa;    # synthesize what would be returned by traceback.extract_stack at the call to &#xa;    # user's parse action 'func', so that we don't incur call penalty at parse time&#xa;    &#xa;    LINE_DIFF = 6&#xa;    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND &#xa;    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!&#xa;    this_line = extract_stack(limit=2)[-1]&#xa;    pa_call_line_synth = (this_line[0], this_line[1]+LINE_DIFF)&#xa;&#xa;    def wrapper(*args):&#xa;        while 1:&#xa;            try:&#xa;                ret = func(*args[limit[0]:])&#xa;                foundArity[0] = True&#xa;                return ret&#xa;            except TypeError:&#xa;                # re-raise TypeErrors if they did not come from our arity testing&#xa;                if foundArity[0]:&#xa;                    raise&#xa;                else:&#xa;                    try:&#xa;                        tb = sys.exc_info()[-1]&#xa;                        if not extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth:&#xa;                            raise&#xa;                    finally:&#xa;                        del tb&#xa;&#xa;                if limit[0] <= maxargs:&#xa;                    limit[0] += 1&#xa;                    continue&#xa;                raise&#xa;&#xa;    # copy func name to wrapper for sensible debug output&#xa;    func_name = ""<parse action>""&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    wrapper.__name__ = func_name&#xa;&#xa;    return wrapper&#xa;&#xa;class ParserElement(object):&#xa;    """"""Abstract base level parser element class.""""""&#xa;    DEFAULT_WHITE_CHARS = "" \n\t\r""&#xa;    verbose_stacktrace = False&#xa;&#xa;    @staticmethod&#xa;    def setDefaultWhitespaceChars( chars ):&#xa;        r""""""&#xa;        Overrides the default whitespace chars&#xa;&#xa;        Example::&#xa;            # default whitespace chars are space, <TAB> and newline&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def', 'ghi', 'jkl']&#xa;            &#xa;            # change to just treat newline as significant&#xa;            ParserElement.setDefaultWhitespaceChars("" \t"")&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def']&#xa;        """"""&#xa;        ParserElement.DEFAULT_WHITE_CHARS = chars&#xa;&#xa;    @staticmethod&#xa;    def inlineLiteralsUsing(cls):&#xa;        """"""&#xa;        Set class to be used for inclusion of string literals into a parser.&#xa;        &#xa;        Example::&#xa;            # default literal class used is Literal&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;&#xa;            # change to Suppress&#xa;            ParserElement.inlineLiteralsUsing(Suppress)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '12', '31']&#xa;        """"""&#xa;        ParserElement._literalStringClass = cls&#xa;&#xa;    def __init__( self, savelist=False ):&#xa;        self.parseAction = list()&#xa;        self.failAction = None&#xa;        #~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall&#xa;        self.strRepr = None&#xa;        self.resultsName = None&#xa;        self.saveAsList = savelist&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        self.copyDefaultWhiteChars = True&#xa;        self.mayReturnEmpty = False # used when checking for left-recursion&#xa;        self.keepTabs = False&#xa;        self.ignoreExprs = list()&#xa;        self.debug = False&#xa;        self.streamlined = False&#xa;        self.mayIndexError = True # used to optimize exception handling for subclasses that don't advance parse index&#xa;        self.errmsg = """"&#xa;        self.modalResults = True # used to mark results names as modal (report only last) or cumulative (list all)&#xa;        self.debugActions = ( None, None, None ) #custom debug actions&#xa;        self.re = None&#xa;        self.callPreparse = True # used to avoid redundant calls to preParse&#xa;        self.callDuringTry = False&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Make a copy of this C{ParserElement}.  Useful for defining different parse actions&#xa;        for the same parsing pattern, using copies of the original parse element.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress(""K"")&#xa;            integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;            &#xa;            print(OneOrMore(integerK | integerM | integer).parseString(""5K 100 640K 256M""))&#xa;        prints::&#xa;            [5120, 100, 655360, 268435456]&#xa;        Equivalent form of C{expr.copy()} is just C{expr()}::&#xa;            integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;        """"""&#xa;        cpy = copy.copy( self )&#xa;        cpy.parseAction = self.parseAction[:]&#xa;        cpy.ignoreExprs = self.ignoreExprs[:]&#xa;        if self.copyDefaultWhiteChars:&#xa;            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        return cpy&#xa;&#xa;    def setName( self, name ):&#xa;        """"""&#xa;        Define name for this expression, makes debugging and exception messages clearer.&#xa;        &#xa;        Example::&#xa;            Word(nums).parseString(""ABC"")  # -> Exception: Expected W:(0123...) (at char 0), (line:1, col:1)&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")  # -> Exception: Expected integer (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        self.name = name&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        if hasattr(self,""exception""):&#xa;            self.exception.msg = self.errmsg&#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        """"""&#xa;        Define name for referencing matching tokens as a nested attribute&#xa;        of the returned parse results.&#xa;        NOTE: this returns a *copy* of the original C{ParserElement} object;&#xa;        this is so that the client can define a basic element, such as an&#xa;        integer, and reference it in multiple places with different names.&#xa;&#xa;        You can also set results names using the abbreviated syntax,&#xa;        C{expr(""name"")} in place of C{expr.setResultsName(""name"")} - &#xa;        see L{I{__call__}<__call__>}.&#xa;&#xa;        Example::&#xa;            date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;&#xa;            # equivalent form:&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;        """"""&#xa;        newself = self.copy()&#xa;        if name.endswith(""*""):&#xa;            name = name[:-1]&#xa;            listAllMatches=True&#xa;        newself.resultsName = name&#xa;        newself.modalResults = not listAllMatches&#xa;        return newself&#xa;&#xa;    def setBreak(self,breakFlag = True):&#xa;        """"""Method to invoke the Python pdb debugger when this element is&#xa;           about to be parsed. Set C{breakFlag} to True to enable, False to&#xa;           disable.&#xa;        """"""&#xa;        if breakFlag:&#xa;            _parseMethod = self._parse&#xa;            def breaker(instring, loc, doActions=True, callPreParse=True):&#xa;                import pdb&#xa;                pdb.set_trace()&#xa;                return _parseMethod( instring, loc, doActions, callPreParse )&#xa;            breaker._originalParseMethod = _parseMethod&#xa;            self._parse = breaker&#xa;        else:&#xa;            if hasattr(self._parse,""_originalParseMethod""):&#xa;                self._parse = self._parse._originalParseMethod&#xa;        return self&#xa;&#xa;    def setParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Define action to perform when successfully matching parse element definition.&#xa;        Parse action fn is a callable method with 0-3 arguments, called as C{fn(s,loc,toks)},&#xa;        C{fn(loc,toks)}, C{fn(toks)}, or just C{fn()}, where:&#xa;         - s   = the original string being parsed (see note below)&#xa;         - loc = the location of the matching substring&#xa;         - toks = a list of the matched tokens, packaged as a C{L{ParseResults}} object&#xa;        If the functions in fns modify the tokens, they can return them as the return&#xa;        value from fn, and the modified list of tokens will replace the original.&#xa;        Otherwise, fn does not need to return any value.&#xa;&#xa;        Optional keyword arguments:&#xa;         - callDuringTry = (default=C{False}) indicate if parse action should be run during lookaheads and alternate testing&#xa;&#xa;        Note: the default parsing behavior is to expand tabs in the input string&#xa;        before starting the parsing process.  See L{I{parseString}<parseString>} for more information&#xa;        on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;        consistent view of the parsed string, the parse location, and line and column&#xa;        positions within the parsed string.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;            # use parse action to convert to ints at parse time&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            # note that integer fields are now ints, not strings&#xa;            date_str.parseString(""1999/12/31"")  # -> [1999, '/', 12, '/', 31]&#xa;        """"""&#xa;        self.parseAction = list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Add parse action to expression's list of parse actions. See L{I{setParseAction}<setParseAction>}.&#xa;        &#xa;        See examples in L{I{copy}<copy>}.&#xa;        """"""&#xa;        self.parseAction += list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addCondition(self, *fns, **kwargs):&#xa;        """"""Add a boolean predicate function to expression's list of parse actions. See &#xa;        L{I{setParseAction}<setParseAction>} for function call signatures. Unlike C{setParseAction}, &#xa;        functions passed to C{addCondition} need to return boolean success/fail of the condition.&#xa;&#xa;        Optional keyword arguments:&#xa;         - message = define a custom message to be used in the raised exception&#xa;         - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException&#xa;         &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            year_int = integer.copy()&#xa;            year_int.addCondition(lambda toks: toks[0] >= 2000, message=""Only support years 2000 and later"")&#xa;            date_str = year_int + '/' + integer + '/' + integer&#xa;&#xa;            result = date_str.parseString(""1999/12/31"")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        msg = kwargs.get(""message"", ""failed user-defined condition"")&#xa;        exc_type = ParseFatalException if kwargs.get(""fatal"", False) else ParseException&#xa;        for fn in fns:&#xa;            def pa(s,l,t):&#xa;                if not bool(_trim_arity(fn)(s,l,t)):&#xa;                    raise exc_type(s,l,msg)&#xa;            self.parseAction.append(pa)&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def setFailAction( self, fn ):&#xa;        """"""Define action to perform if parsing fails at this expression.&#xa;           Fail acton fn is a callable function that takes the arguments&#xa;           C{fn(s,loc,expr,err)} where:&#xa;            - s = string being parsed&#xa;            - loc = location where expression match was attempted and failed&#xa;            - expr = the parse expression that failed&#xa;            - err = the exception thrown&#xa;           The function returns no value.  It may throw C{L{ParseFatalException}}&#xa;           if it is desired to stop parsing immediately.""""""&#xa;        self.failAction = fn&#xa;        return self&#xa;&#xa;    def _skipIgnorables( self, instring, loc ):&#xa;        exprsFound = True&#xa;        while exprsFound:&#xa;            exprsFound = False&#xa;            for e in self.ignoreExprs:&#xa;                try:&#xa;                    while 1:&#xa;                        loc,dummy = e._parse( instring, loc )&#xa;                        exprsFound = True&#xa;                except ParseException:&#xa;                    pass&#xa;        return loc&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if self.ignoreExprs:&#xa;            loc = self._skipIgnorables( instring, loc )&#xa;&#xa;        if self.skipWhitespace:&#xa;            wt = self.whiteChars&#xa;            instrlen = len(instring)&#xa;            while loc < instrlen and instring[loc] in wt:&#xa;                loc += 1&#xa;&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        return loc, []&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return tokenlist&#xa;&#xa;    #~ @profile&#xa;    def _parseNoCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        debugging = ( self.debug ) #and doActions )&#xa;&#xa;        if debugging or self.failAction:&#xa;            #~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))&#xa;            if (self.debugActions[0] ):&#xa;                self.debugActions[0]( instring, loc, self )&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            try:&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            except ParseBaseException as err:&#xa;                #~ print (""Exception raised:"", err)&#xa;                if self.debugActions[2]:&#xa;                    self.debugActions[2]( instring, tokensStart, self, err )&#xa;                if self.failAction:&#xa;                    self.failAction( instring, tokensStart, self, err )&#xa;                raise&#xa;        else:&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            if self.mayIndexError or loc >= len(instring):&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            else:&#xa;                loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;&#xa;        tokens = self.postParse( instring, loc, tokens )&#xa;&#xa;        retTokens = ParseResults( tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults )&#xa;        if self.parseAction and (doActions or self.callDuringTry):&#xa;            if debugging:&#xa;                try:&#xa;                    for fn in self.parseAction:&#xa;                        tokens = fn( instring, tokensStart, retTokens )&#xa;                        if tokens is not None:&#xa;                            retTokens = ParseResults( tokens,&#xa;                                                      self.resultsName,&#xa;                                                      asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                      modal=self.modalResults )&#xa;                except ParseBaseException as err:&#xa;                    #~ print ""Exception raised in user parse action:"", err&#xa;                    if (self.debugActions[2] ):&#xa;                        self.debugActions[2]( instring, tokensStart, self, err )&#xa;                    raise&#xa;            else:&#xa;                for fn in self.parseAction:&#xa;                    tokens = fn( instring, tokensStart, retTokens )&#xa;                    if tokens is not None:&#xa;                        retTokens = ParseResults( tokens,&#xa;                                                  self.resultsName,&#xa;                                                  asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                  modal=self.modalResults )&#xa;&#xa;        if debugging:&#xa;            #~ print (""Matched"",self,""->"",retTokens.asList())&#xa;            if (self.debugActions[1] ):&#xa;                self.debugActions[1]( instring, tokensStart, loc, self, retTokens )&#xa;&#xa;        return loc, retTokens&#xa;&#xa;    def tryParse( self, instring, loc ):&#xa;        try:&#xa;            return self._parse( instring, loc, doActions=False )[0]&#xa;        except ParseFatalException:&#xa;            raise ParseException( instring, loc, self.errmsg, self)&#xa;    &#xa;    def canParseNext(self, instring, loc):&#xa;        try:&#xa;            self.tryParse(instring, loc)&#xa;        except (ParseException, IndexError):&#xa;            return False&#xa;        else:&#xa;            return True&#xa;&#xa;    class _UnboundedCache(object):&#xa;        def __init__(self):&#xa;            cache = {}&#xa;            self.not_in_cache = not_in_cache = object()&#xa;&#xa;            def get(self, key):&#xa;                return cache.get(key, not_in_cache)&#xa;&#xa;            def set(self, key, value):&#xa;                cache[key] = value&#xa;&#xa;            def clear(self):&#xa;                cache.clear()&#xa;&#xa;            self.get = types.MethodType(get, self)&#xa;            self.set = types.MethodType(set, self)&#xa;            self.clear = types.MethodType(clear, self)&#xa;&#xa;    if _OrderedDict is not None:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = _OrderedDict()&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.popitem(False)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    else:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = {}&#xa;                key_fifo = collections.deque([], size)&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.pop(key_fifo.popleft(), None)&#xa;                    key_fifo.append(key)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;                    key_fifo.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    # argument cache for optimizing repeated calls when backtracking through recursive expressions&#xa;    packrat_cache = {} # this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail&#xa;    packrat_cache_lock = RLock()&#xa;    packrat_cache_stats = [0, 0]&#xa;&#xa;    # this method gets repeatedly called during backtracking with the same arguments -&#xa;    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression&#xa;    def _parseCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        HIT, MISS = 0, 1&#xa;        lookup = (self, instring, loc, callPreParse, doActions)&#xa;        with ParserElement.packrat_cache_lock:&#xa;            cache = ParserElement.packrat_cache&#xa;            value = cache.get(lookup)&#xa;            if value is cache.not_in_cache:&#xa;                ParserElement.packrat_cache_stats[MISS] += 1&#xa;                try:&#xa;                    value = self._parseNoCache(instring, loc, doActions, callPreParse)&#xa;                except ParseBaseException as pe:&#xa;                    # cache a copy of the exception, without the traceback&#xa;                    cache.set(lookup, pe.__class__(*pe.args))&#xa;                    raise&#xa;                else:&#xa;                    cache.set(lookup, (value[0], value[1].copy()))&#xa;                    return value&#xa;            else:&#xa;                ParserElement.packrat_cache_stats[HIT] += 1&#xa;                if isinstance(value, Exception):&#xa;                    raise value&#xa;                return (value[0], value[1].copy())&#xa;&#xa;    _parse = _parseNoCache&#xa;&#xa;    @staticmethod&#xa;    def resetCache():&#xa;        ParserElement.packrat_cache.clear()&#xa;        ParserElement.packrat_cache_stats[:] = [0] * len(ParserElement.packrat_cache_stats)&#xa;&#xa;    _packratEnabled = False&#xa;    @staticmethod&#xa;    def enablePackrat(cache_size_limit=128):&#xa;        """"""Enables ""packrat"" parsing, which adds memoizing to the parsing logic.&#xa;           Repeated parse attempts at the same string location (which happens&#xa;           often in many complex grammars) can immediately return a cached value,&#xa;           instead of re-executing parsing/validating code.  Memoizing is done of&#xa;           both valid results and parsing exceptions.&#xa;           &#xa;           Parameters:&#xa;            - cache_size_limit - (default=C{128}) - if an integer value is provided&#xa;              will limit the size of the packrat cache; if None is passed, then&#xa;              the cache size will be unbounded; if 0 is passed, the cache will&#xa;              be effectively disabled.&#xa;            &#xa;           This speedup may break existing programs that use parse actions that&#xa;           have side-effects.  For this reason, packrat parsing is disabled when&#xa;           you first import pyparsing.  To activate the packrat feature, your&#xa;           program must call the class method C{ParserElement.enablePackrat()}.  If&#xa;           your program uses C{psyco} to ""compile as you go"", you must call&#xa;           C{enablePackrat} before calling C{psyco.full()}.  If you do not do this,&#xa;           Python will crash.  For best results, call C{enablePackrat()} immediately&#xa;           after importing pyparsing.&#xa;           &#xa;           Example::&#xa;               import pyparsing&#xa;               pyparsing.ParserElement.enablePackrat()&#xa;        """"""&#xa;        if not ParserElement._packratEnabled:&#xa;            ParserElement._packratEnabled = True&#xa;            if cache_size_limit is None:&#xa;                ParserElement.packrat_cache = ParserElement._UnboundedCache()&#xa;            else:&#xa;                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)&#xa;            ParserElement._parse = ParserElement._parseCache&#xa;&#xa;    def parseString( self, instring, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression with the given string.&#xa;        This is the main interface to the client code, once the complete&#xa;        expression has been built.&#xa;&#xa;        If you want the grammar to require that the entire input string be&#xa;        successfully parsed, then set C{parseAll} to True (equivalent to ending&#xa;        the grammar with C{L{StringEnd()}}).&#xa;&#xa;        Note: C{parseString} implicitly calls C{expandtabs()} on the input string,&#xa;        in order to report proper column numbers in parse actions.&#xa;        If the input string contains tabs and&#xa;        the grammar uses parse actions that use the C{loc} argument to index into the&#xa;        string being parsed, you can ensure you have a consistent view of the input&#xa;        string by:&#xa;         - calling C{parseWithTabs} on your grammar before calling C{parseString}&#xa;           (see L{I{parseWithTabs}<parseWithTabs>})&#xa;         - define your parse action using the full C{(s,loc,toks)} signature, and&#xa;           reference the input string using the parse action's C{s} argument&#xa;         - explictly expand the tabs in your input string before calling&#xa;           C{parseString}&#xa;        &#xa;        Example::&#xa;            Word('a').parseString('aaaaabaaa')  # -> ['aaaaa']&#xa;            Word('a').parseString('aaaaabaaa', parseAll=True)  # -> Exception: Expected end of text&#xa;        """"""&#xa;        ParserElement.resetCache()&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;            #~ self.saveAsList = True&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;        if not self.keepTabs:&#xa;            instring = instring.expandtabs()&#xa;        try:&#xa;            loc, tokens = self._parse( instring, 0 )&#xa;            if parseAll:&#xa;                loc = self.preParse( instring, loc )&#xa;                se = Empty() + StringEnd()&#xa;                se._parse( instring, loc )&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;        else:&#xa;            return tokens&#xa;&#xa;    def scanString( self, instring, maxMatches=_MAX_INT, overlap=False ):&#xa;        """"""&#xa;        Scan the input string for expression matches.  Each match will return the&#xa;        matching tokens, start location, and end location.  May be called with optional&#xa;        C{maxMatches} argument, to clip scanning after 'n' matches are found.  If&#xa;        C{overlap} is specified, then overlapping matches will be reported.&#xa;&#xa;        Note that the start and end locations are reported relative to the string&#xa;        being parsed.  See L{I{parseString}<parseString>} for more information on parsing&#xa;        strings with embedded tabs.&#xa;&#xa;        Example::&#xa;            source = ""sldjf123lsdjjkf345sldkjf879lkjsfd987""&#xa;            print(source)&#xa;            for tokens,start,end in Word(alphas).scanString(source):&#xa;                print(' '*start + '^'*(end-start))&#xa;                print(' '*start + tokens[0])&#xa;        &#xa;        prints::&#xa;        &#xa;            sldjf123lsdjjkf345sldkjf879lkjsfd987&#xa;            ^^^^^&#xa;            sldjf&#xa;                    ^^^^^^^&#xa;                    lsdjjkf&#xa;                              ^^^^^^&#xa;                              sldkjf&#xa;                                       ^^^^^^&#xa;                                       lkjsfd&#xa;        """"""&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;&#xa;        if not self.keepTabs:&#xa;            instring = _ustr(instring).expandtabs()&#xa;        instrlen = len(instring)&#xa;        loc = 0&#xa;        preparseFn = self.preParse&#xa;        parseFn = self._parse&#xa;        ParserElement.resetCache()&#xa;        matches = 0&#xa;        try:&#xa;            while loc <= instrlen and matches < maxMatches:&#xa;                try:&#xa;                    preloc = preparseFn( instring, loc )&#xa;                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )&#xa;                except ParseException:&#xa;                    loc = preloc+1&#xa;                else:&#xa;                    if nextLoc > loc:&#xa;                        matches += 1&#xa;                        yield tokens, preloc, nextLoc&#xa;                        if overlap:&#xa;                            nextloc = preparseFn( instring, loc )&#xa;                            if nextloc > loc:&#xa;                                loc = nextLoc&#xa;                            else:&#xa;                                loc += 1&#xa;                        else:&#xa;                            loc = nextLoc&#xa;                    else:&#xa;                        loc = preloc+1&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def transformString( self, instring ):&#xa;        """"""&#xa;        Extension to C{L{scanString}}, to modify matching text with modified tokens that may&#xa;        be returned from a parse action.  To use C{transformString}, define a grammar and&#xa;        attach a parse action to it that modifies the returned token list.&#xa;        Invoking C{transformString()} on a target string will then scan for matches,&#xa;        and replace the matched text patterns according to the logic in the parse&#xa;        action.  C{transformString()} returns the resulting transformed string.&#xa;        &#xa;        Example::&#xa;            wd = Word(alphas)&#xa;            wd.setParseAction(lambda toks: toks[0].title())&#xa;            &#xa;            print(wd.transformString(""now is the winter of our discontent made glorious summer by this sun of york.""))&#xa;        Prints::&#xa;            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.&#xa;        """"""&#xa;        out = []&#xa;        lastE = 0&#xa;        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to&#xa;        # keep string locs straight between transformString and scanString&#xa;        self.keepTabs = True&#xa;        try:&#xa;            for t,s,e in self.scanString( instring ):&#xa;                out.append( instring[lastE:s] )&#xa;                if t:&#xa;                    if isinstance(t,ParseResults):&#xa;                        out += t.asList()&#xa;                    elif isinstance(t,list):&#xa;                        out += t&#xa;                    else:&#xa;                        out.append(t)&#xa;                lastE = e&#xa;            out.append(instring[lastE:])&#xa;            out = [o for o in out if o]&#xa;            return """".join(map(_ustr,_flatten(out)))&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def searchString( self, instring, maxMatches=_MAX_INT ):&#xa;        """"""&#xa;        Another extension to C{L{scanString}}, simplifying the access to the tokens found&#xa;        to match the given parse expression.  May be called with optional&#xa;        C{maxMatches} argument, to clip searching after 'n' matches are found.&#xa;        &#xa;        Example::&#xa;            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters&#xa;            cap_word = Word(alphas.upper(), alphas.lower())&#xa;            &#xa;            print(cap_word.searchString(""More than Iron, more than Lead, more than Gold I need Electricity""))&#xa;        prints::&#xa;            ['More', 'Iron', 'Lead', 'Gold', 'I']&#xa;        """"""&#xa;        try:&#xa;            return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):&#xa;        """"""&#xa;        Generator method to split a string using the given expression as a separator.&#xa;        May be called with optional C{maxsplit} argument, to limit the number of splits;&#xa;        and the optional C{includeSeparators} argument (default=C{False}), if the separating&#xa;        matching text should be included in the split results.&#xa;        &#xa;        Example::        &#xa;            punc = oneOf(list("".,;:/-!?""))&#xa;            print(list(punc.split(""This, this?, this sentence, is badly punctuated!"")))&#xa;        prints::&#xa;            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']&#xa;        """"""&#xa;        splits = 0&#xa;        last = 0&#xa;        for t,s,e in self.scanString(instring, maxMatches=maxsplit):&#xa;            yield instring[last:s]&#xa;            if includeSeparators:&#xa;                yield t[0]&#xa;            last = e&#xa;        yield instring[last:]&#xa;&#xa;    def __add__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator - returns C{L{And}}. Adding strings to a ParserElement&#xa;        converts them to L{Literal}s by default.&#xa;        &#xa;        Example::&#xa;            greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;            hello = ""Hello, World!""&#xa;            print (hello, ""->"", greet.parseString(hello))&#xa;        Prints::&#xa;            Hello, World! -> ['Hello', ',', 'World', '!']&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, other ] )&#xa;&#xa;    def __radd__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other + self&#xa;&#xa;    def __sub__(self, other):&#xa;        """"""&#xa;        Implementation of - operator, returns C{L{And}} with error stop&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, And._ErrorStop(), other ] )&#xa;&#xa;    def __rsub__(self, other ):&#xa;        """"""&#xa;        Implementation of - operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other - self&#xa;&#xa;    def __mul__(self,other):&#xa;        """"""&#xa;        Implementation of * operator, allows use of C{expr * 3} in place of&#xa;        C{expr + expr + expr}.  Expressions may also me multiplied by a 2-integer&#xa;        tuple, similar to C{{min,max}} multipliers in regular expressions.  Tuples&#xa;        may also include C{None} as in:&#xa;         - C{expr*(n,None)} or C{expr*(n,)} is equivalent&#xa;              to C{expr*n + L{ZeroOrMore}(expr)}&#xa;              (read as ""at least n instances of C{expr}"")&#xa;         - C{expr*(None,n)} is equivalent to C{expr*(0,n)}&#xa;              (read as ""0 to n instances of C{expr}"")&#xa;         - C{expr*(None,None)} is equivalent to C{L{ZeroOrMore}(expr)}&#xa;         - C{expr*(1,None)} is equivalent to C{L{OneOrMore}(expr)}&#xa;&#xa;        Note that C{expr*(None,n)} does not raise an exception if&#xa;        more than n exprs exist in the input stream; that is,&#xa;        C{expr*(None,n)} does not enforce a maximum number of expr&#xa;        occurrences.  If this behavior is desired, then write&#xa;        C{expr*(None,n) + ~expr}&#xa;        """"""&#xa;        if isinstance(other,int):&#xa;            minElements, optElements = other,0&#xa;        elif isinstance(other,tuple):&#xa;            other = (other + (None, None))[:2]&#xa;            if other[0] is None:&#xa;                other = (0, other[1])&#xa;            if isinstance(other[0],int) and other[1] is None:&#xa;                if other[0] == 0:&#xa;                    return ZeroOrMore(self)&#xa;                if other[0] == 1:&#xa;                    return OneOrMore(self)&#xa;                else:&#xa;                    return self*other[0] + ZeroOrMore(self)&#xa;            elif isinstance(other[0],int) and isinstance(other[1],int):&#xa;                minElements, optElements = other&#xa;                optElements -= minElements&#xa;            else:&#xa;                raise TypeError(""cannot multiply 'ParserElement' and ('%s','%s') objects"", type(other[0]),type(other[1]))&#xa;        else:&#xa;            raise TypeError(""cannot multiply 'ParserElement' and '%s' objects"", type(other))&#xa;&#xa;        if minElements < 0:&#xa;            raise ValueError(""cannot multiply ParserElement by negative value"")&#xa;        if optElements < 0:&#xa;            raise ValueError(""second tuple value must be greater or equal to first tuple value"")&#xa;        if minElements == optElements == 0:&#xa;            raise ValueError(""cannot multiply ParserElement by 0 or (0,0)"")&#xa;&#xa;        if (optElements):&#xa;            def makeOptionalList(n):&#xa;                if n>1:&#xa;                    return Optional(self + makeOptionalList(n-1))&#xa;                else:&#xa;                    return Optional(self)&#xa;            if minElements:&#xa;                if minElements == 1:&#xa;                    ret = self + makeOptionalList(optElements)&#xa;                else:&#xa;                    ret = And([self]*minElements) + makeOptionalList(optElements)&#xa;            else:&#xa;                ret = makeOptionalList(optElements)&#xa;        else:&#xa;            if minElements == 1:&#xa;                ret = self&#xa;            else:&#xa;                ret = And([self]*minElements)&#xa;        return ret&#xa;&#xa;    def __rmul__(self, other):&#xa;        return self.__mul__(other)&#xa;&#xa;    def __or__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator - returns C{L{MatchFirst}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return MatchFirst( [ self, other ] )&#xa;&#xa;    def __ror__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other | self&#xa;&#xa;    def __xor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator - returns C{L{Or}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Or( [ self, other ] )&#xa;&#xa;    def __rxor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other ^ self&#xa;&#xa;    def __and__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator - returns C{L{Each}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Each( [ self, other ] )&#xa;&#xa;    def __rand__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other & self&#xa;&#xa;    def __invert__( self ):&#xa;        """"""&#xa;        Implementation of ~ operator - returns C{L{NotAny}}&#xa;        """"""&#xa;        return NotAny( self )&#xa;&#xa;    def __call__(self, name=None):&#xa;        """"""&#xa;        Shortcut for C{L{setResultsName}}, with C{listAllMatches=False}.&#xa;        &#xa;        If C{name} is given with a trailing C{'*'} character, then C{listAllMatches} will be&#xa;        passed as C{True}.&#xa;           &#xa;        If C{name} is omitted, same as calling C{L{copy}}.&#xa;&#xa;        Example::&#xa;            # these are equivalent&#xa;            userdata = Word(alphas).setResultsName(""name"") + Word(nums+""-"").setResultsName(""socsecno"")&#xa;            userdata = Word(alphas)(""name"") + Word(nums+""-"")(""socsecno"")             &#xa;        """"""&#xa;        if name is not None:&#xa;            return self.setResultsName(name)&#xa;        else:&#xa;            return self.copy()&#xa;&#xa;    def suppress( self ):&#xa;        """"""&#xa;        Suppresses the output of this C{ParserElement}; useful to keep punctuation from&#xa;        cluttering up returned output.&#xa;        """"""&#xa;        return Suppress( self )&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""&#xa;        Disables the skipping of whitespace before matching the characters in the&#xa;        C{ParserElement}'s defined pattern.  This is normally only used internally by&#xa;        the pyparsing module, but may be needed in some whitespace-sensitive grammars.&#xa;        """"""&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def setWhitespaceChars( self, chars ):&#xa;        """"""&#xa;        Overrides the default whitespace chars&#xa;        """"""&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = chars&#xa;        self.copyDefaultWhiteChars = False&#xa;        return self&#xa;&#xa;    def parseWithTabs( self ):&#xa;        """"""&#xa;        Overrides default behavior to expand C{<TAB>}s to spaces before parsing the input string.&#xa;        Must be called before C{parseString} when the input grammar contains elements that&#xa;        match C{<TAB>} characters.&#xa;        """"""&#xa;        self.keepTabs = True&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        """"""&#xa;        Define expression to be ignored (e.g., comments) while doing pattern&#xa;        matching; may be called repeatedly, to define multiple comment or other&#xa;        ignorable patterns.&#xa;        &#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']&#xa;            &#xa;            patt.ignore(cStyleComment)&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']&#xa;        """"""&#xa;        if isinstance(other, basestring):&#xa;            other = Suppress(other)&#xa;&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                self.ignoreExprs.append(other)&#xa;        else:&#xa;            self.ignoreExprs.append( Suppress( other.copy() ) )&#xa;        return self&#xa;&#xa;    def setDebugActions( self, startAction, successAction, exceptionAction ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        """"""&#xa;        self.debugActions = (startAction or _defaultStartDebugAction,&#xa;                             successAction or _defaultSuccessDebugAction,&#xa;                             exceptionAction or _defaultExceptionDebugAction)&#xa;        self.debug = True&#xa;        return self&#xa;&#xa;    def setDebug( self, flag=True ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        Set C{flag} to True to enable, False to disable.&#xa;&#xa;        Example::&#xa;            wd = Word(alphas).setName(""alphaword"")&#xa;            integer = Word(nums).setName(""numword"")&#xa;            term = wd | integer&#xa;            &#xa;            # turn on debugging for wd&#xa;            wd.setDebug()&#xa;&#xa;            OneOrMore(term).parseString(""abc 123 xyz 890"")&#xa;        &#xa;        prints::&#xa;            Match alphaword at loc 0(1,1)&#xa;            Matched alphaword -> ['abc']&#xa;            Match alphaword at loc 3(1,4)&#xa;            Exception raised:Expected alphaword (at char 4), (line:1, col:5)&#xa;            Match alphaword at loc 7(1,8)&#xa;            Matched alphaword -> ['xyz']&#xa;            Match alphaword at loc 11(1,12)&#xa;            Exception raised:Expected alphaword (at char 12), (line:1, col:13)&#xa;            Match alphaword at loc 15(1,16)&#xa;            Exception raised:Expected alphaword (at char 15), (line:1, col:16)&#xa;&#xa;        The output shown is that produced by the default debug actions - custom debug actions can be&#xa;        specified using L{setDebugActions}. Prior to attempting&#xa;        to match the C{wd} expression, the debugging message C{""Match <exprname> at loc <n>(<line>,<col>)""}&#xa;        is shown. Then if the parse succeeds, a C{""Matched""} message is shown, or an C{""Exception raised""}&#xa;        message is shown. Also note the use of L{setName} to assign a human-readable name to the expression,&#xa;        which makes debugging and exception messages easier to understand - for instance, the default&#xa;        name created for the C{Word} expression without calling C{setName} is C{""W:(ABCD...)""}.&#xa;        """"""&#xa;        if flag:&#xa;            self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )&#xa;        else:&#xa;            self.debug = False&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        return self.name&#xa;&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;&#xa;    def streamline( self ):&#xa;        self.streamlined = True&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        pass&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        """"""&#xa;        Check defined expressions for valid structure, check for infinite recursive definitions.&#xa;        """"""&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def parseFile( self, file_or_filename, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression on the given file or filename.&#xa;        If a filename is specified (instead of a file object),&#xa;        the entire file is opened, read, and closed before parsing.&#xa;        """"""&#xa;        try:&#xa;            file_contents = file_or_filename.read()&#xa;        except AttributeError:&#xa;            with open(file_or_filename, ""r"") as f:&#xa;                file_contents = f.read()&#xa;        try:&#xa;            return self.parseString(file_contents, parseAll)&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def __eq__(self,other):&#xa;        if isinstance(other, ParserElement):&#xa;            return self is other or vars(self) == vars(other)&#xa;        elif isinstance(other, basestring):&#xa;            return self.matches(other)&#xa;        else:&#xa;            return super(ParserElement,self)==other&#xa;&#xa;    def __ne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def __hash__(self):&#xa;        return hash(id(self))&#xa;&#xa;    def __req__(self,other):&#xa;        return self == other&#xa;&#xa;    def __rne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def matches(self, testString, parseAll=True):&#xa;        """"""&#xa;        Method for quick testing of a parser against a test string. Good for simple &#xa;        inline microtests of sub expressions while building up larger parser.&#xa;           &#xa;        Parameters:&#xa;         - testString - to test against this expression for a match&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests&#xa;            &#xa;        Example::&#xa;            expr = Word(nums)&#xa;            assert expr.matches(""100"")&#xa;        """"""&#xa;        try:&#xa;            self.parseString(_ustr(testString), parseAll=parseAll)&#xa;            return True&#xa;        except ParseBaseException:&#xa;            return False&#xa;                &#xa;    def runTests(self, tests, parseAll=True, comment='#', fullDump=True, printResults=True, failureTests=False):&#xa;        """"""&#xa;        Execute the parse expression on a series of test strings, showing each&#xa;        test, the parsed results or where the parse failed. Quick and easy way to&#xa;        run a parse expression against a list of sample strings.&#xa;           &#xa;        Parameters:&#xa;         - tests - a list of separate test strings, or a multiline string of test strings&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests           &#xa;         - comment - (default=C{'#'}) - expression for indicating embedded comments in the test &#xa;              string; pass None to disable comment filtering&#xa;         - fullDump - (default=C{True}) - dump results as list followed by results names in nested outline;&#xa;              if False, only dump nested list&#xa;         - printResults - (default=C{True}) prints test output to stdout&#xa;         - failureTests - (default=C{False}) indicates if these tests are expected to fail parsing&#xa;&#xa;        Returns: a (success, results) tuple, where success indicates that all tests succeeded&#xa;        (or failed if C{failureTests} is True), and the results contain a list of lines of each &#xa;        test's output&#xa;        &#xa;        Example::&#xa;            number_expr = pyparsing_common.number.copy()&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # unsigned integer&#xa;                100&#xa;                # negative integer&#xa;                -100&#xa;                # float with scientific notation&#xa;                6.02e23&#xa;                # integer with scientific notation&#xa;                1e-12&#xa;                ''')&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # stray character&#xa;                100Z&#xa;                # missing leading digit before '.'&#xa;                -.100&#xa;                # too many '.'&#xa;                3.14.159&#xa;                ''', failureTests=True)&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;        prints::&#xa;            # unsigned integer&#xa;            100&#xa;            [100]&#xa;&#xa;            # negative integer&#xa;            -100&#xa;            [-100]&#xa;&#xa;            # float with scientific notation&#xa;            6.02e23&#xa;            [6.02e+23]&#xa;&#xa;            # integer with scientific notation&#xa;            1e-12&#xa;            [1e-12]&#xa;&#xa;            Success&#xa;            &#xa;            # stray character&#xa;            100Z&#xa;               ^&#xa;            FAIL: Expected end of text (at char 3), (line:1, col:4)&#xa;&#xa;            # missing leading digit before '.'&#xa;            -.100&#xa;            ^&#xa;            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)&#xa;&#xa;            # too many '.'&#xa;            3.14.159&#xa;                ^&#xa;            FAIL: Expected end of text (at char 4), (line:1, col:5)&#xa;&#xa;            Success&#xa;&#xa;        Each test string must be on a single line. If you want to test a string that spans multiple&#xa;        lines, create a test like this::&#xa;&#xa;            expr.runTest(r""this is a test\\n of strings that spans \\n 3 lines"")&#xa;        &#xa;        (Note that this is a raw string literal, you must include the leading 'r'.)&#xa;        """"""&#xa;        if isinstance(tests, basestring):&#xa;            tests = list(map(str.strip, tests.rstrip().splitlines()))&#xa;        if isinstance(comment, basestring):&#xa;            comment = Literal(comment)&#xa;        allResults = []&#xa;        comments = []&#xa;        success = True&#xa;        for t in tests:&#xa;            if comment is not None and comment.matches(t, False) or comments and not t:&#xa;                comments.append(t)&#xa;                continue&#xa;            if not t:&#xa;                continue&#xa;            out = ['\n'.join(comments), t]&#xa;            comments = []&#xa;            try:&#xa;                t = t.replace(r'\n','\n')&#xa;                result = self.parseString(t, parseAll=parseAll)&#xa;                out.append(result.dump(full=fullDump))&#xa;                success = success and not failureTests&#xa;            except ParseBaseException as pe:&#xa;                fatal = ""(FATAL)"" if isinstance(pe, ParseFatalException) else """"&#xa;                if '\n' in t:&#xa;                    out.append(line(pe.loc, t))&#xa;                    out.append(' '*(col(pe.loc,t)-1) + '^' + fatal)&#xa;                else:&#xa;                    out.append(' '*pe.loc + '^' + fatal)&#xa;                out.append(""FAIL: "" + str(pe))&#xa;                success = success and failureTests&#xa;                result = pe&#xa;            except Exception as exc:&#xa;                out.append(""FAIL-EXCEPTION: "" + str(exc))&#xa;                success = success and failureTests&#xa;                result = exc&#xa;&#xa;            if printResults:&#xa;                if fullDump:&#xa;                    out.append('')&#xa;                print('\n'.join(out))&#xa;&#xa;            allResults.append((t, result))&#xa;        &#xa;        return success, allResults&#xa;&#xa;        &#xa;class Token(ParserElement):&#xa;    """"""&#xa;    Abstract C{ParserElement} subclass, for defining atomic matching patterns.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Token,self).__init__( savelist=False )&#xa;&#xa;&#xa;class Empty(Token):&#xa;    """"""&#xa;    An empty token, will always match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Empty,self).__init__()&#xa;        self.name = ""Empty""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;&#xa;class NoMatch(Token):&#xa;    """"""&#xa;    A token that will never match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(NoMatch,self).__init__()&#xa;        self.name = ""NoMatch""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.errmsg = ""Unmatchable token""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Literal(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string.&#xa;    &#xa;    Example::&#xa;        Literal('blah').parseString('blah')  # -> ['blah']&#xa;        Literal('blah').parseString('blahfooblah')  # -> ['blah']&#xa;        Literal('blah').parseString('bla')  # -> Exception: Expected ""blah""&#xa;    &#xa;    For case-insensitive matching, use L{CaselessLiteral}.&#xa;    &#xa;    For keyword matching (force word break before and after the matched string),&#xa;    use L{Keyword} or L{CaselessKeyword}.&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(Literal,self).__init__()&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Literal; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;            self.__class__ = Empty&#xa;        self.name = '""%s""' % _ustr(self.match)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;&#xa;    # Performance tuning: this routine gets called a *lot*&#xa;    # if this is a single character match string  and the first character matches,&#xa;    # short-circuit as quickly as possible, and avoid calling startswith&#xa;    #~ @profile&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if (instring[loc] == self.firstMatchChar and&#xa;            (self.matchLen==1 or instring.startswith(self.match,loc)) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;_L = Literal&#xa;ParserElement._literalStringClass = Literal&#xa;&#xa;class Keyword(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string as a keyword, that is, it must be&#xa;    immediately followed by a non-keyword character.  Compare with C{L{Literal}}:&#xa;     - C{Literal(""if"")} will match the leading C{'if'} in C{'ifAndOnlyIf'}.&#xa;     - C{Keyword(""if"")} will not; it will only match the leading C{'if'} in C{'if x=1'}, or C{'if(y==2)'}&#xa;    Accepts two optional constructor arguments in addition to the keyword string:&#xa;     - C{identChars} is a string of characters that would be valid identifier characters,&#xa;          defaulting to all alphanumerics + ""_"" and ""$""&#xa;     - C{caseless} allows case-insensitive matching, default is C{False}.&#xa;       &#xa;    Example::&#xa;        Keyword(""start"").parseString(""start"")  # -> ['start']&#xa;        Keyword(""start"").parseString(""starting"")  # -> Exception&#xa;&#xa;    For case-insensitive matching, use L{CaselessKeyword}.&#xa;    """"""&#xa;    DEFAULT_KEYWORD_CHARS = alphanums+""_$""&#xa;&#xa;    def __init__( self, matchString, identChars=None, caseless=False ):&#xa;        super(Keyword,self).__init__()&#xa;        if identChars is None:&#xa;            identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Keyword; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;        self.name = '""%s""' % self.match&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;        self.caseless = caseless&#xa;        if caseless:&#xa;            self.caselessmatch = matchString.upper()&#xa;            identChars = identChars.upper()&#xa;        self.identChars = set(identChars)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.caseless:&#xa;            if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;                 (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) and&#xa;                 (loc == 0 or instring[loc-1].upper() not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        else:&#xa;            if (instring[loc] == self.firstMatchChar and&#xa;                (self.matchLen==1 or instring.startswith(self.match,loc)) and&#xa;                (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen] not in self.identChars) and&#xa;                (loc == 0 or instring[loc-1] not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;    def copy(self):&#xa;        c = super(Keyword,self).copy()&#xa;        c.identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        return c&#xa;&#xa;    @staticmethod&#xa;    def setDefaultKeywordChars( chars ):&#xa;        """"""Overrides the default Keyword chars&#xa;        """"""&#xa;        Keyword.DEFAULT_KEYWORD_CHARS = chars&#xa;&#xa;class CaselessLiteral(Literal):&#xa;    """"""&#xa;    Token to match a specified string, ignoring case of letters.&#xa;    Note: the matched results will always be in the case of the given&#xa;    match string, NOT the case of the input text.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessLiteral(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessKeyword}.)&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(CaselessLiteral,self).__init__( matchString.upper() )&#xa;        # Preserve the defining literal.&#xa;        self.returnString = matchString&#xa;        self.name = ""'%s'"" % self.returnString&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[ loc:loc+self.matchLen ].upper() == self.match:&#xa;            return loc+self.matchLen, self.returnString&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CaselessKeyword(Keyword):&#xa;    """"""&#xa;    Caseless version of L{Keyword}.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessKeyword(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessLiteral}.)&#xa;    """"""&#xa;    def __init__( self, matchString, identChars=None ):&#xa;        super(CaselessKeyword,self).__init__( matchString, identChars, caseless=True )&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;             (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CloseMatch(Token):&#xa;    """"""&#xa;    A variation on L{Literal} which matches ""close"" matches, that is, &#xa;    strings with at most 'n' mismatching characters. C{CloseMatch} takes parameters:&#xa;     - C{match_string} - string to be matched&#xa;     - C{maxMismatches} - (C{default=1}) maximum number of mismatches allowed to count as a match&#xa;    &#xa;    The results from a successful parse will contain the matched text from the input string and the following named results:&#xa;     - C{mismatches} - a list of the positions within the match_string where mismatches were found&#xa;     - C{original} - the original match_string used to compare against the input string&#xa;    &#xa;    If C{mismatches} is an empty list, then the match was an exact match.&#xa;    &#xa;    Example::&#xa;        patt = CloseMatch(""ATCATCGAATGGA"")&#xa;        patt.parseString(""ATCATCGAAXGGA"") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)&#xa;&#xa;        # exact match&#xa;        patt.parseString(""ATCATCGAATGGA"") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})&#xa;&#xa;        # close match allowing up to 2 mismatches&#xa;        patt = CloseMatch(""ATCATCGAATGGA"", maxMismatches=2)&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})&#xa;    """"""&#xa;    def __init__(self, match_string, maxMismatches=1):&#xa;        super(CloseMatch,self).__init__()&#xa;        self.name = match_string&#xa;        self.match_string = match_string&#xa;        self.maxMismatches = maxMismatches&#xa;        self.errmsg = ""Expected %r (with up to %d mismatches)"" % (self.match_string, self.maxMismatches)&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        start = loc&#xa;        instrlen = len(instring)&#xa;        maxloc = start + len(self.match_string)&#xa;&#xa;        if maxloc <= instrlen:&#xa;            match_string = self.match_string&#xa;            match_stringloc = 0&#xa;            mismatches = []&#xa;            maxMismatches = self.maxMismatches&#xa;&#xa;            for match_stringloc,s_m in enumerate(zip(instring[loc:maxloc], self.match_string)):&#xa;                src,mat = s_m&#xa;                if src != mat:&#xa;                    mismatches.append(match_stringloc)&#xa;                    if len(mismatches) > maxMismatches:&#xa;                        break&#xa;            else:&#xa;                loc = match_stringloc + 1&#xa;                results = ParseResults([instring[start:loc]])&#xa;                results['original'] = self.match_string&#xa;                results['mismatches'] = mismatches&#xa;                return loc, results&#xa;&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Word(Token):&#xa;    """"""&#xa;    Token for matching words composed of allowed character sets.&#xa;    Defined with string containing all allowed initial characters,&#xa;    an optional string containing allowed body characters (if omitted,&#xa;    defaults to the initial character set), and an optional minimum,&#xa;    maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction. An optional&#xa;    C{excludeChars} parameter can list characters that might be found in &#xa;    the input C{bodyChars} string; useful to define a word of all printables&#xa;    except for one or two characters, for instance.&#xa;    &#xa;    L{srange} is useful for defining custom character set strings for defining &#xa;    C{Word} expressions, using range notation from regular expression character sets.&#xa;    &#xa;    A common mistake is to use C{Word} to match a specific literal string, as in &#xa;    C{Word(""Address"")}. Remember that C{Word} uses the string argument to define&#xa;    I{sets} of matchable characters. This expression would match ""Add"", ""AAA"",&#xa;    ""dAred"", or any other word made up of the characters 'A', 'd', 'r', 'e', and 's'.&#xa;    To match an exact literal string, use L{Literal} or L{Keyword}.&#xa;&#xa;    pyparsing includes helper strings for building Words:&#xa;     - L{alphas}&#xa;     - L{nums}&#xa;     - L{alphanums}&#xa;     - L{hexnums}&#xa;     - L{alphas8bit} (alphabetic characters in ASCII range 128-255 - accented, tilded, umlauted, etc.)&#xa;     - L{punc8bit} (non-alphabetic characters in ASCII range 128-255 - currency, symbols, superscripts, diacriticals, etc.)&#xa;     - L{printables} (any non-whitespace character)&#xa;&#xa;    Example::&#xa;        # a word composed of digits&#xa;        integer = Word(nums) # equivalent to Word(""0123456789"") or Word(srange(""0-9""))&#xa;        &#xa;        # a word with a leading capital, and zero or more lowercase&#xa;        capital_word = Word(alphas.upper(), alphas.lower())&#xa;&#xa;        # hostnames are alphanumeric, with leading alpha, and '-'&#xa;        hostname = Word(alphas, alphanums+'-')&#xa;        &#xa;        # roman numeral (not a strict parser, accepts invalid mix of characters)&#xa;        roman = Word(""IVXLCDM"")&#xa;        &#xa;        # any string of non-whitespace characters, except for ','&#xa;        csv_value = Word(printables, excludeChars="","")&#xa;    """"""&#xa;    def __init__( self, initChars, bodyChars=None, min=1, max=0, exact=0, asKeyword=False, excludeChars=None ):&#xa;        super(Word,self).__init__()&#xa;        if excludeChars:&#xa;            initChars = ''.join(c for c in initChars if c not in excludeChars)&#xa;            if bodyChars:&#xa;                bodyChars = ''.join(c for c in bodyChars if c not in excludeChars)&#xa;        self.initCharsOrig = initChars&#xa;        self.initChars = set(initChars)&#xa;        if bodyChars :&#xa;            self.bodyCharsOrig = bodyChars&#xa;            self.bodyChars = set(bodyChars)&#xa;        else:&#xa;            self.bodyCharsOrig = initChars&#xa;            self.bodyChars = set(initChars)&#xa;&#xa;        self.maxSpecified = max > 0&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(Word()) if zero-length word is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.asKeyword = asKeyword&#xa;&#xa;        if ' ' not in self.initCharsOrig+self.bodyCharsOrig and (min==1 and max==0 and exact==0):&#xa;            if self.bodyCharsOrig == self.initCharsOrig:&#xa;                self.reString = ""[%s]+"" % _escapeRegexRangeChars(self.initCharsOrig)&#xa;            elif len(self.initCharsOrig) == 1:&#xa;                self.reString = ""%s[%s]*"" % \&#xa;                                      (re.escape(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            else:&#xa;                self.reString = ""[%s][%s]*"" % \&#xa;                                      (_escapeRegexRangeChars(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            if self.asKeyword:&#xa;                self.reString = r""\b""+self.reString+r""\b""&#xa;            try:&#xa;                self.re = re.compile( self.reString )&#xa;            except Exception:&#xa;                self.re = None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.re:&#xa;            result = self.re.match(instring,loc)&#xa;            if not result:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;            loc = result.end()&#xa;            return loc, result.group()&#xa;&#xa;        if not(instring[ loc ] in self.initChars):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        instrlen = len(instring)&#xa;        bodychars = self.bodyChars&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, instrlen )&#xa;        while loc < maxloc and instring[loc] in bodychars:&#xa;            loc += 1&#xa;&#xa;        throwException = False&#xa;        if loc - start < self.minLen:&#xa;            throwException = True&#xa;        if self.maxSpecified and loc < instrlen and instring[loc] in bodychars:&#xa;            throwException = True&#xa;        if self.asKeyword:&#xa;            if (start>0 and instring[start-1] in bodychars) or (loc<instrlen and instring[loc] in bodychars):&#xa;                throwException = True&#xa;&#xa;        if throwException:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Word,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;&#xa;        if self.strRepr is None:&#xa;&#xa;            def charsAsStr(s):&#xa;                if len(s)>4:&#xa;                    return s[:4]+""...""&#xa;                else:&#xa;                    return s&#xa;&#xa;            if ( self.initCharsOrig != self.bodyCharsOrig ):&#xa;                self.strRepr = ""W:(%s,%s)"" % ( charsAsStr(self.initCharsOrig), charsAsStr(self.bodyCharsOrig) )&#xa;            else:&#xa;                self.strRepr = ""W:(%s)"" % charsAsStr(self.initCharsOrig)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Regex(Token):&#xa;    """"""&#xa;    Token for matching strings that match a given regular expression.&#xa;    Defined with string specifying the regular expression in a form recognized by the inbuilt Python re module.&#xa;    If the given regex contains named groups (defined using C{(?P<name>...)}), these will be preserved as &#xa;    named parse results.&#xa;&#xa;    Example::&#xa;        realnum = Regex(r""[+-]?\d+\.\d*"")&#xa;        date = Regex(r'(?P<year>\d{4})-(?P<month>\d\d?)-(?P<day>\d\d?)')&#xa;        # ref: http://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression&#xa;        roman = Regex(r""M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})"")&#xa;    """"""&#xa;    compiledREtype = type(re.compile(""[A-Z]""))&#xa;    def __init__( self, pattern, flags=0):&#xa;        """"""The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.""""""&#xa;        super(Regex,self).__init__()&#xa;&#xa;        if isinstance(pattern, basestring):&#xa;            if not pattern:&#xa;                warnings.warn(""null string passed to Regex; use Empty() instead"",&#xa;                        SyntaxWarning, stacklevel=2)&#xa;&#xa;            self.pattern = pattern&#xa;            self.flags = flags&#xa;&#xa;            try:&#xa;                self.re = re.compile(self.pattern, self.flags)&#xa;                self.reString = self.pattern&#xa;            except sre_constants.error:&#xa;                warnings.warn(""invalid pattern (%s) passed to Regex"" % pattern,&#xa;                    SyntaxWarning, stacklevel=2)&#xa;                raise&#xa;&#xa;        elif isinstance(pattern, Regex.compiledREtype):&#xa;            self.re = pattern&#xa;            self.pattern = \&#xa;            self.reString = str(pattern)&#xa;            self.flags = flags&#xa;            &#xa;        else:&#xa;            raise ValueError(""Regex may only be constructed with a string or a compiled RE object"")&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = self.re.match(instring,loc)&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        d = result.groupdict()&#xa;        ret = ParseResults(result.group())&#xa;        if d:&#xa;            for k in d:&#xa;                ret[k] = d[k]&#xa;        return loc,ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Regex,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""Re:(%s)"" % repr(self.pattern)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class QuotedString(Token):&#xa;    r""""""&#xa;    Token for matching strings that are delimited by quoting characters.&#xa;    &#xa;    Defined with the following parameters:&#xa;        - quoteChar - string of one or more characters defining the quote delimiting string&#xa;        - escChar - character to escape quotes, typically backslash (default=C{None})&#xa;        - escQuote - special quote sequence to escape an embedded quote string (such as SQL's """" to escape an embedded "") (default=C{None})&#xa;        - multiline - boolean indicating whether quotes can span multiple lines (default=C{False})&#xa;        - unquoteResults - boolean indicating whether the matched text should be unquoted (default=C{True})&#xa;        - endQuoteChar - string of one or more characters defining the end of the quote delimited string (default=C{None} => same as quoteChar)&#xa;        - convertWhitespaceEscapes - convert escaped whitespace (C{'\t'}, C{'\n'}, etc.) to actual whitespace (default=C{True})&#xa;&#xa;    Example::&#xa;        qs = QuotedString('""')&#xa;        print(qs.searchString('lsjdf ""This is the quote"" sldjf'))&#xa;        complex_qs = QuotedString('{{', endQuoteChar='}}')&#xa;        print(complex_qs.searchString('lsjdf {{This is the ""quote""}} sldjf'))&#xa;        sql_qs = QuotedString('""', escQuote='""""')&#xa;        print(sql_qs.searchString('lsjdf ""This is the quote with """"embedded"""" quotes"" sldjf'))&#xa;    prints::&#xa;        [['This is the quote']]&#xa;        [['This is the ""quote""']]&#xa;        [['This is the quote with ""embedded"" quotes']]&#xa;    """"""&#xa;    def __init__( self, quoteChar, escChar=None, escQuote=None, multiline=False, unquoteResults=True, endQuoteChar=None, convertWhitespaceEscapes=True):&#xa;        super(QuotedString,self).__init__()&#xa;&#xa;        # remove white space from quote chars - wont work anyway&#xa;        quoteChar = quoteChar.strip()&#xa;        if not quoteChar:&#xa;            warnings.warn(""quoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;            raise SyntaxError()&#xa;&#xa;        if endQuoteChar is None:&#xa;            endQuoteChar = quoteChar&#xa;        else:&#xa;            endQuoteChar = endQuoteChar.strip()&#xa;            if not endQuoteChar:&#xa;                warnings.warn(""endQuoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;                raise SyntaxError()&#xa;&#xa;        self.quoteChar = quoteChar&#xa;        self.quoteCharLen = len(quoteChar)&#xa;        self.firstQuoteChar = quoteChar[0]&#xa;        self.endQuoteChar = endQuoteChar&#xa;        self.endQuoteCharLen = len(endQuoteChar)&#xa;        self.escChar = escChar&#xa;        self.escQuote = escQuote&#xa;        self.unquoteResults = unquoteResults&#xa;        self.convertWhitespaceEscapes = convertWhitespaceEscapes&#xa;&#xa;        if multiline:&#xa;            self.flags = re.MULTILINE | re.DOTALL&#xa;            self.pattern = r'%s(?:[^%s%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        else:&#xa;            self.flags = 0&#xa;            self.pattern = r'%s(?:[^%s\n\r%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        if len(self.endQuoteChar) > 1:&#xa;            self.pattern += (&#xa;                '|(?:' + ')|(?:'.join(""%s[^%s]"" % (re.escape(self.endQuoteChar[:i]),&#xa;                                               _escapeRegexRangeChars(self.endQuoteChar[i]))&#xa;                                    for i in range(len(self.endQuoteChar)-1,0,-1)) + ')'&#xa;                )&#xa;        if escQuote:&#xa;            self.pattern += (r'|(?:%s)' % re.escape(escQuote))&#xa;        if escChar:&#xa;            self.pattern += (r'|(?:%s.)' % re.escape(escChar))&#xa;            self.escCharReplacePattern = re.escape(self.escChar)+""(.)""&#xa;        self.pattern += (r')*%s' % re.escape(self.endQuoteChar))&#xa;&#xa;        try:&#xa;            self.re = re.compile(self.pattern, self.flags)&#xa;            self.reString = self.pattern&#xa;        except sre_constants.error:&#xa;            warnings.warn(""invalid pattern (%s) passed to Regex"" % self.pattern,&#xa;                SyntaxWarning, stacklevel=2)&#xa;            raise&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = instring[loc] == self.firstQuoteChar and self.re.match(instring,loc) or None&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        ret = result.group()&#xa;&#xa;        if self.unquoteResults:&#xa;&#xa;            # strip off quotes&#xa;            ret = ret[self.quoteCharLen:-self.endQuoteCharLen]&#xa;&#xa;            if isinstance(ret,basestring):&#xa;                # replace escaped whitespace&#xa;                if '\\' in ret and self.convertWhitespaceEscapes:&#xa;                    ws_map = {&#xa;                        r'\t' : '\t',&#xa;                        r'\n' : '\n',&#xa;                        r'\f' : '\f',&#xa;                        r'\r' : '\r',&#xa;                    }&#xa;                    for wslit,wschar in ws_map.items():&#xa;                        ret = ret.replace(wslit, wschar)&#xa;&#xa;                # replace escaped characters&#xa;                if self.escChar:&#xa;                    ret = re.sub(self.escCharReplacePattern,""\g<1>"",ret)&#xa;&#xa;                # replace escaped quotes&#xa;                if self.escQuote:&#xa;                    ret = ret.replace(self.escQuote, self.endQuoteChar)&#xa;&#xa;        return loc, ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(QuotedString,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""quoted string, starting with %s ending with %s"" % (self.quoteChar, self.endQuoteChar)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class CharsNotIn(Token):&#xa;    """"""&#xa;    Token for matching words composed of characters I{not} in a given set (will&#xa;    include whitespace in matched characters if not listed in the provided exclusion set - see example).&#xa;    Defined with string containing all disallowed characters, and an optional&#xa;    minimum, maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction.&#xa;&#xa;    Example::&#xa;        # define a comma-separated-value as anything that is not a ','&#xa;        csv_value = CharsNotIn(',')&#xa;        print(delimitedList(csv_value).parseString(""dkls,lsdkjf,s12 34,@!#,213""))&#xa;    prints::&#xa;        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']&#xa;    """"""&#xa;    def __init__( self, notChars, min=1, max=0, exact=0 ):&#xa;        super(CharsNotIn,self).__init__()&#xa;        self.skipWhitespace = False&#xa;        self.notChars = notChars&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(CharsNotIn()) if zero-length char group is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = ( self.minLen == 0 )&#xa;        self.mayIndexError = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[loc] in self.notChars:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        notchars = self.notChars&#xa;        maxlen = min( start+self.maxLen, len(instring) )&#xa;        while loc < maxlen and \&#xa;              (instring[loc] not in notchars):&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(CharsNotIn, self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            if len(self.notChars) > 4:&#xa;                self.strRepr = ""!W:(%s...)"" % self.notChars[:4]&#xa;            else:&#xa;                self.strRepr = ""!W:(%s)"" % self.notChars&#xa;&#xa;        return self.strRepr&#xa;&#xa;class White(Token):&#xa;    """"""&#xa;    Special matching class for matching whitespace.  Normally, whitespace is ignored&#xa;    by pyparsing grammars.  This class is included when some whitespace structures&#xa;    are significant.  Define with a string containing the whitespace characters to be&#xa;    matched; default is C{"" \\t\\r\\n""}.  Also takes optional C{min}, C{max}, and C{exact} arguments,&#xa;    as defined for the C{L{Word}} class.&#xa;    """"""&#xa;    whiteStrs = {&#xa;        "" "" : ""<SPC>"",&#xa;        ""\t"": ""<TAB>"",&#xa;        ""\n"": ""<LF>"",&#xa;        ""\r"": ""<CR>"",&#xa;        ""\f"": ""<FF>"",&#xa;        }&#xa;    def __init__(self, ws="" \t\r\n"", min=1, max=0, exact=0):&#xa;        super(White,self).__init__()&#xa;        self.matchWhite = ws&#xa;        self.setWhitespaceChars( """".join(c for c in self.whiteChars if c not in self.matchWhite) )&#xa;        #~ self.leaveWhitespace()&#xa;        self.name = ("""".join(White.whiteStrs[c] for c in self.matchWhite))&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if not(instring[ loc ] in self.matchWhite):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        start = loc&#xa;        loc += 1&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, len(instring) )&#xa;        while loc < maxloc and instring[loc] in self.matchWhite:&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;&#xa;class _PositionToken(Token):&#xa;    def __init__( self ):&#xa;        super(_PositionToken,self).__init__()&#xa;        self.name=self.__class__.__name__&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;class GoToColumn(_PositionToken):&#xa;    """"""&#xa;    Token to advance to a specific column of input text; useful for tabular report scraping.&#xa;    """"""&#xa;    def __init__( self, colno ):&#xa;        super(GoToColumn,self).__init__()&#xa;        self.col = colno&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if col(loc,instring) != self.col:&#xa;            instrlen = len(instring)&#xa;            if self.ignoreExprs:&#xa;                loc = self._skipIgnorables( instring, loc )&#xa;            while loc < instrlen and instring[loc].isspace() and col( loc, instring ) != self.col :&#xa;                loc += 1&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        thiscol = col( loc, instring )&#xa;        if thiscol > self.col:&#xa;            raise ParseException( instring, loc, ""Text not in expected column"", self )&#xa;        newloc = loc + self.col - thiscol&#xa;        ret = instring[ loc: newloc ]&#xa;        return newloc, ret&#xa;&#xa;&#xa;class LineStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of a line within the parse string&#xa;    &#xa;    Example::&#xa;    &#xa;        test = '''\&#xa;        AAA this line&#xa;        AAA and this line&#xa;          AAA but not this one&#xa;        B AAA and definitely not this one&#xa;        '''&#xa;&#xa;        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):&#xa;            print(t)&#xa;    &#xa;    Prints::&#xa;        ['AAA', ' this line']&#xa;        ['AAA', ' and this line']    &#xa;&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineStart,self).__init__()&#xa;        self.errmsg = ""Expected start of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if col(loc, instring) == 1:&#xa;            return loc, []&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class LineEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of a line within the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineEnd,self).__init__()&#xa;        self.setWhitespaceChars( ParserElement.DEFAULT_WHITE_CHARS.replace(""\n"","""") )&#xa;        self.errmsg = ""Expected end of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc<len(instring):&#xa;            if instring[loc] == ""\n"":&#xa;                return loc+1, ""\n""&#xa;            else:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class StringStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringStart,self).__init__()&#xa;        self.errmsg = ""Expected start of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            # see if entire string up to here is just whitespace and ignoreables&#xa;            if loc != self.preParse( instring, 0 ):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class StringEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringEnd,self).__init__()&#xa;        self.errmsg = ""Expected end of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc < len(instring):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        elif loc > len(instring):&#xa;            return loc, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class WordStart(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the beginning of a Word, and&#xa;    is not preceded by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordStart(alphanums)}. C{WordStart} will also match at the beginning of&#xa;    the string being parsed, or at the beginning of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordStart,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.errmsg = ""Not at the start of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            if (instring[loc-1] in self.wordChars or&#xa;                instring[loc] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class WordEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the end of a Word, and&#xa;    is not followed by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordEnd(alphanums)}. C{WordEnd} will also match at the end of&#xa;    the string being parsed, or at the end of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordEnd,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.skipWhitespace = False&#xa;        self.errmsg = ""Not at the end of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        instrlen = len(instring)&#xa;        if instrlen>0 and loc<instrlen:&#xa;            if (instring[loc] in self.wordChars or&#xa;                instring[loc-1] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;&#xa;class ParseExpression(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of ParserElement, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(ParseExpression,self).__init__(savelist)&#xa;        if isinstance( exprs, _generatorType ):&#xa;            exprs = list(exprs)&#xa;&#xa;        if isinstance( exprs, basestring ):&#xa;            self.exprs = [ ParserElement._literalStringClass( exprs ) ]&#xa;        elif isinstance( exprs, collections.Iterable ):&#xa;            exprs = list(exprs)&#xa;            # if sequence of strings provided, wrap with Literal&#xa;            if all(isinstance(expr, basestring) for expr in exprs):&#xa;                exprs = map(ParserElement._literalStringClass, exprs)&#xa;            self.exprs = list(exprs)&#xa;        else:&#xa;            try:&#xa;                self.exprs = list( exprs )&#xa;            except TypeError:&#xa;                self.exprs = [ exprs ]&#xa;        self.callPreparse = False&#xa;&#xa;    def __getitem__( self, i ):&#xa;        return self.exprs[i]&#xa;&#xa;    def append( self, other ):&#xa;        self.exprs.append( other )&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""Extends C{leaveWhitespace} defined in base class, and also invokes C{leaveWhitespace} on&#xa;           all contained expressions.""""""&#xa;        self.skipWhitespace = False&#xa;        self.exprs = [ e.copy() for e in self.exprs ]&#xa;        for e in self.exprs:&#xa;            e.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseExpression, self).ignore( other )&#xa;                for e in self.exprs:&#xa;                    e.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseExpression, self).ignore( other )&#xa;            for e in self.exprs:&#xa;                e.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseExpression,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.exprs) )&#xa;        return self.strRepr&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseExpression,self).streamline()&#xa;&#xa;        for e in self.exprs:&#xa;            e.streamline()&#xa;&#xa;        # collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )&#xa;        # but only if there are no parse actions or resultsNames on the nested And's&#xa;        # (likewise for Or's and MatchFirst's)&#xa;        if ( len(self.exprs) == 2 ):&#xa;            other = self.exprs[0]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = other.exprs[:] + [ self.exprs[1] ]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;            other = self.exprs[-1]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = self.exprs[:-1] + other.exprs[:]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;        self.errmsg = ""Expected "" + _ustr(self)&#xa;        &#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        ret = super(ParseExpression,self).setResultsName(name,listAllMatches)&#xa;        return ret&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        for e in self.exprs:&#xa;            e.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;        &#xa;    def copy(self):&#xa;        ret = super(ParseExpression,self).copy()&#xa;        ret.exprs = [e.copy() for e in self.exprs]&#xa;        return ret&#xa;&#xa;class And(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found in the given order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'+'} operator.&#xa;    May also be constructed using the C{'-'} operator, which will suppress backtracking.&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        name_expr = OneOrMore(Word(alphas))&#xa;&#xa;        expr = And([integer(""id""),name_expr(""name""),integer(""age"")])&#xa;        # more easily written as:&#xa;        expr = integer(""id"") + name_expr(""name"") + integer(""age"")&#xa;    """"""&#xa;&#xa;    class _ErrorStop(Empty):&#xa;        def __init__(self, *args, **kwargs):&#xa;            super(And._ErrorStop,self).__init__(*args, **kwargs)&#xa;            self.name = '-'&#xa;            self.leaveWhitespace()&#xa;&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(And,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.setWhitespaceChars( self.exprs[0].whiteChars )&#xa;        self.skipWhitespace = self.exprs[0].skipWhitespace&#xa;        self.callPreparse = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        # pass False as last arg to _parse for first element, since we already&#xa;        # pre-parsed the string as part of our And pre-parsing&#xa;        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )&#xa;        errorStop = False&#xa;        for e in self.exprs[1:]:&#xa;            if isinstance(e, And._ErrorStop):&#xa;                errorStop = True&#xa;                continue&#xa;            if errorStop:&#xa;                try:&#xa;                    loc, exprtokens = e._parse( instring, loc, doActions )&#xa;                except ParseSyntaxException:&#xa;                    raise&#xa;                except ParseBaseException as pe:&#xa;                    pe.__traceback__ = None&#xa;                    raise ParseSyntaxException._from_exception(pe)&#xa;                except IndexError:&#xa;                    raise ParseSyntaxException(instring, len(instring), self.errmsg, self)&#xa;            else:&#xa;                loc, exprtokens = e._parse( instring, loc, doActions )&#xa;            if exprtokens or exprtokens.haskeys():&#xa;                resultlist += exprtokens&#xa;        return loc, resultlist&#xa;&#xa;    def __iadd__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #And( [ self, other ] )&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;            if not e.mayReturnEmpty:&#xa;                break&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Or(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the expression that matches the longest string will be used.&#xa;    May be constructed using the C{'^'} operator.&#xa;&#xa;    Example::&#xa;        # construct Or using '^' operator&#xa;        &#xa;        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789""))&#xa;    prints::&#xa;        [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(Or,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        matches = []&#xa;        for e in self.exprs:&#xa;            try:&#xa;                loc2 = e.tryParse( instring, loc )&#xa;            except ParseException as err:&#xa;                err.__traceback__ = None&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;            else:&#xa;                # save match among all matches, to retry longest to shortest&#xa;                matches.append((loc2, e))&#xa;&#xa;        if matches:&#xa;            matches.sort(key=lambda x: -x[0])&#xa;            for _,e in matches:&#xa;                try:&#xa;                    return e._parse( instring, loc, doActions )&#xa;                except ParseException as err:&#xa;                    err.__traceback__ = None&#xa;                    if err.loc > maxExcLoc:&#xa;                        maxException = err&#xa;                        maxExcLoc = err.loc&#xa;&#xa;        if maxException is not None:&#xa;            maxException.msg = self.errmsg&#xa;            raise maxException&#xa;        else:&#xa;            raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;&#xa;    def __ixor__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #Or( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" ^ "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class MatchFirst(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the first one listed is the one that will match.&#xa;    May be constructed using the C{'|'} operator.&#xa;&#xa;    Example::&#xa;        # construct MatchFirst using '|' operator&#xa;        &#xa;        # watch the order of expressions to match&#xa;        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789"")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]&#xa;&#xa;        # put more selective expression first&#xa;        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)&#xa;        print(number.searchString(""123 3.1416 789"")) #  Better -> [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(MatchFirst,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        for e in self.exprs:&#xa;            try:&#xa;                ret = e._parse( instring, loc, doActions )&#xa;                return ret&#xa;            except ParseException as err:&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;&#xa;        # only got here if no expression matched, raise exception for match that made it the furthest&#xa;        else:&#xa;            if maxException is not None:&#xa;                maxException.msg = self.errmsg&#xa;                raise maxException&#xa;            else:&#xa;                raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;    def __ior__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #MatchFirst( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" | "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class Each(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found, but in any order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'&'} operator.&#xa;&#xa;    Example::&#xa;        color = oneOf(""RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN"")&#xa;        shape_type = oneOf(""SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON"")&#xa;        integer = Word(nums)&#xa;        shape_attr = ""shape:"" + shape_type(""shape"")&#xa;        posn_attr = ""posn:"" + Group(integer(""x"") + ',' + integer(""y""))(""posn"")&#xa;        color_attr = ""color:"" + color(""color"")&#xa;        size_attr = ""size:"" + integer(""size"")&#xa;&#xa;        # use Each (using operator '&') to accept attributes in any order &#xa;        # (shape and posn are required, color and size are optional)&#xa;        shape_spec = shape_attr & posn_attr & Optional(color_attr) & Optional(size_attr)&#xa;&#xa;        shape_spec.runTests('''&#xa;            shape: SQUARE color: BLACK posn: 100, 120&#xa;            shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;            color:GREEN size:20 shape:TRIANGLE posn:20,40&#xa;            '''&#xa;            )&#xa;    prints::&#xa;        shape: SQUARE color: BLACK posn: 100, 120&#xa;        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]&#xa;        - color: BLACK&#xa;        - posn: ['100', ',', '120']&#xa;          - x: 100&#xa;          - y: 120&#xa;        - shape: SQUARE&#xa;&#xa;&#xa;        shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]&#xa;        - color: BLUE&#xa;        - posn: ['50', ',', '80']&#xa;          - x: 50&#xa;          - y: 80&#xa;        - shape: CIRCLE&#xa;        - size: 50&#xa;&#xa;&#xa;        color: GREEN size: 20 shape: TRIANGLE posn: 20,40&#xa;        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]&#xa;        - color: GREEN&#xa;        - posn: ['20', ',', '40']&#xa;          - x: 20&#xa;          - y: 40&#xa;        - shape: TRIANGLE&#xa;        - size: 20&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(Each,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.skipWhitespace = True&#xa;        self.initExprGroups = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.initExprGroups:&#xa;            self.opt1map = dict((id(e.expr),e) for e in self.exprs if isinstance(e,Optional))&#xa;            opt1 = [ e.expr for e in self.exprs if isinstance(e,Optional) ]&#xa;            opt2 = [ e for e in self.exprs if e.mayReturnEmpty and not isinstance(e,Optional)]&#xa;            self.optionals = opt1 + opt2&#xa;            self.multioptionals = [ e.expr for e in self.exprs if isinstance(e,ZeroOrMore) ]&#xa;            self.multirequired = [ e.expr for e in self.exprs if isinstance(e,OneOrMore) ]&#xa;            self.required = [ e for e in self.exprs if not isinstance(e,(Optional,ZeroOrMore,OneOrMore)) ]&#xa;            self.required += self.multirequired&#xa;            self.initExprGroups = False&#xa;        tmpLoc = loc&#xa;        tmpReqd = self.required[:]&#xa;        tmpOpt  = self.optionals[:]&#xa;        matchOrder = []&#xa;&#xa;        keepMatching = True&#xa;        while keepMatching:&#xa;            tmpExprs = tmpReqd + tmpOpt + self.multioptionals + self.multirequired&#xa;            failed = []&#xa;            for e in tmpExprs:&#xa;                try:&#xa;                    tmpLoc = e.tryParse( instring, tmpLoc )&#xa;                except ParseException:&#xa;                    failed.append(e)&#xa;                else:&#xa;                    matchOrder.append(self.opt1map.get(id(e),e))&#xa;                    if e in tmpReqd:&#xa;                        tmpReqd.remove(e)&#xa;                    elif e in tmpOpt:&#xa;                        tmpOpt.remove(e)&#xa;            if len(failed) == len(tmpExprs):&#xa;                keepMatching = False&#xa;&#xa;        if tmpReqd:&#xa;            missing = "", "".join(_ustr(e) for e in tmpReqd)&#xa;            raise ParseException(instring,loc,""Missing one or more required elements (%s)"" % missing )&#xa;&#xa;        # add any unmatched Optionals, in case they have default values defined&#xa;        matchOrder += [e for e in self.exprs if isinstance(e,Optional) and e.expr in tmpOpt]&#xa;&#xa;        resultlist = []&#xa;        for e in matchOrder:&#xa;            loc,results = e._parse(instring,loc,doActions)&#xa;            resultlist.append(results)&#xa;&#xa;        finalResults = sum(resultlist, ParseResults([]))&#xa;        return loc, finalResults&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" & "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class ParseElementEnhance(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of C{ParserElement}, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(ParseElementEnhance,self).__init__(savelist)&#xa;        if isinstance( expr, basestring ):&#xa;            if issubclass(ParserElement._literalStringClass, Token):&#xa;                expr = ParserElement._literalStringClass(expr)&#xa;            else:&#xa;                expr = ParserElement._literalStringClass(Literal(expr))&#xa;        self.expr = expr&#xa;        self.strRepr = None&#xa;        if expr is not None:&#xa;            self.mayIndexError = expr.mayIndexError&#xa;            self.mayReturnEmpty = expr.mayReturnEmpty&#xa;            self.setWhitespaceChars( expr.whiteChars )&#xa;            self.skipWhitespace = expr.skipWhitespace&#xa;            self.saveAsList = expr.saveAsList&#xa;            self.callPreparse = expr.callPreparse&#xa;            self.ignoreExprs.extend(expr.ignoreExprs)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr is not None:&#xa;            return self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        else:&#xa;            raise ParseException("""",loc,self.errmsg,self)&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        self.expr = self.expr.copy()&#xa;        if self.expr is not None:&#xa;            self.expr.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseElementEnhance, self).ignore( other )&#xa;                if self.expr is not None:&#xa;                    self.expr.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseElementEnhance, self).ignore( other )&#xa;            if self.expr is not None:&#xa;                self.expr.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseElementEnhance,self).streamline()&#xa;        if self.expr is not None:&#xa;            self.expr.streamline()&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        if self in parseElementList:&#xa;            raise RecursiveGrammarException( parseElementList+[self] )&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        if self.expr is not None:&#xa;            self.expr.checkRecursion( subRecCheckList )&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        if self.expr is not None:&#xa;            self.expr.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseElementEnhance,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None and self.expr is not None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.expr) )&#xa;        return self.strRepr&#xa;&#xa;&#xa;class FollowedBy(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead matching of the given parse expression.  C{FollowedBy}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression matches at the current&#xa;    position.  C{FollowedBy} always returns a null token list.&#xa;&#xa;    Example::&#xa;        # use FollowedBy to match a label only if it is followed by a ':'&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        OneOrMore(attr_expr).parseString(""shape: SQUARE color: BLACK posn: upper left"").pprint()&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(FollowedBy,self).__init__(expr)&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self.expr.tryParse( instring, loc )&#xa;        return loc, []&#xa;&#xa;&#xa;class NotAny(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead to disallow matching with the given parse expression.  C{NotAny}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression does I{not} match at the current&#xa;    position.  Also, C{NotAny} does I{not} skip over leading whitespace. C{NotAny}&#xa;    always returns a null token list.  May be constructed using the '~' operator.&#xa;&#xa;    Example::&#xa;        &#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(NotAny,self).__init__(expr)&#xa;        #~ self.leaveWhitespace()&#xa;        self.skipWhitespace = False  # do NOT use self.leaveWhitespace(), don't want to propagate to exprs&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Found unwanted token, ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr.canParseNext(instring, loc):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""~{"" + _ustr(self.expr) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _MultipleMatch(ParseElementEnhance):&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(_MultipleMatch, self).__init__(expr)&#xa;        self.saveAsList = True&#xa;        ender = stopOn&#xa;        if isinstance(ender, basestring):&#xa;            ender = ParserElement._literalStringClass(ender)&#xa;        self.not_ender = ~ender if ender is not None else None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self_expr_parse = self.expr._parse&#xa;        self_skip_ignorables = self._skipIgnorables&#xa;        check_ender = self.not_ender is not None&#xa;        if check_ender:&#xa;            try_not_ender = self.not_ender.tryParse&#xa;        &#xa;        # must be at least one (but first see if we are the stopOn sentinel;&#xa;        # if so, fail)&#xa;        if check_ender:&#xa;            try_not_ender(instring, loc)&#xa;        loc, tokens = self_expr_parse( instring, loc, doActions, callPreParse=False )&#xa;        try:&#xa;            hasIgnoreExprs = (not not self.ignoreExprs)&#xa;            while 1:&#xa;                if check_ender:&#xa;                    try_not_ender(instring, loc)&#xa;                if hasIgnoreExprs:&#xa;                    preloc = self_skip_ignorables( instring, loc )&#xa;                else:&#xa;                    preloc = loc&#xa;                loc, tmptokens = self_expr_parse( instring, preloc, doActions )&#xa;                if tmptokens or tmptokens.haskeys():&#xa;                    tokens += tmptokens&#xa;        except (ParseException,IndexError):&#xa;            pass&#xa;&#xa;        return loc, tokens&#xa;        &#xa;class OneOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Repetition of one or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match one or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: BLACK""&#xa;        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]&#xa;&#xa;        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]&#xa;        &#xa;        # could also be written as&#xa;        (attr_expr * (1,)).parseString(text).pprint()&#xa;    """"""&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + _ustr(self.expr) + ""}...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class ZeroOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Optional repetition of zero or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example: similar to L{OneOrMore}&#xa;    """"""&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(ZeroOrMore,self).__init__(expr, stopOn=stopOn)&#xa;        self.mayReturnEmpty = True&#xa;        &#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)&#xa;        except (ParseException,IndexError):&#xa;            return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _NullToken(object):&#xa;    def __bool__(self):&#xa;        return False&#xa;    __nonzero__ = __bool__&#xa;    def __str__(self):&#xa;        return """"&#xa;&#xa;_optionalNotMatched = _NullToken()&#xa;class Optional(ParseElementEnhance):&#xa;    """"""&#xa;    Optional matching of the given expression.&#xa;&#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - default (optional) - value to be returned if the optional expression is not found.&#xa;&#xa;    Example::&#xa;        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier&#xa;        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))&#xa;        zip.runTests('''&#xa;            # traditional ZIP code&#xa;            12345&#xa;            &#xa;            # ZIP+4 form&#xa;            12101-0001&#xa;            &#xa;            # invalid ZIP&#xa;            98765-&#xa;            ''')&#xa;    prints::&#xa;        # traditional ZIP code&#xa;        12345&#xa;        ['12345']&#xa;&#xa;        # ZIP+4 form&#xa;        12101-0001&#xa;        ['12101-0001']&#xa;&#xa;        # invalid ZIP&#xa;        98765-&#xa;             ^&#xa;        FAIL: Expected end of text (at char 5), (line:1, col:6)&#xa;    """"""&#xa;    def __init__( self, expr, default=_optionalNotMatched ):&#xa;        super(Optional,self).__init__( expr, savelist=False )&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.defaultValue = default&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        except (ParseException,IndexError):&#xa;            if self.defaultValue is not _optionalNotMatched:&#xa;                if self.expr.resultsName:&#xa;                    tokens = ParseResults([ self.defaultValue ])&#xa;                    tokens[self.expr.resultsName] = self.defaultValue&#xa;                else:&#xa;                    tokens = [ self.defaultValue ]&#xa;            else:&#xa;                tokens = []&#xa;        return loc, tokens&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class SkipTo(ParseElementEnhance):&#xa;    """"""&#xa;    Token for skipping over all undefined text until the matched expression is found.&#xa;&#xa;    Parameters:&#xa;     - expr - target expression marking the end of the data to be skipped&#xa;     - include - (default=C{False}) if True, the target expression is also parsed &#xa;          (the skipped text and target expression are returned as a 2-element list).&#xa;     - ignore - (default=C{None}) used to define grammars (typically quoted strings and &#xa;          comments) that might contain false matches to the target expression&#xa;     - failOn - (default=C{None}) define expressions that are not allowed to be &#xa;          included in the skipped test; if found before the target expression is found, &#xa;          the SkipTo is not a match&#xa;&#xa;    Example::&#xa;        report = '''&#xa;            Outstanding Issues Report - 1 Jan 2000&#xa;&#xa;               # | Severity | Description                               |  Days Open&#xa;            -----+----------+-------------------------------------------+-----------&#xa;             101 | Critical | Intermittent system crash                 |          6&#xa;              94 | Cosmetic | Spelling error on Login ('log|n')         |         14&#xa;              79 | Minor    | System slow when running too many reports |         47&#xa;            '''&#xa;        integer = Word(nums)&#xa;        SEP = Suppress('|')&#xa;        # use SkipTo to simply match everything up until the next SEP&#xa;        # - ignore quoted strings, so that a '|' character inside a quoted string does not match&#xa;        # - parse action will call token.strip() for each matched token, i.e., the description body&#xa;        string_data = SkipTo(SEP, ignore=quotedString)&#xa;        string_data.setParseAction(tokenMap(str.strip))&#xa;        ticket_expr = (integer(""issue_num"") + SEP &#xa;                      + string_data(""sev"") + SEP &#xa;                      + string_data(""desc"") + SEP &#xa;                      + integer(""days_open""))&#xa;        &#xa;        for tkt in ticket_expr.searchString(report):&#xa;            print tkt.dump()&#xa;    prints::&#xa;        ['101', 'Critical', 'Intermittent system crash', '6']&#xa;        - days_open: 6&#xa;        - desc: Intermittent system crash&#xa;        - issue_num: 101&#xa;        - sev: Critical&#xa;        ['94', 'Cosmetic', ""Spelling error on Login ('log|n')"", '14']&#xa;        - days_open: 14&#xa;        - desc: Spelling error on Login ('log|n')&#xa;        - issue_num: 94&#xa;        - sev: Cosmetic&#xa;        ['79', 'Minor', 'System slow when running too many reports', '47']&#xa;        - days_open: 47&#xa;        - desc: System slow when running too many reports&#xa;        - issue_num: 79&#xa;        - sev: Minor&#xa;    """"""&#xa;    def __init__( self, other, include=False, ignore=None, failOn=None ):&#xa;        super( SkipTo, self ).__init__( other )&#xa;        self.ignoreExpr = ignore&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.includeMatch = include&#xa;        self.asList = False&#xa;        if isinstance(failOn, basestring):&#xa;            self.failOn = ParserElement._literalStringClass(failOn)&#xa;        else:&#xa;            self.failOn = failOn&#xa;        self.errmsg = ""No match found for ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        startloc = loc&#xa;        instrlen = len(instring)&#xa;        expr = self.expr&#xa;        expr_parse = self.expr._parse&#xa;        self_failOn_canParseNext = self.failOn.canParseNext if self.failOn is not None else None&#xa;        self_ignoreExpr_tryParse = self.ignoreExpr.tryParse if self.ignoreExpr is not None else None&#xa;        &#xa;        tmploc = loc&#xa;        while tmploc <= instrlen:&#xa;            if self_failOn_canParseNext is not None:&#xa;                # break if failOn expression matches&#xa;                if self_failOn_canParseNext(instring, tmploc):&#xa;                    break&#xa;                    &#xa;            if self_ignoreExpr_tryParse is not None:&#xa;                # advance past ignore expressions&#xa;                while 1:&#xa;                    try:&#xa;                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)&#xa;                    except ParseBaseException:&#xa;                        break&#xa;            &#xa;            try:&#xa;                expr_parse(instring, tmploc, doActions=False, callPreParse=False)&#xa;            except (ParseException, IndexError):&#xa;                # no match, advance loc in string&#xa;                tmploc += 1&#xa;            else:&#xa;                # matched skipto expr, done&#xa;                break&#xa;&#xa;        else:&#xa;            # ran off the end of the input string without matching skipto expr, fail&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        # build up return values&#xa;        loc = tmploc&#xa;        skiptext = instring[startloc:loc]&#xa;        skipresult = ParseResults(skiptext)&#xa;        &#xa;        if self.includeMatch:&#xa;            loc, mat = expr_parse(instring,loc,doActions,callPreParse=False)&#xa;            skipresult += mat&#xa;&#xa;        return loc, skipresult&#xa;&#xa;class Forward(ParseElementEnhance):&#xa;    """"""&#xa;    Forward declaration of an expression to be defined later -&#xa;    used for recursive grammars, such as algebraic infix notation.&#xa;    When the expression is known, it is assigned to the C{Forward} variable using the '<<' operator.&#xa;&#xa;    Note: take care when assigning to C{Forward} not to overlook precedence of operators.&#xa;    Specifically, '|' has a lower precedence than '<<', so that::&#xa;        fwdExpr << a | b | c&#xa;    will actually be evaluated as::&#xa;        (fwdExpr << a) | b | c&#xa;    thereby leaving b and c out as parseable alternatives.  It is recommended that you&#xa;    explicitly group the values inserted into the C{Forward}::&#xa;        fwdExpr << (a | b | c)&#xa;    Converting to use the '<<=' operator instead will avoid this problem.&#xa;&#xa;    See L{ParseResults.pprint} for an example of a recursive parser created using&#xa;    C{Forward}.&#xa;    """"""&#xa;    def __init__( self, other=None ):&#xa;        super(Forward,self).__init__( other, savelist=False )&#xa;&#xa;    def __lshift__( self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass(other)&#xa;        self.expr = other&#xa;        self.strRepr = None&#xa;        self.mayIndexError = self.expr.mayIndexError&#xa;        self.mayReturnEmpty = self.expr.mayReturnEmpty&#xa;        self.setWhitespaceChars( self.expr.whiteChars )&#xa;        self.skipWhitespace = self.expr.skipWhitespace&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.ignoreExprs.extend(self.expr.ignoreExprs)&#xa;        return self&#xa;        &#xa;    def __ilshift__(self, other):&#xa;        return self << other&#xa;    &#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        if not self.streamlined:&#xa;            self.streamlined = True&#xa;            if self.expr is not None:&#xa;                self.expr.streamline()&#xa;        return self&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        if self not in validateTrace:&#xa;            tmp = validateTrace[:]+[self]&#xa;            if self.expr is not None:&#xa;                self.expr.validate(tmp)&#xa;        self.checkRecursion([])&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;        return self.__class__.__name__ + "": ...""&#xa;&#xa;        # stubbed out for now - creates awful memory and perf issues&#xa;        self._revertClass = self.__class__&#xa;        self.__class__ = _ForwardNoRecurse&#xa;        try:&#xa;            if self.expr is not None:&#xa;                retString = _ustr(self.expr)&#xa;            else:&#xa;                retString = ""None""&#xa;        finally:&#xa;            self.__class__ = self._revertClass&#xa;        return self.__class__.__name__ + "": "" + retString&#xa;&#xa;    def copy(self):&#xa;        if self.expr is not None:&#xa;            return super(Forward,self).copy()&#xa;        else:&#xa;            ret = Forward()&#xa;            ret <<= self&#xa;            return ret&#xa;&#xa;class _ForwardNoRecurse(Forward):&#xa;    def __str__( self ):&#xa;        return ""...""&#xa;&#xa;class TokenConverter(ParseElementEnhance):&#xa;    """"""&#xa;    Abstract subclass of C{ParseExpression}, for converting parsed results.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(TokenConverter,self).__init__( expr )#, savelist )&#xa;        self.saveAsList = False&#xa;&#xa;class Combine(TokenConverter):&#xa;    """"""&#xa;    Converter to concatenate all matching tokens to a single string.&#xa;    By default, the matching patterns must also be contiguous in the input string;&#xa;    this can be disabled by specifying C{'adjacent=False'} in the constructor.&#xa;&#xa;    Example::&#xa;        real = Word(nums) + '.' + Word(nums)&#xa;        print(real.parseString('3.1416')) # -> ['3', '.', '1416']&#xa;        # will also erroneously match the following&#xa;        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']&#xa;&#xa;        real = Combine(Word(nums) + '.' + Word(nums))&#xa;        print(real.parseString('3.1416')) # -> ['3.1416']&#xa;        # no match when there are internal spaces&#xa;        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)&#xa;    """"""&#xa;    def __init__( self, expr, joinString="""", adjacent=True ):&#xa;        super(Combine,self).__init__( expr )&#xa;        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself&#xa;        if adjacent:&#xa;            self.leaveWhitespace()&#xa;        self.adjacent = adjacent&#xa;        self.skipWhitespace = True&#xa;        self.joinString = joinString&#xa;        self.callPreparse = True&#xa;&#xa;    def ignore( self, other ):&#xa;        if self.adjacent:&#xa;            ParserElement.ignore(self, other)&#xa;        else:&#xa;            super( Combine, self).ignore( other )&#xa;        return self&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        retToks = tokenlist.copy()&#xa;        del retToks[:]&#xa;        retToks += ParseResults([ """".join(tokenlist._asStringList(self.joinString)) ], modal=self.modalResults)&#xa;&#xa;        if self.resultsName and retToks.haskeys():&#xa;            return [ retToks ]&#xa;        else:&#xa;            return retToks&#xa;&#xa;class Group(TokenConverter):&#xa;    """"""&#xa;    Converter to return the matched tokens as a list - useful for returning tokens of C{L{ZeroOrMore}} and C{L{OneOrMore}} expressions.&#xa;&#xa;    Example::&#xa;        ident = Word(alphas)&#xa;        num = Word(nums)&#xa;        term = ident | num&#xa;        func = ident + Optional(delimitedList(term))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', 'a', 'b', '100']&#xa;&#xa;        func = ident + Group(Optional(delimitedList(term)))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', ['a', 'b', '100']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Group,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return [ tokenlist ]&#xa;&#xa;class Dict(TokenConverter):&#xa;    """"""&#xa;    Converter to return a repetitive expression as a list, but also as a dictionary.&#xa;    Each element can also be referenced using the first token in the expression as its key.&#xa;    Useful for tabular report scraping when the first column can be used as a item key.&#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        # print attributes as plain groups&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names&#xa;        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)&#xa;        print(result.dump())&#xa;        &#xa;        # access named fields as dict entries, or output as dict&#xa;        print(result['shape'])        &#xa;        print(result.asDict())&#xa;    prints::&#xa;        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']&#xa;&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}&#xa;    See more examples at L{ParseResults} of accessing fields by results name.&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Dict,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        for i,tok in enumerate(tokenlist):&#xa;            if len(tok) == 0:&#xa;                continue&#xa;            ikey = tok[0]&#xa;            if isinstance(ikey,int):&#xa;                ikey = _ustr(tok[0]).strip()&#xa;            if len(tok)==1:&#xa;                tokenlist[ikey] = _ParseResultsWithOffset("""",i)&#xa;            elif len(tok)==2 and not isinstance(tok[1],ParseResults):&#xa;                tokenlist[ikey] = _ParseResultsWithOffset(tok[1],i)&#xa;            else:&#xa;                dictvalue = tok.copy() #ParseResults(i)&#xa;                del dictvalue[0]&#xa;                if len(dictvalue)!= 1 or (isinstance(dictvalue,ParseResults) and dictvalue.haskeys()):&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue,i)&#xa;                else:&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0],i)&#xa;&#xa;        if self.resultsName:&#xa;            return [ tokenlist ]&#xa;        else:&#xa;            return tokenlist&#xa;&#xa;&#xa;class Suppress(TokenConverter):&#xa;    """"""&#xa;    Converter for ignoring the results of a parsed expression.&#xa;&#xa;    Example::&#xa;        source = ""a, b, c,d""&#xa;        wd = Word(alphas)&#xa;        wd_list1 = wd + ZeroOrMore(',' + wd)&#xa;        print(wd_list1.parseString(source))&#xa;&#xa;        # often, delimiters that are useful during parsing are just in the&#xa;        # way afterward - use Suppress to keep them out of the parsed output&#xa;        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)&#xa;        print(wd_list2.parseString(source))&#xa;    prints::&#xa;        ['a', ',', 'b', ',', 'c', ',', 'd']&#xa;        ['a', 'b', 'c', 'd']&#xa;    (See also L{delimitedList}.)&#xa;    """"""&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return []&#xa;&#xa;    def suppress( self ):&#xa;        return self&#xa;&#xa;&#xa;class OnlyOnce(object):&#xa;    """"""&#xa;    Wrapper for parse actions, to ensure they are only called once.&#xa;    """"""&#xa;    def __init__(self, methodCall):&#xa;        self.callable = _trim_arity(methodCall)&#xa;        self.called = False&#xa;    def __call__(self,s,l,t):&#xa;        if not self.called:&#xa;            results = self.callable(s,l,t)&#xa;            self.called = True&#xa;            return results&#xa;        raise ParseException(s,l,"""")&#xa;    def reset(self):&#xa;        self.called = False&#xa;&#xa;def traceParseAction(f):&#xa;    """"""&#xa;    Decorator for debugging parse actions. &#xa;    &#xa;    When the parse action is called, this decorator will print C{"">> entering I{method-name}(line:I{current_source_line}, I{parse_location}, I{matched_tokens})"".}&#xa;    When the parse action completes, the decorator will print C{""<<""} followed by the returned value, or any exception that the parse action raised.&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;&#xa;        @traceParseAction&#xa;        def remove_duplicate_chars(tokens):&#xa;            return ''.join(sorted(set(''.join(tokens)))&#xa;&#xa;        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)&#xa;        print(wds.parseString(""slkdjs sld sldd sdlf sdljf""))&#xa;    prints::&#xa;        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))&#xa;        <<leaving remove_duplicate_chars (ret: 'dfjkls')&#xa;        ['dfjkls']&#xa;    """"""&#xa;    f = _trim_arity(f)&#xa;    def z(*paArgs):&#xa;        thisFunc = f.__name__&#xa;        s,l,t = paArgs[-3:]&#xa;        if len(paArgs)>3:&#xa;            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc&#xa;        sys.stderr.write( "">>entering %s(line: '%s', %d, %r)\n"" % (thisFunc,line(l,s),l,t) )&#xa;        try:&#xa;            ret = f(*paArgs)&#xa;        except Exception as exc:&#xa;            sys.stderr.write( ""<<leaving %s (exception: %s)\n"" % (thisFunc,exc) )&#xa;            raise&#xa;        sys.stderr.write( ""<<leaving %s (ret: %r)\n"" % (thisFunc,ret) )&#xa;        return ret&#xa;    try:&#xa;        z.__name__ = f.__name__&#xa;    except AttributeError:&#xa;        pass&#xa;    return z&#xa;&#xa;#&#xa;# global helpers&#xa;#&#xa;def delimitedList( expr, delim="","", combine=False ):&#xa;    """"""&#xa;    Helper to define a delimited list of expressions - the delimiter defaults to ','.&#xa;    By default, the list elements and delimiters can have intervening whitespace, and&#xa;    comments, but this can be overridden by passing C{combine=True} in the constructor.&#xa;    If C{combine} is set to C{True}, the matching tokens are returned as a single token&#xa;    string, with the delimiters included; otherwise, the matching tokens are returned&#xa;    as a list of tokens, with the delimiters suppressed.&#xa;&#xa;    Example::&#xa;        delimitedList(Word(alphas)).parseString(""aa,bb,cc"") # -> ['aa', 'bb', 'cc']&#xa;        delimitedList(Word(hexnums), delim=':', combine=True).parseString(""AA:BB:CC:DD:EE"") # -> ['AA:BB:CC:DD:EE']&#xa;    """"""&#xa;    dlName = _ustr(expr)+"" [""+_ustr(delim)+"" ""+_ustr(expr)+""]...""&#xa;    if combine:&#xa;        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)&#xa;    else:&#xa;        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)&#xa;&#xa;def countedArray( expr, intExpr=None ):&#xa;    """"""&#xa;    Helper to define a counted list of expressions.&#xa;    This helper defines a pattern of the form::&#xa;        integer expr expr expr...&#xa;    where the leading integer tells how many expr expressions follow.&#xa;    The matched tokens returns the array of expr tokens as a list - the leading count token is suppressed.&#xa;    &#xa;    If C{intExpr} is specified, it should be a pyparsing expression that produces an integer value.&#xa;&#xa;    Example::&#xa;        countedArray(Word(alphas)).parseString('2 ab cd ef')  # -> ['ab', 'cd']&#xa;&#xa;        # in this parser, the leading integer value is given in binary,&#xa;        # '10' indicating that 2 values are in the array&#xa;        binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))&#xa;        countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')  # -> ['ab', 'cd']&#xa;    """"""&#xa;    arrayExpr = Forward()&#xa;    def countFieldParseAction(s,l,t):&#xa;        n = t[0]&#xa;        arrayExpr << (n and Group(And([expr]*n)) or Group(empty))&#xa;        return []&#xa;    if intExpr is None:&#xa;        intExpr = Word(nums).setParseAction(lambda t:int(t[0]))&#xa;    else:&#xa;        intExpr = intExpr.copy()&#xa;    intExpr.setName(""arrayLen"")&#xa;    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)&#xa;    return ( intExpr + arrayExpr ).setName('(len) ' + _ustr(expr) + '...')&#xa;&#xa;def _flatten(L):&#xa;    ret = []&#xa;    for i in L:&#xa;        if isinstance(i,list):&#xa;            ret.extend(_flatten(i))&#xa;        else:&#xa;            ret.append(i)&#xa;    return ret&#xa;&#xa;def matchPreviousLiteral(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousLiteral(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches a&#xa;    previous literal, will also match the leading C{""1:1""} in C{""1:10""}.&#xa;    If this is not desired, use C{matchPreviousExpr}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        if t:&#xa;            if len(t) == 1:&#xa;                rep << t[0]&#xa;            else:&#xa;                # flatten t tokens&#xa;                tflat = _flatten(t.asList())&#xa;                rep << And(Literal(tt) for tt in tflat)&#xa;        else:&#xa;            rep << Empty()&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def matchPreviousExpr(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousExpr(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches by&#xa;    expressions, will I{not} match the leading C{""1:1""} in C{""1:10""};&#xa;    the expressions are evaluated first, and then compared, so&#xa;    C{""1""} is compared with C{""10""}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    e2 = expr.copy()&#xa;    rep <<= e2&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        matchTokens = _flatten(t.asList())&#xa;        def mustMatchTheseTokens(s,l,t):&#xa;            theseTokens = _flatten(t.asList())&#xa;            if  theseTokens != matchTokens:&#xa;                raise ParseException("""",0,"""")&#xa;        rep.setParseAction( mustMatchTheseTokens, callDuringTry=True )&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def _escapeRegexRangeChars(s):&#xa;    #~  escape these chars: ^-]&#xa;    for c in r""\^-]"":&#xa;        s = s.replace(c,_bslash+c)&#xa;    s = s.replace(""\n"",r""\n"")&#xa;    s = s.replace(""\t"",r""\t"")&#xa;    return _ustr(s)&#xa;&#xa;def oneOf( strs, caseless=False, useRegex=True ):&#xa;    """"""&#xa;    Helper to quickly define a set of alternative Literals, and makes sure to do&#xa;    longest-first testing when there is a conflict, regardless of the input order,&#xa;    but returns a C{L{MatchFirst}} for best performance.&#xa;&#xa;    Parameters:&#xa;     - strs - a string of space-delimited literals, or a collection of string literals&#xa;     - caseless - (default=C{False}) - treat all literals as caseless&#xa;     - useRegex - (default=C{True}) - as an optimization, will generate a Regex&#xa;          object; otherwise, will generate a C{MatchFirst} object (if C{caseless=True}, or&#xa;          if creating a C{Regex} raises an exception)&#xa;&#xa;    Example::&#xa;        comp_oper = oneOf(""< = > <= >= !="")&#xa;        var = Word(alphas)&#xa;        number = Word(nums)&#xa;        term = var | number&#xa;        comparison_expr = term + comp_oper + term&#xa;        print(comparison_expr.searchString(""B = 12  AA=23 B<=AA AA>12""))&#xa;    prints::&#xa;        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]&#xa;    """"""&#xa;    if caseless:&#xa;        isequal = ( lambda a,b: a.upper() == b.upper() )&#xa;        masks = ( lambda a,b: b.upper().startswith(a.upper()) )&#xa;        parseElementClass = CaselessLiteral&#xa;    else:&#xa;        isequal = ( lambda a,b: a == b )&#xa;        masks = ( lambda a,b: b.startswith(a) )&#xa;        parseElementClass = Literal&#xa;&#xa;    symbols = []&#xa;    if isinstance(strs,basestring):&#xa;        symbols = strs.split()&#xa;    elif isinstance(strs, collections.Iterable):&#xa;        symbols = list(strs)&#xa;    else:&#xa;        warnings.warn(""Invalid argument to oneOf, expected string or iterable"",&#xa;                SyntaxWarning, stacklevel=2)&#xa;    if not symbols:&#xa;        return NoMatch()&#xa;&#xa;    i = 0&#xa;    while i < len(symbols)-1:&#xa;        cur = symbols[i]&#xa;        for j,other in enumerate(symbols[i+1:]):&#xa;            if ( isequal(other, cur) ):&#xa;                del symbols[i+j+1]&#xa;                break&#xa;            elif ( masks(cur, other) ):&#xa;                del symbols[i+j+1]&#xa;                symbols.insert(i,other)&#xa;                cur = other&#xa;                break&#xa;        else:&#xa;            i += 1&#xa;&#xa;    if not caseless and useRegex:&#xa;        #~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))&#xa;        try:&#xa;            if len(symbols)==len("""".join(symbols)):&#xa;                return Regex( ""[%s]"" % """".join(_escapeRegexRangeChars(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;            else:&#xa;                return Regex( ""|"".join(re.escape(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;        except Exception:&#xa;            warnings.warn(""Exception creating Regex for oneOf, building MatchFirst"",&#xa;                    SyntaxWarning, stacklevel=2)&#xa;&#xa;&#xa;    # last resort, just use MatchFirst&#xa;    return MatchFirst(parseElementClass(sym) for sym in symbols).setName(' | '.join(symbols))&#xa;&#xa;def dictOf( key, value ):&#xa;    """"""&#xa;    Helper to easily and clearly define a dictionary by specifying the respective patterns&#xa;    for the key and value.  Takes care of defining the C{L{Dict}}, C{L{ZeroOrMore}}, and C{L{Group}} tokens&#xa;    in the proper order.  The key pattern can include delimiting markers or punctuation,&#xa;    as long as they are suppressed, thereby leaving the significant key text.  The value&#xa;    pattern can include named results, so that the C{Dict} results can include named token&#xa;    fields.&#xa;&#xa;    Example::&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        attr_label = label&#xa;        attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)&#xa;&#xa;        # similar to Dict, but simpler call format&#xa;        result = dictOf(attr_label, attr_value).parseString(text)&#xa;        print(result.dump())&#xa;        print(result['shape'])&#xa;        print(result.shape)  # object attribute access works too&#xa;        print(result.asDict())&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        SQUARE&#xa;        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}&#xa;    """"""&#xa;    return Dict( ZeroOrMore( Group ( key + value ) ) )&#xa;&#xa;def originalTextFor(expr, asString=True):&#xa;    """"""&#xa;    Helper to return the original, untokenized text for a given expression.  Useful to&#xa;    restore the parsed fields of an HTML start tag into the raw tag text itself, or to&#xa;    revert separate tokens with intervening whitespace back to the original matching&#xa;    input text. By default, returns astring containing the original parsed text.  &#xa;       &#xa;    If the optional C{asString} argument is passed as C{False}, then the return value is a &#xa;    C{L{ParseResults}} containing any results names that were originally matched, and a &#xa;    single token containing the original matched text from the input string.  So if &#xa;    the expression passed to C{L{originalTextFor}} contains expressions with defined&#xa;    results names, you must set C{asString} to C{False} if you want to preserve those&#xa;    results name values.&#xa;&#xa;    Example::&#xa;        src = ""this is test <b> bold <i>text</i> </b> normal text ""&#xa;        for tag in (""b"",""i""):&#xa;            opener,closer = makeHTMLTags(tag)&#xa;            patt = originalTextFor(opener + SkipTo(closer) + closer)&#xa;            print(patt.searchString(src)[0])&#xa;    prints::&#xa;        ['<b> bold <i>text</i> </b>']&#xa;        ['<i>text</i>']&#xa;    """"""&#xa;    locMarker = Empty().setParseAction(lambda s,loc,t: loc)&#xa;    endlocMarker = locMarker.copy()&#xa;    endlocMarker.callPreparse = False&#xa;    matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")&#xa;    if asString:&#xa;        extractText = lambda s,l,t: s[t._original_start:t._original_end]&#xa;    else:&#xa;        def extractText(s,l,t):&#xa;            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]&#xa;    matchExpr.setParseAction(extractText)&#xa;    matchExpr.ignoreExprs = expr.ignoreExprs&#xa;    return matchExpr&#xa;&#xa;def ungroup(expr): &#xa;    """"""&#xa;    Helper to undo pyparsing's default grouping of And expressions, even&#xa;    if all but one are non-empty.&#xa;    """"""&#xa;    return TokenConverter(expr).setParseAction(lambda t:t[0])&#xa;&#xa;def locatedExpr(expr):&#xa;    """"""&#xa;    Helper to decorate a returned token with its starting and ending locations in the input string.&#xa;    This helper adds the following results names:&#xa;     - locn_start = location where matched expression begins&#xa;     - locn_end = location where matched expression ends&#xa;     - value = the actual parsed results&#xa;&#xa;    Be careful if the input text contains C{<TAB>} characters, you may want to call&#xa;    C{L{ParserElement.parseWithTabs}}&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;        for match in locatedExpr(wd).searchString(""ljsdf123lksdjjf123lkkjj1222""):&#xa;            print(match)&#xa;    prints::&#xa;        [[0, 'ljsdf', 5]]&#xa;        [[8, 'lksdjjf', 15]]&#xa;        [[18, 'lkkjj', 23]]&#xa;    """"""&#xa;    locator = Empty().setParseAction(lambda s,l,t: l)&#xa;    return Group(locator(""locn_start"") + expr(""value"") + locator.copy().leaveWhitespace()(""locn_end""))&#xa;&#xa;&#xa;# convenience constants for positional expressions&#xa;empty       = Empty().setName(""empty"")&#xa;lineStart   = LineStart().setName(""lineStart"")&#xa;lineEnd     = LineEnd().setName(""lineEnd"")&#xa;stringStart = StringStart().setName(""stringStart"")&#xa;stringEnd   = StringEnd().setName(""stringEnd"")&#xa;&#xa;_escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])&#xa;_escapedHexChar = Regex(r""\\0?[xX][0-9a-fA-F]+"").setParseAction(lambda s,l,t:unichr(int(t[0].lstrip(r'\0x'),16)))&#xa;_escapedOctChar = Regex(r""\\0[0-7]+"").setParseAction(lambda s,l,t:unichr(int(t[0][1:],8)))&#xa;_singleChar = _escapedPunc | _escapedHexChar | _escapedOctChar | Word(printables, excludeChars=r'\]', exact=1) | Regex(r""\w"", re.UNICODE)&#xa;_charRange = Group(_singleChar + Suppress(""-"") + _singleChar)&#xa;_reBracketExpr = Literal(""["") + Optional(""^"").setResultsName(""negate"") + Group( OneOrMore( _charRange | _singleChar ) ).setResultsName(""body"") + ""]""&#xa;&#xa;def srange(s):&#xa;    r""""""&#xa;    Helper to easily define string ranges for use in Word construction.  Borrows&#xa;    syntax from regexp '[]' string range definitions::&#xa;        srange(""[0-9]"")   -> ""0123456789""&#xa;        srange(""[a-z]"")   -> ""abcdefghijklmnopqrstuvwxyz""&#xa;        srange(""[a-z$_]"") -> ""abcdefghijklmnopqrstuvwxyz$_""&#xa;    The input string must be enclosed in []'s, and the returned string is the expanded&#xa;    character set joined into a single string.&#xa;    The values enclosed in the []'s may be:&#xa;     - a single character&#xa;     - an escaped character with a leading backslash (such as C{\-} or C{\]})&#xa;     - an escaped hex character with a leading C{'\x'} (C{\x21}, which is a C{'!'} character) &#xa;         (C{\0x##} is also supported for backwards compatibility) &#xa;     - an escaped octal character with a leading C{'\0'} (C{\041}, which is a C{'!'} character)&#xa;     - a range of any of the above, separated by a dash (C{'a-z'}, etc.)&#xa;     - any combination of the above (C{'aeiouy'}, C{'a-zA-Z0-9_$'}, etc.)&#xa;    """"""&#xa;    _expanded = lambda p: p if not isinstance(p,ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]),ord(p[1])+1))&#xa;    try:&#xa;        return """".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)&#xa;    except Exception:&#xa;        return """"&#xa;&#xa;def matchOnlyAtCol(n):&#xa;    """"""&#xa;    Helper method for defining parse actions that require matching at a specific&#xa;    column in the input text.&#xa;    """"""&#xa;    def verifyCol(strg,locn,toks):&#xa;        if col(locn,strg) != n:&#xa;            raise ParseException(strg,locn,""matched token not at column %d"" % n)&#xa;    return verifyCol&#xa;&#xa;def replaceWith(replStr):&#xa;    """"""&#xa;    Helper method for common parse actions that simply return a literal value.  Especially&#xa;    useful when used with C{L{transformString<ParserElement.transformString>}()}.&#xa;&#xa;    Example::&#xa;        num = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;        na = oneOf(""N/A NA"").setParseAction(replaceWith(math.nan))&#xa;        term = na | num&#xa;        &#xa;        OneOrMore(term).parseString(""324 234 N/A 234"") # -> [324, 234, nan, 234]&#xa;    """"""&#xa;    return lambda s,l,t: [replStr]&#xa;&#xa;def removeQuotes(s,l,t):&#xa;    """"""&#xa;    Helper parse action for removing quotation marks from parsed quoted strings.&#xa;&#xa;    Example::&#xa;        # by default, quotation marks are included in parsed results&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""'Now is the Winter of our Discontent'""]&#xa;&#xa;        # use removeQuotes to strip quotation marks from parsed results&#xa;        quotedString.setParseAction(removeQuotes)&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""Now is the Winter of our Discontent""]&#xa;    """"""&#xa;    return t[0][1:-1]&#xa;&#xa;def tokenMap(func, *args):&#xa;    """"""&#xa;    Helper to define a parse action by mapping a function to all elements of a ParseResults list.If any additional &#xa;    args are passed, they are forwarded to the given function as additional arguments after&#xa;    the token, as in C{hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))}, which will convert the&#xa;    parsed data to an integer using base 16.&#xa;&#xa;    Example (compare the last to example in L{ParserElement.transformString}::&#xa;        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))&#xa;        hex_ints.runTests('''&#xa;            00 11 22 aa FF 0a 0d 1a&#xa;            ''')&#xa;        &#xa;        upperword = Word(alphas).setParseAction(tokenMap(str.upper))&#xa;        OneOrMore(upperword).runTests('''&#xa;            my kingdom for a horse&#xa;            ''')&#xa;&#xa;        wd = Word(alphas).setParseAction(tokenMap(str.title))&#xa;        OneOrMore(wd).setParseAction(' '.join).runTests('''&#xa;            now is the winter of our discontent made glorious summer by this sun of york&#xa;            ''')&#xa;    prints::&#xa;        00 11 22 aa FF 0a 0d 1a&#xa;        [0, 17, 34, 170, 255, 10, 13, 26]&#xa;&#xa;        my kingdom for a horse&#xa;        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']&#xa;&#xa;        now is the winter of our discontent made glorious summer by this sun of york&#xa;        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']&#xa;    """"""&#xa;    def pa(s,l,t):&#xa;        return [func(tokn, *args) for tokn in t]&#xa;&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    pa.__name__ = func_name&#xa;&#xa;    return pa&#xa;&#xa;upcaseTokens = tokenMap(lambda t: _ustr(t).upper())&#xa;""""""(Deprecated) Helper parse action to convert tokens to upper case. Deprecated in favor of L{pyparsing_common.upcaseTokens}""""""&#xa;&#xa;downcaseTokens = tokenMap(lambda t: _ustr(t).lower())&#xa;""""""(Deprecated) Helper parse action to convert tokens to lower case. Deprecated in favor of L{pyparsing_common.downcaseTokens}""""""&#xa;    &#xa;def _makeTags(tagStr, xml):&#xa;    """"""Internal helper to construct opening and closing tag expressions, given a tag name""""""&#xa;    if isinstance(tagStr,basestring):&#xa;        resname = tagStr&#xa;        tagStr = Keyword(tagStr, caseless=not xml)&#xa;    else:&#xa;        resname = tagStr.name&#xa;&#xa;    tagAttrName = Word(alphas,alphanums+""_-:"")&#xa;    if (xml):&#xa;        tagAttrValue = dblQuotedString.copy().setParseAction( removeQuotes )&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName + Suppress(""="") + tagAttrValue ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    else:&#xa;        printablesLessRAbrack = """".join(c for c in printables if c not in "">"")&#xa;        tagAttrValue = quotedString.copy().setParseAction( removeQuotes ) | Word(printablesLessRAbrack)&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \&#xa;                Optional( Suppress(""="") + tagAttrValue ) ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    closeTag = Combine(_L(""</"") + tagStr + "">"")&#xa;&#xa;    openTag = openTag.setResultsName(""start""+"""".join(resname.replace("":"","" "").title().split())).setName(""<%s>"" % resname)&#xa;    closeTag = closeTag.setResultsName(""end""+"""".join(resname.replace("":"","" "").title().split())).setName(""</%s>"" % resname)&#xa;    openTag.tag = resname&#xa;    closeTag.tag = resname&#xa;    return openTag, closeTag&#xa;&#xa;def makeHTMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for HTML, given a tag name. Matches&#xa;    tags in either upper or lower case, attributes with namespaces and with quoted or unquoted values.&#xa;&#xa;    Example::&#xa;        text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;        # makeHTMLTags returns pyparsing expressions for the opening and closing tags as a 2-tuple&#xa;        a,a_end = makeHTMLTags(""A"")&#xa;        link_expr = a + SkipTo(a_end)(""link_text"") + a_end&#xa;        &#xa;        for link in link_expr.searchString(text):&#xa;            # attributes in the <A> tag (like ""href"" shown here) are also accessible as named results&#xa;            print(link.link_text, '->', link.href)&#xa;    prints::&#xa;        pyparsing -> http://pyparsing.wikispaces.com&#xa;    """"""&#xa;    return _makeTags( tagStr, False )&#xa;&#xa;def makeXMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for XML, given a tag name. Matches&#xa;    tags only in the given upper/lower case.&#xa;&#xa;    Example: similar to L{makeHTMLTags}&#xa;    """"""&#xa;    return _makeTags( tagStr, True )&#xa;&#xa;def withAttribute(*args,**attrDict):&#xa;    """"""&#xa;    Helper to create a validating parse action to be used with start tags created&#xa;    with C{L{makeXMLTags}} or C{L{makeHTMLTags}}. Use C{withAttribute} to qualify a starting tag&#xa;    with a required attribute value, to avoid false matches on common tags such as&#xa;    C{<TD>} or C{<DIV>}.&#xa;&#xa;    Call C{withAttribute} with a series of attribute names and values. Specify the list&#xa;    of filter attributes names and values as:&#xa;     - keyword arguments, as in C{(align=""right"")}, or&#xa;     - as an explicit dict with C{**} operator, when an attribute name is also a Python&#xa;          reserved word, as in C{**{""class"":""Customer"", ""align"":""right""}}&#xa;     - a list of name-value tuples, as in ( (""ns1:class"", ""Customer""), (""ns2:align"",""right"") )&#xa;    For attribute names with a namespace prefix, you must use the second form.  Attribute&#xa;    names are matched insensitive to upper/lower case.&#xa;       &#xa;    If just testing for C{class} (with or without a namespace), use C{L{withClass}}.&#xa;&#xa;    To verify that the attribute exists, but without specifying a value, pass&#xa;    C{withAttribute.ANY_VALUE} as the value.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div type=""grid"">1 4 0 1 0</div>&#xa;            <div type=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this has no type</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;&#xa;        # only match div tag having a type attribute with value ""grid""&#xa;        div_grid = div().setParseAction(withAttribute(type=""grid""))&#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        # construct a match with any div tag having a type attribute, regardless of the value&#xa;        div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    if args:&#xa;        attrs = args[:]&#xa;    else:&#xa;        attrs = attrDict.items()&#xa;    attrs = [(k,v) for k,v in attrs]&#xa;    def pa(s,l,tokens):&#xa;        for attrName,attrValue in attrs:&#xa;            if attrName not in tokens:&#xa;                raise ParseException(s,l,""no matching attribute "" + attrName)&#xa;            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:&#xa;                raise ParseException(s,l,""attribute '%s' has value '%s', must be '%s'"" %&#xa;                                            (attrName, tokens[attrName], attrValue))&#xa;    return pa&#xa;withAttribute.ANY_VALUE = object()&#xa;&#xa;def withClass(classname, namespace=''):&#xa;    """"""&#xa;    Simplified version of C{L{withAttribute}} when matching on a div class - made&#xa;    difficult because C{class} is a reserved word in Python.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div class=""grid"">1 4 0 1 0</div>&#xa;            <div class=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this &lt;div&gt; has no class</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;        div_grid = div().setParseAction(withClass(""grid""))&#xa;        &#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    classattr = ""%s:class"" % namespace if namespace else ""class""&#xa;    return withAttribute(**{classattr : classname})        &#xa;&#xa;opAssoc = _Constants()&#xa;opAssoc.LEFT = object()&#xa;opAssoc.RIGHT = object()&#xa;&#xa;def infixNotation( baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')') ):&#xa;    """"""&#xa;    Helper method for constructing grammars of expressions made up of&#xa;    operators working in a precedence hierarchy.  Operators may be unary or&#xa;    binary, left- or right-associative.  Parse actions can also be attached&#xa;    to operator expressions. The generated parser will also recognize the use &#xa;    of parentheses to override operator precedences (see example below).&#xa;    &#xa;    Note: if you define a deep operator list, you may see performance issues&#xa;    when using infixNotation. See L{ParserElement.enablePackrat} for a&#xa;    mechanism to potentially improve your parser performance.&#xa;&#xa;    Parameters:&#xa;     - baseExpr - expression representing the most basic element for the nested&#xa;     - opList - list of tuples, one for each operator precedence level in the&#xa;      expression grammar; each tuple is of the form&#xa;      (opExpr, numTerms, rightLeftAssoc, parseAction), where:&#xa;       - opExpr is the pyparsing expression for the operator;&#xa;          may also be a string, which will be converted to a Literal;&#xa;          if numTerms is 3, opExpr is a tuple of two expressions, for the&#xa;          two operators separating the 3 terms&#xa;       - numTerms is the number of terms for this operator (must&#xa;          be 1, 2, or 3)&#xa;       - rightLeftAssoc is the indicator whether the operator is&#xa;          right or left associative, using the pyparsing-defined&#xa;          constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}.&#xa;       - parseAction is the parse action to be associated with&#xa;          expressions matching this operator expression (the&#xa;          parse action tuple member may be omitted)&#xa;     - lpar - expression for matching left-parentheses (default=C{Suppress('(')})&#xa;     - rpar - expression for matching right-parentheses (default=C{Suppress(')')})&#xa;&#xa;    Example::&#xa;        # simple example of four-function arithmetic with ints and variable names&#xa;        integer = pyparsing_common.signed_integer&#xa;        varname = pyparsing_common.identifier &#xa;        &#xa;        arith_expr = infixNotation(integer | varname,&#xa;            [&#xa;            ('-', 1, opAssoc.RIGHT),&#xa;            (oneOf('* /'), 2, opAssoc.LEFT),&#xa;            (oneOf('+ -'), 2, opAssoc.LEFT),&#xa;            ])&#xa;        &#xa;        arith_expr.runTests('''&#xa;            5+3*6&#xa;            (5+3)*6&#xa;            -2--11&#xa;            ''', fullDump=False)&#xa;    prints::&#xa;        5+3*6&#xa;        [[5, '+', [3, '*', 6]]]&#xa;&#xa;        (5+3)*6&#xa;        [[[5, '+', 3], '*', 6]]&#xa;&#xa;        -2--11&#xa;        [[['-', 2], '-', ['-', 11]]]&#xa;    """"""&#xa;    ret = Forward()&#xa;    lastExpr = baseExpr | ( lpar + ret + rpar )&#xa;    for i,operDef in enumerate(opList):&#xa;        opExpr,arity,rightLeftAssoc,pa = (operDef + (None,))[:4]&#xa;        termName = ""%s term"" % opExpr if arity < 3 else ""%s%s term"" % opExpr&#xa;        if arity == 3:&#xa;            if opExpr is None or len(opExpr) != 2:&#xa;                raise ValueError(""if numterms=3, opExpr must be a tuple or list of two expressions"")&#xa;            opExpr1, opExpr2 = opExpr&#xa;        thisExpr = Forward().setName(termName)&#xa;        if rightLeftAssoc == opAssoc.LEFT:&#xa;            if arity == 1:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr) + Group( lastExpr + OneOrMore( opExpr ) )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + lastExpr) + Group( lastExpr + OneOrMore( opExpr + lastExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr+lastExpr) + Group( lastExpr + OneOrMore(lastExpr) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr) + \&#xa;                            Group( lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        elif rightLeftAssoc == opAssoc.RIGHT:&#xa;            if arity == 1:&#xa;                # try to avoid LR with this extra test&#xa;                if not isinstance(opExpr, Optional):&#xa;                    opExpr = Optional(opExpr)&#xa;                matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + thisExpr) + Group( lastExpr + OneOrMore( opExpr + thisExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr + thisExpr) + Group( lastExpr + OneOrMore( thisExpr ) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr) + \&#xa;                            Group( lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        else:&#xa;            raise ValueError(""operator must indicate right or left associativity"")&#xa;        if pa:&#xa;            matchExpr.setParseAction( pa )&#xa;        thisExpr <<= ( matchExpr.setName(termName) | lastExpr )&#xa;        lastExpr = thisExpr&#xa;    ret <<= lastExpr&#xa;    return ret&#xa;&#xa;operatorPrecedence = infixNotation&#xa;""""""(Deprecated) Former name of C{L{infixNotation}}, will be dropped in a future release.""""""&#xa;&#xa;dblQuotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""').setName(""string enclosed in double quotes"")&#xa;sglQuotedString = Combine(Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""string enclosed in single quotes"")&#xa;quotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""'|&#xa;                       Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""quotedString using single or double quotes"")&#xa;unicodeString = Combine(_L('u') + quotedString.copy()).setName(""unicode string literal"")&#xa;&#xa;def nestedExpr(opener=""("", closer="")"", content=None, ignoreExpr=quotedString.copy()):&#xa;    """"""&#xa;    Helper method for defining nested lists enclosed in opening and closing&#xa;    delimiters (""("" and "")"" are the default).&#xa;&#xa;    Parameters:&#xa;     - opener - opening character for a nested list (default=C{""(""}); can also be a pyparsing expression&#xa;     - closer - closing character for a nested list (default=C{"")""}); can also be a pyparsing expression&#xa;     - content - expression for items within the nested lists (default=C{None})&#xa;     - ignoreExpr - expression for ignoring opening and closing delimiters (default=C{quotedString})&#xa;&#xa;    If an expression is not provided for the content argument, the nested&#xa;    expression will capture all whitespace-delimited content between delimiters&#xa;    as a list of separate values.&#xa;&#xa;    Use the C{ignoreExpr} argument to define expressions that may contain&#xa;    opening or closing characters that should not be treated as opening&#xa;    or closing characters for nesting, such as quotedString or a comment&#xa;    expression.  Specify multiple expressions using an C{L{Or}} or C{L{MatchFirst}}.&#xa;    The default is L{quotedString}, but if no expressions are to be ignored,&#xa;    then pass C{None} for this argument.&#xa;&#xa;    Example::&#xa;        data_type = oneOf(""void int short long char float double"")&#xa;        decl_data_type = Combine(data_type + Optional(Word('*')))&#xa;        ident = Word(alphas+'_', alphanums+'_')&#xa;        number = pyparsing_common.number&#xa;        arg = Group(decl_data_type + ident)&#xa;        LPAR,RPAR = map(Suppress, ""()"")&#xa;&#xa;        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))&#xa;&#xa;        c_function = (decl_data_type(""type"") &#xa;                      + ident(""name"")&#xa;                      + LPAR + Optional(delimitedList(arg), [])(""args"") + RPAR &#xa;                      + code_body(""body""))&#xa;        c_function.ignore(cStyleComment)&#xa;        &#xa;        source_code = '''&#xa;            int is_odd(int x) { &#xa;                return (x%2); &#xa;            }&#xa;                &#xa;            int dec_to_hex(char hchar) { &#xa;                if (hchar >= '0' && hchar <= '9') { &#xa;                    return (ord(hchar)-ord('0')); &#xa;                } else { &#xa;                    return (10+ord(hchar)-ord('A'));&#xa;                } &#xa;            }&#xa;        '''&#xa;        for func in c_function.searchString(source_code):&#xa;            print(""%(name)s (%(type)s) args: %(args)s"" % func)&#xa;&#xa;    prints::&#xa;        is_odd (int) args: [['int', 'x']]&#xa;        dec_to_hex (int) args: [['char', 'hchar']]&#xa;    """"""&#xa;    if opener == closer:&#xa;        raise ValueError(""opening and closing strings cannot be the same"")&#xa;    if content is None:&#xa;        if isinstance(opener,basestring) and isinstance(closer,basestring):&#xa;            if len(opener) == 1 and len(closer)==1:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr +&#xa;                                    CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (empty.copy()+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;            else:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr + &#xa;                                    ~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (Combine(OneOrMore(~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;        else:&#xa;            raise ValueError(""opening and closing arguments must be strings if no content expression is given"")&#xa;    ret = Forward()&#xa;    if ignoreExpr is not None:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )&#xa;    else:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )&#xa;    ret.setName('nested %s%s expression' % (opener,closer))&#xa;    return ret&#xa;&#xa;def indentedBlock(blockStatementExpr, indentStack, indent=True):&#xa;    """"""&#xa;    Helper method for defining space-delimited indentation blocks, such as&#xa;    those used to define block statements in Python source code.&#xa;&#xa;    Parameters:&#xa;     - blockStatementExpr - expression defining syntax of statement that&#xa;            is repeated within the indented block&#xa;     - indentStack - list created by caller to manage indentation stack&#xa;            (multiple statementWithIndentedBlock expressions within a single grammar&#xa;            should share a common indentStack)&#xa;     - indent - boolean indicating whether block must be indented beyond the&#xa;            the current level; set to False for block of left-most statements&#xa;            (default=C{True})&#xa;&#xa;    A valid block must contain at least one C{blockStatement}.&#xa;&#xa;    Example::&#xa;        data = '''&#xa;        def A(z):&#xa;          A1&#xa;          B = 100&#xa;          G = A2&#xa;          A2&#xa;          A3&#xa;        B&#xa;        def BB(a,b,c):&#xa;          BB1&#xa;          def BBA():&#xa;            bba1&#xa;            bba2&#xa;            bba3&#xa;        C&#xa;        D&#xa;        def spam(x,y):&#xa;             def eggs(z):&#xa;                 pass&#xa;        '''&#xa;&#xa;&#xa;        indentStack = [1]&#xa;        stmt = Forward()&#xa;&#xa;        identifier = Word(alphas, alphanums)&#xa;        funcDecl = (""def"" + identifier + Group( ""("" + Optional( delimitedList(identifier) ) + "")"" ) + "":"")&#xa;        func_body = indentedBlock(stmt, indentStack)&#xa;        funcDef = Group( funcDecl + func_body )&#xa;&#xa;        rvalue = Forward()&#xa;        funcCall = Group(identifier + ""("" + Optional(delimitedList(rvalue)) + "")"")&#xa;        rvalue << (funcCall | identifier | Word(nums))&#xa;        assignment = Group(identifier + ""="" + rvalue)&#xa;        stmt << ( funcDef | assignment | identifier )&#xa;&#xa;        module_body = OneOrMore(stmt)&#xa;&#xa;        parseTree = module_body.parseString(data)&#xa;        parseTree.pprint()&#xa;    prints::&#xa;        [['def',&#xa;          'A',&#xa;          ['(', 'z', ')'],&#xa;          ':',&#xa;          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],&#xa;         'B',&#xa;         ['def',&#xa;          'BB',&#xa;          ['(', 'a', 'b', 'c', ')'],&#xa;          ':',&#xa;          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],&#xa;         'C',&#xa;         'D',&#xa;         ['def',&#xa;          'spam',&#xa;          ['(', 'x', 'y', ')'],&#xa;          ':',&#xa;          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]] &#xa;    """"""&#xa;    def checkPeerIndent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if curCol != indentStack[-1]:&#xa;            if curCol > indentStack[-1]:&#xa;                raise ParseFatalException(s,l,""illegal nesting"")&#xa;            raise ParseException(s,l,""not a peer entry"")&#xa;&#xa;    def checkSubIndent(s,l,t):&#xa;        curCol = col(l,s)&#xa;        if curCol > indentStack[-1]:&#xa;            indentStack.append( curCol )&#xa;        else:&#xa;            raise ParseException(s,l,""not a subentry"")&#xa;&#xa;    def checkUnindent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if not(indentStack and curCol < indentStack[-1] and curCol <= indentStack[-2]):&#xa;            raise ParseException(s,l,""not an unindent"")&#xa;        indentStack.pop()&#xa;&#xa;    NL = OneOrMore(LineEnd().setWhitespaceChars(""\t "").suppress())&#xa;    INDENT = (Empty() + Empty().setParseAction(checkSubIndent)).setName('INDENT')&#xa;    PEER   = Empty().setParseAction(checkPeerIndent).setName('')&#xa;    UNDENT = Empty().setParseAction(checkUnindent).setName('UNINDENT')&#xa;    if indent:&#xa;        smExpr = Group( Optional(NL) +&#xa;            #~ FollowedBy(blockStatementExpr) +&#xa;            INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT)&#xa;    else:&#xa;        smExpr = Group( Optional(NL) +&#xa;            (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) )&#xa;    blockStatementExpr.ignore(_bslash + LineEnd())&#xa;    return smExpr.setName('indented block')&#xa;&#xa;alphas8bit = srange(r""[\0xc0-\0xd6\0xd8-\0xf6\0xf8-\0xff]"")&#xa;punc8bit = srange(r""[\0xa1-\0xbf\0xd7\0xf7]"")&#xa;&#xa;anyOpenTag,anyCloseTag = makeHTMLTags(Word(alphas,alphanums+""_:"").setName('any tag'))&#xa;_htmlEntityMap = dict(zip(""gt lt amp nbsp quot apos"".split(),'><& ""\''))&#xa;commonHTMLEntity = Regex('&(?P<entity>' + '|'.join(_htmlEntityMap.keys()) +"");"").setName(""common HTML entity"")&#xa;def replaceHTMLEntity(t):&#xa;    """"""Helper parser action to replace common HTML entities with their special characters""""""&#xa;    return _htmlEntityMap.get(t.entity)&#xa;&#xa;# it's easy to get these comment structures wrong - they're very common, so may as well make them available&#xa;cStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/').setName(""C style comment"")&#xa;""Comment of the form C{/* ... */}""&#xa;&#xa;htmlComment = Regex(r""<!--[\s\S]*?-->"").setName(""HTML comment"")&#xa;""Comment of the form C{<!-- ... -->}""&#xa;&#xa;restOfLine = Regex(r"".*"").leaveWhitespace().setName(""rest of line"")&#xa;dblSlashComment = Regex(r""//(?:\\\n|[^\n])*"").setName(""// comment"")&#xa;""Comment of the form C{// ... (to end of line)}""&#xa;&#xa;cppStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/'| dblSlashComment).setName(""C++ style comment"")&#xa;""Comment of either form C{L{cStyleComment}} or C{L{dblSlashComment}}""&#xa;&#xa;javaStyleComment = cppStyleComment&#xa;""Same as C{L{cppStyleComment}}""&#xa;&#xa;pythonStyleComment = Regex(r""#.*"").setName(""Python style comment"")&#xa;""Comment of the form C{# ... (to end of line)}""&#xa;&#xa;_commasepitem = Combine(OneOrMore(Word(printables, excludeChars=',') +&#xa;                                  Optional( Word("" \t"") +&#xa;                                            ~Literal("","") + ~LineEnd() ) ) ).streamline().setName(""commaItem"")&#xa;commaSeparatedList = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""commaSeparatedList"")&#xa;""""""(Deprecated) Predefined expression of 1 or more printable words or quoted strings, separated by commas.&#xa;   This expression is deprecated in favor of L{pyparsing_common.comma_separated_list}.""""""&#xa;&#xa;# some other useful expressions - using lower-case class name since we are really using this as a namespace&#xa;class pyparsing_common:&#xa;    """"""&#xa;    Here are some common low-level expressions that may be useful in jump-starting parser development:&#xa;     - numeric forms (L{integers<integer>}, L{reals<real>}, L{scientific notation<sci_real>})&#xa;     - common L{programming identifiers<identifier>}&#xa;     - network addresses (L{MAC<mac_address>}, L{IPv4<ipv4_address>}, L{IPv6<ipv6_address>})&#xa;     - ISO8601 L{dates<iso8601_date>} and L{datetime<iso8601_datetime>}&#xa;     - L{UUID<uuid>}&#xa;     - L{comma-separated list<comma_separated_list>}&#xa;    Parse actions:&#xa;     - C{L{convertToInteger}}&#xa;     - C{L{convertToFloat}}&#xa;     - C{L{convertToDate}}&#xa;     - C{L{convertToDatetime}}&#xa;     - C{L{stripHTMLTags}}&#xa;     - C{L{upcaseTokens}}&#xa;     - C{L{downcaseTokens}}&#xa;&#xa;    Example::&#xa;        pyparsing_common.number.runTests('''&#xa;            # any int or real number, returned as the appropriate type&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.fnumber.runTests('''&#xa;            # any int or real number, returned as float&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.hex_integer.runTests('''&#xa;            # hex numbers&#xa;            100&#xa;            FF&#xa;            ''')&#xa;&#xa;        pyparsing_common.fraction.runTests('''&#xa;            # fractions&#xa;            1/2&#xa;            -3/4&#xa;            ''')&#xa;&#xa;        pyparsing_common.mixed_integer.runTests('''&#xa;            # mixed fractions&#xa;            1&#xa;            1/2&#xa;            -3/4&#xa;            1-3/4&#xa;            ''')&#xa;&#xa;        import uuid&#xa;        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;        pyparsing_common.uuid.runTests('''&#xa;            # uuid&#xa;            12345678-1234-5678-1234-567812345678&#xa;            ''')&#xa;    prints::&#xa;        # any int or real number, returned as the appropriate type&#xa;        100&#xa;        [100]&#xa;&#xa;        -100&#xa;        [-100]&#xa;&#xa;        +100&#xa;        [100]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # any int or real number, returned as float&#xa;        100&#xa;        [100.0]&#xa;&#xa;        -100&#xa;        [-100.0]&#xa;&#xa;        +100&#xa;        [100.0]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # hex numbers&#xa;        100&#xa;        [256]&#xa;&#xa;        FF&#xa;        [255]&#xa;&#xa;        # fractions&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        # mixed fractions&#xa;        1&#xa;        [1]&#xa;&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        1-3/4&#xa;        [1.75]&#xa;&#xa;        # uuid&#xa;        12345678-1234-5678-1234-567812345678&#xa;        [UUID('12345678-1234-5678-1234-567812345678')]&#xa;    """"""&#xa;&#xa;    convertToInteger = tokenMap(int)&#xa;    """"""&#xa;    Parse action for converting parsed integers to Python int&#xa;    """"""&#xa;&#xa;    convertToFloat = tokenMap(float)&#xa;    """"""&#xa;    Parse action for converting parsed numbers to Python float&#xa;    """"""&#xa;&#xa;    integer = Word(nums).setName(""integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an unsigned integer, returns an int""""""&#xa;&#xa;    hex_integer = Word(hexnums).setName(""hex integer"").setParseAction(tokenMap(int,16))&#xa;    """"""expression that parses a hexadecimal integer, returns an int""""""&#xa;&#xa;    signed_integer = Regex(r'[+-]?\d+').setName(""signed integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an integer with optional leading sign, returns an int""""""&#xa;&#xa;    fraction = (signed_integer().setParseAction(convertToFloat) + '/' + signed_integer().setParseAction(convertToFloat)).setName(""fraction"")&#xa;    """"""fractional expression of an integer divided by an integer, returns a float""""""&#xa;    fraction.addParseAction(lambda t: t[0]/t[-1])&#xa;&#xa;    mixed_integer = (fraction | signed_integer + Optional(Optional('-').suppress() + fraction)).setName(""fraction or mixed integer-fraction"")&#xa;    """"""mixed integer of the form 'integer - fraction', with optional leading integer, returns float""""""&#xa;    mixed_integer.addParseAction(sum)&#xa;&#xa;    real = Regex(r'[+-]?\d+\.\d*').setName(""real number"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number and returns a float""""""&#xa;&#xa;    sci_real = Regex(r'[+-]?\d+([eE][+-]?\d+|\.\d*([eE][+-]?\d+)?)').setName(""real number with scientific notation"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number with optional scientific notation and returns a float""""""&#xa;&#xa;    # streamlining this expression makes the docs nicer-looking&#xa;    number = (sci_real | real | signed_integer).streamline()&#xa;    """"""any numeric expression, returns the corresponding Python type""""""&#xa;&#xa;    fnumber = Regex(r'[+-]?\d+\.?\d*([eE][+-]?\d+)?').setName(""fnumber"").setParseAction(convertToFloat)&#xa;    """"""any int or real number, returned as float""""""&#xa;    &#xa;    identifier = Word(alphas+'_', alphanums+'_').setName(""identifier"")&#xa;    """"""typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')""""""&#xa;    &#xa;    ipv4_address = Regex(r'(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}').setName(""IPv4 address"")&#xa;    ""IPv4 address (C{0.0.0.0 - 255.255.255.255})""&#xa;&#xa;    _ipv6_part = Regex(r'[0-9a-fA-F]{1,4}').setName(""hex_integer"")&#xa;    _full_ipv6_address = (_ipv6_part + (':' + _ipv6_part)*7).setName(""full IPv6 address"")&#xa;    _short_ipv6_address = (Optional(_ipv6_part + (':' + _ipv6_part)*(0,6)) + ""::"" + Optional(_ipv6_part + (':' + _ipv6_part)*(0,6))).setName(""short IPv6 address"")&#xa;    _short_ipv6_address.addCondition(lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8)&#xa;    _mixed_ipv6_address = (""::ffff:"" + ipv4_address).setName(""mixed IPv6 address"")&#xa;    ipv6_address = Combine((_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).setName(""IPv6 address"")).setName(""IPv6 address"")&#xa;    ""IPv6 address (long, short, or mixed form)""&#xa;    &#xa;    mac_address = Regex(r'[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\1[0-9a-fA-F]{2}){4}').setName(""MAC address"")&#xa;    ""MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)""&#xa;&#xa;    @staticmethod&#xa;    def convertToDate(fmt=""%Y-%m-%d""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed date string to Python datetime.date&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%d""})&#xa;&#xa;        Example::&#xa;            date_expr = pyparsing_common.iso8601_date.copy()&#xa;            date_expr.setParseAction(pyparsing_common.convertToDate())&#xa;            print(date_expr.parseString(""1999-12-31""))&#xa;        prints::&#xa;            [datetime.date(1999, 12, 31)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt).date()&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    @staticmethod&#xa;    def convertToDatetime(fmt=""%Y-%m-%dT%H:%M:%S.%f""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed datetime string to Python datetime.datetime&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%dT%H:%M:%S.%f""})&#xa;&#xa;        Example::&#xa;            dt_expr = pyparsing_common.iso8601_datetime.copy()&#xa;            dt_expr.setParseAction(pyparsing_common.convertToDatetime())&#xa;            print(dt_expr.parseString(""1999-12-31T23:59:59.999""))&#xa;        prints::&#xa;            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt)&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    iso8601_date = Regex(r'(?P<year>\d{4})(?:-(?P<month>\d\d)(?:-(?P<day>\d\d))?)?').setName(""ISO8601 date"")&#xa;    ""ISO8601 date (C{yyyy-mm-dd})""&#xa;&#xa;    iso8601_datetime = Regex(r'(?P<year>\d{4})-(?P<month>\d\d)-(?P<day>\d\d)[T ](?P<hour>\d\d):(?P<minute>\d\d)(:(?P<second>\d\d(\.\d*)?)?)?(?P<tz>Z|[+-]\d\d:?\d\d)?').setName(""ISO8601 datetime"")&#xa;    ""ISO8601 datetime (C{yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)}) - trailing seconds, milliseconds, and timezone optional; accepts separating C{'T'} or C{' '}""&#xa;&#xa;    uuid = Regex(r'[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}').setName(""UUID"")&#xa;    ""UUID (C{xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx})""&#xa;&#xa;    _html_stripper = anyOpenTag.suppress() | anyCloseTag.suppress()&#xa;    @staticmethod&#xa;    def stripHTMLTags(s, l, tokens):&#xa;        """"""&#xa;        Parse action to remove HTML tags from web page HTML source&#xa;&#xa;        Example::&#xa;            # strip HTML links from normal text &#xa;            text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;            td,td_end = makeHTMLTags(""TD"")&#xa;            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)(""body"") + td_end&#xa;            &#xa;            print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'&#xa;        """"""&#xa;        return pyparsing_common._html_stripper.transformString(tokens[0])&#xa;&#xa;    _commasepitem = Combine(OneOrMore(~Literal("","") + ~LineEnd() + Word(printables, excludeChars=',') &#xa;                                        + Optional( White("" \t"") ) ) ).streamline().setName(""commaItem"")&#xa;    comma_separated_list = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""comma separated list"")&#xa;    """"""Predefined expression of 1 or more printable words or quoted strings, separated by commas.""""""&#xa;&#xa;    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))&#xa;    """"""Parse action to convert tokens to upper case.""""""&#xa;&#xa;    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))&#xa;    """"""Parse action to convert tokens to lower case.""""""&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    selectToken    = CaselessLiteral(""select"")&#xa;    fromToken      = CaselessLiteral(""from"")&#xa;&#xa;    ident          = Word(alphas, alphanums + ""_$"")&#xa;&#xa;    columnName     = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    columnNameList = Group(delimitedList(columnName)).setName(""columns"")&#xa;    columnSpec     = ('*' | columnNameList)&#xa;&#xa;    tableName      = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    tableNameList  = Group(delimitedList(tableName)).setName(""tables"")&#xa;    &#xa;    simpleSQL      = selectToken(""command"") + columnSpec(""columns"") + fromToken + tableNameList(""tables"")&#xa;&#xa;    # demo runTests method, including embedded comments in test string&#xa;    simpleSQL.runTests(""""""&#xa;        # '*' as column list and dotted table name&#xa;        select * from SYS.XYZZY&#xa;&#xa;        # caseless match on ""SELECT"", and casts back to ""select""&#xa;        SELECT * from XYZZY, ABC&#xa;&#xa;        # list of column names, and mixed case SELECT keyword&#xa;        Select AA,BB,CC from Sys.dual&#xa;&#xa;        # multiple tables&#xa;        Select A, B, C from Sys.dual, Table2&#xa;&#xa;        # invalid SELECT keyword - should fail&#xa;        Xelect A, B, C from Sys.dual&#xa;&#xa;        # incomplete command - should fail&#xa;        Select&#xa;&#xa;        # invalid column name - should fail&#xa;        Select ^^^ frox Sys.dual&#xa;&#xa;        """""")&#xa;&#xa;    pyparsing_common.number.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    # any int or real number, returned as float&#xa;    pyparsing_common.fnumber.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    pyparsing_common.hex_integer.runTests(""""""&#xa;        100&#xa;        FF&#xa;        """""")&#xa;&#xa;    import uuid&#xa;    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;    pyparsing_common.uuid.runTests(""""""&#xa;        12345678-1234-5678-1234-567812345678&#xa;        """""")&#xa;"
15546273|"#!/usr/bin/python&#xa;&#xa;import json&#xa;import os&#xa;import re&#xa;import subprocess as subp # for shell commands&#xa;import math&#xa;from operator import itemgetter # for sorting lists by dict value&#xa;from lxml import etree as et&#xa;&#xa;try:&#xa;    # Python 3&#xa;    import html.parser as HTMLParser&#xa;except:&#xa;    # Python 2&#xa;    import HTMLParser&#xa;&#xa;from pkg_resources import get_distribution&#xa;&#xa;import pcbmode.config as config&#xa;&#xa;# pcbmode modules&#xa;from .point import Point&#xa;from . import messages as msg&#xa;import hashlib&#xa;&#xa;&#xa;&#xa;&#xa;def dictToStyleText(style_dict):&#xa;    """"""&#xa;    Convert a dictionary into an SVG/CSS style attribute&#xa;    """"""&#xa;&#xa;    style = ''&#xa;    for key in style_dict:&#xa;        style += ""%s:%s;"" % (key, style_dict[key])&#xa;&#xa;    return style&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def openBoardSVG():&#xa;    """"""&#xa;    Opens the built PCBmodE board SVG.&#xa;    Returns an ElementTree object&#xa;    """"""&#xa;&#xa;    filename = os.path.join(config.cfg['base-dir'],&#xa;                            config.cfg['locations']['build'],&#xa;                            config.cfg['name'] + '.svg')&#xa;    try:&#xa;        data = et.ElementTree(file=filename) &#xa;    except IOError as e:&#xa;        msg.error(""Cannot open %s; has the board been made using the '-m' option yet?"" % filename)&#xa;&#xa;    return data&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parseDimension(string):&#xa;    """"""&#xa;    Parses a dimention recieved from the source files, separating the units,&#xa;    if specified, from the value&#xa;    """"""&#xa;    if string != None:&#xa;        result = re.match('(-?\d*\.?\d+)\s?(\w+)?', string)&#xa;        value = float(result.group(1))&#xa;        unit = result.group(2)&#xa;    else:&#xa;        value = None&#xa;        unit = None&#xa;    return value, unit&#xa;&#xa;&#xa;&#xa;&#xa;def to_Point(coord=[0, 0]):&#xa;    """"""&#xa;    Takes a coordinate in the form of [x,y] and&#xa;    returns a Point type&#xa;    """"""&#xa;    return Point(coord[0], coord[1])&#xa;&#xa;&#xa;&#xa;&#xa;def toPoint(coord=[0, 0]):&#xa;    """"""&#xa;    Takes a coordinate in the form of [x,y] and&#xa;    returns a Point type&#xa;    """"""&#xa;    if coord == None:&#xa;        return None&#xa;    else:&#xa;        return Point(coord[0], coord[1])&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def get_git_revision():&#xa;&#xa;    return get_distribution('pcbmode').version&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def makePngs():&#xa;    """"""&#xa;    Creates a PNG of the board using Inkscape&#xa;    """"""&#xa;&#xa;    # Directory for storing the Gerbers within the build path&#xa;    images_path = os.path.join(config.cfg['base-dir'], &#xa;                               config.cfg['locations']['build'], &#xa;                               'images')&#xa;    # Create it if it doesn't exist&#xa;    create_dir(images_path)&#xa;&#xa;    # create individual PNG files for layers&#xa;    png_dpi = 600&#xa;    msg.subInfo(""Generating PNGs for each layer of the board"")&#xa;&#xa;    command = ['inkscape', &#xa;               '--without-gui', &#xa;               '--file=%s' % os.path.join(config.cfg['base-dir'], &#xa;                                          config.cfg['locations']['build'], &#xa;                                          config.cfg['name'] + '.svg'), &#xa;               '--export-png=%s' % os.path.join(images_path, config.cfg['name'] + '_rev_' + &#xa;                                                config.brd['config']['rev'] +&#xa;                                                '.png'),&#xa;               '--export-dpi=%s' % str(png_dpi),&#xa;               '--export-area-drawing',&#xa;               '--export-background=#FFFFFF']&#xa;    &#xa;    try:&#xa;        subp.call(command)&#xa;    except OSError as e:&#xa;        msg.error(""Cannot find, or run, Inkscape in commandline mode"")&#xa;&#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;# get_json_data_from_file&#xa;def dictFromJsonFile(filename, error=True):&#xa;    """"""&#xa;    Open a json file and returns its content as a dict&#xa;    """"""&#xa;&#xa;    def checking_for_unique_keys(pairs):&#xa;        """"""&#xa;        Check if there are duplicate keys defined; this is useful&#xa;        for any hand-edited file&#xa;  &#xa;        This SO answer was useful here:&#xa;          http://stackoverflow.com/questions/16172011/json-in-python-receive-check-duplicate-key-error&#xa;        """"""&#xa;        result = dict()&#xa;        for key,value in pairs:&#xa;            if key in result:&#xa;                msg.error(""duplicate key ('%s') specified in %s"" % (key, filename), KeyError)&#xa;            result[key] = value&#xa;        return result&#xa;&#xa;    try:&#xa;        with open(filename, 'r') as f:&#xa;            json_data = json.load(f, object_pairs_hook=checking_for_unique_keys)&#xa;    except (IOError, OSError):&#xa;        if error == True:&#xa;            msg.error(""Couldn't open JSON file: %s"" % filename, IOError)&#xa;        else:&#xa;            msg.info(""Couldn't open JSON file: %s"" % filename, IOError)&#xa;&#xa;    return json_data&#xa;&#xa;&#xa;&#xa;&#xa;def getLayerList():&#xa;    """"""&#xa;    """"""&#xa;    layer_list = []&#xa;    for record in config.stk['stackup']:&#xa;        if record['type'] == 'signal-layer-surface' or record['type'] == 'signal-layer-internal':&#xa;            layer_list.append(record)&#xa;&#xa;    layer_names = []&#xa;    for record in layer_list:&#xa;        layer_names.append(record['name'])&#xa;&#xa;    return layer_list, layer_names&#xa;&#xa;&#xa;&#xa;def getSurfaceLayers():&#xa;    """"""&#xa;    Returns a list of surface layer names&#xa;    Only here until this function is purged from the&#xa;    codebase&#xa;    """"""    &#xa;    return config.stk['surface-layer-names']&#xa;&#xa;&#xa;&#xa;&#xa;def getInternalLayers():&#xa;    """"""&#xa;    Returns a list of internal layer names&#xa;    Only here until this function is purged from the&#xa;    codebase&#xa;    """"""    &#xa;    return config.stk['internal-layer-names']&#xa;&#xa;&#xa;&#xa;&#xa;def getExtendedLayerList(layers):&#xa;    """"""&#xa;    For the list of layers we may get a list of all&#xa;    internal layers ('internal-1', 'internal-2, etc.) or&#xa;    simply 'internal', meaning that that shape is meant&#xa;    to go into all internal layers, which is the most&#xa;    common case. The following 'expands' the layer list&#xa;    """"""&#xa;    if 'internal' in layers:&#xa;        layers.remove('internal')&#xa;        layers.extend(config.stk['internal-layer-names']) &#xa;    return layers&#xa;&#xa;&#xa;&#xa;&#xa;def getExtendedSheetList(layer, sheet):&#xa;    """"""&#xa;    We may want multiple sheets of the same type, such as two&#xa;    soldermask layers on the same physical layer. This function&#xa;    expands the list if such layers are defined in the stackup&#xa;    """"""&#xa;    &#xa;    for layer_dict in config.stk['layers-dict']:&#xa;        if layer_dict['name'] == layer:&#xa;            break&#xa;    stack_sheets = layer_dict['stack']&#xa;&#xa;    sheet_names = []&#xa;    for stack_sheet in stack_sheets:&#xa;        sheet_names.append(stack_sheet['name'])&#xa;&#xa;    new_list = []&#xa;    for sheet_name in sheet_names:&#xa;        if sheet_name.startswith(sheet):&#xa;            new_list.append(sheet_name)&#xa;&#xa;    return new_list&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def create_dir(path):&#xa;    """"""&#xa;    Checks if a directory exists, and creates one if not&#xa;    """"""&#xa;    &#xa;    try:&#xa;        # try to create directory first; this prevents TOCTTOU-type race condition&#xa;        os.makedirs(path)&#xa;    except OSError:&#xa;        # if the dir exists, pass&#xa;        if os.path.isdir(path):&#xa;            pass&#xa;        else:&#xa;            print(""ERROR: couldn't create build path %s"" % path)&#xa;            raise&#xa;&#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def add_dict_values(d1, d2):&#xa;    """"""&#xa;    Add the values of two dicts&#xa;    Helpful code here:&#xa;      http://stackoverflow.com/questions/1031199/adding-dictionaries-in-python&#xa;    """"""&#xa;&#xa;    return dict((n, d1.get(n, 0)+d2.get(n, 0)) for n in set(d1)|set(d2) )&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def process_meander_type(type_string, meander_type):&#xa;    """"""&#xa;    Extract meander path type parameters and return them as a dict&#xa;    """"""&#xa;&#xa;    if (meander_type == 'meander-round'):&#xa;        look_for = ['radius', 'theta', 'bus-width', 'pitch']&#xa;    elif (meander_type == 'meander-sawtooth'):&#xa;        look_for = ['base-length', 'amplitude', 'bus-width', 'pitch']&#xa;    else:&#xa;        print(""ERROR: unrecognised meander type"")&#xa;        reaise&#xa;&#xa;    meander = {}&#xa;&#xa;    regex = '\s*%s\s*:\s*(?P<v>[^;]*)'&#xa;&#xa;    for param in look_for:&#xa;        tmp = re.search(regex % param, type_string)&#xa;        if tmp is not None:&#xa;            meander[param] = float(tmp.group('v'))&#xa;&#xa;    # add optional fields as 'None'&#xa;    for param in look_for:    &#xa;        if meander.get(param) is None:&#xa;            meander[param] = None&#xa;&#xa;    return meander&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def checkForPoursInLayer(layer):&#xa;    """"""&#xa;    Returns True or False if there are pours in the specified layer&#xa;    """"""&#xa;&#xa;    # In case there are no 'shapes' defined&#xa;    try:&#xa;        pours = config.brd['shapes'].get('pours')&#xa;    except:&#xa;        pours = {}&#xa;&#xa;    if pours is not None:&#xa;        for pour_dict in pours:&#xa;            layers = getExtendedLayerList(pour_dict.get('layers'))&#xa;            if layer in layers:&#xa;                return True&#xa; &#xa;    #return False&#xa;    return True&#xa;&#xa;&#xa;&#xa;&#xa;def interpret_svg_matrix(matrix_data):&#xa;    """"""&#xa;    Takes an array for six SVG parameters and returns angle, scale &#xa;    and placement coordinate&#xa;&#xa;    This SO answer was helpful here:&#xa;      http://stackoverflow.com/questions/15546273/svg-matrix-to-rotation-degrees&#xa;    """"""&#xa;&#xa;    # apply float() to all elements, just in case&#xa;    matrix_data = [ float(x) for x in matrix_data ]&#xa;&#xa;    coord = Point(matrix_data[4], -matrix_data[5])&#xa;    if matrix_data[0] == 0:&#xa;        angle = math.degrees(0)&#xa;    else:&#xa;        angle = math.atan(matrix_data[2] / matrix_data[0])&#xa;    &#xa;    scale = Point(math.fabs(matrix_data[0] / math.cos(angle)), &#xa;                  math.fabs(matrix_data[3] / math.cos(angle)))&#xa;&#xa;    # convert angle to degrees&#xa;    angle = math.degrees(angle)&#xa;&#xa;    # Inkscape rotates anti-clockwise, PCBmodE ""thinks"" clockwise. The following &#xa;    # adjusts these two views, although at some point we'd&#xa;    # need to have the same view, or make it configurable&#xa;    angle = -angle&#xa;&#xa;    return coord, angle, scale&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parse_refdef(refdef):&#xa;    """"""&#xa;    Parses a reference designator and returns the refdef categoty,&#xa;    number, and extra characters&#xa;    """"""&#xa;&#xa;    regex = r'^(?P<t>[a-zA-z\D]+?)(?P<n>\d+)(?P<e>[\-\s].*)?'&#xa;    parse = re.match(regex, refdef)&#xa;&#xa;    # TODO: there has to be a more elegant way for doing this!&#xa;    if parse == None:&#xa;        return None, None, None&#xa;    else:&#xa;        t = parse.group('t')&#xa;        n = int(parse.group('n'))&#xa;        e = parse.group('e')&#xa;        return t, n, e&#xa;    &#xa;&#xa;&#xa;&#xa;&#xa;&#xa;#def renumber_refdefs(cfg, order):&#xa;def renumberRefdefs(order):&#xa;    """"""&#xa;    Renumber the refdefs in the specified order&#xa;    """"""&#xa;&#xa;    components = config.brd['components']&#xa;    comp_dict = {}&#xa;    new_dict = {}&#xa;&#xa;    for refdef in components:&#xa;&#xa;        rd_type, rd_number, rd_extra = parse_refdef(refdef)&#xa;        location = to_Point(components[refdef].get('location') or [0, 0])&#xa;        tmp = {}&#xa;        tmp['record'] = components[refdef]&#xa;        tmp['type'] = rd_type&#xa;        tmp['number'] = rd_number&#xa;        tmp['extra'] = rd_extra&#xa;        tmp['coord-x'] = location.x&#xa;        tmp['coord-y'] = location.y&#xa;&#xa;        if comp_dict.get(rd_type) == None:&#xa;            comp_dict[rd_type] = []&#xa;        comp_dict[rd_type].append(tmp)&#xa;&#xa;        # Sort list according to 'order'&#xa;        for comp_type in comp_dict:&#xa;           if order == 'left-to-right':&#xa;               reverse = False&#xa;               itemget_param = 'coord_x'&#xa;           elif order == 'right-to-left':&#xa;               reverse = True&#xa;               itemget_param = 'coord_x'&#xa;           elif order == 'top-to-bottom':&#xa;               reverse = True&#xa;               itemget_param = 'coord-y'&#xa;           elif order == 'bottom-to-top':&#xa;               reverse = False&#xa;               itemget_param = 'coord-y'&#xa;           else:&#xa;               msg.error('Unrecognised renumbering order %s' % (order)) &#xa; &#xa;           sorted_list = sorted(comp_dict[comp_type], &#xa;                                key=itemgetter(itemget_param), &#xa;                                reverse=reverse)&#xa;&#xa;&#xa;           for i, record in enumerate(sorted_list):&#xa;               new_refdef = ""%s%s"" % (record['type'], i+1)&#xa;               if record['extra'] is not None:&#xa;                   new_refdef += ""%s"" % (record['extra'])&#xa;               new_dict[new_refdef] = record['record']&#xa; &#xa;    config.brd['components'] = new_dict&#xa;&#xa;    # Save board config to file (everything is saved, not only the&#xa;    # component data)&#xa;    filename = os.path.join(config.cfg['locations']['boards'], &#xa;                            config.cfg['name'], &#xa;                            config.cfg['name'] + '.json')&#xa;    try:&#xa;        with open(filename, 'wb') as f:&#xa;            f.write(json.dumps(config.brd, sort_keys=True, indent=2))&#xa;    except:&#xa;        msg.error(""Cannot save file %s"" % filename)&#xa; &#xa; &#xa;    return&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def getTextParams(font_size, letter_spacing, line_height):&#xa;    try:&#xa;        letter_spacing, letter_spacing_unit = parseDimension(letter_spacing)&#xa;    except:&#xa;        msg.error(""There's a problem with parsing the 'letter-spacing' property with value '%s'. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '-2 mm' should work."" % letter_spacing)&#xa;&#xa;    if letter_spacing_unit == None:&#xa;        letter_spacing_unit = 'mm'&#xa;&#xa;    try:&#xa;        line_height, line_height_unit = parseDimension(line_height)&#xa;    except:&#xa;        msg.error(""There's a problem parsing the 'line-height' property with value '%s'. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '-2 mm' should work."" % line_height)&#xa;&#xa;    if line_height_unit == None:&#xa;        line_height_unit = 'mm'&#xa;    &#xa;    try:&#xa;        font_size, font_size_unit = parseDimension(font_size)&#xa;    except:&#xa;        throw(""There's a problem parsing the 'font-size'. It's most likely missing. The format should be an integer or float followed by 'mm' (the only unit supported). For example, '0.3mm' or '2 mm' should work. Of course, it needs to be a positive figure."")&#xa;&#xa;    if font_size_unit == None:&#xa;        font_size_unit = 'mm'&#xa;&#xa;    return float(font_size), float(letter_spacing), float(line_height)&#xa;&#xa;&#xa;&#xa;&#xa;def textToPath(font_data, text, letter_spacing, line_height, scale_factor):&#xa;    from .svgpath import SvgPath&#xa;    """"""&#xa;    Convert a text string (unicode and newlines allowed) to a path.&#xa;    The 'scale_factor' is needed in order to scale rp 'letter_spacing' and 'line_height'&#xa;    to the original scale of the font.&#xa;    """"""&#xa;&#xa;    # This the horizontal advance that applied to all glyphs unless there's a specification for&#xa;    # for the glyph itself&#xa;    font_horiz_adv_x = float(font_data.find(""//n:font"", namespaces={'n': config.cfg['namespace']['svg']}).get('horiz-adv-x'))&#xa;    &#xa;    # This is the number if 'units' per 'em'. The default, in the absence of a definition is 1000&#xa;    # according to the SVG spec&#xa;    units_per_em = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('units-per-em')) or 1000&#xa;&#xa;    glyph_ascent = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('ascent'))&#xa;    glyph_decent = float(font_data.find(""//n:font-face"", namespaces={'n': config.cfg['namespace']['svg']}).get('descent'))&#xa; &#xa;    text_width = 0&#xa;    text_path = ''&#xa;&#xa;    # split text into charcters and find unicade chars&#xa;    try:&#xa;        text = re.findall(r'(\&#x[0-9abcdef]*;|.|\n)', text)&#xa;    except:&#xa;        throw(""There's a problem parsing the text '%s'. Unicode and \\n newline should be fine, by the way."" % text)&#xa; &#xa;&#xa;    # instantiate HTML parser&#xa;    htmlpar = HTMLParser.HTMLParser()&#xa;    gerber_lp = ''&#xa;    text_height = 0&#xa;&#xa;    if line_height == None:&#xa;        line_height = units_per_em&#xa;&#xa;    for i, symbol in enumerate(text[:]):&#xa;&#xa;        symbol = htmlpar.unescape(symbol)&#xa;        # get the glyph definition from the file&#xa;        if symbol == '\n':&#xa;            text_width = 0&#xa;            text_height += units_per_em + (line_height/scale_factor-units_per_em)&#xa;        else:&#xa;            glyph = font_data.find(u'//n:glyph[@unicode=""%s""]' % symbol, namespaces={'n': config.cfg['namespace']['svg']})&#xa;            if glyph == None:&#xa;                utils.throw(""Damn, there's no glyph definition for '%s' in the '%s' font :("" % (symbol, font))&#xa;            else:&#xa;                # Unless the glyph has its own width, use the global font width&#xa;                glyph_width = float(glyph.get('horiz-adv-x') or font_horiz_adv_x)&#xa;                if symbol != ' ':&#xa;                    glyph_path = SvgPath(glyph.get('d'))&#xa;                    first_point = glyph_path.getFirstPoint()&#xa;                    offset_x = float(first_point[0])&#xa;                    offset_y = float(first_point[1])&#xa;                    path = glyph_path.getRelative()&#xa;                    path = re.sub('^(m\s?[-\d\.]+\s?,\s?[-\d\.]+)', 'M %s,%s' % (str(text_width+offset_x), str(offset_y-text_height)), path)&#xa;                    gerber_lp += (glyph.get('gerber-lp') or &#xa;                                  glyph.get('gerber_lp') or &#xa;                                  ""%s"" % 'd'*glyph_path.getNumberOfSegments())&#xa;                    text_path += ""%s "" % (path)&#xa;&#xa;            text_width += glyph_width+letter_spacing/scale_factor&#xa;&#xa;&#xa;    # Mirror text &#xa;    text_path = SvgPath(text_path)&#xa;    text_path.transform()&#xa;    text_path = text_path.getTransformedMirrored()&#xa;&#xa;    return text_path, gerber_lp&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def digest(string):&#xa;    digits = config.cfg['digest-digits']&#xa;    return hashlib.md5(string.encode()).hexdigest()[:digits-1]&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def getStyleAttrib(style, attrib):&#xa;    """"""&#xa;    """"""&#xa;    regex = r"".*?%s:\s?(?P<s>[^;]*)(?:;|$)""&#xa;    match = re.match(regex % attrib, style)&#xa;    if match == None:&#xa;        return None&#xa;    else:&#xa;        return match.group('s')&#xa;&#xa;&#xa;&#xa;&#xa;def niceFloat(f):&#xa;    if f.is_integer():&#xa;        return int(f)&#xa;    else:&#xa;        return round(f, 6)&#xa;&#xa;&#xa;&#xa;&#xa;def parseTransform(transform):&#xa;    """"""&#xa;    Returns a Point() for the input transform&#xa;    """"""&#xa;    data = {}&#xa;    if transform == None:&#xa;        data['type'] = 'translate'&#xa;        data['location'] = Point()&#xa;    elif 'translate' in transform.lower():&#xa;        regex = r"".*?translate\s?\(\s?(?P<x>[+-]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?)\s?[\s,]\s?(?P<y>[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?)\s?\).*""&#xa;#       regex = r"".*?translate\s?\(\s?(?P<x>-?[0-9]*\.?[0-9]+)\s?[\s,]\s?(?P<y>-?[0-9]*\.?[0-9]+\s?)\s?\).*""&#xa;        coord = re.match(regex, transform)&#xa;        data['type'] = 'translate'&#xa;        x = coord.group('x')&#xa;        y = coord.group('y')&#xa;        data['location'] = Point(x,y)&#xa;    elif 'matrix' in transform.lower():&#xa;        data['type'] = 'matrix'&#xa;        data['location'], data['rotate'], data['scale'] = parseSvgMatrix(transform)&#xa;    else:&#xa;        msg.error(""Found a path transform that cannot be handled, %s. SVG stansforms should be in the form of 'translate(num,num)' or 'matrix(num,num,num,num,num,num)"" % transform)&#xa;&#xa;    return data &#xa;&#xa;&#xa;&#xa;&#xa;&#xa;def parseSvgMatrix(matrix):&#xa;    """"""&#xa;    Takes an array for six SVG parameters and returns angle, scale &#xa;    and placement coordinate&#xa;&#xa;    This SO answer was helpful here:&#xa;      http://stackoverflow.com/questions/15546273/svg-matrix-to-rotation-degrees&#xa;    """"""&#xa;    regex = r"".*?matrix\((?P<m>.*?)\).*""&#xa;    matrix = re.match(regex, matrix)&#xa;    matrix = matrix.group('m')&#xa;    matrix = matrix.split(',')&#xa;&#xa;    # Apply float() to all elements&#xa;    matrix = [ float(x) for x in matrix ]&#xa;&#xa;    coord = Point(matrix[4], matrix[5])&#xa;    if matrix[0] == 0:&#xa;        angle = math.degrees(0)&#xa;    else:&#xa;        angle = math.atan(matrix[2] / matrix[0])&#xa;    &#xa;    #scale = Point(math.fabs(matrix[0] / math.cos(angle)), &#xa;    #              math.fabs(matrix[3] / math.cos(angle)))&#xa;    scale_x = math.sqrt(matrix[0]*matrix[0] + matrix[1]*matrix[1]),&#xa;    scale_y = math.sqrt(matrix[2]*matrix[2] + matrix[3]*matrix[3]),    &#xa;&#xa;    scale = max(scale_x, scale_y)[0]&#xa;&#xa;    # convert angle to degrees&#xa;    angle = math.degrees(angle)&#xa;&#xa;    # Inkscape rotates anti-clockwise, PCBmodE ""thinks"" clockwise. The following &#xa;    # adjusts these two views, although at some point we'd&#xa;    # need to have the same view, or make it configurable&#xa;    angle = -angle&#xa;&#xa;    return coord, angle, scale&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;"
3209233|"#!/usr/bin/env python&#xa;""""""&#xa;# Inbound Proxy Module developed by Bharadwaj Machiraju (blog.tunnelshade.in) as a part of Google Summer of Code 2013.&#xa;""""""&#xa;&#xa;import os&#xa;import re&#xa;import ssl&#xa;import socket&#xa;import datetime&#xa;from multiprocessing import Value&#xa;&#xa;import pycurl&#xa;&#xa;import tornado.httpserver&#xa;import tornado.ioloop&#xa;import tornado.iostream&#xa;import tornado.web&#xa;import tornado.httpclient&#xa;import tornado.curl_httpclient&#xa;import tornado.escape&#xa;import tornado.httputil&#xa;import tornado.options&#xa;import tornado.template&#xa;import tornado.websocket&#xa;import tornado.gen&#xa;&#xa;from socket_wrapper import wrap_socket&#xa;from cache_handler import CacheHandler&#xa;from framework.dependency_management.dependency_resolver import BaseComponent, ComponentNotFoundException&#xa;from framework.utils import FileOperations&#xa;from framework.lib.owtf_process import OWTFProcess&#xa;&#xa;&#xa;def prepare_curl_callback(curl):&#xa;    curl.setopt(pycurl.PROXYTYPE, pycurl.PROXYTYPE_SOCKS5)&#xa;&#xa;&#xa;class ProxyHandler(tornado.web.RequestHandler):&#xa;&#xa;    """"""This RequestHandler processes all the requests that the application received.""""""&#xa;&#xa;    SUPPORTED_METHODS = ['GET', 'POST', 'CONNECT', 'HEAD', 'PUT', 'DELETE', 'OPTIONS', 'TRACE']&#xa;    server = None&#xa;    restricted_request_headers = None&#xa;    restricted_response_headers = None&#xa;&#xa;    def __new__(cls, application, request, **kwargs):&#xa;        # http://stackoverflow.com/questions/3209233/how-to-replace-an-instance-in-init-with-a-different-object&#xa;        # Based on upgrade header, websocket request handler must be used&#xa;        try:&#xa;            if request.headers['Upgrade'].lower() == 'websocket':&#xa;                return CustomWebSocketHandler(application, request, **kwargs)&#xa;        except KeyError:&#xa;            pass&#xa;        return tornado.web.RequestHandler.__new__(cls, application, request, **kwargs)&#xa;&#xa;    def set_default_headers(self):&#xa;        # Automatically called by Tornado,&#xa;        # Used to remove ""Server"" header set by tornado&#xa;        del self._headers[""Server""]&#xa;&#xa;    def set_status(self, status_code, reason=None):&#xa;        """"""Sets the status code for our response.&#xa;&#xa;        Overriding is done so as to handle unknown response codes gracefully.&#xa;        """"""&#xa;        self._status_code = status_code&#xa;        if reason is not None:&#xa;            self._reason = tornado.escape.native_str(reason)&#xa;        else:&#xa;            try:&#xa;                self._reason = tornado.httputil.responses[status_code]&#xa;            except KeyError:&#xa;                self._reason = tornado.escape.native_str(""Server Not Found"")&#xa;&#xa;    def finish_response(self, response):&#xa;        """"""Write a new response and cache it.""""""&#xa;        self.set_status(response.code)&#xa;        for header, value in response.headers.get_all():&#xa;            if header == ""Set-Cookie"":&#xa;                self.add_header(header, value)&#xa;            else:&#xa;                if header not in ProxyHandler.restricted_response_headers:&#xa;                    self.set_header(header, value)&#xa;        self.finish()&#xa;&#xa;    def handle_data_chunk(self, data):&#xa;        """"""Callback when a small chunk is received.""""""&#xa;        if data:&#xa;            self.write(data)&#xa;            self.request.response_buffer += data&#xa;&#xa;    @tornado.web.asynchronous&#xa;    @tornado.gen.coroutine&#xa;    def get(self):&#xa;        """"""Handle all requests except the connect request.&#xa;&#xa;        Once ssl stream is formed between browser and proxy, the requests are then processed by this function.&#xa;        """"""&#xa;        # The flow starts here&#xa;        self.request.local_timestamp = datetime.datetime.now()&#xa;        self.request.response_buffer = ''&#xa;&#xa;        # The requests that come through ssl streams are relative requests, so transparent proxying is required. The&#xa;        # following snippet decides the url that should be passed to the async client&#xa;        if self.request.uri.startswith(self.request.protocol, 0):  # Normal Proxy Request.&#xa;            self.request.url = self.request.uri&#xa;        else:  # Transparent Proxy Request.&#xa;            self.request.url = ""%s://%s"" % (self.request.protocol, self.request.host)&#xa;            if self.request.uri != '/':  # Add uri only if needed.&#xa;                self.request.url += self.request.uri&#xa;&#xa;        # This block here checks for already cached response and if present returns one&#xa;        self.cache_handler = CacheHandler(self.application.cache_dir, self.request, self.application.cookie_regex,&#xa;                                          self.application.cookie_blacklist)&#xa;        request_hash = yield tornado.gen.Task(self.cache_handler.calculate_hash)&#xa;        self.cached_response = self.cache_handler.load()&#xa;&#xa;        if self.cached_response:&#xa;            if self.cached_response.body:&#xa;                self.write(self.cached_response.body)&#xa;            self.finish_response(self.cached_response)&#xa;        else:&#xa;            # Request header cleaning&#xa;            for header in ProxyHandler.restricted_request_headers:&#xa;                try:&#xa;                    del self.request.headers[header]&#xa;                except:&#xa;                    continue&#xa;            # HTTP auth if exists&#xa;            http_auth_username = None&#xa;            http_auth_password = None&#xa;            http_auth_mode = None&#xa;            if self.application.http_auth:&#xa;                host = self.request.host&#xa;                # If default ports are not provided, they are added&#xa;                if ':' not in self.request.host:&#xa;                    default_ports = {'http': '80', 'https': '443'}&#xa;                    if self.request.protocol in default_ports:&#xa;                        host = '%s:%s' % (self.request.host, default_ports[self.request.protocol])&#xa;                # Check if auth is provided for that host&#xa;                try:&#xa;                    index = self.application.http_auth_hosts.index(host)&#xa;                    http_auth_username = self.application.http_auth_usernames[index]&#xa;                    http_auth_password = self.application.http_auth_passwords[index]&#xa;                    http_auth_mode = self.application.http_auth_modes[index]&#xa;                except ValueError:&#xa;                    pass&#xa;&#xa;            # pycurl is needed for curl client&#xa;            async_client = tornado.curl_httpclient.CurlAsyncHTTPClient()&#xa;            # httprequest object is created and then passed to async client with a callback&#xa;            success_response = False  # is used to check the response in the botnet mode&#xa;&#xa;            while not success_response:&#xa;                # Proxy Switching (botnet_mode) code&#xa;                if self.application.proxy_manager:&#xa;                    proxy = self.application.proxy_manager.get_next_available_proxy()&#xa;                    self.application.outbound_ip = proxy[""proxy""][0]&#xa;                    self.application.outbound_port = int(proxy[""proxy""][1])&#xa;                # httprequest object is created and then passed to async client with a callback&#xa;                callback = None&#xa;                if self.application.outbound_proxy_type == 'socks':&#xa;                    callback = prepare_curl_callback  # socks callback function.&#xa;                body = self.request.body or None&#xa;                request = tornado.httpclient.HTTPRequest(&#xa;                    url=self.request.url,&#xa;                    method=self.request.method,&#xa;                    body=body,&#xa;                    headers=self.request.headers,&#xa;                    auth_username=http_auth_username,&#xa;                    auth_password=http_auth_password,&#xa;                    auth_mode=http_auth_mode,&#xa;                    follow_redirects=False,&#xa;                    use_gzip=True,&#xa;                    streaming_callback=self.handle_data_chunk,&#xa;                    header_callback=None,&#xa;                    proxy_host=self.application.outbound_ip,&#xa;                    proxy_port=self.application.outbound_port,&#xa;                    proxy_username=self.application.outbound_username,&#xa;                    proxy_password=self.application.outbound_password,&#xa;                    allow_nonstandard_methods=True,&#xa;                    prepare_curl_callback=callback,&#xa;                    validate_cert=False&#xa;                )&#xa;                try:&#xa;                    response = yield tornado.gen.Task(async_client.fetch, request)&#xa;                except Exception:&#xa;                    response = None&#xa;                    pass&#xa;                # Request retries&#xa;                for i in range(0, 3):&#xa;                    if (response is None) or response.code in [408, 599]:&#xa;                        self.request.response_buffer = ''&#xa;                        response = yield tornado.gen.Task(async_client.fetch, request)&#xa;                    else:&#xa;                        success_response = True&#xa;                        break&#xa;&#xa;                # Botnet mode code (proxy switching).&#xa;                # Checking the status of the proxy (asynchronous).&#xa;                if self.application.proxy_manager and not success_response:&#xa;                    proxy_check_req = tornado.httpclient.HTTPRequest(&#xa;                        url=self.application.proxy_manager.testing_url,  # testing url is google.com.&#xa;                        use_gzip=True,&#xa;                        proxy_host=self.application.outbound_ip,&#xa;                        proxy_port=self.application.outbound_port,&#xa;                        proxy_username=self.application.outbound_username,&#xa;                        proxy_password=self.application.outbound_password,&#xa;                        prepare_curl_callback=callback,  # socks callback function.&#xa;                        validate_cert=False&#xa;                    )&#xa;                    try:&#xa;                        proxy_check_resp = yield tornado.gen.Task(async_client.fetch, proxy_check_req)&#xa;                    except Exception:&#xa;                        pass&#xa;&#xa;                    if proxy_check_resp.code != 200:&#xa;                        self.application.proxy_manager.remove_proxy(proxy[""index""])&#xa;                    else:&#xa;                        success_response = True&#xa;                else:&#xa;                    success_response = True&#xa;&#xa;            self.finish_response(response)&#xa;            # Cache the response after finishing the response, so caching time is not included in response time&#xa;            self.cache_handler.dump(response)&#xa;&#xa;    # The following 5 methods can be handled through the above implementation.&#xa;    @tornado.web.asynchronous&#xa;    def post(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def head(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def put(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def delete(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def options(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def trace(self):&#xa;        return self.get()&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def connect(self):&#xa;        """"""Gets called when a connect request is received.&#xa;&#xa;        * The host and port are obtained from the request uri&#xa;        * A socket is created, wrapped in ssl and then added to SSLIOStream&#xa;        * This stream is used to connect to speak to the remote host on given port&#xa;        * If the server speaks ssl on that port, callback start_tunnel is called&#xa;        * An OK response is written back to client&#xa;        * The client side socket is wrapped in ssl&#xa;        * If the wrapping is successful, a new SSLIOStream is made using that socket&#xa;        * The stream is added back to the server for monitoring&#xa;        """"""&#xa;        host, port = self.request.uri.split(':')&#xa;&#xa;        def start_tunnel():&#xa;            try:&#xa;                self.request.connection.stream.write(b""HTTP/1.1 200 Connection established\r\n\r\n"")&#xa;                wrap_socket(&#xa;                    self.request.connection.stream.socket,&#xa;                    host,&#xa;                    self.application.ca_cert,&#xa;                    self.application.ca_key,&#xa;                    self.application.ca_key_pass,&#xa;                    self.application.certs_folder,&#xa;                    success=ssl_success&#xa;                )&#xa;            except tornado.iostream.StreamClosedError:&#xa;                pass&#xa;&#xa;        def ssl_success(client_socket):&#xa;            client = tornado.iostream.SSLIOStream(client_socket)&#xa;            ProxyHandler.server.handle_stream(client, self.application.inbound_ip)&#xa;&#xa;        # Tiny Hack to satisfy proxychains CONNECT request to HTTP port.&#xa;        # HTTPS fail check has to be improvised&#xa;        def ssl_fail():&#xa;            try:&#xa;                self.request.connection.stream.write(b""HTTP/1.1 200 Connection established\r\n\r\n"")&#xa;            except tornado.iostream.StreamClosedError:&#xa;                pass&#xa;            ProxyHandler.server.handle_stream(self.request.connection.stream, self.application.inbound_ip)&#xa;&#xa;        # Hacking to be done here, so as to check for ssl using proxy and auth&#xa;        try:&#xa;            # Adds a fix for check_hostname errors in Tornado 4.3.0&#xa;            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)&#xa;            context.check_hostname = False&#xa;            context.load_default_certs()&#xa;            # When connecting through a new socket, no need to wrap the socket before passing&#xa;            # to SSIOStream&#xa;            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM, 0)&#xa;            upstream = tornado.iostream.SSLIOStream(s, ssl_options=context)&#xa;            upstream.set_close_callback(ssl_fail)&#xa;            upstream.connect((host, int(port)), start_tunnel)&#xa;        except Exception:&#xa;            self.finish()&#xa;&#xa;&#xa;class CustomWebSocketHandler(tornado.websocket.WebSocketHandler):&#xa;&#xa;    """"""Class is used for handling websocket traffic.&#xa;&#xa;    * Object of this class replaces the main request handler for a request with header => ""Upgrade: websocket""&#xa;    * wss:// - CONNECT request is handled by main handler&#xa;    """"""&#xa;&#xa;    def upstream_connect(self, io_loop=None, callback=None):&#xa;        """"""Custom alternative to tornado.websocket.websocket_connect.&#xa;&#xa;        Returns a future.&#xa;        """"""&#xa;        # io_loop is needed or it won't work with Tornado.&#xa;        if io_loop is None:&#xa;            io_loop = tornado.ioloop.IOLoop.current()&#xa;&#xa;        # During secure communication, we get relative URI, so make them absolute&#xa;        if self.request.uri.startswith(self.request.protocol, 0):  # Normal Proxy Request.&#xa;            self.request.url = self.request.uri&#xa;        # Transparent Proxy Request&#xa;        else:&#xa;            self.request.url = ""%s://%s%s"" % (self.request.protocol, self.request.host, self.request.uri)&#xa;        self.request.url = self.request.url.replace(""http"", ""ws"", 1)&#xa;&#xa;        # Have to add cookies and stuff&#xa;        request_headers = tornado.httputil.HTTPHeaders()&#xa;        for name, value in self.request.headers.items():&#xa;            if name not in ProxyHandler.restricted_request_headers:&#xa;                request_headers.add(name, value)&#xa;        # Build a custom request&#xa;        request = tornado.httpclient.HTTPRequest(&#xa;            url=self.request.url,&#xa;            headers=request_headers,&#xa;            proxy_host=self.application.outbound_ip,&#xa;            proxy_port=self.application.outbound_port,&#xa;            proxy_username=self.application.outbound_username,&#xa;            proxy_password=self.application.outbound_password&#xa;        )&#xa;        self.upstream_connection = CustomWebSocketClientConnection(io_loop, request)&#xa;        if callback is not None:&#xa;            io_loop.add_future(self.upstream_connection.connect_future, callback)&#xa;        return self.upstream_connection.connect_future&#xa;&#xa;    def _execute(self, transforms, *args, **kwargs):&#xa;        """"""Overriding of a method of WebSocketHandler.""""""&#xa;&#xa;        def start_tunnel(future):&#xa;            """"""A callback which is called when connection to url is successful.""""""&#xa;            # We need upstream to write further messages&#xa;            self.upstream = future.result()&#xa;            # HTTPRequest needed for caching&#xa;            self.handshake_request = self.upstream_connection.request&#xa;            # Needed for websocket data & compliance with cache_handler stuff&#xa;            self.handshake_request.response_buffer = ''&#xa;            # Tiny hack to protect caching (according to websocket standards)&#xa;            self.handshake_request.version = 'HTTP/1.1'&#xa;            # XXX: I dont know why a None is coming&#xa;            self.handshake_request.body = self.handshake_request.body or ''&#xa;            # The regular procedures are to be done&#xa;            tornado.websocket.WebSocketHandler._execute(self, transforms, *args, **kwargs)&#xa;&#xa;        # We try to connect to provided URL & then we proceed with connection on client side.&#xa;        self.upstream = self.upstream_connect(callback=start_tunnel)&#xa;&#xa;    def store_upstream_data(self, message):&#xa;        """"""Save websocket data sent from client to server.&#xa;&#xa;        i.e add it to HTTPRequest.response_buffer with direction (>>)&#xa;        """"""&#xa;        try:  # Cannot write binary content as a string, so catch it&#xa;            self.handshake_request.response_buffer += ("">>> %s\r\n"" % message)&#xa;        except TypeError:&#xa;            self.handshake_request.response_buffer += ("">>> May be binary\r\n"")&#xa;&#xa;    def store_downstream_data(self, message):&#xa;        """"""Save websocket data sent from client to server.&#xa;&#xa;        i.e add it to HTTPRequest.response_buffer with direction (<<)&#xa;        """"""&#xa;        try:  # Cannot write binary content as a string, so catch it.&#xa;            self.handshake_request.response_buffer += (""<<< %s\r\n"" % message)&#xa;        except TypeError:&#xa;            self.handshake_request.response_buffer += (""<<< May be binary\r\n"")&#xa;&#xa;    def on_message(self, message):&#xa;        """"""Everytime a message is received from client side, this instance method is called.""""""&#xa;        self.upstream.write_message(message)  # The obtained message is written to upstream.&#xa;        self.store_upstream_data(message)&#xa;&#xa;        # The following check ensures that if a callback is added for reading message from upstream, another one is not&#xa;        # added.&#xa;        if not self.upstream.read_future:&#xa;            # A callback is added to read the data when upstream responds.&#xa;            self.upstream.read_message(callback=self.on_response)&#xa;&#xa;    def on_response(self, message):&#xa;        """"""A callback when a message is recieved from upstream.""""""&#xa;        # The following check ensures that if a callback is added for reading message from upstream, another one is not&#xa;        # added&#xa;        if not self.upstream.read_future:&#xa;            self.upstream.read_message(callback=self.on_response)&#xa;        if self.ws_connection:  # Check if connection still exists.&#xa;            if message.result():  # Check if it is not NULL (indirect checking of upstream connection).&#xa;                self.write_message(message.result())  # Write obtained message to client.&#xa;                self.store_downstream_data(message.result())&#xa;            else:&#xa;                self.close()&#xa;&#xa;    def on_close(self):&#xa;        """"""Called when websocket is closed.&#xa;&#xa;        So handshake request-response pair along with websocket data as response body is saved&#xa;        """"""&#xa;        # Required for cache_handler&#xa;        self.handshake_response = tornado.httpclient.HTTPResponse(&#xa;            self.handshake_request,&#xa;            self.upstream_connection.code,&#xa;            headers=self.upstream_connection.headers,&#xa;            request_time=0&#xa;        )&#xa;        # Procedure for dumping a tornado request-response&#xa;        self.cache_handler = CacheHandler(&#xa;            self.application.cache_dir,&#xa;            self.handshake_request,&#xa;            self.application.cookie_regex,&#xa;            self.application.cookie_blacklist&#xa;        )&#xa;        self.cached_response = self.cache_handler.load()&#xa;        self.cache_handler.dump(self.handshake_response)&#xa;&#xa;&#xa;class CustomWebSocketClientConnection(tornado.websocket.WebSocketClientConnection):&#xa;    def _handle_1xx(self, code):&#xa;        # Had to extract response code, so it is necessary to override.&#xa;        self.code = code&#xa;        super(CustomWebSocketClientConnection, self)._handle_1xx(code)&#xa;&#xa;&#xa;class CommandHandler(tornado.web.RequestHandler):&#xa;&#xa;    """"""This handles the python function calls issued with relative url ""/JSON/?cmd="".&#xa;&#xa;    Responses are in JSON&#xa;    """"""&#xa;&#xa;    @tornado.web.asynchronous&#xa;    def get(self, relative_url):&#xa;        # Currently only get requests are sufficient for providing PnH service commands.&#xa;        command_list = self.get_arguments(""cmd"")&#xa;        info = {}&#xa;        for command in command_list:&#xa;            if command.startswith(""Core""):&#xa;                command = ""self.application.%s"" % command&#xa;                info[command] = eval(command)&#xa;            if command.startswith(""setattr""):&#xa;                info[command] = eval(command)&#xa;        self.write(info)&#xa;        self.finish()&#xa;&#xa;&#xa;class ProxyProcess(OWTFProcess, BaseComponent):&#xa;&#xa;    def initialize(self, outbound_options=[], outbound_auth=""""):&#xa;        # The tornado application, which is used to pass variables to request handler&#xa;        self.application = tornado.web.Application(handlers=[(r'.*', ProxyHandler)], debug=False, gzip=True,)&#xa;        self.config = self.get_component(""config"")&#xa;        self.db_config = self.get_component(""db_config"")&#xa;        # All required variables in request handler&#xa;        # Required variables are added as attributes to application, so that request handler can access these&#xa;        self.application.Core = self.get_component(""core"")&#xa;        try:&#xa;            self.proxy_manager = self.get_component(""proxy_manager"")&#xa;        except ComponentNotFoundException:&#xa;            self.proxy_manager = None&#xa;        self.application.proxy_manager = self.proxy_manager&#xa;        # ctypes object allocated from shared memory to verify if proxy must inject probe code or not&#xa;        # 'i' means ctypes type is integer, initialization value is 0&#xa;        # if lock is True then a new recursive lock object is created to&#xa;        # synchronize access to the value&#xa;        self.application.Core.pnh_inject = Value('i', 0, lock=True)&#xa;        self.application.inbound_ip = self.db_config.Get('INBOUND_PROXY_IP')&#xa;        self.application.inbound_port = int(self.db_config.Get('INBOUND_PROXY_PORT'))&#xa;&#xa;        if self.proxy_manager:&#xa;            self.instances = ""1""  # Botnet mode needs only one proxy process.&#xa;        else:&#xa;            self.instances = self.db_config.Get(""INBOUND_PROXY_PROCESSES"")&#xa;&#xa;        # Proxy CACHE&#xa;        # Cache related settings, including creating required folders according to cache folder structure&#xa;        self.application.cache_dir = self.db_config.Get(""INBOUND_PROXY_CACHE_DIR"")&#xa;        # Clean possible older cache directory.&#xa;        if os.path.exists(self.application.cache_dir):&#xa;            FileOperations.rm_tree(self.application.cache_dir)&#xa;        FileOperations.make_dirs(self.application.cache_dir)&#xa;&#xa;        # SSL MiTM&#xa;        # SSL certs, keys and other settings (os.path.expanduser because they are stored in users home directory&#xa;        # ~/.owtf/proxy)&#xa;        self.application.ca_cert = os.path.expanduser(self.db_config.Get('CA_CERT'))&#xa;        self.application.ca_key = os.path.expanduser(self.db_config.Get('CA_KEY'))&#xa;        # To stop OWTF from breaking for our beloved users :P&#xa;        try:&#xa;            self.application.ca_key_pass = FileOperations.open(os.path.expanduser(self.db_config.Get('CA_PASS_FILE')),&#xa;                                                               'r', owtf_clean=False).read().strip()&#xa;        except IOError:&#xa;            self.application.ca_key_pass = ""owtf""  # XXX: Legacy CA key pass for older versions.&#xa;        self.application.proxy_folder = os.path.dirname(self.application.ca_cert)&#xa;        self.application.certs_folder = os.path.expanduser(self.db_config.Get('CERTS_FOLDER'))&#xa;&#xa;        try:  # Ensure CA.crt and Key exist.&#xa;            assert os.path.exists(self.application.ca_cert)&#xa;            assert os.path.exists(self.application.ca_key)&#xa;        except AssertionError:&#xa;            self.get_component(""error_handler"").FrameworkAbort(""Files required for SSL MiTM are missing.""&#xa;                                                               "" Please run the install script"")&#xa;&#xa;        try:  # If certs folder missing, create that.&#xa;            assert os.path.exists(self.application.certs_folder)&#xa;        except AssertionError:&#xa;            FileOperations.make_dirs(self.application.certs_folder)&#xa;&#xa;        # Blacklist (or) Whitelist Cookies&#xa;        # Building cookie regex to be used for cookie filtering for caching&#xa;        if self.db_config.Get('WHITELIST_COOKIES') == 'None':&#xa;            cookies_list = self.db_config.Get('BLACKLIST_COOKIES').split(',')&#xa;            self.application.cookie_blacklist = True&#xa;        else:&#xa;            cookies_list = self.db_config.Get('WHITELIST_COOKIES').split(',')&#xa;            self.application.cookie_blacklist = False&#xa;        if self.application.cookie_blacklist:&#xa;            regex_cookies_list = [cookie + ""=([^;]+;?)"" for cookie in cookies_list]&#xa;        else:&#xa;            regex_cookies_list = [""("" + cookie + ""=[^;]+;?)"" for cookie in self.db_config.Get('COOKIES_LIST')]&#xa;        regex_string = '|'.join(regex_cookies_list)&#xa;        self.application.cookie_regex = re.compile(regex_string)&#xa;&#xa;        # Outbound Proxy&#xa;        # Outbound proxy settings to be used inside request handler&#xa;        if outbound_options:&#xa;            if len(outbound_options) == 3:&#xa;                self.application.outbound_proxy_type = outbound_options[0]&#xa;                self.application.outbound_ip = outbound_options[1]&#xa;                self.application.outbound_port = int(outbound_options[2])&#xa;            else:&#xa;                self.application.outbound_proxy_type = ""http""&#xa;                self.application.outbound_ip = outbound_options[0]&#xa;                self.application.outbound_port = int(outbound_options[1])&#xa;        else:&#xa;            self.application.outbound_ip = None&#xa;            self.application.outbound_port = None&#xa;            self.application.outbound_proxy_type = None&#xa;        if outbound_auth:&#xa;            self.application.outbound_username, self.application.outbound_password = outbound_auth.split("":"")&#xa;        else:&#xa;            self.application.outbound_username = None&#xa;            self.application.outbound_password = None&#xa;&#xa;        self.server = tornado.httpserver.HTTPServer(self.application)&#xa;        # server has to be a class variable, because it is used inside request handler to attach sockets for monitoring&#xa;        ProxyHandler.server = self.server&#xa;&#xa;        # Header filters&#xa;        # Restricted headers are picked from framework/config/framework_config.cfg&#xa;        # These headers are removed from the response obtained from webserver, before sending it to browser&#xa;        restricted_response_headers = self.config.FrameworkConfigGet(""PROXY_RESTRICTED_RESPONSE_HEADERS"").split("","")&#xa;        ProxyHandler.restricted_response_headers = restricted_response_headers&#xa;        # These headers are removed from request obtained from browser, before sending it to webserver&#xa;        restricted_request_headers = self.config.FrameworkConfigGet(""PROXY_RESTRICTED_REQUEST_HEADERS"").split("","")&#xa;        ProxyHandler.restricted_request_headers = restricted_request_headers&#xa;&#xa;        # HTTP Auth options&#xa;        if self.db_config.Get(""HTTP_AUTH_HOST"") != ""None"":&#xa;            self.application.http_auth = True&#xa;            # All the variables are lists&#xa;            self.application.http_auth_hosts = self.db_config.Get(""HTTP_AUTH_HOST"").strip().split(',')&#xa;            self.application.http_auth_usernames = self.db_config.Get(""HTTP_AUTH_USERNAME"").strip().split(',')&#xa;            self.application.http_auth_passwords = self.db_config.Get(""HTTP_AUTH_PASSWORD"").strip().split(',')&#xa;            self.application.http_auth_modes = self.db_config.Get(""HTTP_AUTH_MODE"").strip().split(',')&#xa;        else:&#xa;            self.application.http_auth = False&#xa;&#xa;    def pseudo_run(self):&#xa;        self.application.Core.disable_console_logging()&#xa;        try:&#xa;            self.server.bind(self.application.inbound_port, address=self.application.inbound_ip)&#xa;            # Useful for using custom loggers because of relative paths in secure requests&#xa;            # http://www.joet3ch.com/blog/2011/09/08/alternative-tornado-logging/&#xa;            tornado.options.parse_command_line(&#xa;                args=[""dummy_arg"", ""--log_file_prefix=%s"" % self.db_config.Get(""PROXY_LOG""), ""--logging=info""])&#xa;            # To run any number of instances&#xa;            # ""0"" equals the number of cores present in a machine&#xa;            self.server.start(int(self.instances))&#xa;            tornado.ioloop.IOLoop.instance().start()&#xa;        except:&#xa;            # Cleanup code&#xa;            self.clean_up()&#xa;&#xa;    def clean_up(self):&#xa;        self.server.stop()&#xa;        tornado.ioloop.IOLoop.instance().stop()&#xa;"
4494404|"# -*- coding: utf-8 -*-&#xa;&#xa;import os, sys&#xa;print(""CWD: "" + os.getcwd() )&#xa;&#xa;config_path = os.path.abspath('../matplotlib/')&#xa;sys.path.append(config_path)&#xa;lib_path = os.path.abspath('../../lib')&#xa;sys.path.append(lib_path)&#xa;&#xa;# Load configuration file (before pyplot)&#xa;import configuration as config&#xa;&#xa;&#xa;import numpy as np&#xa;import scipy.ndimage as ndi&#xa;import cv2&#xa;import matplotlib.pyplot as plt&#xa;&#xa;&#xa;import framemanager_python&#xa;&#xa;# Force reloading of external library (convenient during active development)&#xa;reload(framemanager_python)&#xa;&#xa;&#xa;# Taken from http://stackoverflow.com/questions/4494404/find-large-number-of-consecutive-values-fulfilling-condition-in-a-numpy-array&#xa;# Author: Joe Kington&#xa;def contiguous_regions(condition):&#xa;    """"""Finds contiguous True regions of the boolean array ""condition"". Returns&#xa;    a 2D array where the first column is the start index of the region and the&#xa;    second column is the end index.""""""&#xa;&#xa;    # Find the indicies of changes in ""condition""&#xa;    d = np.diff(condition)&#xa;    idx, = d.nonzero() &#xa;&#xa;    # We need to start things after the change in ""condition"". Therefore, &#xa;    # we'll shift the index by 1 to the right.&#xa;    idx += 1&#xa;&#xa;    if condition[0]:&#xa;        # If the start of condition is True prepend a 0&#xa;        idx = np.r_[0, idx]&#xa;&#xa;    if condition[-1]:&#xa;        # If the end of condition is True, append the length of the array&#xa;        idx = np.r_[idx, condition.size] # Edit&#xa;&#xa;    # Reshape the result into two columns&#xa;    idx.shape = (-1,2)&#xa;    return idx&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;profileName = os.path.abspath(""some_steps.dsa"")&#xa;frameManager = framemanager_python.FrameManagerWrapper()&#xa;frameManager.load_profile(profileName);&#xa;&#xa;&#xa;numTSFrames = frameManager.get_tsframe_count();&#xa;starttime = frameManager.get_tsframe_timestamp(0)&#xa;stoptime = frameManager.get_tsframe_timestamp(numTSFrames)&#xa;&#xa;max_matrix_1 = frameManager.get_max_matrix_list(1)&#xa;max_matrix_5 = frameManager.get_max_matrix_list(5)&#xa;&#xa;# Time stamps&#xa;timestamps = frameManager.get_tsframe_timestamp_list()&#xa;timestamps = (timestamps-timestamps[0]) / 1000.0 # Relative timestamps in seconds&#xa;&#xa;&#xa;&#xa;# Simple smoothing&#xa;#filtered_matrix_5 = ndi.filters.median_filter(max_matrix_5, size=5, mode='reflect')&#xa;#filtered_matrix_5 = ndi.uniform_filter1d(max_matrix_5, size=10, mode='reflect')&#xa;&#xa;# Edge detection&#xa;sobel = ndi.sobel(max_matrix_5, mode='reflect')  &#xa;#laplace = ndi.laplace(max_matrix_5, mode='reflect') # Too sensitive to noise&#xa;#gaussian_laplace = ndi.filters.gaussian_laplace(max_matrix_5, sigma=1.0, mode='reflect')&#xa;#gaussian_gradient_magnitude = ndi.filters.gaussian_gradient_magnitude(max_matrix_5, sigma=1.0, mode='reflect')&#xa;&#xa;max_matrix_5_cv = (max_matrix_5/(4096.0/255.0)).astype(np.uint8) # Scale [0..255], convert to CV_8U&#xa;canny = cv2.Canny(max_matrix_5_cv, 10, 20) # Hysteresis Thresholding: &#xa;canny = canny.astype(np.float64) * (sobel.max()/255.0) # Scale to comparable scale&#xa;&#xa;&#xa;#---------------------------------&#xa;# Simple step detection algorithm&#xa;#---------------------------------&#xa;# Find all non-zero sequences&#xa;# Throw small sequencs away. Actual grasps are remaining&#xa;# For more elaborated methods: http://en.wikipedia.org/wiki/Step_detection&#xa;&#xa;thresh_sequence = 10 # Minimum length of a sequence to be considered a ""grasp""&#xa;grasp_begin = []&#xa;grasp_end = []&#xa;for start, stop in contiguous_regions(max_matrix_5 != 0):&#xa;    if (stop-start) > thresh_sequence:&#xa;        grasp_begin.append([start, max_matrix_5[start]])&#xa;        grasp_end.append([stop-1, max_matrix_5[stop-1]])&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;############&#xa;# Plotting&#xa;############&#xa;text_width = 6.30045 # LaTeX text width in inches&#xa;golden_ratio = (1 + np.sqrt(5) ) / 2.0&#xa;&#xa;size_factor = 1.0&#xa;figure_width = size_factor*text_width&#xa;#figure_height = (figure_width / golden_ratio)&#xa;figure_height = 1.3 * figure_width&#xa;figure_size = [figure_width, figure_height]&#xa;config.load_config_large()&#xa;&#xa;&#xa;&#xa;&#xa;&#xa;fig = plt.figure(figsize=figure_size)&#xa;&#xa;# Axis 1&#xa;ax1 = fig.add_subplot(2,1,1)&#xa;&#xa;ax1.plot(max_matrix_5, ""-"", label=""Max Matrix 5"")&#xa;#ax1.plot(filtered_matrix_5, ""-"", marker=""x"", markersize=4, label=""Median Matrix 5"")&#xa;&#xa;ax1.plot([p[0] for p in grasp_begin], [p[1] for p in grasp_begin], ""o"", markersize=8, color=""green"", label=""Grasp begin"")&#xa;ax1.plot([p[0] for p in grasp_end], [p[1] for p in grasp_end], ""o"", markersize=8, color=""red"", label=""Grasp end"")&#xa;&#xa;&#xa;#ax1.set_xlim([xmin,xmax])&#xa;ax1.set_ylim([0, 1.2*np.max(max_matrix_5)])&#xa;ax1.set_xlabel(""# Frames"")&#xa;ax1.set_ylabel(""Raw Sensor Value"", rotation=90)&#xa;ax1.set_title(""Step detection by finding long non-zero sequences in tactile sensor readings"", y=1.10)&#xa;&#xa;# Second axis for time&#xa;ax1_time = ax1.twiny()&#xa;dummy = ax1_time.plot(timestamps, np.ones([timestamps.size]))&#xa;dummy.pop(0).remove()&#xa;ax1_time.set_xlabel(""Time [s]"")&#xa;ax1.legend(loc = 'upper left')&#xa;&#xa;&#xa;&#xa;&#xa;# Axis 2&#xa;ax2 = fig.add_subplot(2,1,2, sharex=ax1)&#xa;&#xa;ax2.plot(sobel, label=""Sobel"")&#xa;#ax2.plot(laplace, label=""Laplace"")&#xa;#ax2.plot(gaussian_laplace, label=""Gaussian laplace"")&#xa;#ax2.plot(gaussian_gradient_magnitude, label=""Gaussian gradient magnitude"")&#xa;ax2.plot(canny, label=""Canny"")&#xa;&#xa;ax2.set_xlabel(""# Frames"")&#xa;ax2.set_ylabel(""Filtered"", rotation=90)&#xa;ax2.legend(loc = 'lower left')&#xa;&#xa;&#xa;fig.tight_layout()&#xa;#plt.show() &#xa;&#xa;&#xa;plotname = ""step_detection_tactile_sensors""&#xa;fig.savefig(plotname+"".pdf"", pad_inches=0, dpi=fig.dpi) # pdf&#xa;#fig.savefig(plotname+"".pgf"", pad_inches=0, dpi=fig.dpi) # pgf&#xa;plt.close()&#xa;"
29082268|"# Copyright 2017 Battelle Energy Alliance, LLC&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;# http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;""""""&#xa;Created on Mar 5, 2013&#xa;&#xa;@author: alfoa, cogljj, crisr&#xa;""""""&#xa;#for future compatibility with Python 3-----------------------------------------&#xa;from __future__ import division, print_function, unicode_literals, absolute_import&#xa;import warnings&#xa;warnings.simplefilter('default',DeprecationWarning)&#xa;if not 'xrange' in dir(__builtins__):&#xa;  xrange = range&#xa;#End compatibility block for Python 3-------------------------------------------&#xa;&#xa;#External Modules---------------------------------------------------------------&#xa;import time&#xa;import collections&#xa;import subprocess&#xa;&#xa;import os&#xa;import copy&#xa;import sys&#xa;import abc&#xa;import threading&#xa;import random&#xa;import socket&#xa;#External Modules End-----------------------------------------------------------&#xa;&#xa;#Internal Modules---------------------------------------------------------------&#xa;from utils import utils&#xa;from BaseClasses import BaseType&#xa;import MessageHandler&#xa;import Runners&#xa;import Models&#xa;# for internal parallel&#xa;if sys.version_info.major == 2:&#xa;  import pp&#xa;  import ppserver&#xa;else:&#xa;  print(""pp does not support python3"")&#xa;# end internal parallel module&#xa;#Internal Modules End-----------------------------------------------------------&#xa;&#xa;&#xa;## FIXME: Finished jobs can bog down the queue waiting for other objects to take&#xa;## them away. Can we shove them onto a different list and free up the job queue?&#xa;&#xa;class JobHandler(MessageHandler.MessageUser):&#xa;  """"""&#xa;    JobHandler class. This handles the execution of any job in the RAVEN&#xa;    framework&#xa;  """"""&#xa;  def __init__(self):&#xa;    """"""&#xa;      Init method&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    self.printTag         = 'Job Handler'&#xa;    self.runInfoDict      = {}&#xa;&#xa;    self.isParallelPythonInitialized = False&#xa;&#xa;    self.sleepTime  = 0.005&#xa;    self.completed = False&#xa;&#xa;    ## Determines whether to collect and print job timing summaries at the end of job runs.&#xa;    self.__profileJobs = False&#xa;&#xa;    ## Prevents the pending queue from growing indefinitely, but also allowing&#xa;    ## extra jobs to be queued to prevent starving parallelized environments of&#xa;    ## jobs.&#xa;    self.maxQueueSize = None&#xa;&#xa;    ############################################################################&#xa;    ## The following variables are protected by the __queueLock&#xa;&#xa;    ## Placeholders for each actively running job. When a job finishes, its&#xa;    ## spot in one of these lists will be reset to None and the next Runner will&#xa;    ## be placed in a free None spot, and set to start&#xa;    self.__running       = []&#xa;    self.__clientRunning = []&#xa;&#xa;    ## Queue of jobs to be run, when something on the list above opens up, the&#xa;    ## corresponding queue will pop a job (Runner) and put it into that location&#xa;    ## and set it to start&#xa;    self.__queue       = collections.deque()&#xa;    self.__clientQueue = collections.deque()&#xa;&#xa;    ## A counter used for uniquely identifying the next id for an ExternalRunner&#xa;    ## InternalRunners will increment this counter, but do not use it currently&#xa;    self.__nextId = 0&#xa;&#xa;    ## List of finished jobs. When a job finishes, it is placed here until&#xa;    ## something from the main thread can remove them.&#xa;    self.__finished = []&#xa;&#xa;    ## End block of __queueLock protected variables&#xa;    ############################################################################&#xa;&#xa;    self.__queueLock = threading.RLock()&#xa;&#xa;    ## List of submitted job identifiers, includes jobs that have completed as&#xa;    ## this list is not cleared until a new step is entered&#xa;    self.__submittedJobs = []&#xa;    ## Dict of failed jobs of the form { identifer: metadata }&#xa;    self.__failedJobs = {}&#xa;&#xa;    #self.__noResourcesJobs = []&#xa;&#xa;  def initialize(self, runInfoDict, messageHandler):&#xa;    """"""&#xa;      Method to initialize the JobHandler&#xa;      @ In, runInfoDict, dict, dictionary of run info settings&#xa;      @ In, messageHandler, MessageHandler object, instance of the global RAVEN&#xa;        message handler&#xa;      @ Out, None&#xa;    """"""&#xa;    self.runInfoDict = runInfoDict&#xa;    self.messageHandler = messageHandler&#xa;    # set the maximum queue size (number of jobs to queue past the running number)&#xa;    self.maxQueueSize = runInfoDict['maxQueueSize']&#xa;    # defaults to None; if None, then use batchSize instead&#xa;    if self.maxQueueSize is None:&#xa;      self.maxQueueSize = runInfoDict['batchSize']&#xa;    # if requsted max size less than 1, we can't do that, so take 1 instead&#xa;    if self.maxQueueSize < 1:&#xa;      self.raiseAWarning('maxQueueSize was set to be less than 1!  Setting to 1...')&#xa;      self.maxQueueSize = 1&#xa;    self.raiseADebug('Setting maxQueueSize to',self.maxQueueSize)&#xa;&#xa;    #initialize PBS&#xa;    with self.__queueLock:&#xa;      self.__running       = [None]*self.runInfoDict['batchSize']&#xa;      self.__clientRunning = [None]*self.runInfoDict['batchSize']&#xa;&#xa;  def __checkAndRemoveFinished(self, running):&#xa;    """"""&#xa;      Method to check if a run is finished and remove it from the queque&#xa;      @ In, running, instance, the job instance (InternalRunner or ExternalRunner)&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      returnCode = running.getReturnCode()&#xa;      if returnCode != 0:&#xa;        metadataFailedRun = running.getMetadata()&#xa;        metadataToKeep = metadataFailedRun&#xa;        if metadataFailedRun is not None:&#xa;          metadataKeys      = metadataFailedRun.keys()&#xa;          if 'jobHandler' in metadataKeys:&#xa;            metadataKeys.pop(metadataKeys.index(""jobHandler""))&#xa;            metadataToKeep = { keepKey: metadataFailedRun[keepKey] for keepKey in metadataKeys }&#xa;        ## FIXME: The running.command was always internal now, so I removed it.&#xa;        ## We should probably find a way to give more pertinent information.&#xa;        self.raiseAMessage("" Process Failed "" + str(running) + "" internal returnCode "" + str(returnCode))&#xa;        self.__failedJobs[running.identifier]=(returnCode,copy.deepcopy(metadataToKeep))&#xa;&#xa;  def __initializeParallelPython(self):&#xa;    """"""&#xa;      Internal method that is aimed to initialize the internal parallel system.&#xa;      It initilizes the paralle python implementation (with socketing system) in&#xa;      case RAVEN is run in a cluster with multiple nodes or the NumMPI > 1,&#xa;      otherwise multi-threading is used.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    ## Check if the list of unique nodes is present and, in case, initialize the&#xa;    ## socket&#xa;    if self.runInfoDict['internalParallel']:&#xa;      if len(self.runInfoDict['Nodes']) > 0:&#xa;        availableNodes = [nodeId.strip() for nodeId in self.runInfoDict['Nodes']]&#xa;&#xa;        ## Set the initial port randomly among the user accessible ones&#xa;        ## Is there any problem if we select the same port as something else?&#xa;        randomPort = random.randint(1024,65535)&#xa;&#xa;        ## Get localHost and servers&#xa;        localHostName, ppservers = self.__runRemoteListeningSockets(randomPort)&#xa;        self.raiseADebug(""Local host is ""+ localHostName)&#xa;&#xa;        if len(ppservers) == 0:&#xa;          ## We are on a single node&#xa;          self.ppserver = pp.Server(ncpus=len(availableNodes))&#xa;        else:&#xa;          ## We are using multiple nodes&#xa;          self.raiseADebug(""Servers found are "" + ','.join(ppservers))&#xa;          self.raiseADebug(""Server port in use is "" + str(randomPort))&#xa;          self.ppserver = pp.Server(ncpus=0, ppservers=tuple(ppservers))&#xa;      else:&#xa;         ## We are using the parallel python system&#xa;        self.ppserver = pp.Server(ncpus=int(self.runInfoDict['totalNumCoresUsed']))&#xa;    else:&#xa;      ## We are just using threading&#xa;      self.ppserver = None&#xa;&#xa;    self.isParallelPythonInitialized = True&#xa;&#xa;  def __getLocalAndRemoteMachineNames(self):&#xa;    """"""&#xa;      Method to get the qualified host and remote nodes' names&#xa;      @ In, None&#xa;      @ Out, hostNameMapping, dict, dictionary containing the qualified names&#xa;        {'local':hostName,'remote':{nodeName1:IP1,nodeName2:IP2,etc}}&#xa;    """"""&#xa;    hostNameMapping = {'local':"""",'remote':{}}&#xa;&#xa;    ## Store the local machine name as its fully-qualified domain name (FQDN)&#xa;    hostNameMapping['local'] = str(socket.getfqdn()).strip()&#xa;    self.raiseADebug(""Local Host is "" + hostNameMapping['local'])&#xa;&#xa;    ## collect the qualified hostnames for each remote node&#xa;    for nodeId in list(set(self.runInfoDict['Nodes'])):&#xa;      hostNameMapping['remote'][nodeId.strip()] = socket.gethostbyname(nodeId.strip())&#xa;      self.raiseADebug(""Remote Host identified "" + hostNameMapping['remote'][nodeId.strip()])&#xa;&#xa;    return hostNameMapping&#xa;&#xa;  def __runRemoteListeningSockets(self,newPort):&#xa;    """"""&#xa;      Method to activate the remote sockets for parallel python&#xa;      @ In, newPort, integer, the comunication port to use&#xa;      @ Out, (qualifiedHostName, ppservers), tuple, tuple containining:&#xa;             - in position 0 the host name and&#xa;             - in position 1 the list containing the nodes in which the remote&#xa;               sockets have been activated&#xa;    """"""&#xa;    ## Get the local machine name and the remote nodes one&#xa;    hostNameMapping = self.__getLocalAndRemoteMachineNames()&#xa;    qualifiedHostName =  hostNameMapping['local']&#xa;    remoteNodesIP = hostNameMapping['remote']&#xa;&#xa;    ## Strip out the nodes' names&#xa;    availableNodes = [node.strip() for node in self.runInfoDict['Nodes']]&#xa;&#xa;    ## Get unique nodes&#xa;    uniqueNodes    = list(set(availableNodes))&#xa;    ppservers      = []&#xa;&#xa;    if len(uniqueNodes) > 1:&#xa;      ## There are remote nodes that need to be activated&#xa;&#xa;      ## Locate the ppserver script to be executed&#xa;      ppserverScript = os.path.join(self.runInfoDict['FrameworkDir'],""contrib"",""pp"",""ppserver.py"")&#xa;&#xa;      ## Modify the python path used by the local environment&#xa;      localenv = os.environ.copy()&#xa;      pathSeparator = os.pathsep&#xa;      localenv[""PYTHONPATH""] = pathSeparator.join(sys.path)&#xa;&#xa;      for nodeId in uniqueNodes:&#xa;        ## Build the filename&#xa;        outFileName = nodeId.strip()+""_port:""+str(newPort)+""_server_out.log""&#xa;        outFileName = os.path.join(self.runInfoDict['WorkingDir'], outFileName)&#xa;&#xa;        outFile = open(outFileName, 'w')&#xa;&#xa;        ## Check how many processors are available in the node&#xa;        ntasks = availableNodes.count(nodeId)&#xa;        remoteHostName =  remoteNodesIP[nodeId]&#xa;&#xa;        ## Activate the remote socketing system&#xa;&#xa;        ## Next line is a direct execute of a ppserver:&#xa;        #subprocess.Popen(['ssh', nodeId, ""python2.7"", ppserverScript,""-w"",str(ntasks),""-i"",remoteHostName,""-p"",str(newPort),""-t"",""1000"",""-g"",localenv[""PYTHONPATH""],""-d""],shell=False,stdout=outFile,stderr=outFile,env=localenv)&#xa;&#xa;        ## Instead, let's build the command and then call the os-agnostic version&#xa;        command="" "".join([""python"",ppserverScript,""-w"",str(ntasks),""-i"",remoteHostName,""-p"",str(newPort),""-t"",""50000"",""-g"",localenv[""PYTHONPATH""],""-d""])&#xa;        utils.pickleSafeSubprocessPopen(['ssh',nodeId,""COMMAND='""+command+""'"",self.runInfoDict['RemoteRunCommand']],shell=False,stdout=outFile,stderr=outFile,env=localenv)&#xa;        ## e.g., ssh nodeId COMMAND='python ppserverScript -w stuff'&#xa;&#xa;        ## update list of servers&#xa;        ppservers.append(nodeId+"":""+str(newPort))&#xa;&#xa;    return qualifiedHostName, ppservers&#xa;&#xa;  def startLoop(self):&#xa;    """"""&#xa;    This function begins the polling loop for the JobHandler where it will&#xa;    constantly fill up its running queue with jobs in its pending queue and&#xa;    unload finished jobs into its finished queue to be extracted by&#xa;    """"""&#xa;    while not self.completed:&#xa;      self.fillJobQueue()&#xa;      self.cleanJobQueue()&#xa;      ## TODO May want to revisit this:&#xa;      ## http://stackoverflow.com/questions/29082268/python-time-sleep-vs-event-wait&#xa;      ## probably when we move to Python 3.&#xa;      time.sleep(self.sleepTime)&#xa;&#xa;  def addJob(self, args, functionToRun, identifier, metadata=None, modulesToImport = [], forceUseThreads = False, uniqueHandler=""any"", clientQueue = False):&#xa;    """"""&#xa;      Method to add an internal run (function execution)&#xa;      @ In, args, dict, this is a list of arguments that will be passed as&#xa;        function parameters into whatever method is stored in functionToRun.&#xa;        e.g., functionToRun(*args)&#xa;      @ In, functionToRun,function or method, the function that needs to be&#xa;        executed&#xa;      @ In, identifier, string, the job identifier&#xa;      @ In, metadata, dict, optional, dictionary of metadata associated to this&#xa;        run&#xa;      @ In, modulesToImport, list, optional, list of modules that need to be&#xa;        imported for internal parallelization (parallel python). This list&#xa;        should be generated with the method returnImportModuleString in utils.py&#xa;      @ In, forceUseThreads, bool, optional, flag that, if True, is going to&#xa;        force the usage of multi-threading even if parallel python is activated&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        this runner. For example, if present, to retrieve this runner using the&#xa;        method jobHandler.getFinished, the uniqueHandler needs to be provided.&#xa;        If uniqueHandler == 'any', every ""client"" can get this runner&#xa;      @ In, clientQueue, boolean, optional, if this run needs to be added in the&#xa;        clientQueue&#xa;      @ Out, None&#xa;    """"""&#xa;    ## internal server is initialized only in case an internal calc is requested&#xa;    if not self.isParallelPythonInitialized:&#xa;      self.__initializeParallelPython()&#xa;&#xa;    if self.ppserver is None or forceUseThreads:&#xa;      internalJob = Runners.SharedMemoryRunner(self.messageHandler, args,&#xa;                                               functionToRun,&#xa;                                               identifier, metadata,&#xa;                                               uniqueHandler,&#xa;                                               profile=self.__profileJobs)&#xa;    else:&#xa;      skipFunctions = [utils.metaclass_insert(abc.ABCMeta,BaseType)]&#xa;      internalJob = Runners.DistributedMemoryRunner(self.messageHandler,&#xa;                                                    self.ppserver, args,&#xa;                                                    functionToRun,&#xa;                                                    modulesToImport, identifier,&#xa;                                                    metadata, skipFunctions,&#xa;                                                    uniqueHandler,&#xa;                                                    profile=self.__profileJobs)&#xa;&#xa;    # set the client info&#xa;    internalJob.clientRunner = clientQueue&#xa;    # add the runner in the Queue&#xa;    self.reAddJob(internalJob)&#xa;&#xa;  def reAddJob(self, runner):&#xa;    """"""&#xa;      Method to add a runner object in the queue&#xa;      @ In, runner, Runner Instance, this is the instance of the runner that we want to readd in the queque&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      if not runner.clientRunner:&#xa;        self.__queue.append(runner)&#xa;      else:&#xa;        self.__clientQueue.append(runner)&#xa;      if self.__profileJobs:&#xa;        runner.trackTime('queue')&#xa;      self.__submittedJobs.append(runner.identifier)&#xa;&#xa;  def addClientJob(self, args, functionToRun, identifier, metadata=None, modulesToImport = [], uniqueHandler=""any""):&#xa;    """"""&#xa;      Method to add an internal run (function execution), without consuming&#xa;      resources (free spots). This can be used for client handling (see&#xa;      metamodel)&#xa;      @ In, args, dict, this is a list of arguments that will be passed as&#xa;        function parameters into whatever method is stored in functionToRun.&#xa;        e.g., functionToRun(*args)&#xa;      @ In, functionToRun,function or method, the function that needs to be&#xa;        executed&#xa;      @ In, identifier, string, the job identifier&#xa;      @ In, metadata, dict, optional, dictionary of metadata associated to this&#xa;        run&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        this runner. For example, if present, to retrieve this runner using the&#xa;        method jobHandler.getFinished, the uniqueHandler needs to be provided.&#xa;        If uniqueHandler == 'any', every ""client"" can get this runner.&#xa;      @ Out, None&#xa;    """"""&#xa;    self.addJob(args, functionToRun, identifier, metadata, modulesToImport,&#xa;                forceUseThreads = True, uniqueHandler = uniqueHandler,&#xa;                clientQueue = True)&#xa;&#xa;  def isFinished(self):&#xa;    """"""&#xa;      Method to check if all the runs in the queue are finished&#xa;      @ In, None&#xa;      @ Out, isFinished, bool, True all the runs in the queue are finished&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      ## If there is still something left in the queue, we are not done yet.&#xa;      if len(self.__queue) > 0 or len(self.__clientQueue) > 0:&#xa;        return False&#xa;&#xa;      ## Otherwise, let's look at our running lists and see if there is a job&#xa;      ## that is not done.&#xa;      for run in self.__running+self.__clientRunning:&#xa;        if run:&#xa;          return False&#xa;&#xa;    ## Are there runs that need to be claimed? If so, then I cannot say I am&#xa;    ## done.&#xa;    if len(self.getFinishedNoPop()) > 0:&#xa;      return False&#xa;&#xa;    return True&#xa;&#xa;  def availability(self, client=False):&#xa;    """"""&#xa;      Returns the number of runs that can be added until we consider our queue&#xa;      saturated&#xa;      @ In, client, bool, if true, then return the values for the&#xa;      __clientQueue, otherwise use __queue&#xa;      @ Out, availability, int the number of runs that can be added until we&#xa;      reach saturation&#xa;    """"""&#xa;    ## Due to possibility of memory explosion, we should include the finished&#xa;    ## queue when considering whether we should add a new job. There was an&#xa;    ## issue when running on a distributed system where we saw that this list&#xa;    ## seemed to be growing indefinitely as the main thread was unable to clear&#xa;    ## that list within a reasonable amount of time. The issue on the main thread&#xa;    ## should also be addressed, but at least we can prevent it on this end since&#xa;    ## the main thread's issue may be legitimate.&#xa;&#xa;    maxCount = self.maxQueueSize&#xa;    finishedCount = len(self.__finished)&#xa;&#xa;    if client:&#xa;      if maxCount is None:&#xa;        maxCount = self.__clientRunning.count(None)&#xa;      queueCount = len(self.__clientQueue)&#xa;    else:&#xa;      if maxCount is None:&#xa;        maxCount = self.__running.count(None)&#xa;      queueCount = len(self.__queue)&#xa;&#xa;    availability = maxCount - queueCount - finishedCount&#xa;    return availability&#xa;&#xa;  def isThisJobFinished(self, identifier):&#xa;    """"""&#xa;      Method to check if the run identified by ""identifier"" is finished&#xa;      @ In, identifier, string, identifier&#xa;      @ Out, isFinished, bool, True if the job identified by ""identifier"" is&#xa;        finished&#xa;    """"""&#xa;    identifier = identifier.strip()&#xa;    with self.__queueLock:&#xa;      ## Look through the finished jobs and attempt to find a matching&#xa;      ## identifier. If the job exists here, it is finished&#xa;      for run in self.__finished:&#xa;        if run.identifier == identifier:&#xa;          return True&#xa;&#xa;      ## Look through the pending jobs and attempt to find a matching identifier&#xa;      ## If the job exists here, it is not finished&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        for run in queue:&#xa;          if run.identifier == identifier:&#xa;            return False&#xa;&#xa;      ## Look through the running jobs and attempt to find a matching identifier&#xa;      ## If the job exists here, it is not finished&#xa;      for run in self.__running+self.__clientRunning:&#xa;        if run is not None and run.identifier == identifier:&#xa;          return False&#xa;&#xa;    ##  If you made it here and we still have not found anything, we have got&#xa;    ## problems.&#xa;    self.raiseAnError(RuntimeError,""Job ""+identifier+"" is unknown!"")&#xa;&#xa;  def areTheseJobsFinished(self, uniqueHandler=""any""):&#xa;    """"""&#xa;      Method to check if all the runs in the queue are finished&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        each runner. If provided, just the jobs that have the uniqueIdentifier&#xa;        will be retrieved. By default uniqueHandler = 'any' => all the jobs for&#xa;        which no uniqueIdentifier has been set up are going to be retrieved&#xa;      @ Out, isFinished, bool, True all the runs in the queue are finished&#xa;    """"""&#xa;    uniqueHandler = uniqueHandler.strip()&#xa;    with self.__queueLock:&#xa;      for run in self.__finished:&#xa;        if run.uniqueHandler == uniqueHandler:&#xa;          return False&#xa;&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        for run in queue:&#xa;          if run.uniqueHandler == uniqueHandler:&#xa;            return False&#xa;&#xa;      for run in self.__running + self.__clientRunning:&#xa;        if run is not None and run.uniqueHandler == uniqueHandler:&#xa;          return False&#xa;&#xa;    self.raiseADebug(""The jobs with uniqueHandler "", uniqueHandler, ""are finished"")&#xa;&#xa;    return True&#xa;&#xa;  def getFailedJobs(self):&#xa;    """"""&#xa;      Method to get list of failed jobs&#xa;      @ In, None&#xa;      @ Out, __failedJobs, list, list of the identifiers (jobs) that failed&#xa;    """"""&#xa;    return self.__failedJobs&#xa;&#xa;  def getFinished(self, removeFinished=True, jobIdentifier = '', uniqueHandler = ""any""):&#xa;    """"""&#xa;      Method to get the list of jobs that ended (list of objects)&#xa;      @ In, removeFinished, bool, optional, flag to control if the finished jobs&#xa;        need to be removed from the queue&#xa;      @ In, jobIdentifier, string, optional, if specified, only collects&#xa;        finished runs that start with this text. If not specified collect all.&#xa;      @ In, uniqueHandler, string, optional, it is a special keyword attached to&#xa;        each runner. If provided, just the jobs that have the uniqueIdentifier&#xa;        will be retrieved. By default uniqueHandler = 'any' => all the jobs for&#xa;        which no uniqueIdentifier has been set up are going to be retrieved&#xa;      @ Out, finished, list, list of finished jobs (InternalRunner or&#xa;        ExternalRunner objects) (if jobIdentifier is None), else the finished&#xa;        jobs matching the base case jobIdentifier&#xa;    """"""&#xa;    finished = []&#xa;&#xa;    ## If the user does not specify a jobIdentifier, then set it to the empty&#xa;    ## string because every job will match this starting string.&#xa;    if jobIdentifier is None:&#xa;      jobIdentifier = ''&#xa;&#xa;    with self.__queueLock:&#xa;      runsToBeRemoved = []&#xa;      for i,run in enumerate(self.__finished):&#xa;        ## If the jobIdentifier does not match or the uniqueHandler does not&#xa;        ## match, then don't bother trying to do anything with it&#xa;        if not run.identifier.startswith(jobIdentifier) \&#xa;        or uniqueHandler != run.uniqueHandler:&#xa;          continue&#xa;&#xa;        finished.append(run)&#xa;        if removeFinished:&#xa;          runsToBeRemoved.append(i)&#xa;          self.__checkAndRemoveFinished(run)&#xa;&#xa;      ##Since these indices are sorted, reverse them to ensure that when we&#xa;      ## delete something it will not shift anything to the left (lower index)&#xa;      ## than it.&#xa;      for i in reversed(runsToBeRemoved):&#xa;        self.__finished[i].trackTime('collected')&#xa;        del self.__finished[i]&#xa;    ## end with self.__queueLock&#xa;&#xa;    return finished&#xa;&#xa;  def getFinishedNoPop(self):&#xa;    """"""&#xa;      Method to get the list of jobs that ended (list of objects) without&#xa;      removing them from the queue&#xa;      @ In, None&#xa;      @ Out, finished, list, list of finished jobs (InternalRunner or&#xa;        ExternalRunner objects)&#xa;    """"""&#xa;    finished = self.getFinished(False)&#xa;    return finished&#xa;&#xa;  ## Deprecating this function because I don't think it is doing the right thing&#xa;  ## People using the job handler should be asking for what is available not the&#xa;  ## number of free spots in the running block. Only the job handler should be&#xa;  ## able to internally alter or query the running and clientRunning queues.&#xa;  ## The outside environment can only access the queue and clientQueue variables.&#xa;  # def numFreeSpots(self, client=False):&#xa;&#xa;  def numRunning(self):&#xa;    """"""&#xa;      Returns the number of runs currently running.&#xa;      @ In, None&#xa;      @ Out, activeRuns, int, number of active runs&#xa;    """"""&#xa;    #with self.__queueLock:&#xa;    ## The size of the list does not change, only its contents, so I don't&#xa;    ## think there should be any conflict if we are reading a variable from&#xa;    ## one thread and updating it on the other thread.&#xa;    activeRuns = sum(run is not None for run in self.__running)&#xa;    return activeRuns&#xa;&#xa;  def numSubmitted(self):&#xa;    """"""&#xa;      Method to get the number of submitted jobs&#xa;      @ In, None&#xa;      @ Out, len(self.__submittedJobs), int, number of submitted jobs&#xa;    """"""&#xa;    return len(self.__submittedJobs)&#xa;&#xa;  def fillJobQueue(self):&#xa;    """"""&#xa;      Method to start running the jobs in queue.  If there are empty slots&#xa;      takes jobs out of the queue and starts running them.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;&#xa;    ## Only the jobHandler's startLoop thread should have write access to the&#xa;    ## self.__running variable, so we should be able to safely query this outside&#xa;    ## of the lock given that this function is called only on that thread as well.&#xa;    emptySlots = [i for i,run in enumerate(self.__running) if run is None]&#xa;&#xa;    ## Don't bother acquiring the lock if there are no empty spots or nothing&#xa;    ## in the queue (this could be simultaneously added to by the main thread,&#xa;    ## but I will be back here after a short wait on this thread so I am not&#xa;    ## concerned about this potential inconsistency)&#xa;    if len(emptySlots) > 0 and len(self.__queue) > 0:&#xa;      with self.__queueLock:&#xa;        for i in emptySlots:&#xa;          ## The queue could be emptied during this loop, so we will to break&#xa;          ## out as soon as that happens so we don't hog the lock.&#xa;          if len(self.__queue) > 0:&#xa;            item = self.__queue.popleft()&#xa;&#xa;            ## Okay, this is a little tricky, but hang with me here. Whenever&#xa;            ## a code model is run, we need to replace some of its command&#xa;            ## parameters. The way we do this is by looking at the job instance&#xa;            ## and checking if the first argument (the self in&#xa;            ## self.evaluateSample) is an instance of Code, if so, then we need&#xa;            ## to replace the execution command. Is this fragile? Possibly. We may&#xa;            ## want to revisit this on the next iteration of this code.&#xa;            if len(item.args) > 0 and isinstance(item.args[0], Models.Code):&#xa;              kwargs = {}&#xa;              kwargs['INDEX'] = str(i)&#xa;              kwargs['INDEX1'] = str(i+i)&#xa;              kwargs['CURRENT_ID'] = str(self.__nextId)&#xa;              kwargs['CURRENT_ID1'] = str(self.__nextId+1)&#xa;              kwargs['SCRIPT_DIR'] = self.runInfoDict['ScriptDir']&#xa;              kwargs['FRAMEWORK_DIR'] = self.runInfoDict['FrameworkDir']&#xa;              ## This will not be used since the Code will create a new&#xa;              ## directory for its specific files and will spawn a process there&#xa;              ## so we will let the Code fill that in. Note, the line below&#xa;              ## represents the WRONG directory for an instance of a code!&#xa;              ## It is however the correct directory for a MultiRun step&#xa;              ## -- DPM 5/4/17&#xa;              kwargs['WORKING_DIR'] = item.args[0].workingDir&#xa;              kwargs['BASE_WORKING_DIR'] = self.runInfoDict['WorkingDir']&#xa;              kwargs['METHOD'] = os.environ.get(""METHOD"",""opt"")&#xa;              kwargs['NUM_CPUS'] = str(self.runInfoDict['NumThreads'])&#xa;              item.args[3].update(kwargs)&#xa;&#xa;            self.__running[i] = item&#xa;            self.__running[i].start()&#xa;            self.__running[i].trackTime('started')&#xa;            self.__nextId += 1&#xa;          else:&#xa;            break&#xa;&#xa;    ## Repeat the same process above, only for the clientQueue&#xa;    emptySlots = [i for i,run in enumerate(self.__clientRunning) if run is None]&#xa;    if len(emptySlots) > 0 and len(self.__clientQueue) > 0:&#xa;      with self.__queueLock:&#xa;        for i in emptySlots:&#xa;          if len(self.__clientQueue) > 0:&#xa;            self.__clientRunning[i] = self.__clientQueue.popleft()&#xa;            self.__clientRunning[i].start()&#xa;            self.__clientRunning[i].trackTime('jobHandler_started')&#xa;            self.__nextId += 1&#xa;          else:&#xa;            break&#xa;&#xa;  def cleanJobQueue(self):&#xa;    """"""&#xa;    Method that will remove finished jobs from the queue and place them into the&#xa;    finished queue to be read by some other thread.&#xa;    @ In, None&#xa;    @ Out, None&#xa;    """"""&#xa;    ## The code handling these two lists was the exact same, I have taken the&#xa;    ## liberty of condensing these loops into one and removing some of the&#xa;    ## redundant checks to make this code a bit simpler.&#xa;    for runList in [self.__running, self.__clientRunning]:&#xa;      for i,run in enumerate(runList):&#xa;        if run is not None and run.isDone():&#xa;          ## We should only need the lock if we are touching the finished queue&#xa;          ## which is cleared by the main thread. Again, the running queues&#xa;          ## should not be modified by the main thread, however they may inquire&#xa;          ## it by calling numRunning.&#xa;          with self.__queueLock:&#xa;            self.__finished.append(run)&#xa;            self.__finished[-1].trackTime('jobHandler_finished')&#xa;            runList[i] = None&#xa;&#xa;  def setProfileJobs(self,profile=False):&#xa;    """"""&#xa;      Sets whether profiles for jobs are printed or not.&#xa;      @ In, profile, bool, optional, if True then print timings for jobs when they are garbage collected&#xa;      @ Out, None&#xa;    """"""&#xa;    self.__profileJobs = profile&#xa;&#xa;  def startingNewStep(self):&#xa;    """"""&#xa;      Method to reset the __submittedJobs to an empty list.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      self.__submittedJobs = []&#xa;&#xa;  def shutdown(self):&#xa;    """"""&#xa;    This function will mark the job handler as done, so it can shutdown its&#xa;    polling thread.&#xa;    @ In, None&#xa;    @ Out, None&#xa;    """"""&#xa;    self.completed = True&#xa;&#xa;  def terminateAll(self):&#xa;    """"""&#xa;      Method to clear out the queue by killing all running processes.&#xa;      @ In, None&#xa;      @ Out, None&#xa;    """"""&#xa;    with self.__queueLock:&#xa;      for queue in [self.__queue, self.__clientQueue]:&#xa;        queue.clear()&#xa;&#xa;      for runList in [self.__running, self.__clientRunning]:&#xa;        unfinishedRuns = [run for run in runList if run is not None]&#xa;        for run in unfinishedRuns:&#xa;          run.kill()&#xa;"
5599254|"# -*- coding: utf-8 -*-&#xa;&#xa;import os&#xa;import sys&#xa;import sphinx_bootstrap_theme&#xa;&#xa;# only needed for Autobahn|Python&#xa;sys.path.insert(0, os.path.abspath('./_extensions'))&#xa;sys.path.insert(0, os.path.abspath('..'))&#xa;&#xa;extensions = [&#xa;   'sphinx.ext.autodoc',&#xa;   'sphinx.ext.doctest',&#xa;   'sphinx.ext.intersphinx',&#xa;   'sphinx.ext.viewcode',&#xa;   'sphinx.ext.ifconfig',&#xa;   'sphinx.ext.todo',&#xa;   'sphinxcontrib.spelling',&#xa;   'txsphinx' # only needed for Autobahn|Python&#xa;]&#xa;&#xa;spelling_lang = 'en_US'&#xa;spelling_show_suggestions = False&#xa;spelling_word_list_filename = 'spelling_wordlist.txt'&#xa;&#xa;# Add any paths that contain templates here, relative to this directory.&#xa;templates_path = ['_templates']&#xa;&#xa;# The suffix of source filenames.&#xa;source_suffix = '.rst'&#xa;&#xa;# The master toctree document.&#xa;master_doc = 'index'&#xa;&#xa;# General information about the project.&#xa;project = u'AutobahnPython'&#xa;copyright = u'Tavendo GmbH'&#xa;&#xa;# The version info for the project you're documenting, acts as replacement for&#xa;# |version| and |release|, also used in various other places throughout the&#xa;# built documents.&#xa;#&#xa;base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))&#xa;init = {}&#xa;with open(os.path.join(base_dir, ""autobahn"", ""__init__.py"")) as f:&#xa;   exec(f.read(), init)&#xa;&#xa;version = release = init[""__version__""]&#xa;&#xa;&#xa;# The language for content autogenerated by Sphinx. Refer to documentation&#xa;# for a list of supported languages.&#xa;#language = None&#xa;&#xa;# There are two options for replacing |today|: either, you set today to some&#xa;# non-false value, then it is used:&#xa;#today = ''&#xa;# Else, today_fmt is used as the format for a strftime call.&#xa;#today_fmt = '%B %d, %Y'&#xa;&#xa;# List of patterns, relative to source directory, that match files and&#xa;# directories to ignore when looking for source files.&#xa;exclude_patterns = ['_build', 'work']&#xa;&#xa;# The name of the Pygments (syntax highlighting) style to use.&#xa;pygments_style = 'sphinx'&#xa;&#xa;## Sphinx-Bootstrap Theme&#xa;##&#xa;## http://sphinx-bootstrap-theme.readthedocs.org/en/latest/README.html&#xa;##&#xa;html_theme = 'bootstrap'&#xa;html_theme_path = sphinx_bootstrap_theme.get_html_theme_path()&#xa;&#xa;html_theme_options = {&#xa;    # Navigation bar title. (Default: ``project`` value)&#xa;    'navbar_title': "" "",&#xa;&#xa;    # Tab name for entire site. (Default: ""Site"")&#xa;    'navbar_site_name': ""Site"",&#xa;&#xa;    # A list of tuples containing pages or urls to link to.&#xa;    # Valid tuples should be in the following forms:&#xa;    #    (name, page)                 # a link to a page&#xa;    #    (name, ""/aa/bb"", 1)          # a link to an arbitrary relative url&#xa;    #    (name, ""http://example.com"", True) # arbitrary absolute url&#xa;    # Note the ""1"" or ""True"" value above as the third argument to indicate&#xa;    # an arbitrary url.&#xa;    'navbar_links': [&#xa;        #(""Examples"", ""examples""),&#xa;        #(""Link"", ""http://example.com"", True),&#xa;    ],&#xa;&#xa;    # Render the next and previous page links in navbar. (Default: true)&#xa;    'navbar_sidebarrel': True,&#xa;&#xa;    # Render the current pages TOC in the navbar. (Default: true)&#xa;    'navbar_pagenav': True,&#xa;&#xa;    # Tab name for the current pages TOC. (Default: ""Page"")&#xa;    #'navbar_pagenav_name': ""Page"",&#xa;&#xa;    # Global TOC depth for ""site"" navbar tab. (Default: 1)&#xa;    # Switching to -1 shows all levels.&#xa;    'globaltoc_depth': 1,&#xa;&#xa;    # Include hidden TOCs in Site navbar?&#xa;    #&#xa;    # Note: If this is ""false"", you cannot have mixed ``:hidden:`` and&#xa;    # non-hidden ``toctree`` directives in the same page, or else the build&#xa;    # will break.&#xa;    #&#xa;    # Values: ""true"" (default) or ""false""&#xa;    'globaltoc_includehidden': ""true"",&#xa;&#xa;    # HTML navbar class (Default: ""navbar"") to attach to <div> element.&#xa;    # For black navbar, do ""navbar navbar-inverse""&#xa;    #'navbar_class': ""navbar navbar-inverse"",&#xa;    'navbar_class': ""navbar"",&#xa;&#xa;    # Fix navigation bar to top of page?&#xa;    # Values: ""true"" (default) or ""false""&#xa;    'navbar_fixed_top': ""true"",&#xa;&#xa;    # Location of link to source.&#xa;    # Options are ""nav"" (default), ""footer"" or anything else to exclude.&#xa;    'source_link_position': ""nav"",&#xa;&#xa;    # Bootswatch (http://bootswatch.com/) theme.&#xa;    #&#xa;    # Options are nothing with """" (default) or the name of a valid theme&#xa;    # such as ""amelia"" or ""cosmo"".&#xa;    'bootswatch_theme': """",&#xa;&#xa;    # Choose Bootstrap version.&#xa;    # Values: ""3"" (default) or ""2"" (in quotes)&#xa;    'bootstrap_version': ""3"",&#xa;}&#xa;&#xa;# Add any paths that contain custom themes here, relative to this directory.&#xa;#html_theme_path = []&#xa;&#xa;# The name for this set of Sphinx documents.  If None, it defaults to&#xa;# ""<project> v<release> documentation"".&#xa;#html_title = None&#xa;&#xa;# A shorter title for the navigation bar.  Default is the same as html_title.&#xa;#html_short_title = None&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top&#xa;# of the sidebar.&#xa;#html_logo = None&#xa;&#xa;# The name of an image file (within the static path) to use as favicon of the&#xa;# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32&#xa;# pixels large.&#xa;#html_favicon = None&#xa;&#xa;# Add any paths that contain custom static files (such as style sheets) here,&#xa;# relative to this directory. They are copied after the builtin static files,&#xa;# so a file named ""default.css"" will overwrite the builtin ""default.css"".&#xa;html_static_path = ['_static']&#xa;&#xa;&#xa;## additional variables which become accessible in RST (e.g. .. ifconfig:: not no_network)&#xa;##&#xa;def setup(app):&#xa;   app.add_config_value('no_network', False, True)&#xa;&#xa;no_network = None&#xa;&#xa;## additional variables which become accessible in the template engine's&#xa;## context for all pages&#xa;##&#xa;html_context = {&#xa;   #'widgeturl': 'https://demo.crossbar.io/clandeckwidget'&#xa;   #'widgeturl': 'http://127.0.0.1:8090/widget'&#xa;   'widgeturl': None,&#xa;   'no_network': False,&#xa;   #'cstatic': 'http://127.0.0.1:8888',&#xa;   'cstatic': '//tavendo-common-static.s3-eu-west-1.amazonaws.com',&#xa;}&#xa;&#xa;# (Optional) Logo. Should be small enough to fit the navbar (ideally 24x24).&#xa;# Path should be relative to the ``_static`` files directory.&#xa;html_logo = None&#xa;&#xa;&#xa;# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,&#xa;# using the given strftime format.&#xa;#html_last_updated_fmt = '%b %d, %Y'&#xa;&#xa;# If true, SmartyPants will be used to convert quotes and dashes to&#xa;# typographically correct entities.&#xa;#html_use_smartypants = True&#xa;&#xa;# Custom sidebar templates, maps document names to template names.&#xa;html_sidebars = {&#xa;   '**': [&#xa;      'side-primary.html'&#xa;   ]&#xa;}&#xa;&#xa;# Additional templates that should be rendered to pages, maps page names to&#xa;# template names.&#xa;#html_additional_pages = {}&#xa;&#xa;# If false, no module index is generated.&#xa;#html_domain_indices = True&#xa;&#xa;# If false, no index is generated.&#xa;#html_use_index = True&#xa;&#xa;# If true, the index is split into individual pages for each letter.&#xa;#html_split_index = False&#xa;&#xa;# If true, links to the reST sources are added to the pages.&#xa;#html_show_sourcelink = True&#xa;&#xa;# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.&#xa;#html_show_sphinx = True&#xa;&#xa;# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.&#xa;#html_show_copyright = True&#xa;&#xa;# If true, an OpenSearch description file will be output, and all pages will&#xa;# contain a <link> tag referring to it.  The value of this option must be the&#xa;# base URL from which the finished HTML is served.&#xa;#html_use_opensearch = ''&#xa;&#xa;# This is the file name suffix for HTML files (e.g. "".xhtml"").&#xa;#html_file_suffix = None&#xa;&#xa;# Output file base name for HTML help builder.&#xa;htmlhelp_basename = 'AutobahnPython'&#xa;&#xa;&#xa;# http://sphinx-doc.org/ext/intersphinx.html&#xa;intersphinx_mapping = {&#xa;   'py2': ('http://docs.python.org/2', None),&#xa;   'py3': ('http://docs.python.org/3', None),&#xa;   'six': ('https://pythonhosted.org/six/', None),&#xa;}&#xa;&#xa;rst_epilog = """"""&#xa;.. |ab| replace:: Autobahn&#xa;.. |Ab| replace:: **Autobahn**&#xa;.. |abL| replace:: Autobahn|Python&#xa;.. |AbL| replace:: **Autobahn**\|Python&#xa;.. _Autobahn: http://autobahn.ws&#xa;.. _AutobahnJS: http://autobahn.ws/js&#xa;.. _AutobahnPython: **Autobahn**\|Python&#xa;.. _WebSocket: http://tools.ietf.org/html/rfc6455&#xa;.. _RFC6455: http://tools.ietf.org/html/rfc6455&#xa;.. _WAMP: http://wamp.ws/&#xa;.. _Twisted: http://twistedmatrix.com/&#xa;.. _asyncio: http://docs.python.org/3.4/library/asyncio.html&#xa;.. _CPython: http://python.org/&#xa;.. _PyPy: http://pypy.org/&#xa;.. _Jython: http://jython.org/&#xa;.. _WAMP: http://wamp.ws/&#xa;.. _WAMPv1: http://wamp.ws/spec/wamp1/&#xa;.. _WAMPv2: https://github.com/tavendo/WAMP/blob/master/spec/README.md&#xa;.. _AutobahnTestsuite: http://autobahn.ws/testsuite&#xa;.. _trollius: https://pypi.python.org/pypi/trollius/&#xa;.. _tulip: https://pypi.python.org/pypi/asyncio/&#xa;""""""&#xa;&#xa;rst_prolog = """"""&#xa;""""""&#xa;&#xa;# http://stackoverflow.com/questions/5599254/how-to-use-sphinxs-autodoc-to-document-a-classs-init-self-method&#xa;autoclass_content = 'both'&#xa;&#xa;autodoc_member_order = 'bysource'&#xa;"
5209087|"from __future__ import absolute_import, print_function&#xa;&#xa;from collections import OrderedDict&#xa;from six.moves.urllib import request as urllib2&#xa;import io&#xa;import pandas as pd&#xa;from .. import charts&#xa;from . import help_messages as hm&#xa;&#xa;&#xa;def keep_source_input_sync(filepath, callback, start=0):&#xa;    """""" Monitor file at filepath checking for new lines (similar to&#xa;    tail -f) and calls callback on every new line found.&#xa;&#xa;    Args:&#xa;        filepath (str): path to the series data file (&#xa;            i.e.: /source/to/my/data.csv)&#xa;        callback (callable): function to be called with the a DataFrame&#xa;            created from the new lines found from file at filepath&#xa;            starting byte start&#xa;        start (int): specifies where to start reading from the file at&#xa;            filepath.&#xa;            Default: 0&#xa;&#xa;    Returns:&#xa;        DataFrame created from data read from filepath&#xa;    """"""&#xa;    if filepath is None:&#xa;        msg = ""No Input! Please specify --source_filename or --buffer t""&#xa;        raise IOError(msg)&#xa;&#xa;    if filepath.lower().startswith('http'):&#xa;        # Create a request for the given URL.&#xa;&#xa;        while True:&#xa;            request = urllib2.Request(filepath)&#xa;            data = get_data_from_url(request, start)&#xa;&#xa;            f = io.BytesIO(data)&#xa;            f.seek(start)&#xa;            line = f.readline()     # See note below&#xa;&#xa;            if not line:&#xa;                continue   # No data, try again&#xa;&#xa;            callback(line)&#xa;            start = len(data)&#xa;    else:&#xa;        f = open(filepath, 'r')&#xa;        f.seek(start)&#xa;        while True:&#xa;            line = f.readline()     # See note below&#xa;            if not line:&#xa;                continue   # No data, try again&#xa;            callback(line)&#xa;&#xa;    source = pd.read_csv(filepath)&#xa;    return source&#xa;&#xa;# Try to get the response. This will raise a urllib2.URLError if there is a&#xa;# problem (e.g., invalid URL).&#xa;# Reference:&#xa;# - http://stackoverflow.com/questions/5209087/python-seek-in-http-response-stream&#xa;# - http://stackoverflow.com/questions/1971240/python-seek-on-remote-file-using-http&#xa;def get_data_from_url(request, start=0, length=0):&#xa;    """""" Read from request after adding headers to retrieve data from byte&#xa;    specified in start.&#xa;&#xa;    request (urllib2.Request): request object related to the data to read&#xa;    start (int, optional): byte to start reading from.&#xa;        Default: 0&#xa;    length: length of the data range to read from start. If 0 it reads&#xa;        until the end of the stream.&#xa;        Default: 0&#xa;&#xa;    Returns:&#xa;        String read from request&#xa;    """"""&#xa;    # Add the header to specify the range to download.&#xa;    if start and length:&#xa;        request.add_header(""Range"", ""bytes=%d-%d"" % (start, start + length - 1))&#xa;    elif start:&#xa;        request.add_header(""Range"", ""bytes=%s-"" % start)&#xa;&#xa;    response = urllib2.urlopen(request)&#xa;    # If a content-range header is present, partial retrieval worked.&#xa;    if ""content-range"" in response.headers:&#xa;        print(""Partial retrieval successful."")&#xa;&#xa;        # The header contains the string 'bytes', followed by a space, then the&#xa;        # range in the format 'start-end', followed by a slash and then the total&#xa;        # size of the page (or an asterix if the total size is unknown). Lets get&#xa;        # the range and total size from this.&#xa;        _range, total = response.headers['content-range'].split(' ')[-1].split('/')&#xa;        # Print a message giving the range information.&#xa;        if total == '*':&#xa;            print(""Bytes %s of an unknown total were retrieved."" % _range)&#xa;        else:&#xa;            print(""Bytes %s of a total of %s were retrieved."" % (_range, total))&#xa;&#xa;    # # No header, so partial retrieval was unsuccessful.&#xa;    # else:&#xa;    #     print ""Unable to use partial retrieval.""&#xa;    data = response.read()&#xa;&#xa;    return data&#xa;&#xa;def parse_output_config(output):&#xa;    """"""Parse the output specification string and return the related chart&#xa;    output attribute.&#xa;&#xa;    Attr:&#xa;        output (str): String with the syntax convention specified for the&#xa;            cli output option is as follows: <output_type>://<type_arg>&#xa;            Valid values:&#xa;                output_type: file or server&#xa;                type_arg:&#xa;                    file_path if output_type is file&#xa;                    serve path if output_type is server&#xa;&#xa;    Returns:&#xa;        dictionary containing the output arguments to pass to a chart object&#xa;    """"""&#xa;    output_type, output_options = output.split('://')&#xa;&#xa;    if output_type == 'file':&#xa;        return {'filename': output_options}&#xa;&#xa;    elif output_type == 'server':&#xa;        # TODO: check if server configuration is as flexible as with plotting&#xa;        #       interface and add support for url/name if so.&#xa;        out_opt = output_options.split(""@"")&#xa;        attrnames = ['server', 'url', 'name']&#xa;&#xa;        # unpack server output parametrs in order to pass them to the plot&#xa;        # creation function&#xa;        kws = dict((attrn, val) for attrn, val in zip( attrnames, out_opt))&#xa;        return {'server': kws['server']}&#xa;&#xa;    else:&#xa;        msg = ""Unknown output type %s found. Please use: file|server""&#xa;        print (msg % output_type)&#xa;        return {}&#xa;&#xa;&#xa;def get_chart_params(title, output, show_legend=False):&#xa;    """"""Parse output type and output options and return related chart&#xa;    parameters. For example: returns filename if output_type is file&#xa;    or server it output_type is server&#xa;&#xa;    Args:&#xa;        title (str): the title of your plot.&#xa;        output (str): selected output. Follows the following convention:&#xa;            <output_type>://<type_arg> where output_type can be&#xa;            `file` (in that case type_arg specifies the file path) or&#xa;            `server` (in that case type_arg specify the server name).&#xa;&#xa;&#xa;    Returns:&#xa;        dictionary containing the arguments to pass to a chart object&#xa;        related to title and output options&#xa;    """"""&#xa;    params = {'title': title, 'legend': show_legend}&#xa;    output_params = parse_output_config(output)&#xa;    if output_params:&#xa;        params.update(output_params)&#xa;&#xa;    return params&#xa;&#xa;&#xa;def get_data_series(series, source, indexes):&#xa;    """"""Generate an OrderedDict from the source series excluding index&#xa;    and all series not specified in series.&#xa;&#xa;    Args:&#xa;        series (list(str)): list of strings specifying the names of the&#xa;            series to keep from source&#xa;        source (DataFrame): pandas DataFrame with the data series to be&#xa;            plotted&#xa;        indexes (lst(str)): name of the series of source to be used as index.&#xa;&#xa;    Returns:&#xa;        OrderedDict with the data series from source&#xa;    """"""&#xa;    series = define_series(series, source, indexes)&#xa;    # generate charts data&#xa;    data_series = OrderedDict()&#xa;    for i, colname in enumerate(series+indexes):&#xa;        try:&#xa;            data_series[colname] = source[colname]&#xa;        except KeyError:&#xa;            raise KeyError(hm.ERR_MSG_SERIES_NOT_FOUND % (colname, source.keys()))&#xa;&#xa;    return data_series&#xa;&#xa;&#xa;def define_series(series, source, indexes):&#xa;    """"""If series is empty returns source_columns excluding the column&#xa;    where column == index. Otherwise returns the series.split(',')&#xa;&#xa;    Args:&#xa;        series (str): string that contains the names of the&#xa;            series to keep from source, separated by `,`&#xa;        source (DataFrame): pandas DataFrame with the data series to be&#xa;            plotted&#xa;        indexes (lst(str)): name of the series of source to be used as index.&#xa;&#xa;    Returns:&#xa;        list of the names (as str) of the series except index&#xa;    """"""&#xa;    if not series:&#xa;        return [c for c in source.columns if c not in indexes]&#xa;    else:&#xa;        return series.split(',')&#xa;&#xa;&#xa;def get_charts_mapping():&#xa;    """"""Return a dict with chart classes names (lower case) as keys and&#xa;    their related class as values.&#xa;&#xa;    Returns:&#xa;        dict mapping chart classes names to chart classes&#xa;    """"""&#xa;    mapping = {}&#xa;    for (clsname, cls) in charts.__dict__.items():&#xa;        try:&#xa;            # TODO: We may need to restore the objects filtering&#xa;            # when charts creators (or builders registration) is added&#xa;            # to the charts API&#xa;            mapping[clsname.lower()] = cls&#xa;        except TypeError:&#xa;            pass&#xa;    return mapping&#xa;"
5347065|"#!/bin/env python&#xa;&#xa;""""""&#xa;Copyright (c) 2016 Christopher M. Bruns&#xa;&#xa;Permission is hereby granted, free of charge, to any person obtaining a copy&#xa;of this software and associated documentation files (the ""Software""), to deal&#xa;in the Software without restriction, including without limitation the rights&#xa;to use, copy, modify, merge, publish, distribute, sublicense, and/or sell&#xa;copies of the Software, and to permit persons to whom the Software is&#xa;furnished to do so, subject to the following conditions:&#xa;&#xa;The above copyright notice and this permission notice shall be included in all&#xa;copies or substantial portions of the Software.&#xa;&#xa;THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR&#xa;IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,&#xa;FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE&#xa;AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER&#xa;LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,&#xa;OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE&#xa;SOFTWARE.&#xa;""""""&#xa;&#xa;# built-in python modules&#xa;import sys&#xa;import io&#xa;import time&#xa;from glob import glob&#xa;import os&#xa;import math&#xa;import datetime&#xa;&#xa;# third party python modules&#xa;from OpenGL import GL&#xa;from tifffile import TiffFile&#xa;import tifffile&#xa;import numpy&#xa;&#xa;# local python modules&#xa;import ktx&#xa;from ktx.util import create_mipmaps, mipmap_dimension, interleave_channel_arrays, downsample_array_xy&#xa;&#xa;""""""&#xa;TODO: For converting rendered octree blocks, include the following precomputed:&#xa;  * all mipmap levels&#xa;  * optional intensity downsampling, with affine reestimation parameters&#xa;  * optional spatial downsampling&#xa;  * other metadata:&#xa;      * distance units e.g. ""micrometers"", for all the transforms below&#xa;      * transform from texture coordinates to Cartesian reference space&#xa;      * Optional transform from texture coordinates to Allen reference space&#xa;      * center xyz in reference space&#xa;      * bounding radius&#xa;      * nominal spatial resolution range at this level&#xa;      * specimen ID, e.g. ""2015-06-19-johan-full""&#xa;      * parent tile/block ID, e.g. ""/1/5/4/8/default.[0,1].tif""&#xa;      * relation to parent tile/block, e.g. ""downsampled 2X in XY; rescaled intensity to 8 bits; sub-block (1,2) of (6,6)&#xa;      * multiscale level ID (int)&#xa;          * of total multiscale level count (int)&#xa;      * per channel&#xa;          * affine parameters to approximate background level of first channel, for dynamic unmixing&#xa;          * min, max, average, median intensities&#xa;          * proportion of zero/NaN in this block&#xa;      * creation time&#xa;      * name of program used to create this block&#xa;      * version of program used to create this block&#xa;      * texture coordinate bounds for display (because there might be padding...)&#xa;""""""&#xa;&#xa;def test_downsample_xy(filter_='arthur'):&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data = tif.asarray()&#xa;    t0 = time.time()&#xa;    downsampled = downsample_array_xy(data, filter_=filter_)&#xa;    t1 = time.time()&#xa;    print (t1-t0, "" seconds elapsed time to downsample volume in XY"")&#xa;    tifffile.imsave(""downsampled.tif"", downsampled)&#xa;    t2 = time.time()&#xa;    print (t2-t1, "" seconds elapsed time to save downsampled volume in tiff format to disk"")&#xa;def test_interleave_channel_arrays():&#xa;    a = numpy.array( (1,2,3,4,5,), dtype='uint16' )&#xa;    b = numpy.array( (6,7,8,9,10,), dtype='uint16' )&#xa;    # print (a)&#xa;    # print (b)&#xa;    c = interleave_channel_arrays( (a,b,) )&#xa;    # print (c)&#xa;    assert numpy.array_equal(c, numpy.array(&#xa;        [[ 1,  6],&#xa;         [ 2,  7],&#xa;         [ 3,  8],&#xa;         [ 4,  9],&#xa;         [ 5, 10]]))&#xa;&#xa;def test_create_mipmaps(filter_='arthur'):&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data = tif.asarray()&#xa;    data = downsample_array_xy(data, filter_=filter_)&#xa;    t0 = time.time()&#xa;    mipmaps = create_mipmaps(data, filter_=filter_)&#xa;    t1 = time.time()&#xa;    print (t1-t0, "" seconds elapsed time to compute mipmaps"")&#xa;    for i in range(len(mipmaps)):&#xa;        tifffile.imsave(""test_mipmap%02d.tif"" % i, mipmaps[i])&#xa;    t2 = time.time()&#xa;    print (t2-t1, "" seconds elapsed time to save mipmaps in tiff format to disk"")&#xa;&#xa;def test_create_tiff():&#xa;    # https://pypi.python.org/pypi/tifffile&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data1 = tif.asarray()&#xa;        # tifffile.imsave('test1.tif', data1)&#xa;    fname = ""E:/brunsc/projects/ktxtiff/octree_tip/default.1.tif""&#xa;    with TiffFile(fname) as tif:&#xa;        data2 = tif.asarray()&#xa;        # tifffile.imsave('test2.tif', data2)&#xa;    # TODO unmixing test&#xa;    # compute channel 1/2 unmixing parameters&#xa;    # For lower end of mapping, just use lower quartile intensity (non-zero!)&#xa;    lower1 = numpy.percentile(data1[data1 != 0], 40)&#xa;    lower2 = numpy.percentile(data2[data2 != 0], 40)&#xa;    print (lower1, lower2)&#xa;    # For upper end of mapping, use voxels that are bright in BOTH channels&#xa;    m_a = numpy.median(data1[data1 != 0])&#xa;    m_b = numpy.median(data2[data2 != 0])&#xa;    s_a = numpy.std(data1[data1 != 0])&#xa;    s_b = numpy.std(data2[data2 != 0])&#xa;    upper1 = numpy.median(data1[(data1 > m_a + 2*s_a) & (data2 > m_b + 2*s_b)])&#xa;    upper2 = numpy.median(data2[(data1 > m_a + 2*s_a) & (data2 > m_b + 2*s_b)])&#xa;    print (upper1, upper2)&#xa;    # transform data2 to match data1&#xa;    scale = (upper1 - lower1) / (upper2 - lower2)&#xa;    offset = upper1 - upper2 * scale&#xa;    scale2 = (upper2 - lower2) / (upper1 - lower1)&#xa;    offset2 = upper2 - upper1 * scale2&#xa;    data2b = numpy.array(data2, dtype='float32')&#xa;    data2b *= scale&#xa;    data2b += offset&#xa;    data2b[data2 == 0] = 0&#xa;    data2b[data2b <= 0] = 0&#xa;    data2b = numpy.array(data2b, dtype=data1.dtype)&#xa;    # TODO ktx to tiff&#xa;    # Needs 1 or 3 channels for Fiji to load it OK&#xa;    # data3 = numpy.zeros_like(data1)&#xa;    tissue = numpy.minimum(data1, data2)&#xa;    tissue_base = numpy.percentile(tissue[tissue != 0], 4) - 1&#xa;    tissue = numpy.array(tissue, dtype='float32') # so we can handle negative numbers&#xa;    print (tissue_base)&#xa;    tissue -= tissue_base&#xa;    tissue[tissue <= 0] = 0&#xa;    tissue = numpy.array(tissue, dtype=data1.dtype)&#xa;    #&#xa;    unmixed1 = numpy.array(data1, dtype='float32')&#xa;    unmixed1 -= data2b&#xa;    # unmixed1 += s_a # tweak background up to show more stuff&#xa;    unmixed1[unmixed1 <= 0] = 0&#xa;    unmixed1 = numpy.array(unmixed1, dtype=data1.dtype)&#xa;    #&#xa;    data1b = numpy.array(data1, dtype='float32')&#xa;    data1b *= scale2&#xa;    data1b += offset2&#xa;    data1b[data1 == 0] = 0&#xa;    data1b[data1b <= 0] = 0&#xa;    data1b = numpy.array(data1b, dtype=data1.dtype)&#xa;    unmixed2 = numpy.array(data2, dtype='float32')&#xa;    unmixed2 -= data1b&#xa;    # unmixed2 += s_b # tweak background up to show more stuff&#xa;    unmixed2[unmixed2 <= 0] = 0&#xa;    unmixed2 = numpy.array(unmixed2, dtype=data1.dtype)&#xa;    #&#xa;    print (tissue.shape)&#xa;    data123 = interleave_channel_arrays( (data2, data1b, unmixed2) )&#xa;    # print (data123.shape)&#xa;    tifffile.imsave('test123.tif', data123)&#xa;&#xa;def ktx_from_mouselight_octree_folder(input_folder_name,&#xa;                              output_folder_name,&#xa;                              num_levels=1, # '0' means 'all'&#xa;                              mipmap_filter='max', &#xa;                              downsample_xy=True, &#xa;                              downsample_intensity=False):&#xa;    # Parse geometry data from top level transform.txt&#xa;    metadata = dict()&#xa;    with io.open(os.path.join(input_folder_name, ""transform.txt""), 'r') as transform_file:&#xa;        for line in transform_file:&#xa;            fields = line.split("": "")&#xa;            if len(fields) != 2:&#xa;                continue&#xa;            metadata[fields[0].strip()] = fields[1].strip()&#xa;    # Get original tiff file dimensions, to help compute geometry correctly&#xa;    with tifffile.TiffFile(os.path.join(input_folder_name, ""default.0.tif"")) as tif:&#xa;        original_tiff_dimensions = tif.asarray().shape&#xa;    # for k, v in metadata.items():&#xa;    #     print (k, v)&#xa;    if num_levels == 0:&#xa;        num_levels = int(metadata[""nl""])&#xa;    assert num_levels > 0&#xa;    folder = input_folder_name&#xa;    for level in range(num_levels):&#xa;        tiffs = glob(os.path.join(folder, ""default.*.tif""))&#xa;        ktx_obj = ktx_from_tiff_channel_files(tiffs, mipmap_filter, downsample_xy, downsample_intensity)&#xa;        # Populate custom block metadata&#xa;        kh = ktx_obj.header&#xa;        # kv = ktx_obj.header.key_value_metadata&#xa;        # kv[b'distance_units'] = b'micrometers'&#xa;        kh[""distance_units""] = ""micrometers""&#xa;        umFromNm = 1.0/1000.0&#xa;        # Origin of volume (corner of corner voxel)&#xa;        ox = umFromNm*float(metadata['ox'])&#xa;        oy = umFromNm*float(metadata['oy'])&#xa;        oz = umFromNm*float(metadata['oz'])&#xa;        # Size of entire volume&#xa;        # Use original dimensions, to account for downsampling...&#xa;        sx = umFromNm * original_tiff_dimensions[2] * float(metadata['sx']) &#xa;        sy = umFromNm * original_tiff_dimensions[1] * float(metadata['sy'])&#xa;        sz = umFromNm * original_tiff_dimensions[0] * float(metadata['sz'])&#xa;        xform = numpy.array([&#xa;                [sx, 0, 0, ox],&#xa;                [0, sy, 0, oy],&#xa;                [0, 0, sz, oz],&#xa;                [0, 0, 0, 1],], dtype='float32')&#xa;        # print(xform)&#xa;        kh[""xyz_from_texcoord_xform""] = xform&#xa;        # print (kh[""xyz_from_texcoord_xform""])&#xa;        #&#xa;        center = numpy.array( (ox + 0.5*sx, oy + 0.5*sy, oz + 0.5*sz,), )&#xa;        radius = math.sqrt(sx*sx + sy*sy + sz*sz)/16.0&#xa;        kh['bounding_sphere_center'] = center&#xa;        kh['bounding_sphere_radius'] = radius&#xa;        # Nominal resolution&#xa;        resX = sx / ktx_obj.header.pixel_width&#xa;        resY = sy / ktx_obj.header.pixel_height&#xa;        resZ = sz / ktx_obj.header.pixel_depth&#xa;        rms = math.sqrt(numpy.mean(numpy.square([resX, resY, resZ],)))&#xa;        kh['nominal_resolution'] = rms&#xa;        # print (kh['nominal_resolution'])&#xa;        # Specimen ID&#xa;        kh['specimen_id'] = os.path.split(input_folder_name)[-1]&#xa;        # print (kh['specimen_id'])&#xa;        # TODO: octree block ID&#xa;        # Relation to parent tile/block&#xa;        kh['mipmap_filter'] = mipmap_filter&#xa;        relations = list()&#xa;        if downsample_xy:&#xa;            relations.append(""downsampled 2X in X & Y"")&#xa;        if downsample_intensity:&#xa;            relations.append(""rescaled intensity to 8 bits"")&#xa;        if len(relations) == 0:&#xa;            relations.append(""unchanged"")&#xa;        kh['relation_to_parent'] = "";"".join(relations)&#xa;        # print (kh['relation_to_parent'])&#xa;        kh['multiscale_level_id'] = level&#xa;        kh['multiscale_total_levels'] = metadata['nl']&#xa;        # TODO: Per channel statistics&#xa;        kh['ktx_file_creation_date'] = datetime.datetime.now()&#xa;        # print (kh['ktx_file_creation_date'])&#xa;        import __main__ #@UnresolvedImport&#xa;        kh['ktx_file_creation_program'] = __main__.__file__&#xa;        # print (kh['ktx_file_creation_program'])&#xa;        kh['pyktx_version'] = ktx.__version__&#xa;        # print (kh['ktx_package_version'])&#xa;        # TODO: Texture coordinate bounds for display&#xa;        # Write LZ4-compressed KTX file&#xa;        t1 = time.time()&#xa;        with io.open('test.ktx', 'wb') as ktx_out:&#xa;            temp = io.BytesIO()&#xa;            ktx_obj.write_stream(temp)&#xa;            ktx_out.write(temp.getvalue())        # Create tiff file for sanity check testing&#xa;        t2 = time.time()&#xa;        print (""Creating uncompressed ktx file took %.3f seconds"" % (t2 - t1))&#xa;        # TODO: create tiffFromKtx.py as a separate tool&#xa;        # tifffile.imsave('test.tif', ktx_obj.asarray(0))&#xa;&#xa;def ktx_from_tiff_channel_files(channel_tiff_names, mipmap_filter='max', downsample_xy=True, downsample_intensity=False):&#xa;    """"""&#xa;    Load multiple single-channel tiff files, and create a multichannel Ktx object.&#xa;    """"""&#xa;    t0 = time.time()&#xa;    channels = list()&#xa;    for fname in channel_tiff_names:&#xa;        with TiffFile(fname) as tif:&#xa;            arr = tif.asarray()&#xa;            if downsample_xy:&#xa;                arr = downsample_array_xy(arr, mipmap_filter)&#xa;            channels.append(arr)&#xa;    t1 = time.time()&#xa;    print (""loading tiff files took %.3f seconds"" % (t1 - t0))&#xa;    if downsample_intensity:&#xa;        new_channels = list()&#xa;        channel_transforms = list()&#xa;        for channel in channels:&#xa;            min_ = numpy.min(channel[channel != 0])&#xa;            max_ = numpy.max(channel[channel != 0])&#xa;            scale = 1.0&#xa;            offset = min_ - 1&#xa;            if max_ - min_ > 255: # need a lossy contraction of intensities&#xa;                # Discard dimmest 2% of intensities&#xa;                min_ = numpy.percentile(channel[channel != 0], 2)&#xa;                median = numpy.median(channel[channel != 0])&#xa;                max_ = numpy.max(channel[channel != 0])&#xa;                # Discard intensities above 90% of max&#xa;                max_ = median + 0.90 * (max_ - median)&#xa;                print(min_, median, max_)&#xa;                scale = (max_ - min_) / 255.0&#xa;                offset = min_ - 1&#xa;            if channel.dtype.itemsize == 2:&#xa;                c = numpy.array(channel, dtype='float32')&#xa;                c -= offset&#xa;                c /= scale&#xa;                c[c<0] = 0&#xa;                if channel.dtype == numpy.uint16:&#xa;                    dt = numpy.uint8&#xa;                else:&#xa;                    raise # TODO: more cases&#xa;                c = numpy.array(c, dtype=dt)&#xa;                new_channels.append(c)&#xa;                channel_transforms.append( tuple([scale, offset]) )&#xa;            else:&#xa;                raise # TODO:&#xa;        channels = new_channels&#xa;    combined = interleave_channel_arrays(channels)&#xa;    ktx_obj = ktx.Ktx.from_ndarray(combined, mipmap_filter=mipmap_filter)&#xa;    # Include metadata for reconstructing original intensities&#xa;    if downsample_intensity:&#xa;        c = 0&#xa;        for ct in channel_transforms:&#xa;            ktx_obj.header['intensity_transform_%d'%c] = ct&#xa;            c += 1&#xa;    t2 = time.time()&#xa;    print (""creating swizzled mipmapped ktx data took %.3f seconds"" % (t2 - t1))    &#xa;    return ktx_obj&#xa;&#xa;def main():&#xa;    test_mipmap_dimension()&#xa;    test_downsample_xy()&#xa;    return&#xa;    ""Interleave multiple single channel tiff files into a multi-channel KTX file""&#xa;    arrays = list()&#xa;    for arg in sys.argv[1:]:&#xa;        for fname in glob(arg):&#xa;            print (fname)&#xa;            with TiffFile(fname) as tif:&#xa;                print (len(tif.pages))&#xa;                data = tif.asarray()&#xa;                print (data.shape)&#xa;                arrays.append(data)&#xa;                # print (numpy.percentile(data[data != 0], [25, 99]))&#xa;    a = arrays[0]&#xa;    b = arrays[1]&#xa;    # TODO: generate linear unmixing parameters appropriate for both dim and bright sections&#xa;    # Use only non-zero locations for basic statistics&#xa;    m_a = numpy.median(a[a != 0])&#xa;    s_a = numpy.std(a[a != 0])&#xa;    m_b = numpy.median(b[b != 0])&#xa;    s_b = numpy.std(b[b != 0])&#xa;    # Statistic for locations where both channels are bright at the same location&#xa;    h_a = numpy.median(a[(a > m_a + 2*s_a) & (b > m_b + 2*s_b)])&#xa;    print (m_a, s_a, h_a)&#xa;    h_b = numpy.median(b[(a > m_a + 2*s_a) & (b > m_b + 2*s_b)])&#xa;    print (m_b, s_b, h_b)&#xa;    # Interleave two channels&#xa;    # http://stackoverflow.com/questions/5347065/interweaving-two-numpy-arrays&#xa;    c = numpy.empty( shape=(a.shape[0], a.shape[1], a.shape[2], len(arrays)), dtype=a.dtype)&#xa;    for i in range(len(arrays)):&#xa;        c[:,:,:,i] = arrays[i]&#xa;    print (c.shape)&#xa;    dt = c.dtype&#xa;    ktx_obj = ktx.Ktx()&#xa;    kh = ktx_obj.header&#xa;    if dt.byteorder == '<':&#xa;        kh.little_endian = True&#xa;    elif dt.byteorder == '=':&#xa;        kh.little_endian = sys.byteorder == 'little'&#xa;    else:&#xa;        raise # TODO&#xa;    print (dt.byteorder)&#xa;    print (kh.little_endian)&#xa;    if dt.kind == 'u':&#xa;        if dt.itemsize == 2:&#xa;            kh.gl_type = GL.GL_UNSIGNED_SHORT&#xa;        elif dt.itemsize == 1:&#xa;            kh.gl_type = GL.GL_UNSIGNED_BYTE&#xa;        else:&#xa;            raise # TODO&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    kh.gl_type_size = dt.itemsize&#xa;    #&#xa;    if c.shape[3] == 1:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RED&#xa;    elif c.shape[3] == 2:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RG&#xa;    elif c.shape[3] == 3:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RGB&#xa;    elif c.shape[3] == 4:&#xa;        kh.gl_format = kh.gl_base_internal_format = GL.GL_RGBA&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    if kh.gl_base_internal_format == GL.GL_RG and kh.gl_type == GL.GL_UNSIGNED_SHORT:&#xa;        kh.gl_internal_format = GL.GL_RG16UI&#xa;    else:&#xa;        raise # TODO&#xa;    #&#xa;    kh.pixel_width = c.shape[2]&#xa;    kh.pixel_height = c.shape[1]&#xa;    kh.pixel_depth = c.shape[0]&#xa;    kh.number_of_array_elements = 0&#xa;    kh.number_of_faces = 0&#xa;    kh.number_of_mipmap_levels = 1 # TODO zero for autogenerate?&#xa;    # TODO - key/value pairs for provenance&#xa;    ktx_obj.image_data.mipmaps.clear()&#xa;    ktx_obj.image_data.mipmaps.append(c.tostring())&#xa;    &#xa;if __name__ == ""__main__"":&#xa;    """"""&#xa;    ktx_from_tiff_channel_files(&#xa;            (""E:/brunsc/projects/ktxtiff/octree_tip/default.1.tif"",&#xa;            ""E:/brunsc/projects/ktxtiff/octree_tip/default.0.tif"",&#xa;            ), )&#xa;    """"""&#xa;    """"""&#xa;    ktx_from_mouselight_octree_folder(&#xa;            input_folder_name='//fxt/nobackup2/mouselight/2015-06-19-johan-full', &#xa;            output_folder_name='',&#xa;            mipmap_filter='arthur', &#xa;            downsample_xy=True,&#xa;            downsample_intensity=True)&#xa;    """"""&#xa;    test_create_mipmaps('arthur')&#xa;"
1550560|"import datetime&#xa;import struct&#xa;import logging&#xa;import numpy as np&#xa;import os&#xa;import time&#xa;import re&#xa;import pytz&#xa;import scipy.spatial.distance as distance&#xa;from tzlocal import get_localzone&#xa;import warnings&#xa;&#xa;&#xa;def get_filename_meta_data(fn):&#xa;    """"""&#xa;    This function retrieves the meta information from a filename given the typical formatting in the Time Travel Task.&#xa;&#xa;    :param fn: a filename to parse for meta-data&#xa;    :return: a dictionary containing keys 'subID', 'trial', 'phase', 'inverse' and 'datetime' of types string&#xa;             with the exception of datetime which is of type datetime&#xa;    """"""&#xa;    parts = fn.split('_')&#xa;    dt = datetime.datetime.strptime(parts[4] + '_' + parts[5].split('.')[0], '%Y-%m-%d_%H-%M-%S')&#xa;    return {""subID"": parts[0], ""trial"": parts[1], ""phase"": parts[2], ""inverse"": parts[3], ""datetime"": dt}&#xa;&#xa;&#xa;def phase_num_to_str(phase):&#xa;    """"""&#xa;    This function converts a phase integer into a nameable phase string.&#xa;&#xa;    :param phase: an integer which represents the phase type to be converted to a string&#xa;    :return: 'VR Practice', 'VR Study', 'VR Test', 'VE Practice', 'VE Study', 'VE Test', '2D Practice', '2D Study',&#xa;             '2D Test', in order, from 0 to 8.&#xa;    """"""&#xa;    names = ['VR Practice', 'VR Study', 'VR Test', 'VE Practice', 'VE Study', 'VE Test',&#xa;             '2D Practice', '2D Study', '2D Test']&#xa;    lookup = phase&#xa;    # noinspection PyCompatibility&#xa;    if isinstance(lookup, str):&#xa;        lookup = int(lookup)&#xa;    return names[lookup]&#xa;&#xa;&#xa;def decode_7bit_int_length(fp):&#xa;    """"""&#xa;    From:&#xa;    http://stackoverflow.com/questions/1550560/encoding-an-integer-in-7-bit-format-of-c-sharp-binaryreader-readstring&#xa;&#xa;    This function takes a file pointer and extracts the appropriate next information which is expected to contain a&#xa;    .NET 7bit binary datetime encoded value and extracts the length of that datetime value.&#xa;&#xa;    :param fp: a file pointer whose next expected element is a 7bit integer length of a binary datetime in .NET&#xa;    :return: a length value representing the string length of a binary datetime in .NET&#xa;    """"""&#xa;    string_length = 0&#xa;    string_length_parsed = False&#xa;    step = 0&#xa;    while not string_length_parsed:&#xa;        part = ord(fp.read(1))&#xa;        string_length_parsed = ((part >> 7) == 0)&#xa;        part_cutter = part & 127&#xa;        to_add = part_cutter << (step * 7)&#xa;        string_length += to_add&#xa;        step += 1&#xa;    return string_length&#xa;&#xa;&#xa;def datetime_from_dot_net_binary(data):&#xa;    """"""&#xa;    From http://stackoverflow.com/questions/15919598/serialize-datetime-as-binary&#xa;&#xa;    This function converts data from a .NET datetime binary representation to a python datetime object&#xa;&#xa;    :param data: some binary data which is expected to convert to a datetime value&#xa;&#xa;    :return: a datetime value corresponding to the binary .NET datetime representation from the input data&#xa;    """"""&#xa;    kind = (data % 2 ** 64) >> 62  # This says about UTC and stuff...&#xa;    ticks = data & 0x3FFFFFFFFFFFFFFF&#xa;    seconds = float(ticks) / 10000000.0&#xa;    tz = pytz.utc&#xa;    if kind == 0:&#xa;        tz = get_localzone()&#xa;    return datetime.datetime(1, 1, 1, tzinfo=tz) + datetime.timedelta(seconds=seconds)&#xa;&#xa;&#xa;def read_binary_file(path):&#xa;    """"""&#xa;    This function reads a Time Travel Task binary file in its entirety and converts it into a list of iterations which&#xa;    can be parsed independently.&#xa;&#xa;    :param path: a string absolute path to a Time Travel Task binary file&#xa;&#xa;    :return: a list of iterations (dictionaries containing values:&#xa;             'version' - an integer version number&#xa;&#xa;             'datetime' - a python datetime object for this iteration&#xa;&#xa;             'time_val' - a time for this iteration&#xa;&#xa;             'timescale' - a timescale which the current time is proceding through&#xa;&#xa;             'x', 'y', 'z' - x, y, and z spatial coordinates in which the participant resides&#xa;&#xa;             'rx', 'ry', 'rz', 'rw' - x, y, z, and w rotation quaternion coordinates in which the participant resides&#xa;             'keys', 'buttons', 'keylabels', 'buttonlabels' 0 the key states, button states, key labels and button&#xa;             labels for every key and button which is registered to be logged&#xa;&#xa;             'itemsx', 'itemsy', 'itemsz', 'itemsactive', 'itemsclicked', 'itemsevent', 'itemstime' - the item x, y, z&#xa;             spatial coordinates, the item active state (enabled or disabled in the environment), the item event,&#xa;             and time&#xa;             'boundarystate', 'br', 'bg', 'bb' - the boundary state and Red, Green, and Blue color intensities of the&#xa;             boundary&#xa;&#xa;             'inventoryitemnumber'- the item numbers in the inventory this iteration&#xa;&#xa;             'activeinventoryitemnumber','activeinventoryeventinder' - the active item number and event number this&#xa;             iteration)&#xa;    """"""&#xa;    iterations = []&#xa;    with open(path, 'rb') as f:&#xa;        header_length = decode_7bit_int_length(f)&#xa;        header = f.read(header_length)&#xa;        split_header = header.split(',')&#xa;        if split_header[0] != 'version':  # Beta version with new version prefix&#xa;            num_keys = header.count('key')&#xa;            num_buttons = header.count('button')&#xa;            num_items = header.count('itemXYZAC')&#xa;&#xa;            while f.read(1):  # Look ahead for end of file&#xa;                f.seek(-1, 1)  # Go back one to undo the look-ahead&#xa;&#xa;                # Extract time_val information&#xa;                date_time = datetime_from_dot_net_binary(struct.unpack_from('q', f.read(8))[0])&#xa;                time_val = struct.unpack_from('f', f.read(4))[0]&#xa;                time_scale = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract position information&#xa;                x = struct.unpack_from('f', f.read(4))[0]&#xa;                y = struct.unpack_from('f', f.read(4))[0]&#xa;                z = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract rotation information&#xa;                rx = struct.unpack_from('f', f.read(4))[0]&#xa;                ry = struct.unpack_from('f', f.read(4))[0]&#xa;                rz = struct.unpack_from('f', f.read(4))[0]&#xa;                rw = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract key, button, and item information according to expected numbers of each&#xa;                keys = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_keys):&#xa;                    keys.append(struct.unpack_from('?', f.read(1))[0])&#xa;                buttons = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_buttons):&#xa;                    buttons.append(struct.unpack_from('?', f.read(1))[0])&#xa;                ix = []&#xa;                iy = []&#xa;                iz = []&#xa;                i_active = []&#xa;                i_clicked = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_items):&#xa;                    ix.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iy.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iz.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    i_active.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_clicked.append(struct.unpack_from('?', f.read(1))[0])&#xa;&#xa;                # Extract boundary information&#xa;                boundary_state = struct.unpack_from('i', f.read(4))[0]&#xa;                br = struct.unpack_from('f', f.read(4))[0]&#xa;                bg = struct.unpack_from('f', f.read(4))[0]&#xa;                bb = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Store all information in simple dictionary and add to list of iterations&#xa;                iterations.append({""version"": 0,&#xa;                                   ""datetime"": date_time, ""time_val"": time_val, ""timescale"": time_scale,&#xa;                                   ""x"": x, ""y"": y, ""z"": z,&#xa;                                   ""rx"": rx, ""ry"": ry, ""rz"": rz, ""rw"": rw,&#xa;                                   ""keys"": keys, ""buttons"": buttons,&#xa;                                   ""itemsx"": ix, ""itemsy"": iy, ""itemsz"": iz, ""itemsactive"": i_active,&#xa;                                   ""itemsclicked"": i_clicked,&#xa;                                   ""boundarystate"": boundary_state, ""br"": br, ""bg"": bg, ""bb"": bb})&#xa;        elif split_header[1] == '2':  # Version 2&#xa;            num_keys = header.count('key')&#xa;            num_buttons = header.count('button')&#xa;            num_items = header.count('itemXYZActiveClickedEventTime')&#xa;            key_labels = []&#xa;            key_split = header.split('key')&#xa;            for i in range(1, len(key_split)):&#xa;                key_labels.append(key_split[i].split('_')[0])&#xa;            button_labels = []&#xa;            button_split = header.split('button')&#xa;            for i in range(1, len(button_split)):&#xa;                button_labels.append(button_split[i].split('_')[0])&#xa;            while f.read(1):  # Look ahead for end of file&#xa;                f.seek(-1, 1)  # Go back one to undo the look-ahead&#xa;&#xa;                # Extract time_val information&#xa;                date_time = datetime_from_dot_net_binary(struct.unpack_from('q', f.read(8))[0])&#xa;                time_val = struct.unpack_from('f', f.read(4))[0]&#xa;                time_scale = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract position information&#xa;                x = struct.unpack_from('f', f.read(4))[0]&#xa;                y = struct.unpack_from('f', f.read(4))[0]&#xa;                z = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract rotation information&#xa;                rx = struct.unpack_from('f', f.read(4))[0]&#xa;                ry = struct.unpack_from('f', f.read(4))[0]&#xa;                rz = struct.unpack_from('f', f.read(4))[0]&#xa;                rw = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract key, button, and item information according to expected numbers of each&#xa;                keys = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_keys):&#xa;                    keys.append(struct.unpack_from('?', f.read(1))[0])&#xa;                buttons = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_buttons):&#xa;                    buttons.append(struct.unpack_from('?', f.read(1))[0])&#xa;                ix = []&#xa;                iy = []&#xa;                iz = []&#xa;                i_active = []&#xa;                i_clicked = []&#xa;                i_event_type = []&#xa;                i_event_time = []&#xa;                # noinspection PyRedeclaration&#xa;                for i in range(0, num_items):&#xa;                    ix.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iy.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    iz.append(struct.unpack_from('f', f.read(4))[0])&#xa;                    i_active.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_clicked.append(struct.unpack_from('?', f.read(1))[0])&#xa;                    i_event_type.append(struct.unpack_from('i', f.read(4))[0])&#xa;                    i_event_time.append(struct.unpack_from('f', f.read(4))[0])&#xa;&#xa;                # Extract boundary information&#xa;                boundary_state = struct.unpack_from('i', f.read(4))[0]&#xa;                br = struct.unpack_from('f', f.read(4))[0]&#xa;                bg = struct.unpack_from('f', f.read(4))[0]&#xa;                bb = struct.unpack_from('f', f.read(4))[0]&#xa;&#xa;                # Extract inventory state&#xa;                inventory_item_numbers = []&#xa;                for i in range(0, num_items):&#xa;                    inventory_item_numbers.append(struct.unpack_from('i', f.read(4))[0])&#xa;                active_inventory_item_number = struct.unpack_from('i', f.read(4))[0]&#xa;                active_inventory_event_index = struct.unpack_from('i', f.read(4))[0]&#xa;&#xa;                # Store all information in simple dictionary and add to list of iterations&#xa;                iterations.append({""version"": 2,&#xa;                                   ""datetime"": date_time, ""time_val"": time_val, ""timescale"": time_scale,&#xa;                                   ""x"": x, ""y"": y, ""z"": z,&#xa;                                   ""rx"": rx, ""ry"": ry, ""rz"": rz, ""rw"": rw,&#xa;                                   ""keys"": keys, ""buttons"": buttons,&#xa;                                   'keylabels': key_labels, 'buttonlabels': button_labels,&#xa;                                   ""itemsx"": ix, ""itemsy"": iy, ""itemsz"": iz, ""itemsactive"": i_active,&#xa;                                   ""itemsclicked"": i_clicked, 'itemsevent': i_event_type, 'itemstime': i_event_time,&#xa;                                   ""boundarystate"": boundary_state, ""br"": br, ""bg"": bg, ""bb"": bb,&#xa;                                   'inventoryitemnumbers': inventory_item_numbers,&#xa;                                   'activeinventoryitemnumber': active_inventory_item_number,&#xa;                                   'activeinventoryeventindex': active_inventory_event_index})&#xa;&#xa;        return iterations&#xa;&#xa;&#xa;def find_last(lst, sought_elt):&#xa;    """"""&#xa;    This function finds the last index that an element of a particular value appears in a list.&#xa;&#xa;    :param lst: the list to search&#xa;    :param sought_elt: the element for which we search&#xa;    :return: the index at which the last element matching sought_elt resides&#xa;    """"""&#xa;    for r_idx, elt in enumerate(reversed(lst)):&#xa;        if elt == sought_elt:&#xa;            return len(lst) - 1 - r_idx&#xa;&#xa;&#xa;def parse_test_items(iterations, cols, item_number_label, event_state_labels):&#xa;    """"""&#xa;    This function takes in a set of iterations parsed from read_binary_file, a set of color values, a set of item&#xa;    labels, and a set of event labels and produces the items, reconstruction items and the order description for&#xa;    each item in the reconstruction.&#xa;&#xa;    :param iterations: the iterations output from read_binary_file&#xa;    :param cols: the colors of each item in canonical ordering&#xa;    :param item_number_label: the labels for each item in canonical ordering&#xa;    :param event_state_labels: the event labels for each item in canonical ordering&#xa;&#xa;    :return: a dictionary containing:&#xa;             ""direction"" - a numeric value representing the up, down or stationary state&#xa;             ""pos"" - the x, z, and t values representing the 2D position and time coordinates&#xa;             ""color"" - the color value representing the indexed color value from cols associated with the item&#xa;    """"""&#xa;    descrambler = [1, 2, 4, 7, 0, 3, 5, 6, 8, 9]&#xa;    descrambler_type = [2, 2, 2, 2, 1, 1, 1, 1, 0, 0]&#xa;    reconstruction_items = [None] * len(item_number_label)&#xa;    if iterations[0]['version'] == 0:&#xa;        # pos = np.empty((len(items), 3))&#xa;        # size = np.empty((len(items)))&#xa;        # color = np.empty((len(items), 4))&#xa;        # end_time = 60  # End time for convenience&#xa;&#xa;        ################################################################################################################&#xa;        # BEGIN DESCRAMBLER&#xa;        ################################################################################################################&#xa;&#xa;        # start_state = iterations[0]  # Store first iteration&#xa;        # end_state = iterations[len(iterations) - 1]  # Store last iteration&#xa;        # prev_active = start_state['itemsactive']  # Create activity array&#xa;&#xa;        # Event state tracker variables (this works great)&#xa;        number_placed = 0&#xa;        event_state = 0&#xa;        prev_event_btn_state = False&#xa;        prev_drop_button_state = False&#xa;        prev_inventory_button_state = False&#xa;        inventory_index = 0&#xa;        inventory = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&#xa;        numbers_placed = [-1] * 10&#xa;        flag_for_same_place_override = False&#xa;        flag_for_same_place_location = None&#xa;        for iterations_idx, i in enumerate(iterations):&#xa;            if iterations_idx == len(iterations) - 1:&#xa;                break&#xa;            # Get the event state button (keys, 4, buttons, 4)&#xa;            event_button_state = i['buttons'][4]&#xa;            if event_button_state and not prev_event_btn_state:  # On rising edge&#xa;                event_state = (event_state + 1) % 3  # Set the event state appropriately&#xa;            prev_event_btn_state = event_button_state  # Update prev state for edge detection&#xa;            # Get the item drop button state (keys 1, buttons 1)&#xa;            drop_button_state = i['buttons'][1]&#xa;            inventory_button_state = i['buttons'][3]&#xa;            # Find the value of the index of this item in the descrambler, this is the correct item value (I think)&#xa;            # Coordinate comparison&#xa;            current_item_coords = []&#xa;            for xx, zz in zip(i['itemsx'], i['itemsz']):&#xa;                current_item_coords.append((xx, zz))&#xa;            next_iteration = iterations[iterations_idx + 1]&#xa;            next_item_coords = []&#xa;            for xx, zz in zip(next_iteration['itemsx'], next_iteration['itemsz']):&#xa;                next_item_coords.append((xx, zz))&#xa;&#xa;            # Check for changed state (at this point, active and button press simultaneously)&#xa;            if (current_item_coords != next_item_coords) and (not iterations_idx == 0):&#xa;                logging.debug(','.join(map(str, current_item_coords)))&#xa;                logging.debug(','.join(map(str, next_item_coords)))&#xa;                present_checklist = [False] * len(current_item_coords)&#xa;                missing_item_index = -1&#xa;                for idxx, (first, second) in enumerate(zip(current_item_coords, next_item_coords)):&#xa;                    if first in next_item_coords:&#xa;                        present_checklist[idxx] = True&#xa;                    if next_item_coords.count(first) > 1 or current_item_coords.count(second) > 1:&#xa;                        logging.warning('Items were found to be placed on top of each other. ' +&#xa;                                        'This will likely make the item identities during reconstruction inaccurate.')&#xa;                        logging.debug('CASE: Multiple items in same location, ' +&#xa;                                      'consulting Active list for differentiation.')&#xa;                        active_list = i['itemsactive']&#xa;                        next_active_list = iterations[iterations_idx + 1]['itemsactive']&#xa;                        logging.debug(','.join(map(str, active_list)))&#xa;                        logging.debug(','.join(map(str, next_active_list)))&#xa;                        logging.debug(event_state)&#xa;                        logging.debug(','.join(map(str, descrambler_type)))&#xa;                        for idxxx, (a, b) in enumerate(zip(current_item_coords, next_item_coords)):&#xa;                            if not a == b:&#xa;                                present_checklist[idxxx] = False&#xa;                                logging.debug('{0} found as move index'.format(idxxx))&#xa;                                break&#xa;                for idxx, check in enumerate(present_checklist):&#xa;                    if not check:&#xa;                        if current_item_coords.count(current_item_coords[idxx]) > 1:&#xa;                            flag_for_same_place_override = True&#xa;                            flag_for_same_place_location = current_item_coords[idxx]&#xa;                        missing_item_index = idxx&#xa;                logging.debug('{0} is missing item index'.format(missing_item_index))&#xa;                if drop_button_state and not prev_drop_button_state:  # Rising edge, item dropped&#xa;                    logging.info('item dropped/picked up.')&#xa;                    if current_item_coords == next_item_coords:  # item picked up&#xa;                        # noinspection PyTypeChecker&#xa;                        inventory.insert(inventory_index, '?')&#xa;                        logging.info(&#xa;                            'item picked up to inventory index {0}: inventory state: {1}'.format(inventory_index,&#xa;                                                                                                 inventory))&#xa;                    else:&#xa;                        if inventory:&#xa;                            inventory.pop(inventory_index)&#xa;                        numbers_placed[descrambler[missing_item_index]] = number_placed&#xa;                        number_placed += 1&#xa;                        logging.info(&#xa;                            'item dropped from inventory index {0}: inventory state: {1}'.format(inventory_index,&#xa;                                                                                                 inventory))&#xa;                if inventory_button_state and not prev_inventory_button_state:&#xa;                    inventory_index = (inventory_index + 1) % len(inventory)&#xa;&#xa;                # Now we know which item (according to the previous descrambler state) had its location change&#xa;                # and which index it WAS in. We also know the event type it was placed as so we can compute its NEW&#xa;                # position in the list.&#xa;                # missing_item_index is the index of the placed item (according to current descrambler)&#xa;                # event_state is the type of event which was placed&#xa;                # Edge cases include if an item is placed precisely back where it was and if multiple items are&#xa;                # placed in the same place... unfortunately this happens in 44.6% of test files...&#xa;                # MORE NOTES:&#xa;                # I think it is possible to completely descramble because if an item is inserted into a&#xa;                # list of consecutive&#xa;                # identical coordinates, the items form a stack (where the latest placed item is picked up first)...&#xa;                # when picked up, we now know it's relative index in the inventory (and we can be relatively sure it's&#xa;                # picked up because it should become inactive). Since it must be picked up to be placed correctly,&#xa;                # if we track its position in the inventory until it is again placed, we can disentangle it from its&#xa;                # identical partners. To track it in inventory, it is necessary to track inventory clicks as well as&#xa;                # the number of items in the inventory so a proper modulus can be established.&#xa;                # Ugh.&#xa;                # DESCRAMBLER LOGIC&#xa;                if flag_for_same_place_override:&#xa;                    tmp_max = -1&#xa;                    tmp_max_index = -1&#xa;                    for x, y in enumerate(numbers_placed):&#xa;                        if current_item_coords[descrambler[x]] == flag_for_same_place_location:&#xa;                            if y > tmp_max:&#xa;                                tmp_max = y&#xa;                                tmp_max_index = x&#xa;                    override_index = tmp_max_index&#xa;                    # override_index = [x for x, y in enumerate(numbers_placed) if y == max(numbers_placed)][0]&#xa;                    override_index_descrambled = [x for x, y in enumerate(descrambler) if y == override_index][0]&#xa;                    logging.info('same place override, most recent placed descramble index is {0} compared to '&#xa;                                 'original missing index of {1}'.format(override_index_descrambled,&#xa;                                                                        missing_item_index))&#xa;                    missing_item_index = override_index_descrambled&#xa;                    flag_for_same_place_override = False&#xa;                # If the current event state is 0 (stationary), move the current item to the end of the list&#xa;                insertion_index = -1&#xa;                val = descrambler[missing_item_index]&#xa;                del descrambler[missing_item_index]&#xa;                del descrambler_type[missing_item_index]&#xa;                if event_state == 0:&#xa;                    descrambler.append(val)&#xa;                    descrambler_type.append(event_state)&#xa;                    insertion_index = len(descrambler) - 1&#xa;                # If the current event state is 1 (up/fly), move the current item to the last fly position&#xa;                elif event_state == 1 or event_state == 2:&#xa;                    last = find_last(descrambler_type, event_state)&#xa;                    if last is None and event_state == 1:&#xa;                        last = find_last(descrambler_type, 2)&#xa;                    elif last is None and event_state == 2:&#xa;                        last = 0&#xa;                    logging.debug('inserting into {0}'.format((last + 1)))&#xa;                    descrambler.insert(last + 1, val)&#xa;                    descrambler_type.insert(last + 1, event_state)&#xa;                    insertion_index = last + 1&#xa;&#xa;                # Generate projected values (time is the important one, the space ones are replaced at the end&#xa;                # according to the descrambler order)&#xa;                placed_x = next_item_coords[insertion_index][0]&#xa;                placed_z = next_item_coords[insertion_index][1]&#xa;                placed_t = i['time_val']&#xa;                # If the event is stationary, the time of placement doesn't matter, ignore it and set to 0&#xa;                if event_state == 0:&#xa;                    placed_t = 0&#xa;                # Add the item to the list using the correct IDX to look up the color&#xa;                reconstruction_items[val] = {'direction': event_state,&#xa;                                             'pos': (placed_x, placed_z, placed_t),&#xa;                                             'color': cols[val]}&#xa;                # Log debug information&#xa;                logging.debug(','.join(map(str, descrambler)))&#xa;                logging.debug(""{0}, {1}, ({2}, {3}, {4})"".format(&#xa;                    item_number_label[val].ljust(11, ' '),&#xa;                    event_state_labels[event_state], placed_x, placed_z, placed_t))&#xa;            prev_drop_button_state = drop_button_state&#xa;            prev_inventory_button_state = inventory_button_state&#xa;&#xa;            # Replace all of the position values with the descrambled position values at the final time point.&#xa;            # Keep the time point the same as it should've been corrected earlier&#xa;            # for idx in range(0, len(reconstruction_items)):&#xa;            #    reconstruction_items[idx]['pos'] = (end_state['itemsx'][descrambler[idx]],&#xa;            #                                        end_state['itemsz'][descrambler[idx]],&#xa;            #                                        reconstruction_items[idx]['pos'][2])&#xa;            #    reconstruction_items[idx]['color'] = cols[idx]&#xa;&#xa;            ############################################################################################################&#xa;            # END DESCRAMBLER&#xa;            ############################################################################################################&#xa;&#xa;    order = [[] for _ in range(0, len(reconstruction_items))]&#xa;    if iterations[0]['version'] == 2:&#xa;&#xa;        end_state = iterations[len(iterations) - 1]&#xa;        for idx, (x, y, z, active, clicked, event, time_val) in \&#xa;                enumerate(zip(end_state['itemsx'], end_state['itemsy'], end_state['itemsz'], end_state['itemsactive'],&#xa;                              end_state['itemsclicked'], end_state['itemsevent'], end_state['itemstime'])):&#xa;            reconstruction_items[idx] = {'direction': event, 'pos': (x, z, time_val), 'color': cols[idx]}&#xa;        order_num = 0&#xa;        for iter_idx, i in enumerate(iterations):&#xa;            for idx, (x, y, z, active, clicked, event, time_val) in enumerate(zip(i['itemsx'], i['itemsy'], i['itemsz'],&#xa;                                                                                  i['itemsactive'], i['itemsclicked'],&#xa;                                                                                  i['itemsevent'], i['itemstime'])):&#xa;                if active and not iterations[iter_idx - 1]['itemsactive'][idx]:&#xa;                    order[idx].append(order_num)&#xa;                    order_num += 1&#xa;    return reconstruction_items, order&#xa;&#xa;&#xa;def get_click_locations_and_indicies(iterations, items, meta):&#xa;    # If Study/Practice, label click events&#xa;    """"""&#xa;    This function takes the iterations from read_binary_file, the items to be searched and the meta information from&#xa;    the file and returns the clicked positions, indices in the iterations, size and colors for visualization.&#xa;&#xa;    :param iterations: the iterations from read_binary_file&#xa;    :param items: the items to be visualized&#xa;    :param meta: the meta information from the filename&#xa;    :return: a dictionary containing:&#xa;             click_pos - the x, z, time coordinates of the click position&#xa;             click_idx - the index in iterations at which time the click happened&#xa;             click_size - the size the click should be visualized as&#xa;             click_color - the color with which the click should be visualized&#xa;    """"""&#xa;    click_idx = np.empty(len(items))&#xa;    click_pos = np.empty((len(items), 3))&#xa;    click_size = np.zeros((len(iterations), len(items)))&#xa;    click_color = np.empty((len(items), 4))&#xa;    if meta['phase'] in ['0', '1', '3', '4', '6', '7']:&#xa;        for idx, i in enumerate(iterations):&#xa;            if idx + 1 < len(iterations):&#xa;                for idxx, (i1, i2) in enumerate(zip(i['itemsclicked'], iterations[idx + 1]['itemsclicked'])):&#xa;                    if i['itemsclicked'][idxx]:&#xa;                        click_size[idx][idxx] = 0.5&#xa;                    if not i1 == i2:&#xa;                        click_idx[idxx] = idx&#xa;                        click_pos[idxx] = (i['x'], i['z'], i['time_val'])&#xa;                        click_color[idxx] = (128, 128, 128, 255)&#xa;            else:&#xa;                for idxx, i1 in enumerate(i['itemsclicked']):&#xa;                    if i['itemsclicked'][idxx]:&#xa;                        click_size[idx][idxx] = 0.5&#xa;    return {'position': click_pos, 'index': click_idx, 'size': click_size, 'color': click_color}&#xa;&#xa;&#xa;def get_items_solutions(meta):&#xa;    """"""&#xa;    This function returns the solution values given a particular meta-file information configuration.&#xa;&#xa;    :param meta: the meta information from get_filename_meta_data&#xa;    :return: a tuple with items, times and directions where times contains numeric time constants, directions contains&#xa;             numeric labels such that 2 is Fall, 1 is Fly, and 0 is Stationary/Stay, and items containts a list of&#xa;             dictionaries containing values:&#xa;             ""direction"" - the 0, 1, or 2 direction value&#xa;             ""pos"" - the x, z, time coordinate of the item&#xa;             ""color"" - the RGB color tuple for the item&#xa;    """"""&#xa;    if meta['phase'] == '0' or meta['phase'] == '3' or meta['phase'] == '6':&#xa;        times = [2, 12, 18, 25]&#xa;        directions = [2, 1, 2, 1]  # Fall = 2, Fly = 1, Stay = 0&#xa;        if meta['inverse'] == '1':&#xa;            times.reverse()&#xa;            directions.reverse()&#xa;        items = [{'direction': directions[0], 'pos': (2, -12, times[0]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[1], 'pos': (2, 13, times[1]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[2], 'pos': (-13, 2, times[2]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[3], 'pos': (-12, -17, times[3]), 'color': (0, 0, 255)},&#xa;                 {'direction': 0, 'pos': (13, 5, 0), 'color': (128, 0, 128)}]&#xa;    # elif meta['phase'] == '7' or meta['phase'] == '8':&#xa;    #    times = [2, 8, 17, 23]&#xa;    #    directions = [2, 1, 1, 2]  # Fall = 2, Fly = 1, Stay = 0&#xa;    #    if meta['inverse'] == '1':&#xa;    #        times.reverse()&#xa;    #        directions.reverse()&#xa;    #    items = [{'direction': directions[0], 'pos': (16, -14, times[0]), 'color': (255, 255, 0)},&#xa;    #             {'direction': directions[1], 'pos': (-10, -2, times[1]), 'color': (255, 0, 0)},&#xa;    #             {'direction': directions[2], 'pos': (15, -8, times[2]), 'color': (0, 255, 0)},&#xa;    #             {'direction': directions[3], 'pos': (-15, -15, times[3]), 'color': (0, 0, 255)},&#xa;    #             {'direction': 0, 'pos': (-2, 10, 0), 'color': (128, 0, 128)}]&#xa;    else:&#xa;        times = [4, 10, 16, 25, 34, 40, 46, 51]&#xa;        directions = [2, 1, 1, 2, 1, 2, 2, 1]  # Fall = 2, Fly = 1, Stay = 0&#xa;        if meta['inverse'] == '1':&#xa;            times.reverse()&#xa;            directions.reverse()&#xa;        items = [{'direction': directions[0], 'pos': (18, -13, times[0]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[1], 'pos': (-13, 9, times[1]), 'color': (255, 255, 0)},&#xa;                 {'direction': directions[2], 'pos': (-10, -2, times[2]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[3], 'pos': (6, -2, times[3]), 'color': (255, 0, 0)},&#xa;                 {'direction': directions[4], 'pos': (17, -8, times[4]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[5], 'pos': (-2, -7, times[5]), 'color': (0, 255, 0)},&#xa;                 {'direction': directions[6], 'pos': (-15, -15, times[6]), 'color': (0, 0, 255)},&#xa;                 {'direction': directions[7], 'pos': (6, 18, times[7]), 'color': (0, 0, 255)},&#xa;                 {'direction': 0, 'pos': (14, 6, 0), 'color': (128, 0, 128)},&#xa;                 {'direction': 0, 'pos': (-2, 10, 0), 'color': (128, 0, 128)}]&#xa;    return items, times, directions&#xa;&#xa;&#xa;def find_data_files_in_directory(directory, file_regex=""""):&#xa;    """"""&#xa;    This function acts as a helper to search a directory for files that match a regular expression. The function&#xa;    will raise an IOError if the input path is not found.&#xa;&#xa;    :param directory: the directory to search&#xa;    :param file_regex: the regular expression to match for files&#xa;&#xa;    :return: a list of files which match the regular expression&#xa;    """"""&#xa;    if not os.path.exists(directory):&#xa;        raise IOError('The input path was not found.')&#xa;&#xa;    start_time = time.time()&#xa;    data_files = []&#xa;    file_index = []&#xa;    file_roots_index = []&#xa;    for root, dirs, files in os.walk(directory):&#xa;        for f in files:&#xa;            file_index.append(f)&#xa;            file_roots_index.append(root)&#xa;&#xa;    regex = re.compile(file_regex)&#xa;    for root, f in zip(file_roots_index, file_index):&#xa;        if regex.search(os.path.basename(f)):&#xa;            logging.debug('Found data file ({0}).'.format(f))&#xa;            data_files.append(os.path.join(root, f))&#xa;    logging.info('Found {0} data files in {1} seconds.'.format(len(data_files), time.time() - start_time))&#xa;    return data_files&#xa;&#xa;&#xa;def get_exploration_metrics(iterations):&#xa;    """"""&#xa;    This function gets the common exploration metrics from an iterations list returned by read_binary_file.&#xa;&#xa;    :param iterations: the iterations from read_binary_file&#xa;    :return: a tuple containing total_time, space_travelled, time_travelled, and space_time_travelled&#xa;    """"""&#xa;    total_time = (iterations[-1]['datetime'] - iterations[0]['datetime']).total_seconds()&#xa;    space_travelled = 0&#xa;    time_travelled = 0&#xa;    space_time_travelled = 0&#xa;    for idx, i in enumerate(iterations):&#xa;        if idx == len(iterations) - 1:&#xa;            break&#xa;        t = iterations[idx]['time_val']&#xa;        xy = [iterations[idx]['x'], iterations[idx]['y']]&#xa;        xyt = xy + [t]&#xa;        t_next = iterations[idx + 1]['time_val']&#xa;        xy_next = [iterations[idx + 1]['x'], iterations[idx + 1]['y']]&#xa;        xyt_next = xy_next + [t_next]&#xa;        space_travelled += distance.euclidean(xy, xy_next)&#xa;        space_time_travelled += distance.euclidean(xyt, xyt_next)&#xa;        time_travelled += distance.euclidean(t, t_next)&#xa;&#xa;    return total_time, space_travelled, time_travelled, space_time_travelled&#xa;&#xa;&#xa;def is_correct_color(t, solution_t, bins=15.0):&#xa;    """"""&#xa;    This function determines if a particular item time is correct given a time, solution time and bins in which a&#xa;    timeline is divided.&#xa;&#xa;    :param bins: the float representing the bins into which the timeline is divided&#xa;    :param t: the time of the item&#xa;    :param solution_t: the correct time of the time&#xa;    :return: a boolean value, true if the item is in the correct time region, false otherwise&#xa;    """"""&#xa;    lower = float(np.floor(float(solution_t) / bins) * bins)&#xa;    upper = float(np.ceil(float(solution_t) / bins) * bins)&#xa;    # noinspection PyTypeChecker&#xa;    return float(lower) < float(t) < float(upper)&#xa;&#xa;&#xa;def compute_accuracy(meta, items):&#xa;    """"""&#xa;    Given some meta information from the file via get_filename_meta_data and the item information, compute the accuracy&#xa;    of the items within and across contexts.&#xa;&#xa;    :param meta: the meta information from get_filename_meta_data&#xa;    :param items: the items from parse_test_items&#xa;    :return: a tuple containing:&#xa;             space_misplacement - the amount of space-only misplacement&#xa;             time_misplacement - the amount of time-only misplacement&#xa;             space_time_misplacement - the total space and time misplacement (treating the values equally)&#xa;             direction_correct_count - the number of correct direction labels&#xa;             mean_context_crossing_excluding_wrong_context_pairs - the mean of the distance between context&#xa;             crossing pairs&#xa;             excluding those which are in the wrong context&#xa;             mean_context_noncrossing_excluding_wrong_context_pairs - the mean of the distance between non-context&#xa;             crossing pairs&#xa;             excluding those which are in the wrong context&#xa;             mean_context_crossing - the mean distance between context crossing pairs with no exclusions&#xa;             mean_noncontext_crossing - the mean distance between non-context-crossing pairs with no exclusions&#xa;    """"""&#xa;    solution_items, times_solution, directions_solution = get_items_solutions(meta)&#xa;    xs = [item['pos'][0] for item in items]&#xa;    zs = [item['pos'][1] for item in items]&#xa;    times = [item['pos'][2] for item in items]&#xa;    directions = [item['direction'] for item in items]&#xa;    xs_solution = [item['pos'][0] for item in solution_items]&#xa;    zs_solution = [item['pos'][1] for item in solution_items]&#xa;&#xa;    space_misplacement = 0&#xa;    time_misplacement = 0&#xa;    space_time_misplacement = 0&#xa;    direction_correct_count = 0&#xa;    for x, z, t, d, solx, solz, solt, sold in zip(xs, zs, times, directions, xs_solution, zs_solution, times_solution,&#xa;                                                  directions_solution):&#xa;        space_misplacement += distance.euclidean((x, z), (solx, solz))&#xa;        time_misplacement += np.abs(t - solt)&#xa;        space_time_misplacement += distance.euclidean((x, z, t), (solx, solz, solt))&#xa;        direction_correct_count += int(d == sold)&#xa;&#xa;    context_crossing_dist_exclude_wrong_colors_pairs = []&#xa;    context_noncrossing_dist_exclude_wrong_colors_pairs = []&#xa;    context_crossing_dist_pairs = []&#xa;    context_noncrossing_dist_pairs = []&#xa;&#xa;    pairs = [(1, 1, 2), (1, 3, 4), (1, 5, 6), (0, 0, 1), (0, 2, 3), (0, 4, 5), (0, 6, 7)]&#xa;    for pair in pairs:&#xa;        crossing = pair[0] != 0&#xa;        idx0 = pair[1]&#xa;        idx1 = pair[2]&#xa;        x0, z0, t0, d0 = (xs[idx0], zs[idx0], times[idx0], directions[idx0])&#xa;        solx0, solz0, solt0, sold0 = (&#xa;            xs_solution[idx0], zs_solution[idx0], times_solution[idx0], directions_solution[idx0])&#xa;        x1, z1, t1, d1 = (xs[idx1], zs[idx1], times[idx1], directions[idx1])&#xa;        solx1, solz1, solt1, sold1 = (&#xa;            xs_solution[idx1], zs_solution[idx1], times_solution[idx1], directions_solution[idx1])&#xa;        dist = np.abs(t0 - t1) / np.abs(solt0 - solt1)&#xa;        if crossing:&#xa;            context_crossing_dist_pairs.append(dist)&#xa;        else:&#xa;            context_noncrossing_dist_pairs.append(dist)&#xa;        if is_correct_color(t0, solt0) and is_correct_color(t1, solt1):&#xa;            if crossing:&#xa;                context_crossing_dist_exclude_wrong_colors_pairs.append(dist)&#xa;            else:&#xa;                context_noncrossing_dist_exclude_wrong_colors_pairs.append(dist)&#xa;&#xa;    with warnings.catch_warnings():&#xa;        warnings.simplefilter(""ignore"", category=RuntimeWarning)&#xa;        return space_misplacement, time_misplacement, space_time_misplacement, direction_correct_count, \&#xa;               np.mean(context_crossing_dist_exclude_wrong_colors_pairs), \&#xa;               np.mean(context_noncrossing_dist_exclude_wrong_colors_pairs), \&#xa;               np.mean(context_crossing_dist_pairs), np.mean(context_noncrossing_dist_pairs), \&#xa;               np.nan, np.nan, np.nan, np.nan  # Patch to fix issue with space return values which are missing...&#xa;&#xa;&#xa;def get_item_details(pastel_factor=127):&#xa;    """"""&#xa;    This function returns detailed information about the item solutions including strings representing the event&#xa;    state, strings with the item labels, and filename image strings (JPG), as well as RGB color tuples for each item.&#xa;&#xa;    :param pastel_factor: a factor to render the RGB values via pastel shades (default 127)&#xa;    :return: a tuple containing:&#xa;             event_state_labels - a set of strings containing the labels for event states given an integer&#xa;             item_number_label - a set of strings containing the labels for items given an integer&#xa;             item_label_filenames - a set of strings containing the filename for JPGs containing the images of items&#xa;             given an integer&#xa;             cols - a set of RGB colors influenced by the input pastel_factor representing the item colors&#xa;    """"""&#xa;    event_state_labels = ['stationary', 'up', 'down']&#xa;    item_number_label = ['bottle', 'icecubetray', 'clover', 'basketball', 'boot', 'crown', 'bandana', 'hammer',&#xa;                         'fireext', 'guitar']&#xa;    item_label_filename = ['bottle.jpg', 'icecubetray.jpg', 'clover.jpg', 'basketball.jpg',&#xa;                           'boot.jpg', 'crown.jpg', 'bandana.jpg', 'hammer.jpg',&#xa;                           'fireextinguisher.jpg', 'guitar.jpg']&#xa;&#xa;    cols = [(255, 255, pastel_factor), (255, 255, pastel_factor),&#xa;            (255, pastel_factor, pastel_factor), (255, pastel_factor, pastel_factor),&#xa;            (pastel_factor, 255, pastel_factor), (pastel_factor, 255, pastel_factor),&#xa;            (pastel_factor, pastel_factor, 255),&#xa;            (pastel_factor, pastel_factor, 255),&#xa;            (128, pastel_factor / 2, 128), (128, pastel_factor / 2, 128)]&#xa;    return event_state_labels, item_number_label, item_label_filename, cols&#xa;"
4939734|"#!/usr/bin/env python&#xa;#try:&#xa;try:&#xa;	import gi, loggy, player&#xa;	gi.require_version('Gst', '1.0')&#xa;	from gi.repository import GObject, Gst, Gtk, GdkPixbuf, GdkX11, GstVideo&#xa;except:&#xa;	loggy.warn('Gui could not import required libraries')&#xa;# import soundblizzard&#xa;# import player, loggy, gst, cairo, aspectimage&#xa;# from gi.repository import Gtk&#xa;# from gi.repository import GdkPixbuf&#xa;# from gi.repository import GdkX11&#xa;# from gi.repository import GObject&#xa;#TODO: look at kiwi for pygtk lists etc.&#xa;#pyGtk.require(""2.0"")&#xa;#except:&#xa;#	print ""gui - Required libraries not found - pyGtk, Gtk, player, loggy, gst, sbdb! Please install\n""&#xa;#TODO: while gtk.gtk_events_pending(): gtk.gtk_main_iteration() - does this work?&#xa;class GTKGui(object):&#xa;	def __init__(self, sb):&#xa;		loggy.log('Gui loading...')&#xa;		#self.play_buttons = []&#xa;		#self.progress_bars    = []&#xa;		#self.position_labels    = []&#xa;		self.volume_scales =[]&#xa;		#self.info_labels = []&#xa;		self.album_arts = []&#xa;		self.main_trees = []&#xa;		self.slave_windows = []&#xa;		self.main_tree_modes = {}&#xa;		self.WIDGETS = 3&#xa;&#xa;&#xa;		self.widgets={'consume_toggles':[],'repeat_toggles':[],'single_toggles':[],'random_toggles':[],'play_buttons':[], 'progress_bars':[], 'position_labels':[], 'info_labels':[], 'fullscreen_widgets':[]}&#xa;		#self.sb = soundblizzard.soundblizzard # fakes for tab completion&#xa;		self.sb = sb&#xa;		self.sb.player.connect('async-done', self.on_async_done)&#xa;		self.master_tree_load()&#xa;&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file(""glade/gui.glade"")&#xa;		self.builder.connect_signals(self)&#xa;		#widgets = self.builder.get_objects()&#xa;		self.window = self.builder.get_object(""window1"")&#xa;		self.window.set_title(""SoundBlizzard"")&#xa;		self.window.connect('delete-event', Gtk.main_quit)&#xa;		#pixbuf = GdkPixbuf.Pixbuf.new_from_file('/home/sam/Code/Eclipse workspace/soundblizzard/logo.png')&#xa;		self.window.set_icon_from_file('logo.png')&#xa;		self.get_widgets('window')&#xa;		for window in self.widgets['window']:&#xa;			window.show()&#xa;			self.window.show()&#xa;		#self.window.fullscreen() #TODO: fullscreen&#xa;&#xa;		#self.get_widgets('albumartdrawingarea')&#xa;		#for alb in self.widgets['albumartdrawingarea']:&#xa;			#self.album_arts.append(alb)&#xa;			#print alb.window&#xa;			#self.sb.player.videosink.set_xwindow_id(alb.window.xid)&#xa;			#print(alb.get_window_xid())&#xa;&#xa;		#self.sb.player.on_update_play_state.append(self.on_play_state_change)&#xa;		#self.sb.player.on_update_volume.append(self.on_volume_change)&#xa;		#self.sb.player.on_update_tags.append(self.on_update_tags)&#xa;		self.sb.player.connect('async-done', self.on_update_tags)&#xa;		self.sb.player.connect('volume-changed', self.on_volume_change)&#xa;		self.sb.player.connect('position-changed', self.on_position_change)&#xa;		self.sb.player.connect('play-state-changed', self.on_play_state_change)&#xa;	def debug(self, data=None):&#xa;		print ('debug got')&#xa;		print (data)&#xa;&#xa;	def get_widgets(self, name):&#xa;		'''Searches Gtkbuilder for widgets with name1, name2, name3 etc and adds them to self.widgets[name]'''&#xa;		self.widgets[name]=[]&#xa;		for x in range(self.WIDGETS):&#xa;			if self.builder.get_object(name + str(x)):&#xa;				self.widgets[name].append(self.builder.get_object(name + str(x)))&#xa;	def get_next(self, widget):&#xa;		self.sb.playlist.get_next()&#xa;	def get_prev(self, widget):&#xa;		self.sb.playlist.get_prev()&#xa;	def on_playbutton_click(self, widget):&#xa;		loggy.debug('gui.on_playbutton_click')&#xa;		widget.set_relief(Gtk.ReliefStyle.NONE)&#xa;		self.sb.player.playpause()&#xa;	def on_position_change(self, player):&#xa;		'''What to do when position change signal recieved'''&#xa;		#loggy.debug('gui.on_position_change')&#xa;		for progress_bar in self.widgets['progress_bars']:&#xa;			progress_bar.set_range(0, self.sb.player.durns)&#xa;			progress_bar.set_value(self.sb.player.posns)&#xa;		label = self.sb.player.posstr + ' / ' + self.sb.player.durstr&#xa;		for position_label in self.widgets['position_labels']:&#xa;			position_label.set_label(label)&#xa;	def on_play_state_change(self, player):&#xa;		#TODO: - make player only emit one signal for each event&#xa;		#state = self.sb.player.getstate()&#xa;		loggy.debug('gui.on_play_state_change ' + self.sb.player.state)&#xa;		if (self.sb.player.state == 'play'):&#xa;			for playbutton in self.widgets['play_buttons']:&#xa;				playbutton.set_label(Gtk.STOCK_MEDIA_PAUSE)&#xa;		elif (self.sb.player.state == 'pause' or self.sb.player.state == 'stop'):&#xa;			for playbutton in self.widgets['play_buttons']:&#xa;				playbutton.set_label(Gtk.STOCK_MEDIA_PLAY)&#xa;		else:&#xa;			loggy.warn('gui.on_play_state_change got unknown state: ' + self.sb.player.state)&#xa;	def is_play_button(self, widget):&#xa;		#print 'playbutton found'&#xa;		self.widgets['play_buttons'].append(widget)#TODO: check for duplicates&#xa;	def is_fullscreen_toggle(self,widget):&#xa;		self.widgets['fullscreen_widgets'].append(widget)&#xa;		widget.get_parent_window()&#xa;		widget.connect('toggled',self.on_fullscreen_toggle)&#xa;		#def change_fullscreen_toggle(self):&#xa;		#widget.get_parent_window().connect('window-state-event',self.on_window_state_event)&#xa;	def on_window_state_event(self):&#xa;		print (widget)&#xa;		print ('LoL')&#xa;	def on_fullscreen_toggle(self, widget):&#xa;		if (widget.get_label() == 'gtk-fullscreen'):&#xa;			widget.get_parent_window().fullscreen()&#xa;			widget.set_label('gtk-leave-fullscreen')&#xa;		else:&#xa;			widget.get_parent_window().unfullscreen()&#xa;			widget.set_label('gtk-fullscreen')&#xa;	def on_progress_bar_change_value (self, value_range, scroll, value, data=None):&#xa;		self.sb.player.setpos(value)&#xa;		#self.sb.player.player.seek_simple(Gst.Format.TIME, Gst.SeekFlags.FLUSH, value)&#xa;	def is_progress_bar(self, widget):&#xa;		#print 'progress bar found'&#xa;		self.widgets['progress_bars'].append(widget)&#xa;	def is_position_label(self, widget):&#xa;		self.widgets['position_labels'].append(widget)&#xa;	def gst_time_string(self, nanosecs):&#xa;		# This method was submitted by Sam Mason.&#xa;		# It's much shorter than the original one.&#xa;		s,ns = divmod(nanosecs, self.sb.player.SECOND)&#xa;		m,s = divmod(s, 60)&#xa;		if m < 60:&#xa;			return ""%02i:%02i"" %(m,s)&#xa;		else:&#xa;			h,m = divmod(m, 60)&#xa;			return ""%i:%02i:%02i"" %(h,m,s)&#xa;	def is_volume_scale(self, widget):&#xa;		self.volume_scales.append(widget)&#xa;		widget.set_adjustment(Gtk.Adjustment(value=self.sb.player.vol, lower=0, upper=100, step_incr=5, page_incr=10, page_size=0))&#xa;		widget.connect('value-changed', self.on_volume_scale_change)&#xa;		#widget.set_value(self.sb.player.getvol())&#xa;		#widget.set_from_icon_name(Gtk.STOCK_OPEN, 36)&#xa;	def on_volume_change(self, caller):&#xa;		for volume_scale in self.volume_scales:&#xa;			if (int(round(volume_scale.get_value()))!=self.sb.player.vol): &#xa;				volume_scale.set_value(self.sb.player.vol)&#xa;	def on_volume_scale_change(self, widget, value):&#xa;		if (int(round(value)) == self.sb.player.vol):&#xa;			return True&#xa;		self.sb.player.setvol(int(round(value)))&#xa;	def is_info_label(self, widget):&#xa;		self.widgets['info_labels'].append(widget)&#xa;	def is_single_toggle(self, widget):&#xa;		self.widgets['single_toggles'].append(widget)&#xa;		self.single_toggle_update(self.sb.playlist.single, self.sb.playlist.single.get(), widget)&#xa;		widget.connect('toggled', self.on_single_toggle)&#xa;		self.sb.playlist.single.connect('changed', self.single_toggle_update, widget)&#xa;	def on_single_toggle(self, widget):&#xa;		if self.sb.playlist.single.get() != widget.get_active():&#xa;			self.sb.playlist.single.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	#TODO: combine this into one function not four&#xa;	def single_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_consume_toggle(self, widget):&#xa;		self.widgets['consume_toggles'].append(widget)&#xa;		self.consume_toggle_update(self.sb.playlist.consume, self.sb.playlist.consume.get(), widget)&#xa;		widget.connect('toggled', self.on_consume_toggle)&#xa;		self.sb.playlist.consume.connect('changed', self.consume_toggle_update, widget)&#xa;	def on_consume_toggle(self, widget):&#xa;		if self.sb.playlist.consume.get != widget.get_active():&#xa;			self.sb.playlist.consume.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def consume_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_repeat_toggle(self, widget):&#xa;		self.widgets['repeat_toggles'].append(widget)&#xa;		self.repeat_toggle_update(self.sb.playlist.repeat, self.sb.playlist.repeat.get(), widget)&#xa;		widget.connect('toggled', self.on_repeat_toggle)&#xa;		self.sb.playlist.repeat.connect('changed', self.repeat_toggle_update, widget)&#xa;	def on_repeat_toggle(self, widget):&#xa;		if self.sb.playlist.repeat.get() != widget.get_active():&#xa;			self.sb.playlist.repeat.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def repeat_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def is_random_toggle(self, widget):&#xa;		self.widgets['random_toggles'].append(widget)&#xa;		self.random_toggle_update(self.sb.playlist.random, self.sb.playlist.random.get(), widget)&#xa;		widget.connect('toggled', self.on_random_toggle)&#xa;		self.sb.playlist.random.connect('changed', self.random_toggle_update, widget)&#xa;	def on_random_toggle(self, widget):&#xa;		if self.sb.playlist.random.get() != widget.get_active():&#xa;			self.sb.playlist.random.set(widget.get_active())&#xa;		loggy.log ('toggle button ' + str(widget.get_active()))&#xa;	def random_toggle_update(self, toggle, value, widget):&#xa;		if widget.get_active() != value:&#xa;			widget.set_active(value) &#xa;	def on_async_done(self, player):&#xa;		loggy.debug('gui.on_async_done')&#xa;		self.on_update_tags()&#xa;	def on_update_tags(self, *player):&#xa;		text = ''&#xa;		if 'title' in self.sb.player.tags: #TODO: do this after async done, not every time&#xa;			text += self.sb.player.tags['title']&#xa;		if 'artist' in self.sb.player.tags:&#xa;			text += '\n by ' + self.sb.player.tags['artist']&#xa;		if 'album' in self.sb.player.tags:&#xa;			text += ' from ' + self.sb.player.tags['album'] #TODO: make font italic&#xa;		#print text&#xa;		for label in self.widgets['info_labels']:&#xa;			label.set_label(text)&#xa;		#TODO: - get this on timer not on async&#xa;		if 'image' in self.sb.player.tags:&#xa;			filename = '/home/sam/.temp.img'#TODO: replace file to temp file or better interface&#xa;			img = open(filename, 'w')&#xa;			img.write(self.sb.player.tags['image'])&#xa;			img.close()&#xa;			for album_art in self.album_arts:&#xa;				pass&#xa;				#album_art.connect('draw', self.draw_album_art, album_art)&#xa;				#cairo.ImageSurface.create_from_png(filename)&#xa;				#album_art.set_from_file(file)&#xa;				#self.on_image_resize(album_art, None)&#xa;#    def draw_album_art(self, widget):&#xa;#        print (widget + 'fart')&#xa;&#xa;	def is_album_art(self, widget):&#xa;		#image = aspectimage.AspectImage('logo.png')&#xa;		image = Gtk.Image()&#xa;		image.set_from_file('logo.png')&#xa;		widget.pack_start(image,True, True, 0)&#xa;		widget.show_all()&#xa;		#widget.add(art)&#xa;		widget.show()&#xa;		self.album_arts.append(widget)&#xa;		#print('got image '+str(widget))&#xa;		#widget.set_from_file('logo16.png')&#xa;		&#xa;		#self.on_image_resize(widget, None)&#xa;#	def redraw_album_art(self, widget, event):&#xa;#		print 'image redraw'&#xa;#		x , y, width, height = event.area&#xa;#		file = self.sbdb.config.get('Main', 'imagefile') #TODO: - does this result in file read?&#xa;#		pixbuf = Gtk.gdk.pixbuf_new_from_file_at_size(file, width , height)&#xa;#		widget.draw_pixbuf(widget.get_style().fg_gc[STATE_NORMAL],pixbuf, x, y, x, y, width, height, Gtk.gdk.RGB_DITHER_NONE, 0 , 0)&#xa;#&#xa;	def on_image_resize(self, widgetty, event): #TODO: on_image_resize&#xa;		None&#xa;#        print 'image resize'&#xa;#        #src_width, src_height = widget.get_pixmap().get_width(), widget.get_pixmap().get_height()&#xa;#        #allocation = widget.get_allocation()# thanks to http://stackoverflow.com/questions/4939734/automatic-image-scaling-on-resize-with-pyGtk&#xa;#        #ben = widget.get_pixbuf()&#xa;#        for widget in self.album_arts:&#xa;#            file = '/home/sam/temp.img' #TODO: - does this result in file read?&#xa;#            pixbuf = Gtk.gdk.pixbuf_new_from_file_at_size(file,widget.allocation.height , widget.allocation.width)&#xa;#            #pixbuf = widget.get_pixbuf().scale_simple(widget.allocation.width, widget.allocation.height, Gtk.gdk.INTERP_BILINEAR)&#xa;#            widget.set_from_pixbuf(pixbuf)&#xa;	def is_video_out(self, widget):&#xa;		loggy.debug('is_video_out')&#xa;		self.sb.player.vidout['xid'] = widget.get_property('window').get_xid() # don't forget to import GdkX11!&#xa;		self.sb.player.videosink.set_xwindow_id(self.sb.player.vidout['xid'])&#xa;		#print (widget)&#xa;		#self.sb.player.add_vid(widget.window.xid)&#xa;	def is_master_tree(self, widget):&#xa;		self.main_trees.append(widget)&#xa;		#self.main_tree_load()&#xa;		widget.set_model(self.main_tree_store)&#xa;		widget.tv_column = Gtk.TreeViewColumn('Spambox')&#xa;		widget.append_column(widget.tv_column)&#xa;		widget.cell = Gtk.CellRendererText()&#xa;		widget.tv_column.pack_start(widget.cell, True)&#xa;		widget.tv_column.add_attribute(widget.cell, 'text', 0)&#xa;		widget.set_reorderable(True)&#xa;		widget.connect('cursor-changed', self.master_tree_cursor_changed)&#xa;	def master_tree_cursor_changed(self, widget):&#xa;		#TODO: set position in all trees&#xa;		loggy.debug('gui.master_tree_cursor_changed')&#xa;		(model, iterat) = widget.get_selection().get_selected()&#xa;		#print self.slave_windows[0].get_children()&#xa;		try:&#xa;			self.slave_view.destroy()&#xa;		except:&#xa;			None&#xa;&#xa;		if iter:&#xa;			self.main_tree_modes[model.get_value(iterat,0)]['open_func']()&#xa;	def master_tree_add(self, name, open_func):&#xa;		self.main_tree_store.append(None, [name])&#xa;		self.main_tree_modes[name] = {'open_func' : open_func}&#xa;&#xa;	def master_tree_load(self):&#xa;		self.main_tree_store = Gtk.TreeStore(str)&#xa;		self.master_tree_add('Now Playing', self.slave_enter_now_playing_view)&#xa;		self.master_tree_add('Media', self.slave_enter_media_view)&#xa;		self.master_tree_add('Preferences', self.slave_enter_preferences_view)&#xa;		self.master_tree_playlist_iter = self.main_tree_store.append(None, ['Playlists'])&#xa;		for playlist in self.sb.playlist.playlists.keys():&#xa;			self.main_tree_store.append(self.master_tree_playlist_iter, [playlist])&#xa;		self.sb.player.connect('new-playlist', self.master_tree_new_playlist)&#xa;		self.sb.player.connect('deleted-playlist', self.master_tree_deleted_playlist)&#xa;	def master_tree_deleted_playlist(self,player, name):&#xa;		while name in self.main_tree_store[self.master_tree_playlist_iter]: self.main_tree_store[self.master_tree_playlist_iter].remove(name)&#xa;		&#xa;		return True #What a line of code!&#xa;	def master_tree_new_playlist(self,player, name):&#xa;		if name in self.main_tree_store[self.master_tree_playlist_iter][0]:&#xa;			return True&#xa;		else:&#xa;			self.main_tree_store.append(self.master_tree_playlist_iter, [name])&#xa;			return True&#xa;		&#xa;		&#xa;#TODO: connect signals other than map automatically&#xa;	def is_slave_area(self, widget):&#xa;		loggy.debug('gui.is_slave_area')&#xa;		self.slave_windows.append(widget) #TODO: allow multiple separate master/slave combos&#xa;	def slave_enter_media_view(self):&#xa;		loggy.debug('gui.slave_enter_media_view')&#xa;		#TODO: check slave_window is a hbox&#xa;		self.slave_view = GTK_media_view(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;	def slave_enter_now_playing_view(self):&#xa;		self.slave_view = GTK_now_playing(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;		loggy.debug('gui.slave_enter_now_playing_view')&#xa;	def slave_enter_preferences_view(self):&#xa;		self.slave_view = GTK_preferences(self.sb)&#xa;		self.slave_windows[0].pack_start(self.slave_view, True, True,0)&#xa;		self.slave_windows[0].show_all()&#xa;		self.builder.connect_signals(self)&#xa;class GTK_media_view(Gtk.HBox):&#xa;	def __init__(self, sb):&#xa;		self.sb = sb&#xa;		self.gui = sb.gtkgui&#xa;		Gtk.HBox.__init__(self) # load glade&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file('glade/media_view.glade')&#xa;		widget = self.builder.get_object(""vbox1"")&#xa;		widget.reparent(self)&#xa;		self.builder.connect_signals(self)&#xa;		self.keystoshow = ('artist', 'title', 'album', 'date', 'genre', 'duration', 'rating','mimetype', 'atime', 'mtime', 'ctime', 'dtime', 'size')&#xa;		self.keystoshowdict = {} # dic of name, col position&#xa;		for i, a in enumerate(self.keystoshow):&#xa;			self.keystoshowdict[a] = i&#xa;		#print self.keystoshowdict&#xa;		#print 'GOOB' + str( player.sbdb.keytypelist )&#xa;		#creates list store with as many string columns as there are keys for&#xa;		#print player.sbdb.get_uri_db_info(""file:///home/sam/Music/POPCORN.MP3"")&#xa;		#self.list_store.append(self.sb.sbdb.get_uri_db_info(""file:///home/sam/Music/POPCORN.MP3""))&#xa;&#xa;		arsy = (GObject.TYPE_STRING,)*len(self.sb.sbdb.keys)&#xa;		self.list_store = Gtk.ListStore(*arsy)&#xa;		print( ' row length = ' + str(self.list_store.get_n_columns()))&#xa;		self.sb.sbdb.iter(self.list_store.append)&#xa;	def is_listview(self, widget):&#xa;		loggy.debug('gui.GTK_media_view.is_listview')&#xa;		self.treeview = widget&#xa;		widget.connect('row-activated', self.treeview_activated)&#xa;		widget.set_model(self.list_store) # init treeview&#xa;&#xa;		widget.tv_columns = {}&#xa;		for i, name in enumerate(self.sb.sbdb.keys): # go through all keys&#xa;			if name in self.keystoshowdict: #check if column is to display&#xa;				widget.tv_columns[name] = Gtk.TreeViewColumn(name)&#xa;				widget.insert_column(widget.tv_columns[name], self.keystoshowdict[name]) # inserts column in order from keystoshow&#xa;				widget.tv_columns[name].cell = Gtk.CellRendererText()&#xa;				widget.tv_columns[name].pack_start(widget.tv_columns[name].cell, True)&#xa;				widget.tv_columns[name].add_attribute(widget.tv_columns[name].cell, 'text', i)&#xa;				widget.tv_columns[name].set_resizable(True)&#xa;				#widget.tv_columns[name].set_clickable(True)&#xa;				widget.tv_columns[name].connect('clicked', self.tv_clicked, i)&#xa;				#widget.tv_columns[name].set_sort_indicator(True)&#xa;		widget.columns_autosize()&#xa;		#widget.set_headers_clickable(True)&#xa;		#widget.tv_column = Gtk.TreeViewColumn('Spambox')&#xa;		#widget.append_column(widget.tv_column)&#xa;		#widget.cell = Gtk.CellRendererText()&#xa;		#widget.tv_column.pack_start(widget.cell, True)&#xa;		#widget.tv_column.add_attribute(widget.cell, 'text', 0)&#xa;		#widget.set_reorderable(True)&#xa;	def treeview_activated(self, treeview, path, view_column):&#xa;		loggy.debug('gui.GTK_media_view.treeview_activated')&#xa;		(model, iterat) = treeview.get_selection().get_selected()&#xa;		if iterat:&#xa;			self.sb.playlist.load_uri(self.list_store.get_value(iterat,len(self.sb.sbdb.keys)-2))&#xa;			self.sb.playlist.playlist = [self.list_store.get_value(iterat,len(self.sb.sbdb.keys)-2)]&#xa;	def tv_clicked(self, widget, i):&#xa;		loggy.debug('gui.GTK_media_view.tv_clicked')&#xa;		widget.set_sort_column_id(i)&#xa;		#prevorder = widget.get_sort_order()&#xa;		#if prevorder = Gtk.SORT_ASCENDING:&#xa;		#	widget.set_sort_order(Gtk.SORT_DESCENDING)&#xa;		#else:&#xa;		#	widget.set_sort_order(Gtk.SORT_ASCENDING)&#xa;&#xa;class GTK_preferences(Gtk.HBox): # thanks to http://stackoverflow.com/questions/2129369/Gtk-builder-and-multiple-glade-files-breaks&#xa;	def __init__(self, sb):&#xa;		self.sb = sb&#xa;		Gtk.HBox.__init__(self)&#xa;		self.builder = Gtk.Builder()&#xa;		self.builder.add_from_file('glade/preferences.glade')&#xa;		some_widget = self.builder.get_object(""notebook1"")&#xa;		some_widget.reparent(self)&#xa;		self.folderview = self.builder.get_object(""treeview1"")&#xa;		self.builder.connect_signals(self)&#xa;		self.folderstore = Gtk.ListStore(str)&#xa;		for folder in self.sb.config.config['libraryfolders']:&#xa;			self.folderstore.append([folder])&#xa;		self.folderview.set_model(self.folderstore)&#xa;		renderer = Gtk.CellRendererText()&#xa;		column = Gtk.TreeViewColumn(""Folders"", renderer, text=0)&#xa;		self.folderview.append_column(column)&#xa;		True&#xa;		#self.show_all()&#xa;		#self.add(some_widget)&#xa;		#some_widget.show()&#xa;		&#xa;#	def __destroy__(self):&#xa;#		loggy.log('GTK_media_view destroyed')&#xa;#		self.builder.destroy()&#xa;	def on_button4_clicked(self, button):&#xa;		loggy.debug('GTK_media_view.on_button4_clicked')&#xa;		self.sb.sbdb.recreate_db()&#xa;		True&#xa;	def on_button3_clicked(self, button):&#xa;		loggy.debug('GTK_media_view.on_button3_clicked')&#xa;		self.sb.sbdb.update_db()&#xa;		True&#xa;	def on_button1_clicked(self, button):&#xa;		Folderbox = Gtk.FileChooserDialog(""Please select a folder containing media"", self.sb.gtkgui.window, Gtk.FileChooserAction.SELECT_FOLDER, (Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL, Gtk.STOCK_OK, Gtk.ResponseType.OK))&#xa;		Folderbox.set_default_size(800,400)&#xa;		Folderbox.set_select_multiple(True)&#xa;		Folderbox.set_local_only(False)&#xa;		&#xa;		response = Folderbox.run()&#xa;		if response == Gtk.ResponseType.OK:&#xa;			loggy.log(""Gui adding media folder: "" + str(Folderbox.get_filenames()))&#xa;			self.sb.config.config['libraryfolders'] = self.sb.config.config['libraryfolders'] + Folderbox.get_filenames()&#xa;			self.sb.config.save_config()&#xa;			for folder in Folderbox.get_filenames():&#xa;				self.folderstore.append([folder])			&#xa;		Folderbox.destroy()&#xa;		True&#xa;	def on_button2_clicked(self, button):&#xa;		selection = self.folderview.get_selection()&#xa;		model, treeiter = selection.get_selected()&#xa;		if treeiter != None:&#xa;			loggy.log (""Removing "" + model[treeiter][0] + ""from folderlist"")&#xa;			self.sb.config.config['libraryfolders'].remove(model[treeiter][0])&#xa;			self.folderstore.remove(treeiter)&#xa;			self.sb.config.save_config()&#xa;			#now redo list store&#xa;		True&#xa;class GTK_now_playing(Gtk.DrawingArea):&#xa;	def __init__(self,sb):&#xa;		#print (""in the monkey"")&#xa;		Gtk.DrawingArea.__init__(self)&#xa;		#sb.gtkgui.is_video_out(self)&#xa;		True&#xa;&#xa;if __name__ == ""__main__"":&#xa;	temp = ''&#xa;	player1 = player.player&#xa;	#sbdb1 = sbdb.sbdb()&#xa;	temp.player = player1&#xa;	loggy.debug_setting = True&#xa;	app = GTKGui(temp)&#xa;	Gtk.main()&#xa;&#xa;"
16834861|"# Copyright (c) 2015 William Lees&#xa;&#xa;# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated&#xa;# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the&#xa;# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit&#xa;# persons to whom the Software is furnished to do so, subject to the following conditions:&#xa;&#xa;# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the&#xa;# Software.&#xa;&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE&#xa;# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR&#xa;# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR&#xa;# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;&#xa;&#xa;# Create just enough of a database file to work with Change-o's DefineClones&#xa;&#xa;__author__ = 'William Lees'&#xa;__docformat__ = ""restructuredtext en""&#xa;&#xa;import os&#xa;import sys&#xa;import argparse&#xa;import csv&#xa;import re&#xa;import numpy as np&#xa;import matplotlib.pyplot as plt&#xa;import matplotlib.colors as mcolors&#xa;import matplotlib.mlab as mlab&#xa;import itertools&#xa;&#xa;def main(argv):&#xa;    parser = argparse.ArgumentParser(description='Read an IgBlastPlus file and plot the contained CDR3 lengths.')&#xa;    parser.add_argument('infiles', help='input files, separated by commas (IgBLASTPlus format, nt or aa)')&#xa;    parser.add_argument('-b', '--barcolour', help='colour or list of colours for bars')&#xa;    parser.add_argument('-c', '--cols', help='Number of columns for plot')&#xa;    parser.add_argument('-d', '--dupheader', help='Prefix for duplicate count, eg ""DUPCOUNT="" for Presto')&#xa;    parser.add_argument('-g', '--gradientfill', help='fill bars with a gradiented colour', action='store_true')    &#xa;    parser.add_argument('-gh', '--grid_horizontal', help='horizontal grid lines', action='store_true')&#xa;    parser.add_argument('-gv', '--grid_vertical', help='vertical grid lines every n bars')&#xa;    parser.add_argument('-ga', '--gauss', help='plot best-fit Gaussian distribution', action='store_true')    &#xa;    parser.add_argument('-s', '--save', help='Save output to file (as opposed to interactive display)')&#xa;    parser.add_argument('-sz', '--size', help='Figure size (x,y)')&#xa;    parser.add_argument('-t', '--titles', help='titles for each plot, separated by commas')&#xa;    parser.add_argument('-u', '--unique', help='only count unique sequences', action='store_true')&#xa;    parser.add_argument('-w', '--width', help='relative bar width (number between 0 and 1)')&#xa;    parser.add_argument('-xmax', '--xmax', help='Max x-value to use on all charts')&#xa;    parser.add_argument('-xmin', '--xmin', help='Min x-value to use on all charts')&#xa;    parser.add_argument('-y', '--ymax', help='Max y-value to use on all charts')&#xa;    args = parser.parse_args()&#xa;    infiles = args.infiles.split(',')&#xa;    titles = args.titles.split(',') if args.titles else [""""]&#xa;    ncols = int(args.cols) if args.cols else 1&#xa;    xmin = int(args.xmin) if args.xmin else 1&#xa;    xmax = int(args.xmax) if args.xmax else None&#xa;    ymax = float(args.ymax) if args.ymax else None&#xa;    outfile = args.save if args.save else None&#xa;    dupheader = args.dupheader&#xa;    bar_width = float(args.width) if args.width else 1.0&#xa;    mapcolour = args.barcolour if args.barcolour else 'blue'&#xa;    mapcolour = mapcolour.split(',')&#xa;    grid_vertical = int(args.grid_vertical) if args.grid_vertical else False&#xa;    gauss = args.gauss&#xa;&#xa;    nrows = len(infiles) / ncols&#xa;    if len(infiles) % ncols != 0:&#xa;        nrows += 1&#xa;    (sizex, sizey) = args.size.split(',') if args.size else (8*ncols,4*nrows)&#xa;&#xa;    lengths = []&#xa;    for infile in infiles:&#xa;        lengths.append(determine_stats(dupheader, infile, args.unique))&#xa;        &#xa;    if not outfile or len(outfile) < 5 or outfile[-4:] != '.csv':&#xa;        plt.figure(figsize=(float(sizex),float(sizey)))&#xa;        plot_number = 1&#xa;        plotted = False&#xa;        for (length, title, colour) in zip(lengths, itertools.cycle(titles), itertools.cycle(mapcolour)):&#xa;            if length is not None:&#xa;                plot_file(length, xmin, xmax, ymax, title, nrows, ncols, plot_number, colour, bar_width, args.gradientfill, args.grid_horizontal, grid_vertical, gauss)&#xa;                plot_number += 1&#xa;                plotted = True&#xa;        if not plotted:&#xa;            quit()&#xa;&#xa;        plt.tight_layout()&#xa;        if outfile:&#xa;            plt.savefig(outfile)&#xa;        else:&#xa;            plt.show()&#xa;    else:&#xa;        minlength = 9999&#xa;        maxlength = 0&#xa;        for length in lengths:&#xa;            if length is not None:&#xa;                maxlength = max(maxlength, max([i for i in range(len(length)) if length[i] > 0]))&#xa;                minlength = min(minlength, min([i for i in range(len(length)) if length[i] > 0]))&#xa;&#xa;        with open(outfile, 'wb') as fo:&#xa;            writer = csv.writer(fo)&#xa;            writer.writerow(['Length'] + range(minlength, maxlength))&#xa;            ititle = iter(titles) if titles else iter(infiles)&#xa;            for length in lengths:&#xa;                if length is not None:&#xa;                    writer.writerow([next(ititle)] + list(length[minlength:maxlength]))&#xa;&#xa;&#xa;def plot_file(lengths, xmin, xmax, ymax, title, nrows, ncols, plot_number, mapcolour, bar_width, gradientfill, grid_horizontal, grid_vertical, gauss):&#xa;    if not xmax:&#xa;        for xmax in range(len(lengths)-1, 0, -1):&#xa;            if lengths[xmax] > 0:&#xa;                break&#xa;    &#xa;    ax = plt.subplot(nrows, ncols, plot_number)&#xa;    ax.tick_params(direction='out', top=False, right=False)&#xa;&#xa;    if title != """":&#xa;        plt.xlabel(title)&#xa;&#xa;    bar_pos = np.arange(xmax + 1)&#xa;    &#xa;    # if xmax is set, all the x axes will be the same, so only put labels on the bottom row&#xa;    # if we have a vertical grid, label just one point in each column&#xa;    &#xa;    if xmax is None or plot_number > (nrows-1)*ncols:&#xa;        if grid_vertical:&#xa;            labels = []&#xa;            if grid_vertical % 2 == 0:&#xa;                midpoint = grid_vertical/2&#xa;            else:&#xa;                midpoint = int(grid_vertical/2)&#xa;            for tick in bar_pos:&#xa;                if tick % grid_vertical == midpoint:&#xa;                    labels.append(str(tick))&#xa;                else:&#xa;                    labels.append(' ')&#xa;        else:&#xa;            labels = ['%d' % x for x in bar_pos]&#xa;    else:&#xa;        labels = []&#xa;            &#xa;    plt.xticks(bar_pos+0.5, labels)&#xa;&#xa;    if ymax:&#xa;        plt.ylim(0, ymax)&#xa;    if xmax or xmin != 0:&#xa;        plt.xlim(xmin, xmax)&#xa;&#xa;    if bar_width < 1.:&#xa;        bar_pos = bar_pos + (1 - bar_width) / 2.&#xa;&#xa;    if grid_horizontal:&#xa;        plt.grid(which='major', axis='y', c='black', linestyle='-', alpha=0.6, zorder=1)&#xa;&#xa;    if gradientfill:&#xa;        gbar(bar_pos, lengths, mapcolour, width=bar_width)&#xa;    else:&#xa;        plt.bar(bar_pos, lengths[:xmax+1], width=bar_width, color=mapcolour, zorder=10)&#xa;&#xa;    if grid_vertical:&#xa;        pos = 0&#xa;        while pos < xmax:&#xa;            ymin, ymax = plt.ylim()&#xa;            plt.plot([bar_pos[pos] - (1 - bar_width)/2, bar_pos[pos] - (1 - bar_width)/2], [ymin, ymax], c='black', linestyle='-', alpha=0.6, zorder=1)&#xa;            pos += grid_vertical&#xa;            &#xa;    # Remove every other y label because we get far too many by default&#xa;    &#xa;    locs, labels = plt.yticks()&#xa;    newlocs = []&#xa;    newlabels = []&#xa;    &#xa;    for i in range(0, len(labels)):&#xa;        if i % 2 != 0:&#xa;            newlocs.append(locs[i])&#xa;            newlabels.append(str(int(locs[i])))&#xa;            &#xa;    plt.yticks(newlocs, newlabels)&#xa;            &#xa;    if gauss:&#xa;        values = []&#xa;        for i in range(len(lengths)-1):&#xa;            if lengths[i] > 0:&#xa;                foo = [i]*lengths[i]&#xa;                values += ([i] * lengths[i])&#xa;        values = np.array(values)&#xa;        mean = np.mean(values)&#xa;        variance = np.var(values)&#xa;        sigma = np.sqrt(variance)&#xa;        x = np.linspace(min(values), max(values), 100)&#xa;        plt.plot(x, mlab.normpdf(x, mean, sigma)*len(values), zorder=20)&#xa;&#xa;    ax.set_aspect('auto')&#xa;    plt.tight_layout()&#xa;&#xa;&#xa;def gbar(x, y, mapcolour, width=1, bottom=0):&#xa;    X = [[.6, .6], [.7, .7]]&#xa;    c = mcolors.ColorConverter().to_rgb&#xa;    cm = make_colormap([c('white'), c(mapcolour)])&#xa;    for left, top in zip(x, y):&#xa;        if top != bottom:&#xa;            right = left + width&#xa;            plt.imshow(X, interpolation='bicubic', cmap=cm, extent=(left, right, bottom, top), alpha=1, zorder=10)&#xa;            plt.plot([left, left], [bottom, top], color='black', linestyle='-', zorder=20)&#xa;            plt.plot([right, right], [bottom, top], color='black', linestyle='-', zorder=20)&#xa;            plt.plot([right, left], [top, top], color='black', linestyle='-', zorder=20)&#xa;&#xa;&#xa;# From http://stackoverflow.com/questions/16834861/create-own-colormap-using-matplotlib-and-plot-color-scale&#xa;def make_colormap(seq):&#xa;    """"""Return a LinearSegmentedColormap&#xa;    seq: a sequence of floats and RGB-tuples. The floats should be increasing&#xa;    and in the interval (0,1).&#xa;    """"""&#xa;    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]&#xa;    cdict = {'red': [], 'green': [], 'blue': []}&#xa;    for i, item in enumerate(seq):&#xa;        if isinstance(item, float):&#xa;            r1, g1, b1 = seq[i - 1]&#xa;            r2, g2, b2 = seq[i + 1]&#xa;            cdict['red'].append([item, r1, r2])&#xa;            cdict['green'].append([item, g1, g2])&#xa;            cdict['blue'].append([item, b1, b2])&#xa;    return mcolors.LinearSegmentedColormap('CustomMap', cdict)&#xa;&#xa;&#xa;def determine_stats(dupheader, infile, unique):&#xa;    seen = {}&#xa;    with open(infile, 'r') as fi:&#xa;        ln = fi.readline()&#xa;        sep = (""\t"" if ""\t"" in ln else "","")&#xa;        fi.seek(0)&#xa;        reader = csv.DictReader(fi, delimiter=sep)&#xa;        lengths = np.zeros(600)&#xa;        first_row = True&#xa;        seen_data = False&#xa;        for row in reader:&#xa;            if first_row:&#xa;                for f in ('Functionality', 'CDR3-IMGT', 'Sequence ID'):&#xa;                    if f not in row:&#xa;                        print 'Error: required field %s not in %s.' % (f, infile)&#xa;                        quit()&#xa;                first_row = False&#xa;            if 'unproductive' not in row['Functionality'] and row['CDR3-IMGT'] and (&#xa;                not unique or row['CDR3-IMGT'] not in seen):&#xa;                lengths[len(row['CDR3-IMGT'])] += (get_size(row['Sequence ID'], dupheader) if dupheader else 1)&#xa;                seen[row['CDR3-IMGT']] = 1&#xa;                seen_data = True&#xa;&#xa;    if not seen_data:&#xa;        print 'Warning: no functional CDR3 records found in file %s.' % infile&#xa;        return None&#xa;&#xa;    return lengths&#xa;&#xa;&#xa;# Find duplicate size, or return 1&#xa;&#xa;def get_size(s, dupheader):&#xa;    count = None&#xa;    if dupheader in s:&#xa;        spl = s.split(dupheader)&#xa;        for i in range(1, len(spl[1])):&#xa;            if spl[1][0:i].isdigit():&#xa;                count = int(spl[1][0:i])&#xa;            else:&#xa;                break&#xa;&#xa;    return count if count else 1    &#xa;&#xa;if __name__==""__main__"":&#xa;    main(sys.argv)&#xa;&#xa;"
21458387|"import time&#xa;&#xa;from django.db import transaction&#xa;from django.db.utils import IntegrityError&#xa;from django.test import TestCase&#xa;&#xa;from main.models import (&#xa;    User, UserFile, UserDir, Chunk, VersionChunk, Option, Storage,&#xa;    ChunkStorage,&#xa;)&#xa;from main.fs.clouds.base import BaseOAuth2APIClient&#xa;from main.fs.array import ArrayClient&#xa;&#xa;&#xa;class UserDirTestCase(TestCase):&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create(email='foo@bar.org')&#xa;&#xa;    def test_create(self):&#xa;        with self.assertRaises(ValueError):&#xa;            UserDir.objects.create(path='/foobar')&#xa;&#xa;        dir1 = UserDir.objects.create(path='/foobar', user=self.user)&#xa;        self.assertEqual('/foobar', dir1.path)&#xa;&#xa;        dir2 = UserDir.objects.create(path='/foobar/foo/bar', user=self.user)&#xa;        self.assertEqual('/foobar/foo/bar', dir2.path)&#xa;&#xa;        dir1.delete()&#xa;&#xa;&#xa;class UserFileTestCase(TestCase):&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create(email='foo@bar.org')&#xa;&#xa;    def test_create(self):&#xa;        with self.assertRaises(ValueError):&#xa;            UserFile.objects.create(path='/foo/bar')&#xa;&#xa;        dir1 = UserDir.objects.create(path='/foo', user=self.user)&#xa;        file1 = UserFile.objects.create(path='/foo/bar.txt', user=self.user)&#xa;&#xa;        self.assertEqual(dir1, UserDir.objects.get(uid=dir1.uid))&#xa;        self.assertEqual(file1, UserFile.objects.get(uid=file1.uid))&#xa;&#xa;        self.assertEqual('.txt', file1.extension)&#xa;&#xa;        # Two to account for ""root""&#xa;        self.assertEqual(2, UserDir.objects.all().count())&#xa;        self.assertEqual(1, UserFile.objects.all().count())&#xa;&#xa;        dir1.delete()&#xa;&#xa;        self.assertEqual(1, UserDir.objects.all().count())&#xa;        self.assertEqual(0, UserFile.objects.all().count())&#xa;&#xa;    def test_chunks(self):&#xa;        file = UserFile.objects.create(&#xa;            path='/foo/bar', user=self.user,&#xa;            parent=UserDir.objects.create(path='/foo', user=self.user))&#xa;&#xa;        chunk1 = Chunk.objects.create(size=1024, user=self.user)&#xa;        chunk2 = Chunk.objects.create(size=1024, user=self.user)&#xa;        versionchunk = file.file.version.add_chunk(chunk1)&#xa;&#xa;        self.assertEqual(1, versionchunk.serial)&#xa;&#xa;        storage = Storage.objects.create(&#xa;            user=self.user, type=Storage.TYPE_DROPBOX)&#xa;        ChunkStorage.objects.create(chunk=chunk1, storage=storage)&#xa;&#xa;        self.assertTrue(&#xa;            VersionChunk.objects.filter(&#xa;                version=file.file.version, chunk=chunk1).exists())&#xa;&#xa;        file.file.version.add_chunk(chunk2)&#xa;&#xa;        self.assertEqual(&#xa;            2, VersionChunk.objects.filter(version=file.file.version).count())&#xa;&#xa;        for i, chunk in enumerate(&#xa;            VersionChunk.objects.filter(&#xa;                version=file.file.version).order_by('serial')):&#xa;            self.assertEqual(i + 1, chunk.serial)&#xa;&#xa;        file.delete()&#xa;&#xa;&#xa;class UserTestCase(TestCase):&#xa;    def test_create(self):&#xa;        user = User.objects.create_user('foo@bar.org', full_name='Foo Bar')&#xa;        self.assertEqual(False, user.is_admin)&#xa;        self.assertEqual(False, user.is_staff)&#xa;        self.assertEqual('Foo', user.first_name)&#xa;&#xa;        # http://stackoverflow.com/questions/21458387/transactionmanagementerror-you-cant-execute-queries-until-the-end-of-the-atom  # noqa&#xa;        with transaction.atomic():&#xa;            with self.assertRaises(IntegrityError):&#xa;                User.objects.create_user('foo@bar.org')&#xa;&#xa;        superuser = User.objects.create_superuser('bar@foo.org', 'foobar',&#xa;                                                  full_name='Bar Foo')&#xa;        self.assertTrue(superuser.is_admin)&#xa;        self.assertTrue(superuser.is_staff)&#xa;        self.assertTrue(superuser.has_perm('any_perm'))&#xa;        self.assertTrue(superuser.has_module_perms('any_app'))&#xa;        self.assertEqual('Bar', superuser.first_name)&#xa;        self.assertEqual('Bar Foo', superuser.get_full_name())&#xa;        self.assertEqual('Bar', superuser.get_short_name())&#xa;&#xa;    def test_fail(self):&#xa;        with self.assertRaises(ValueError):&#xa;            User.objects.create_user('')&#xa;&#xa;    def test_options(self):&#xa;        user = User.objects.create_user('foo@bar.org', full_name='Foo Bar')&#xa;        options = Option.objects.create(user=user)&#xa;        self.assertEqual(1, options.raid_level)&#xa;        self.assertEqual(1, options.raid_replicas)&#xa;&#xa;&#xa;class UserStorageTestCase(TestCase):&#xa;    """"""&#xa;    Test user storage (usable storage instances).&#xa;    """"""&#xa;&#xa;    @classmethod&#xa;    def setUpTestData(cls):&#xa;        cls.user = User.objects.create_user('foo@bar.org',&#xa;                                            full_name='Foo Bar')&#xa;&#xa;    def test_oauth2(self):&#xa;        storage = Storage.objects.create(type=Storage.TYPE_DROPBOX,&#xa;                                         user=self.user)&#xa;&#xa;        client = storage.get_client()&#xa;        self.assertIsInstance(client, BaseOAuth2APIClient)&#xa;&#xa;        kwargs = {&#xa;            'access_token': 'AAAA',&#xa;            'refresh_token': 'BBBB',&#xa;            'expires_in': 10,&#xa;        }&#xa;        storage.auth.update(**kwargs)&#xa;        self.assertEqual('AAAA', storage.auth['access_token'])&#xa;        self.assertEqual('BBBB', storage.auth['refresh_token'])&#xa;        storage.auth.update({&#xa;            'access_token': 'CCCC',&#xa;            'expires_at': time.time()&#xa;        })&#xa;        self.assertEqual('CCCC', storage.auth['access_token'])&#xa;        self.assertEqual('BBBB', storage.auth['refresh_token'])&#xa;&#xa;    def test_array(self):&#xa;        array = Storage.objects.create(type=Storage.TYPE_ARRAY,&#xa;                                       user=self.user)&#xa;&#xa;        # Name should have usable default (is system-provided).&#xa;        self.assertIsNotNone(array.name)&#xa;&#xa;        obj = array.get_client()&#xa;        self.assertIsInstance(obj, ArrayClient)&#xa;&#xa;    def test_basic(self):&#xa;        storage = Storage.objects.create(&#xa;            type=Storage.TYPE_BASIC, user=self.user)&#xa;&#xa;        with self.assertRaises(NotImplementedError):&#xa;            storage.get_client()&#xa;"
136168|"#&#xa;# Licensed to the Apache Software Foundation (ASF) under one&#xa;# or more contributor license agreements.  See the NOTICE file&#xa;# distributed with this work for additional information&#xa;# regarding copyright ownership.  The ASF licenses this file&#xa;# to you under the Apache License, Version 2.0 (the&#xa;# ""License""); you may not use this file except in compliance&#xa;# with the License.  You may obtain a copy of the License at&#xa;#&#xa;#   http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing,&#xa;# software distributed under the License is distributed on an&#xa;# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY&#xa;# KIND, either express or implied.  See the License for the&#xa;# specific language governing permissions and limitations&#xa;# under the License.&#xa;#&#xa;&#xa;from __future__ import print_function&#xa;&#xa;import atexit as _atexit&#xa;import binascii as _binascii&#xa;import codecs as _codecs&#xa;import collections as _collections&#xa;import fnmatch as _fnmatch&#xa;import getpass as _getpass&#xa;import json as _json&#xa;import os as _os&#xa;import random as _random&#xa;import re as _re&#xa;import shlex as _shlex&#xa;import shutil as _shutil&#xa;import signal as _signal&#xa;import socket as _socket&#xa;import subprocess as _subprocess&#xa;import sys as _sys&#xa;import tarfile as _tarfile&#xa;import tempfile as _tempfile&#xa;import time as _time&#xa;import traceback as _traceback&#xa;import types as _types&#xa;import uuid as _uuid&#xa;&#xa;from subprocess import CalledProcessError&#xa;from subprocess import PIPE&#xa;&#xa;# See documentation at http://www.ssorj.net/projects/plano.html&#xa;&#xa;LINE_SEP = _os.linesep&#xa;PATH_SEP = _os.sep&#xa;PATH_VAR_SEP = _os.pathsep&#xa;ENV = _os.environ&#xa;ARGS = _sys.argv&#xa;&#xa;STD_IN = _sys.stdin&#xa;STD_OUT = _sys.stdout&#xa;STD_ERR = _sys.stderr&#xa;NULL_DEV = _os.devnull&#xa;&#xa;_message_levels = (&#xa;    ""debug"",&#xa;    ""notice"",&#xa;    ""warn"",&#xa;    ""error"",&#xa;)&#xa;&#xa;_debug = _message_levels.index(""debug"")&#xa;_notice = _message_levels.index(""notice"")&#xa;_warn = _message_levels.index(""warn"")&#xa;_error = _message_levels.index(""error"")&#xa;&#xa;_message_output = STD_ERR&#xa;_message_threshold = _notice&#xa;&#xa;def set_message_output(writeable):&#xa;    global _message_output&#xa;    _message_output = writeable&#xa;&#xa;def set_message_threshold(level):&#xa;    assert level in _message_levels&#xa;&#xa;    global _message_threshold&#xa;    _message_threshold = _message_levels.index(level)&#xa;&#xa;def fail(message, *args):&#xa;    error(message, *args)&#xa;&#xa;    if isinstance(message, BaseException):&#xa;        raise message&#xa;&#xa;    raise Exception(message.format(*args))&#xa;&#xa;def error(message, *args):&#xa;    _print_message(""Error"", message, args)&#xa;&#xa;def warn(message, *args):&#xa;    if _message_threshold <= _warn:&#xa;        _print_message(""Warning"", message, args)&#xa;&#xa;def notice(message, *args):&#xa;    if _message_threshold <= _notice:&#xa;        _print_message(None, message, args)&#xa;&#xa;def debug(message, *args):&#xa;    if _message_threshold <= _debug:&#xa;        _print_message(""Debug"", message, args)&#xa;&#xa;def exit(arg=None, *args):&#xa;    if arg in (0, None):&#xa;        _sys.exit()&#xa;&#xa;    if _is_string(arg):&#xa;        error(arg, args)&#xa;        _sys.exit(1)&#xa;    elif isinstance(arg, int):&#xa;        if arg > 0:&#xa;            error(""Exiting with code {0}"", arg)&#xa;        else:&#xa;            notice(""Exiting with code {0}"", arg)&#xa;&#xa;        _sys.exit(arg)&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;def _print_message(category, message, args):&#xa;    if _message_output is None:&#xa;        return&#xa;&#xa;    message = _format_message(category, message, args)&#xa;&#xa;    print(message, file=_message_output)&#xa;&#xa;    _message_output.flush()&#xa;&#xa;def _format_message(category, message, args):&#xa;    if not _is_string(message):&#xa;        message = str(message)&#xa;&#xa;    if args:&#xa;        message = message.format(*args)&#xa;&#xa;    if len(message) > 0 and message[0].islower():&#xa;        message = message[0].upper() + message[1:]&#xa;&#xa;    if category:&#xa;        message = ""{0}: {1}"".format(category, message)&#xa;&#xa;    program = program_name()&#xa;    message = ""{0}: {1}"".format(program, message)&#xa;&#xa;    return message&#xa;&#xa;def eprint(*args, **kwargs):&#xa;    print(*args, file=_sys.stderr, **kwargs)&#xa;&#xa;def flush():&#xa;    _sys.stdout.flush()&#xa;    _sys.stderr.flush()&#xa;&#xa;absolute_path = _os.path.abspath&#xa;normalize_path = _os.path.normpath&#xa;real_path = _os.path.realpath&#xa;exists = _os.path.exists&#xa;is_absolute = _os.path.isabs&#xa;is_dir = _os.path.isdir&#xa;is_file = _os.path.isfile&#xa;is_link = _os.path.islink&#xa;file_size = _os.path.getsize&#xa;&#xa;join = _os.path.join&#xa;split = _os.path.split&#xa;split_extension = _os.path.splitext&#xa;&#xa;current_dir = _os.getcwd&#xa;sleep = _time.sleep&#xa;&#xa;def home_dir(user=""""):&#xa;    return _os.path.expanduser(""~{0}"".format(user))&#xa;&#xa;def parent_dir(path):&#xa;    path = normalize_path(path)&#xa;    parent, child = split(path)&#xa;&#xa;    return parent&#xa;&#xa;def file_name(file):&#xa;    file = normalize_path(file)&#xa;    dir, name = split(file)&#xa;&#xa;    return name&#xa;&#xa;def name_stem(file):&#xa;    name = file_name(file)&#xa;&#xa;    if name.endswith("".tar.gz""):&#xa;        name = name[:-3]&#xa;&#xa;    stem, ext = split_extension(name)&#xa;&#xa;    return stem&#xa;&#xa;def name_extension(file):&#xa;    name = file_name(file)&#xa;    stem, ext = split_extension(name)&#xa;&#xa;    return ext&#xa;&#xa;def program_name(command=None):&#xa;    if command is None:&#xa;        args = ARGS&#xa;    else:&#xa;        args = command.split()&#xa;&#xa;    for arg in args:&#xa;        if ""="" not in arg:&#xa;            return file_name(arg)&#xa;&#xa;def which(program_name):&#xa;    for dir in ENV[""PATH""].split(PATH_VAR_SEP):&#xa;        program = join(dir, program_name)&#xa;&#xa;        if _os.access(program, _os.X_OK):&#xa;            return program&#xa;&#xa;def read(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return f.read()&#xa;&#xa;def write(file, string):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        f.write(string)&#xa;&#xa;    return file&#xa;&#xa;def append(file, string):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""a"") as f:&#xa;        f.write(string)&#xa;&#xa;    return file&#xa;&#xa;def prepend(file, string):&#xa;    orig = read(file)&#xa;    prepended = string + orig&#xa;&#xa;    return write(file, prepended)&#xa;&#xa;# XXX Should this work on directories?&#xa;def touch(file):&#xa;    return append(file, """")&#xa;&#xa;def tail(file, n):&#xa;    return """".join(tail_lines(file, n))&#xa;&#xa;def read_lines(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return f.readlines()&#xa;&#xa;def write_lines(file, lines):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        f.writelines(lines)&#xa;&#xa;    return file&#xa;&#xa;def append_lines(file, lines):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""a"") as f:&#xa;        f.writelines(string)&#xa;&#xa;    return file&#xa;&#xa;def prepend_lines(file, lines):&#xa;    orig_lines = read_lines(file)&#xa;&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        f.writelines(lines)&#xa;        f.writelines(orig_lines)&#xa;&#xa;    return file&#xa;&#xa;# Derived from http://stackoverflow.com/questions/136168/get-last-n-lines-of-a-file-with-python-similar-to-tail&#xa;def tail_lines(file, n):&#xa;    assert n >= 0&#xa;&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        pos = n + 1&#xa;        lines = list()&#xa;&#xa;        while len(lines) <= n:&#xa;                try:&#xa;                    f.seek(-pos, 2)&#xa;                except IOError:&#xa;                    f.seek(0)&#xa;                    break&#xa;                finally:&#xa;                    lines = f.readlines()&#xa;&#xa;                pos *= 2&#xa;&#xa;        return lines[-n:]&#xa;&#xa;def read_json(file):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""r"") as f:&#xa;        return _json.load(f)&#xa;&#xa;def write_json(file, obj):&#xa;    with _codecs.open(file, encoding=""utf-8"", mode=""w"") as f:&#xa;        return _json.dump(obj, f, indent=4, separators=("","", "": ""), sort_keys=True)&#xa;&#xa;def make_temp_file(suffix=""""):&#xa;    return _tempfile.mkstemp(prefix=""plano-"", suffix=suffix)[1]&#xa;&#xa;def make_temp_dir(suffix=""""):&#xa;    return _tempfile.mkdtemp(prefix=""plano-"", suffix=suffix)&#xa;&#xa;def make_user_temp_dir():&#xa;    temp_dir = _tempfile.gettempdir()&#xa;    user = _getpass.getuser()&#xa;    user_temp_dir = join(temp_dir, user)&#xa;&#xa;    return make_dir(user_temp_dir)&#xa;&#xa;class temp_file(object):&#xa;    def __init__(self, suffix=""""):&#xa;        self.file = make_temp_file(suffix=suffix)&#xa;&#xa;    def __enter__(self):&#xa;        return self.file&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        if exists(self.file):&#xa;            _os.remove(self.file)&#xa;&#xa;class temp_dir(object):&#xa;    def __init__(self, suffix=""""):&#xa;        self.dir = make_temp_dir(suffix=suffix)&#xa;&#xa;    def __enter__(self):&#xa;        return self.dir&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        if exists(self.dir):&#xa;            _shutil.rmtree(self.dir, ignore_errors=True)&#xa;&#xa;def unique_id(length=16):&#xa;    assert length >= 1&#xa;    assert length <= 16&#xa;&#xa;    uuid_bytes = _uuid.uuid4().bytes&#xa;    uuid_bytes = uuid_bytes[:length]&#xa;&#xa;    return _binascii.hexlify(uuid_bytes).decode(""utf-8"")&#xa;&#xa;def copy(from_path, to_path):&#xa;    notice(""Copying '{0}' to '{1}'"", from_path, to_path)&#xa;&#xa;    if is_dir(to_path):&#xa;        to_path = join(to_path, file_name(from_path))&#xa;    else:&#xa;        make_dir(parent_dir(to_path))&#xa;&#xa;    if is_dir(from_path):&#xa;        _copytree(from_path, to_path, symlinks=True)&#xa;    else:&#xa;        _shutil.copy(from_path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def move(from_path, to_path):&#xa;    notice(""Moving '{0}' to '{1}'"", from_path, to_path)&#xa;&#xa;    if is_dir(to_path):&#xa;        to_path = join(to_path, file_name(from_path))&#xa;    else:&#xa;        make_dir(parent_dir(to_path))&#xa;&#xa;    _shutil.move(from_path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def rename(path, expr, replacement):&#xa;    path = normalize_path(path)&#xa;    parent_dir, name = split(path)&#xa;    to_name = string_replace(name, expr, replacement)&#xa;    to_path = join(parent_dir, to_name)&#xa;&#xa;    notice(""Renaming '{0}' to '{1}'"", path, to_path)&#xa;&#xa;    move(path, to_path)&#xa;&#xa;    return to_path&#xa;&#xa;def remove(path):&#xa;    notice(""Removing '{0}'"", path)&#xa;&#xa;    if not exists(path):&#xa;        return&#xa;&#xa;    if is_dir(path):&#xa;        _shutil.rmtree(path, ignore_errors=True)&#xa;    else:&#xa;        _os.remove(path)&#xa;&#xa;    return path&#xa;&#xa;def make_link(source_path, link_file):&#xa;    notice(""Making link '{0}' to '{1}'"", link_file, source_path)&#xa;&#xa;    if exists(link_file):&#xa;        assert read_link(link_file) == source_path&#xa;        return&#xa;&#xa;    link_dir = parent_dir(link_file)&#xa;&#xa;    if link_dir:&#xa;        make_dir(link_dir)&#xa;&#xa;    _os.symlink(source_path, link_file)&#xa;&#xa;    return link_file&#xa;&#xa;def read_link(file):&#xa;    return _os.readlink(file)&#xa;&#xa;def find(dir, *patterns):&#xa;    matched_paths = set()&#xa;&#xa;    if not patterns:&#xa;        patterns = (""*"",)&#xa;&#xa;    for root, dirs, files in _os.walk(dir):&#xa;        for pattern in patterns:&#xa;            matched_dirs = _fnmatch.filter(dirs, pattern)&#xa;            matched_files = _fnmatch.filter(files, pattern)&#xa;&#xa;            matched_paths.update([join(root, x) for x in matched_dirs])&#xa;            matched_paths.update([join(root, x) for x in matched_files])&#xa;&#xa;    return sorted(matched_paths)&#xa;&#xa;def find_any_one(dir, *patterns):&#xa;    paths = find(dir, *patterns)&#xa;&#xa;    if len(paths) == 0:&#xa;        return&#xa;&#xa;    return paths[0]&#xa;&#xa;def find_only_one(dir, *patterns):&#xa;    paths = find(dir, *patterns)&#xa;&#xa;    if len(paths) == 0:&#xa;        return&#xa;&#xa;    assert len(paths) == 1&#xa;&#xa;    return paths[0]&#xa;&#xa;# find_via_expr?&#xa;&#xa;def string_replace(string, expr, replacement, count=0):&#xa;    return _re.sub(expr, replacement, string, count)&#xa;&#xa;def make_dir(dir):&#xa;    if not exists(dir):&#xa;        _os.makedirs(dir)&#xa;&#xa;    return dir&#xa;&#xa;# Returns the current working directory so you can change it back&#xa;def change_dir(dir):&#xa;    notice(""Changing directory to '{0}'"", dir)&#xa;&#xa;    cwd = current_dir()&#xa;    _os.chdir(dir)&#xa;    return cwd&#xa;&#xa;def list_dir(dir, *patterns):&#xa;    assert is_dir(dir)&#xa;&#xa;    names = _os.listdir(dir)&#xa;&#xa;    if not patterns:&#xa;        return sorted(names)&#xa;&#xa;    matched_names = set()&#xa;&#xa;    for pattern in patterns:&#xa;        matched_names.update(_fnmatch.filter(names, pattern))&#xa;&#xa;    return sorted(matched_names)&#xa;&#xa;class working_dir(object):&#xa;    def __init__(self, dir):&#xa;        self.dir = dir&#xa;        self.prev_dir = None&#xa;&#xa;    def __enter__(self):&#xa;        self.prev_dir = change_dir(self.dir)&#xa;        return self.dir&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        change_dir(self.prev_dir)&#xa;&#xa;class temp_working_dir(working_dir):&#xa;    def __init__(self):&#xa;        super(temp_working_dir, self).__init__(make_temp_dir())&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        super(temp_working_dir, self).__exit__(exc_type, exc_value, traceback)&#xa;&#xa;        if exists(self.dir):&#xa;            _shutil.rmtree(self.dir, ignore_errors=True)&#xa;&#xa;def call(command, *args, **kwargs):&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    check_process(proc)&#xa;&#xa;def call_for_exit_code(command, *args, **kwargs):&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    return wait_for_process(proc)&#xa;&#xa;def call_for_output(command, *args, **kwargs):&#xa;    kwargs[""stdout""] = _subprocess.PIPE&#xa;&#xa;    proc = start_process(command, *args, **kwargs)&#xa;    output = proc.communicate()[0]&#xa;    exit_code = proc.poll()&#xa;&#xa;    # XXX I don't know if None is possible here&#xa;    assert exit_code is not None&#xa;&#xa;    if exit_code not in (None, 0):&#xa;        error = CalledProcessError(exit_code, proc.command_string)&#xa;        error.output = output&#xa;&#xa;        raise error&#xa;&#xa;    return output&#xa;&#xa;def call_and_print_on_error(command, *args, **kwargs):&#xa;    with temp_file() as output_file:&#xa;        try:&#xa;            with open(output_file, ""w"") as out:&#xa;                kwargs[""output""] = out&#xa;                call(command, *args, **kwargs)&#xa;        except CalledProcessError:&#xa;            eprint(read(output_file), end="""")&#xa;            raise&#xa;&#xa;_child_processes = list()&#xa;&#xa;class _Process(_subprocess.Popen):&#xa;    def __init__(self, command, options, name, command_string):&#xa;        super(_Process, self).__init__(command, **options)&#xa;&#xa;        self.name = name&#xa;        self.command_string = command_string&#xa;&#xa;        _child_processes.append(self)&#xa;&#xa;    @property&#xa;    def exit_code(self):&#xa;        return self.returncode&#xa;&#xa;    def __enter__(self):&#xa;        return self&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        stop_process(self.proc)&#xa;&#xa;    def __repr__(self):&#xa;        return ""process {0} ({1})"".format(self.pid, self.name)&#xa;&#xa;def default_sigterm_handler(signum, frame):&#xa;    for proc in _child_processes:&#xa;        if proc.poll() is None:&#xa;            proc.terminate()&#xa;&#xa;    #_remove_temp_dir()&#xa;&#xa;    exit(-(_signal.SIGTERM))&#xa;&#xa;_signal.signal(_signal.SIGTERM, default_sigterm_handler)&#xa;&#xa;def _command_string(command, args):&#xa;    elems = [""\""{0}\"""".format(x) if "" "" in x else x for x in command]&#xa;    string = "" "".join(elems)&#xa;    string = string.format(*args)&#xa;&#xa;    return string&#xa;&#xa;def start_process(command, *args, **kwargs):&#xa;    if _is_string(command):&#xa;        command = command.format(*args)&#xa;        command_args = _shlex.split(command)&#xa;        command_string = command&#xa;    elif isinstance(command, _collections.Iterable):&#xa;        assert len(args) == 0, args&#xa;        command_args = command&#xa;        command_string = _command_string(command, [])&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;    notice(""Calling '{0}'"", command_string)&#xa;&#xa;    name = kwargs.get(""name"", command_args[0])&#xa;&#xa;    kwargs[""stdout""] = kwargs.get(""stdout"", _sys.stdout)&#xa;    kwargs[""stderr""] = kwargs.get(""stderr"", _sys.stderr)&#xa;&#xa;    if ""output"" in kwargs:&#xa;        out = kwargs.pop(""output"")&#xa;&#xa;        kwargs[""stdout""] = out&#xa;        kwargs[""stderr""] = out&#xa;&#xa;    if ""shell"" in kwargs and kwargs[""shell""] is True:&#xa;        proc = _Process(command_string, kwargs, name, command_string)&#xa;    else:&#xa;        proc = _Process(command_args, kwargs, name, command_string)&#xa;&#xa;    debug(""{0} started"", proc)&#xa;&#xa;    return proc&#xa;&#xa;def terminate_process(proc):&#xa;    notice(""Terminating {0}"", proc)&#xa;&#xa;    if proc.poll() is None:&#xa;        proc.terminate()&#xa;    else:&#xa;        debug(""{0} already exited"", proc)&#xa;&#xa;def stop_process(proc):&#xa;    notice(""Stopping {0}"", proc)&#xa;&#xa;    if proc.poll() is not None:&#xa;        if proc.returncode == 0:&#xa;            debug(""{0} already exited normally"", proc)&#xa;        elif proc.returncode == -(_signal.SIGTERM):&#xa;            debug(""{0} was already terminated"", proc)&#xa;        else:&#xa;            debug(""{0} already exited with code {1}"", proc, proc.returncode)&#xa;&#xa;        return&#xa;&#xa;    proc.terminate()&#xa;&#xa;    return wait_for_process(proc)&#xa;&#xa;def wait_for_process(proc):&#xa;    debug(""Waiting for {0} to exit"", proc)&#xa;&#xa;    proc.wait()&#xa;&#xa;    if proc.returncode == 0:&#xa;        debug(""{0} exited normally"", proc)&#xa;    elif proc.returncode == -(_signal.SIGTERM):&#xa;        debug(""{0} exited after termination"", proc)&#xa;    else:&#xa;        debug(""{0} exited with code {1}"", proc, proc.returncode)&#xa;&#xa;    return proc.returncode&#xa;&#xa;def check_process(proc):&#xa;    wait_for_process(proc)&#xa;&#xa;    if proc.returncode != 0:&#xa;        raise CalledProcessError(proc.returncode, proc.command_string)&#xa;&#xa;def exec_process(command, *args):&#xa;    if _is_string(command):&#xa;        command = command.format(*args)&#xa;        command_args = _shlex.split(command)&#xa;        command_string = command&#xa;    elif isinstance(command, _collections.Iterable):&#xa;        assert len(args) == 0, args&#xa;        command_args = command&#xa;        command_string = _command_string(command, [])&#xa;    else:&#xa;        raise Exception()&#xa;&#xa;    notice(""Calling '{0}'"", command_string)&#xa;&#xa;    _os.execvp(command_args[0], command_args[1:])&#xa;&#xa;def make_archive(input_dir, output_dir, archive_stem):&#xa;    # XXX Cleanup temp dir&#xa;&#xa;    temp_dir = make_temp_dir()&#xa;    temp_input_dir = join(temp_dir, archive_stem)&#xa;&#xa;    copy(input_dir, temp_input_dir)&#xa;    make_dir(output_dir)&#xa;&#xa;    output_file = ""{0}.tar.gz"".format(join(output_dir, archive_stem))&#xa;    output_file = absolute_path(output_file)&#xa;&#xa;    with working_dir(temp_dir):&#xa;        call(""tar -czf {0} {1}"", output_file, archive_stem)&#xa;&#xa;    return output_file&#xa;&#xa;def extract_archive(archive_file, output_dir):&#xa;    assert is_file(archive_file)&#xa;&#xa;    if not exists(output_dir):&#xa;        make_dir(output_dir)&#xa;&#xa;    archive_file = absolute_path(archive_file)&#xa;&#xa;    with working_dir(output_dir):&#xa;        call(""tar -xf {0}"", archive_file)&#xa;&#xa;    return output_dir&#xa;&#xa;def rename_archive(archive_file, new_archive_stem):&#xa;    # XXX Cleanup temp dir&#xa;&#xa;    assert is_file(archive_file)&#xa;&#xa;    if name_stem(archive_file) == new_archive_stem:&#xa;        return archive_file&#xa;&#xa;    temp_dir = make_temp_dir()&#xa;&#xa;    extract_archive(archive_file, temp_dir)&#xa;&#xa;    input_name = list_dir(temp_dir)[0]&#xa;    input_dir = join(temp_dir, input_name)&#xa;    output_file = make_archive(input_dir, temp_dir, new_archive_stem)&#xa;    output_name = file_name(output_file)&#xa;    archive_dir = parent_dir(archive_file)&#xa;    new_archive_file = join(archive_dir, output_name)&#xa;&#xa;    move(output_file, new_archive_file)&#xa;    remove(archive_file)&#xa;&#xa;    return new_archive_file&#xa;&#xa;def random_port(min=49152, max=65535):&#xa;    return _random.randint(min, max)&#xa;&#xa;def wait_for_port(port, host="""", timeout=30):&#xa;    if _is_string(port):&#xa;        port = int(port)&#xa;&#xa;    sock = _socket.socket(_socket.AF_INET, _socket.SOCK_STREAM)&#xa;    sock.setsockopt(_socket.SOL_SOCKET, _socket.SO_REUSEADDR, 1)&#xa;&#xa;    start = _time.time()&#xa;&#xa;    try:&#xa;        while True:&#xa;            if sock.connect_ex((host, port)) == 0:&#xa;                return&#xa;&#xa;            sleep(0.1)&#xa;&#xa;            if _time.time() - start > timeout:&#xa;                fail(""Timed out waiting for port {0} to open"", port)&#xa;    finally:&#xa;        sock.close()&#xa;&#xa;# Modified copytree impl that allows for already existing destination&#xa;# dirs&#xa;def _copytree(src, dst, symlinks=False, ignore=None):&#xa;    """"""Recursively copy a directory tree using copy2().&#xa;&#xa;    If exception(s) occur, an Error is raised with a list of reasons.&#xa;&#xa;    If the optional symlinks flag is true, symbolic links in the&#xa;    source tree result in symbolic links in the destination tree; if&#xa;    it is false, the contents of the files pointed to by symbolic&#xa;    links are copied.&#xa;&#xa;    The optional ignore argument is a callable. If given, it&#xa;    is called with the `src` parameter, which is the directory&#xa;    being visited by copytree(), and `names` which is the list of&#xa;    `src` contents, as returned by os.listdir():&#xa;&#xa;        callable(src, names) -> ignored_names&#xa;&#xa;    Since copytree() is called recursively, the callable will be&#xa;    called once for each directory that is copied. It returns a&#xa;    list of names relative to the `src` directory that should&#xa;    not be copied.&#xa;&#xa;    XXX Consider this example code rather than the ultimate tool.&#xa;&#xa;    """"""&#xa;    names = _os.listdir(src)&#xa;    if ignore is not None:&#xa;        ignored_names = ignore(src, names)&#xa;    else:&#xa;        ignored_names = set()&#xa;&#xa;    if not exists(dst):&#xa;        _os.makedirs(dst)&#xa;    errors = []&#xa;    for name in names:&#xa;        if name in ignored_names:&#xa;            continue&#xa;        srcname = _os.path.join(src, name)&#xa;        dstname = _os.path.join(dst, name)&#xa;        try:&#xa;            if symlinks and _os.path.islink(srcname):&#xa;                linkto = _os.readlink(srcname)&#xa;                _os.symlink(linkto, dstname)&#xa;            elif _os.path.isdir(srcname):&#xa;                _copytree(srcname, dstname, symlinks, ignore)&#xa;            else:&#xa;                # Will raise a SpecialFileError for unsupported file types&#xa;                _shutil.copy2(srcname, dstname)&#xa;        # catch the Error from the recursive copytree so that we can&#xa;        # continue with other files&#xa;        except _shutil.Error as err:&#xa;            errors.extend(err.args[0])&#xa;        except EnvironmentError as why:&#xa;            errors.append((srcname, dstname, str(why)))&#xa;    try:&#xa;        _shutil.copystat(src, dst)&#xa;    except OSError as why:&#xa;        if _shutil.WindowsError is not None and isinstance \&#xa;               (why, _shutil.WindowsError):&#xa;            # Copying file access times may fail on Windows&#xa;            pass&#xa;        else:&#xa;            errors.append((src, dst, str(why)))&#xa;    if errors:&#xa;        raise _shutil.Error(errors)&#xa;&#xa;def _is_string(obj):&#xa;    try:&#xa;        return isinstance(obj, basestring)&#xa;    except NameError:&#xa;        return isinstance(obj, str)&#xa;"
764360|"# (c) 2014, Chris Church <chris@ninemoreminutes.com>&#xa;#&#xa;# This file is part of Ansible.&#xa;#&#xa;# Ansible is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Ansible is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.&#xa;from __future__ import (absolute_import, division, print_function)&#xa;__metaclass__ = type&#xa;&#xa;import base64&#xa;import os&#xa;import re&#xa;import random&#xa;import shlex&#xa;import time&#xa;&#xa;from ansible.utils.unicode import to_bytes, to_unicode&#xa;&#xa;_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']&#xa;&#xa;# Primarily for testing, allow explicitly specifying PowerShell version via&#xa;# an environment variable.&#xa;_powershell_version = os.environ.get('POWERSHELL_VERSION', None)&#xa;if _powershell_version:&#xa;    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]&#xa;&#xa;class ShellModule(object):&#xa;&#xa;    def env_prefix(self, **kwargs):&#xa;        return ''&#xa;&#xa;    def join_path(self, *args):&#xa;        parts = []&#xa;        for arg in args:&#xa;            arg = self._unquote(arg).replace('/', '\\')&#xa;            parts.extend([a for a in arg.split('\\') if a])&#xa;        path = '\\'.join(parts)&#xa;        if path.startswith('~'):&#xa;            return path&#xa;        return '""%s""' % path&#xa;&#xa;    def path_has_trailing_slash(self, path):&#xa;        # Allow Windows paths to be specified using either slash.&#xa;        path = self._unquote(path)&#xa;        return path.endswith('/') or path.endswith('\\')&#xa;&#xa;    def chmod(self, mode, path):&#xa;        return ''&#xa;&#xa;    def remove(self, path, recurse=False):&#xa;        path = self._escape(self._unquote(path))&#xa;        if recurse:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force -Recurse;''' % path)&#xa;        else:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force;''' % path)&#xa;&#xa;    def mkdtemp(self, basefile, system=False, mode=None):&#xa;        basefile = self._escape(self._unquote(basefile))&#xa;        # FIXME: Support system temp path!&#xa;        return self._encode_script('''(New-Item -Type Directory -Path $env:temp -Name ""%s"").FullName | Write-Host -Separator '';''' % basefile)&#xa;&#xa;    def expand_user(self, user_home_path):&#xa;        # PowerShell only supports ""~"" (not ""~username"").  Resolve-Path ~ does&#xa;        # not seem to work remotely, though by default we are always starting&#xa;        # in the user's home directory.&#xa;        user_home_path = self._unquote(user_home_path)&#xa;        if user_home_path == '~':&#xa;            script = 'Write-Host (Get-Location).Path'&#xa;        elif user_home_path.startswith('~\\'):&#xa;            script = 'Write-Host ((Get-Location).Path + ""%s"")' % self._escape(user_home_path[1:])&#xa;        else:&#xa;            script = 'Write-Host ""%s""' % self._escape(user_home_path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def checksum(self, path, *args, **kwargs):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path -PathType Leaf ""%(path)s"")&#xa;            {&#xa;                $sp = new-object -TypeName System.Security.Cryptography.SHA1CryptoServiceProvider;&#xa;                $fp = [System.IO.File]::Open(""%(path)s"", [System.IO.Filemode]::Open, [System.IO.FileAccess]::Read);&#xa;                [System.BitConverter]::ToString($sp.ComputeHash($fp)).Replace(""-"", """").ToLower();&#xa;                $fp.Dispose();&#xa;            }&#xa;            ElseIf (Test-Path -PathType Container ""%(path)s"")&#xa;            {&#xa;                Write-Host ""3"";&#xa;            }&#xa;            Else&#xa;            {&#xa;                Write-Host ""1"";&#xa;            }&#xa;        ''' % dict(path=path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def build_module_command(self, env_string, shebang, cmd, rm_tmp=None):&#xa;        cmd_parts = shlex.split(to_bytes(cmd), posix=False)&#xa;        cmd_parts = map(to_unicode, cmd_parts)&#xa;        if shebang and shebang.lower() == '#!powershell':&#xa;            if not self._unquote(cmd_parts[0]).lower().endswith('.ps1'):&#xa;                cmd_parts[0] = '""%s.ps1""' % self._unquote(cmd_parts[0])&#xa;            cmd_parts.insert(0, '&')&#xa;        elif shebang and shebang.startswith('#!'):&#xa;            cmd_parts.insert(0, shebang[2:])&#xa;        catch = '''&#xa;            $_obj = @{ failed = $true; $msg = $_ }&#xa;            echo $_obj | ConvertTo-Json -Compress -Depth 99&#xa;            Exit 1&#xa;        '''&#xa;        script = 'Try { %s }\nCatch { %s }' % (' '.join(cmd_parts), 'throw')&#xa;        if rm_tmp:&#xa;            rm_tmp = self._escape(self._unquote(rm_tmp))&#xa;            rm_cmd = 'Remove-Item ""%s"" -Force -Recurse -ErrorAction SilentlyContinue' % rm_tmp&#xa;            script = '%s\nFinally { %s }' % (script, rm_cmd)&#xa;        return self._encode_script(script)&#xa;&#xa;    def _unquote(self, value):&#xa;        '''Remove any matching quotes that wrap the given value.'''&#xa;        value = to_unicode(value or '')&#xa;        m = re.match(r'^\s*?\'(.*?)\'\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        m = re.match(r'^\s*?""(.*?)""\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        return value&#xa;&#xa;    def _escape(self, value, include_vars=False):&#xa;        '''Return value escaped for use in PowerShell command.'''&#xa;        # http://www.techotopia.com/index.php/Windows_PowerShell_1.0_String_Quoting_and_Escape_Sequences&#xa;        # http://stackoverflow.com/questions/764360/a-list-of-string-replacements-in-python&#xa;        subs = [('\n', '`n'), ('\r', '`r'), ('\t', '`t'), ('\a', '`a'),&#xa;                ('\b', '`b'), ('\f', '`f'), ('\v', '`v'), ('""', '`""'),&#xa;                ('\'', '`\''), ('`', '``'), ('\x00', '`0')]&#xa;        if include_vars:&#xa;            subs.append(('$', '`$'))&#xa;        pattern = '|'.join('(%s)' % re.escape(p) for p, s in subs)&#xa;        substs = [s for p, s in subs]&#xa;        replace = lambda m: substs[m.lastindex - 1]&#xa;        return re.sub(pattern, replace, value)&#xa;&#xa;    def _encode_script(self, script, as_list=False):&#xa;        '''Convert a PowerShell script to a single base64-encoded command.'''&#xa;        script = to_unicode(script)&#xa;        script = '\n'.join([x.strip() for x in script.splitlines() if x.strip()])&#xa;        encoded_script = base64.b64encode(script.encode('utf-16-le'))&#xa;        cmd_parts = _common_args + ['-EncodedCommand', encoded_script]&#xa;        if as_list:&#xa;            return cmd_parts&#xa;        return ' '.join(cmd_parts)&#xa;"
2371436|"from __future__ import division&#xa;from pyparsing import (Literal,CaselessLiteral,Word,Combine,Group,Optional,&#xa;                       ZeroOrMore,Forward,nums,alphas,oneOf)&#xa;import math&#xa;import operator&#xa;&#xa;__author__='Paul McGuire'&#xa;__version__ = '$Revision: 0.0 $'&#xa;__date__ = '$Date: 2009-03-20 $'&#xa;__source__='''http://pyparsing.wikispaces.com/file/view/fourFn.py&#xa;http://pyparsing.wikispaces.com/message/view/home/15549426&#xa;'''&#xa;__note__='''&#xa;Script from: http://stackoverflow.com/questions/2371436/evaluating-a-mathematical-expression-in-a-string&#xa;Original note:&#xa;&#xa;All I've done is rewrap Paul McGuire's fourFn.py as a class, so I can use it&#xa;more easily in other places.&#xa;'''&#xa;&#xa;class NumericStringParser(object):&#xa;    '''&#xa;    Most of this code comes from the fourFn.py pyparsing example&#xa;&#xa;    '''&#xa;    def pushFirst(self, strg, loc, toks ):&#xa;        self.exprStack.append( toks[0] )&#xa;    def pushUMinus(self, strg, loc, toks ):&#xa;        if toks and toks[0]=='-':&#xa;            self.exprStack.append( 'unary -' )&#xa;    def __init__(self):&#xa;        """"""&#xa;        expop   :: '^'&#xa;        multop  :: '*' | '/'&#xa;        addop   :: '+' | '-'&#xa;        integer :: ['+' | '-'] '0'..'9'+&#xa;        atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')'&#xa;        factor  :: atom [ expop factor ]*&#xa;        term    :: factor [ multop factor ]*&#xa;        expr    :: term [ addop term ]*&#xa;        """"""&#xa;        point = Literal( ""."" )&#xa;        e     = CaselessLiteral( ""E"" )&#xa;        fnumber = Combine( Word( ""+-""+nums, nums ) +&#xa;                           Optional( point + Optional( Word( nums ) ) ) +&#xa;                           Optional( e + Word( ""+-""+nums, nums ) ) )&#xa;        ident = Word(alphas, alphas+nums+""_$"")&#xa;        plus  = Literal( ""+"" )&#xa;        minus = Literal( ""-"" )&#xa;        mult  = Literal( ""*"" )&#xa;        div   = Literal( ""/"" )&#xa;        lpar  = Literal( ""("" ).suppress()&#xa;        rpar  = Literal( "")"" ).suppress()&#xa;        addop  = plus | minus&#xa;        multop = mult | div&#xa;        expop = Literal( ""^"" )&#xa;        pi    = CaselessLiteral( ""PI"" )&#xa;        expr = Forward()&#xa;        atom = ((Optional(oneOf(""- +"")) +&#xa;                 (pi|e|fnumber|ident+lpar+expr+rpar).setParseAction(self.pushFirst))&#xa;                | Optional(oneOf(""- +"")) + Group(lpar+expr+rpar)&#xa;                ).setParseAction(self.pushUMinus)&#xa;        # by defining exponentiation as ""atom [ ^ factor ]..."" instead of&#xa;        # ""atom [ ^ atom ]..."", we get right-to-left exponents, instead of left-to-right&#xa;        # that is, 2^3^2 = 2^(3^2), not (2^3)^2.&#xa;        factor = Forward()&#xa;        factor << atom + ZeroOrMore( ( expop + factor ).setParseAction( self.pushFirst ) )&#xa;        term = factor + ZeroOrMore( ( multop + factor ).setParseAction( self.pushFirst ) )&#xa;        expr << term + ZeroOrMore( ( addop + term ).setParseAction( self.pushFirst ) )&#xa;        # addop_term = ( addop + term ).setParseAction( self.pushFirst )&#xa;        # general_term = term + ZeroOrMore( addop_term ) | OneOrMore( addop_term)&#xa;        # expr <<  general_term&#xa;        self.bnf = expr&#xa;        # map operator symbols to corresponding arithmetic operations&#xa;        epsilon = 1e-12&#xa;        self.opn = { ""+"" : operator.add,&#xa;                ""-"" : operator.sub,&#xa;                ""*"" : operator.mul,&#xa;                ""/"" : operator.truediv,&#xa;                ""^"" : operator.pow }&#xa;        self.fn  = { ""sin"" : math.sin,&#xa;                ""cos"" : math.cos,&#xa;                ""tan"" : math.tan,&#xa;                ""abs"" : abs,&#xa;                ""trunc"" : lambda a: int(a),&#xa;                ""round"" : round,&#xa;                ""sgn"" : lambda a: abs(a)>epsilon and cmp(a,0) or 0}&#xa;    def evaluateStack(self, s ):&#xa;        op = s.pop()&#xa;        if op == 'unary -':&#xa;            return -self.evaluateStack( s )&#xa;        if op in ""+-*/^"":&#xa;            op2 = self.evaluateStack( s )&#xa;            op1 = self.evaluateStack( s )&#xa;            return self.opn[op]( op1, op2 )&#xa;        elif op == ""PI"":&#xa;            return math.pi # 3.1415926535&#xa;        elif op == ""E"":&#xa;            return math.e  # 2.718281828&#xa;        elif op in self.fn:&#xa;            return self.fn[op]( self.evaluateStack( s ) )&#xa;        elif op[0].isalpha():&#xa;            return 0&#xa;        else:&#xa;            return float( op )&#xa;    def eval(self,num_string,parseAll=True):&#xa;        self.exprStack=[]&#xa;        results=self.bnf.parseString(num_string,parseAll)&#xa;        val=self.evaluateStack( self.exprStack[:] )&#xa;        return val&#xa;"
16837946|""""""" pydevd_vars deals with variables:&#xa;    resolution/conversion to XML.&#xa;""""""&#xa;import pickle&#xa;from _pydevd_bundle.pydevd_constants import dict_contains, get_frame, get_thread_id, xrange&#xa;&#xa;from _pydevd_bundle.pydevd_custom_frames import get_custom_frame&#xa;from _pydevd_bundle.pydevd_xml import ExceptionOnEvaluate, get_type, var_to_xml&#xa;from _pydev_imps._pydev_saved_modules import thread&#xa;&#xa;try:&#xa;    from StringIO import StringIO&#xa;except ImportError:&#xa;    from io import StringIO&#xa;import sys  # @Reimport&#xa;&#xa;from _pydev_imps._pydev_saved_modules import threading&#xa;import traceback&#xa;from _pydevd_bundle import pydevd_save_locals&#xa;from _pydev_bundle.pydev_imports import Exec, quote, execfile&#xa;from _pydevd_bundle.pydevd_utils import to_string&#xa;&#xa;SENTINEL_VALUE = []&#xa;&#xa;# -------------------------------------------------------------------------- defining true and false for earlier versions&#xa;&#xa;try:&#xa;    __setFalse = False&#xa;except:&#xa;    import __builtin__&#xa;&#xa;    setattr(__builtin__, 'True', 1)&#xa;    setattr(__builtin__, 'False', 0)&#xa;&#xa;&#xa;# ------------------------------------------------------------------------------------------------------ class for errors&#xa;&#xa;class VariableError(RuntimeError): pass&#xa;&#xa;&#xa;class FrameNotFoundError(RuntimeError): pass&#xa;&#xa;&#xa;def _iter_frames(initialFrame):&#xa;    '''NO-YIELD VERSION: Iterates through all the frames starting at the specified frame (which will be the first returned item)'''&#xa;    # cannot use yield&#xa;    frames = []&#xa;&#xa;    while initialFrame is not None:&#xa;        frames.append(initialFrame)&#xa;        initialFrame = initialFrame.f_back&#xa;&#xa;    return frames&#xa;&#xa;&#xa;def dump_frames(thread_id):&#xa;    sys.stdout.write('dumping frames\n')&#xa;    if thread_id != get_thread_id(threading.currentThread()):&#xa;        raise VariableError(""find_frame: must execute on same thread"")&#xa;&#xa;    curFrame = get_frame()&#xa;    for frame in _iter_frames(curFrame):&#xa;        sys.stdout.write('%s\n' % pickle.dumps(frame))&#xa;&#xa;&#xa;# ===============================================================================&#xa;# AdditionalFramesContainer&#xa;# ===============================================================================&#xa;class AdditionalFramesContainer:&#xa;    lock = thread.allocate_lock()&#xa;    additional_frames = {}  # dict of dicts&#xa;&#xa;&#xa;def add_additional_frame_by_id(thread_id, frames_by_id):&#xa;    AdditionalFramesContainer.additional_frames[thread_id] = frames_by_id&#xa;&#xa;&#xa;addAdditionalFrameById = add_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def remove_additional_frame_by_id(thread_id):&#xa;    del AdditionalFramesContainer.additional_frames[thread_id]&#xa;&#xa;&#xa;removeAdditionalFrameById = remove_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def has_additional_frames_by_id(thread_id):&#xa;    return dict_contains(AdditionalFramesContainer.additional_frames, thread_id)&#xa;&#xa;&#xa;def get_additional_frames_by_id(thread_id):&#xa;    return AdditionalFramesContainer.additional_frames.get(thread_id)&#xa;&#xa;&#xa;def find_frame(thread_id, frame_id):&#xa;    """""" returns a frame on the thread that has a given frame_id """"""&#xa;    try:&#xa;        curr_thread_id = get_thread_id(threading.currentThread())&#xa;        if thread_id != curr_thread_id:&#xa;            try:&#xa;                return get_custom_frame(thread_id, frame_id)  # I.e.: thread_id could be a stackless frame id + thread_id.&#xa;            except:&#xa;                pass&#xa;&#xa;            raise VariableError(""find_frame: must execute on same thread (%s != %s)"" % (thread_id, curr_thread_id))&#xa;&#xa;        lookingFor = int(frame_id)&#xa;&#xa;        if AdditionalFramesContainer.additional_frames:&#xa;            if dict_contains(AdditionalFramesContainer.additional_frames, thread_id):&#xa;                frame = AdditionalFramesContainer.additional_frames[thread_id].get(lookingFor)&#xa;&#xa;                if frame is not None:&#xa;                    return frame&#xa;&#xa;        curFrame = get_frame()&#xa;        if frame_id == ""*"":&#xa;            return curFrame  # any frame is specified with ""*""&#xa;&#xa;        frameFound = None&#xa;&#xa;        for frame in _iter_frames(curFrame):&#xa;            if lookingFor == id(frame):&#xa;                frameFound = frame&#xa;                del frame&#xa;                break&#xa;&#xa;            del frame&#xa;&#xa;        # Important: python can hold a reference to the frame from the current context&#xa;        # if an exception is raised, so, if we don't explicitly add those deletes&#xa;        # we might have those variables living much more than we'd want to.&#xa;&#xa;        # I.e.: sys.exc_info holding reference to frame that raises exception (so, other places&#xa;        # need to call sys.exc_clear())&#xa;        del curFrame&#xa;&#xa;        if frameFound is None:&#xa;            msgFrames = ''&#xa;            i = 0&#xa;&#xa;            for frame in _iter_frames(get_frame()):&#xa;                i += 1&#xa;                msgFrames += str(id(frame))&#xa;                if i % 5 == 0:&#xa;                    msgFrames += '\n'&#xa;                else:&#xa;                    msgFrames += '  -  '&#xa;&#xa;            errMsg = '''find_frame: frame not found.&#xa;    Looking for thread_id:%s, frame_id:%s&#xa;    Current     thread_id:%s, available frames:&#xa;    %s\n&#xa;    ''' % (thread_id, lookingFor, curr_thread_id, msgFrames)&#xa;&#xa;            sys.stderr.write(errMsg)&#xa;            return None&#xa;&#xa;        return frameFound&#xa;    except:&#xa;        import traceback&#xa;        traceback.print_exc()&#xa;        return None&#xa;&#xa;&#xa;def getVariable(thread_id, frame_id, scope, attrs):&#xa;    """"""&#xa;    returns the value of a variable&#xa;&#xa;    :scope: can be BY_ID, EXPRESSION, GLOBAL, LOCAL, FRAME&#xa;&#xa;    BY_ID means we'll traverse the list of all objects alive to get the object.&#xa;&#xa;    :attrs: after reaching the proper scope, we have to get the attributes until we find&#xa;            the proper location (i.e.: obj\tattr1\tattr2)&#xa;&#xa;    :note: when BY_ID is used, the frame_id is considered the id of the object to find and&#xa;           not the frame (as we don't care about the frame in this case).&#xa;    """"""&#xa;    if scope == 'BY_ID':&#xa;        if thread_id != get_thread_id(threading.currentThread()):&#xa;            raise VariableError(""getVariable: must execute on same thread"")&#xa;&#xa;        try:&#xa;            import gc&#xa;            objects = gc.get_objects()&#xa;        except:&#xa;            pass  # Not all python variants have it.&#xa;        else:&#xa;            frame_id = int(frame_id)&#xa;            for var in objects:&#xa;                if id(var) == frame_id:&#xa;                    if attrs is not None:&#xa;                        attrList = attrs.split('\t')&#xa;                        for k in attrList:&#xa;                            _type, _typeName, resolver = get_type(var)&#xa;                            var = resolver.resolve(var, k)&#xa;&#xa;                    return var&#xa;&#xa;        # If it didn't return previously, we coudn't find it by id (i.e.: alrceady garbage collected).&#xa;        sys.stderr.write('Unable to find object with id: %s\n' % (frame_id,))&#xa;        return None&#xa;&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return {}&#xa;&#xa;    if attrs is not None:&#xa;        attrList = attrs.split('\t')&#xa;    else:&#xa;        attrList = []&#xa;&#xa;    for attr in attrList:&#xa;        attr.replace(""@_@TAB_CHAR@_@"", '\t')&#xa;&#xa;    if scope == 'EXPRESSION':&#xa;        for count in xrange(len(attrList)):&#xa;            if count == 0:&#xa;                # An Expression can be in any scope (globals/locals), therefore it needs to evaluated as an expression&#xa;                var = evaluate_expression(thread_id, frame_id, attrList[count], False)&#xa;            else:&#xa;                _type, _typeName, resolver = get_type(var)&#xa;                var = resolver.resolve(var, attrList[count])&#xa;    else:&#xa;        if scope == ""GLOBAL"":&#xa;            var = frame.f_globals&#xa;            del attrList[0]  # globals are special, and they get a single dummy unused attribute&#xa;        else:&#xa;            # in a frame access both locals and globals as Python does&#xa;            var = {}&#xa;            var.update(frame.f_globals)&#xa;            var.update(frame.f_locals)&#xa;&#xa;        for k in attrList:&#xa;            _type, _typeName, resolver = get_type(var)&#xa;            var = resolver.resolve(var, k)&#xa;&#xa;    return var&#xa;&#xa;&#xa;def resolve_compound_variable(thread_id, frame_id, scope, attrs):&#xa;    """""" returns the value of the compound variable as a dictionary""""""&#xa;&#xa;    var = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        _type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        sys.stderr.write('Error evaluating: thread_id: %s\nframe_id: %s\nscope: %s\nattrs: %s\n' % (&#xa;            thread_id, frame_id, scope, attrs,))&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def resolve_var(var, attrs):&#xa;    attrList = attrs.split('\t')&#xa;&#xa;    for k in attrList:&#xa;        type, _typeName, resolver = get_type(var)&#xa;&#xa;        var = resolver.resolve(var, k)&#xa;&#xa;    try:&#xa;        type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def custom_operation(thread_id, frame_id, scope, attrs, style, code_or_file, operation_fn_name):&#xa;    """"""&#xa;    We'll execute the code_or_file and then search in the namespace the operation_fn_name to execute with the given var.&#xa;&#xa;    code_or_file: either some code (i.e.: from pprint import pprint) or a file to be executed.&#xa;    operation_fn_name: the name of the operation to execute after the exec (i.e.: pprint)&#xa;    """"""&#xa;    expressionValue = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        namespace = {'__name__': '<custom_operation>'}&#xa;        if style == ""EXECFILE"":&#xa;            namespace['__file__'] = code_or_file&#xa;            execfile(code_or_file, namespace, namespace)&#xa;        else:  # style == EXEC&#xa;            namespace['__file__'] = '<customOperationCode>'&#xa;            Exec(code_or_file, namespace, namespace)&#xa;&#xa;        return str(namespace[operation_fn_name](expressionValue))&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def eval_in_context(expression, globals, locals):&#xa;    result = None&#xa;    try:&#xa;        result = eval(expression, globals, locals)&#xa;    except Exception:&#xa;        s = StringIO()&#xa;        traceback.print_exc(file=s)&#xa;        result = s.getvalue()&#xa;&#xa;        try:&#xa;            try:&#xa;                etype, value, tb = sys.exc_info()&#xa;                result = value&#xa;            finally:&#xa;                etype = value = tb = None&#xa;        except:&#xa;            pass&#xa;&#xa;        result = ExceptionOnEvaluate(result)&#xa;&#xa;        # Ok, we have the initial error message, but let's see if we're dealing with a name mangling error...&#xa;        try:&#xa;            if '__' in expression:&#xa;                # Try to handle '__' name mangling...&#xa;                split = expression.split('.')&#xa;                curr = locals.get(split[0])&#xa;                for entry in split[1:]:&#xa;                    if entry.startswith('__') and not hasattr(curr, entry):&#xa;                        entry = '_%s%s' % (curr.__class__.__name__, entry)&#xa;                    curr = getattr(curr, entry)&#xa;&#xa;                result = curr&#xa;        except:&#xa;            pass&#xa;    return result&#xa;&#xa;&#xa;def evaluate_expression(thread_id, frame_id, expression, doExec):&#xa;    '''returns the result of the evaluated expression&#xa;    @param doExec: determines if we should do an exec or an eval&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    # Not using frame.f_globals because of https://sourceforge.net/tracker2/?func=detail&aid=2541355&group_id=85796&atid=577329&#xa;    # (Names not resolved in generator expression in method)&#xa;    # See message: http://mail.python.org/pipermail/python-list/2009-January/526522.html&#xa;    updated_globals = {}&#xa;    updated_globals.update(frame.f_globals)&#xa;    updated_globals.update(frame.f_locals)  # locals later because it has precedence over the actual globals&#xa;&#xa;    try:&#xa;        expression = str(expression.replace('@LINE@', '\n'))&#xa;&#xa;        if doExec:&#xa;            try:&#xa;                # try to make it an eval (if it is an eval we can print it, otherwise we'll exec it and&#xa;                # it will have whatever the user actually did)&#xa;                compiled = compile(expression, '<string>', 'eval')&#xa;            except:&#xa;                Exec(expression, updated_globals, frame.f_locals)&#xa;                pydevd_save_locals.save_locals(frame)&#xa;            else:&#xa;                result = eval(compiled, updated_globals, frame.f_locals)&#xa;                if result is not None:  # Only print if it's not None (as python does)&#xa;                    sys.stdout.write('%s\n' % (result,))&#xa;            return&#xa;&#xa;        else:&#xa;            return eval_in_context(expression, updated_globals, frame.f_locals)&#xa;    finally:&#xa;        # Should not be kept alive if an exception happens and this frame is kept in the stack.&#xa;        del updated_globals&#xa;        del frame&#xa;&#xa;&#xa;def change_attr_expression(thread_id, frame_id, attr, expression, dbg, value=SENTINEL_VALUE):&#xa;    '''Changes some attribute in a given frame.&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    try:&#xa;        expression = expression.replace('@LINE@', '\n')&#xa;&#xa;        if dbg.plugin and value is SENTINEL_VALUE:&#xa;            result = dbg.plugin.change_variable(frame, attr, expression)&#xa;            if result:&#xa;                return result&#xa;&#xa;        if attr[:7] == ""Globals"":&#xa;            attr = attr[8:]&#xa;            if attr in frame.f_globals:&#xa;                if value is SENTINEL_VALUE:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_globals[attr] = value&#xa;                return frame.f_globals[attr]&#xa;        else:&#xa;            if pydevd_save_locals.is_save_locals_available():&#xa;                if value is SENTINEL_VALUE:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_locals[attr] = value&#xa;                pydevd_save_locals.save_locals(frame)&#xa;                return frame.f_locals[attr]&#xa;&#xa;            # default way (only works for changing it in the topmost frame)&#xa;            if value is SENTINEL_VALUE:&#xa;                value = eval(expression, frame.f_globals, frame.f_locals)&#xa;            result = value&#xa;            Exec('%s=%s' % (attr, expression), frame.f_globals, frame.f_locals)&#xa;            return result&#xa;&#xa;&#xa;    except Exception:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;MAXIMUM_ARRAY_SIZE = 100&#xa;&#xa;def table_like_struct_to_xml(array, name, roffset, coffset, rows, cols, format):&#xa;    _, type_name, _ = get_type(array)&#xa;    if type_name == 'ndarray':&#xa;        array, metaxml, r, c, f = array_to_meta_xml(array, name, format)&#xa;        xml = metaxml&#xa;        format = '%' + f&#xa;        if rows == -1 and cols == -1:&#xa;            rows = r&#xa;            cols = c&#xa;        xml += array_to_xml(array, roffset, coffset, rows, cols, format)&#xa;    elif type_name == 'DataFrame':&#xa;        xml = dataframe_to_xml(array, name, roffset, coffset, rows, cols, format)&#xa;    else:&#xa;        raise VariableError(""Do not know how to convert type %s to table"" % (type_name))&#xa;&#xa;    return ""<xml>%s</xml>"" % xml&#xa;&#xa;&#xa;def array_to_xml(array, roffset, coffset, rows, cols, format):&#xa;    xml = """"&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(cols, MAXIMUM_ARRAY_SIZE)&#xa;&#xa;    # there is no obvious rule for slicing (at least 5 choices)&#xa;    if len(array) == 1 and (rows > 1 or cols > 1):&#xa;        array = array[0]&#xa;    if array.size > len(array):&#xa;        array = array[roffset:, coffset:]&#xa;        rows = min(rows, len(array))&#xa;        cols = min(cols, len(array[0]))&#xa;        if len(array) == 1:&#xa;            array = array[0]&#xa;    elif array.size == len(array):&#xa;        if roffset == 0 and rows == 1:&#xa;            array = array[coffset:]&#xa;            cols = min(cols, len(array))&#xa;        elif coffset == 0 and cols == 1:&#xa;            array = array[roffset:]&#xa;            rows = min(rows, len(array))&#xa;&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>"" % to_string(row)&#xa;        for col in range(cols):&#xa;            value = array&#xa;            if rows == 1 or cols == 1:&#xa;                if rows == 1 and cols == 1:&#xa;                    value = array[0]&#xa;                else:&#xa;                    if rows == 1:&#xa;                        dim = col&#xa;                    else:&#xa;                        dim = row&#xa;                    value = array[dim]&#xa;                    if ""ndarray"" in str(type(value)):&#xa;                        value = value[0]&#xa;            else:&#xa;                value = array[row][col]&#xa;            value = format % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;&#xa;&#xa;def array_to_meta_xml(array, name, format):&#xa;    type = array.dtype.kind&#xa;    slice = name&#xa;    l = len(array.shape)&#xa;&#xa;    # initial load, compute slice&#xa;    if format == '%':&#xa;        if l > 2:&#xa;            slice += '[0]' * (l - 2)&#xa;            for r in range(l - 2):&#xa;                array = array[0]&#xa;        if type == 'f':&#xa;            format = '.5f'&#xa;        elif type == 'i' or type == 'u':&#xa;            format = 'd'&#xa;        else:&#xa;            format = 's'&#xa;    else:&#xa;        format = format.replace('%', '')&#xa;&#xa;    l = len(array.shape)&#xa;    reslice = """"&#xa;    if l > 2:&#xa;        raise Exception(""%s has more than 2 dimensions."" % slice)&#xa;    elif l == 1:&#xa;        # special case with 1D arrays arr[i, :] - row, but arr[:, i] - column with equal shape and ndim&#xa;        # http://stackoverflow.com/questions/16837946/numpy-a-2-rows-1-column-file-loadtxt-returns-1row-2-columns&#xa;        # explanation: http://stackoverflow.com/questions/15165170/how-do-i-maintain-row-column-orientation-of-vectors-in-numpy?rq=1&#xa;        # we use kind of a hack - get information about memory from C_CONTIGUOUS&#xa;        is_row = array.flags['C_CONTIGUOUS']&#xa;&#xa;        if is_row:&#xa;            rows = 1&#xa;            cols = len(array)&#xa;            if cols < len(array):&#xa;                reslice = '[0:%s]' % (cols)&#xa;            array = array[0:cols]&#xa;        else:&#xa;            cols = 1&#xa;            rows = len(array)&#xa;            if rows < len(array):&#xa;                reslice = '[0:%s]' % (rows)&#xa;            array = array[0:rows]&#xa;    elif l == 2:&#xa;        rows = array.shape[-2]&#xa;        cols = array.shape[-1]&#xa;        if cols < array.shape[-1] or rows < array.shape[-2]:&#xa;            reslice = '[0:%s, 0:%s]' % (rows, cols)&#xa;        array = array[0:rows, 0:cols]&#xa;&#xa;    # avoid slice duplication&#xa;    if not slice.endswith(reslice):&#xa;        slice += reslice&#xa;&#xa;    bounds = (0, 0)&#xa;    if type in ""biufc"":&#xa;        bounds = (array.min(), array.max())&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""%s\"" type=\""%s\"" max=\""%s\"" min=\""%s\""/>' % \&#xa;          (slice, rows, cols, format, type, bounds[1], bounds[0])&#xa;    return array, xml, rows, cols, format&#xa;&#xa;&#xa;def array_default_format(type):&#xa;    if type == 'f':&#xa;        return '.5f'&#xa;    elif type == 'i' or type == 'u':&#xa;        return 'd'&#xa;    else:&#xa;        return 's'&#xa;&#xa;&#xa;def dataframe_to_xml(df, name, roffset, coffset, rows, cols, format):&#xa;    """"""&#xa;    :type df: pandas.core.frame.DataFrame&#xa;    :type name: str&#xa;    :type coffset: int&#xa;    :type roffset: int&#xa;    :type rows: int&#xa;    :type cols: int&#xa;    :type format: str&#xa;&#xa;&#xa;    """"""&#xa;    num_rows = df.shape[0]&#xa;    num_cols = df.shape[1]&#xa;    if (num_rows, num_cols) != df.shape:&#xa;        df = df.iloc[0:num_rows, 0: num_cols]&#xa;        slice = '.iloc[0:%s, 0:%s]' % (num_rows, num_cols)&#xa;    else:&#xa;        slice = ''&#xa;    slice = name + slice&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""\"" type=\""\"" max=\""0\"" min=\""0\""/>\n' % \&#xa;          (slice, num_rows, num_cols)&#xa;&#xa;    if (rows, cols) == (-1, -1):&#xa;        rows, cols = num_rows, num_cols&#xa;&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(min(cols, MAXIMUM_ARRAY_SIZE), num_cols)&#xa;    # need to precompute column bounds here before slicing!&#xa;    col_bounds = [None] * cols&#xa;    for col in range(cols):&#xa;        dtype = df.dtypes.iloc[coffset + col].kind&#xa;        if dtype in ""biufc"":&#xa;            cvalues = df.iloc[:, coffset + col]&#xa;            bounds = (cvalues.min(), cvalues.max())&#xa;        else:&#xa;            bounds = (0, 0)&#xa;        col_bounds[col] = bounds&#xa;&#xa;    df = df.iloc[roffset: roffset + rows, coffset: coffset + cols]&#xa;    rows, cols = df.shape&#xa;&#xa;&#xa;    xml += ""<headerdata rows=\""%s\"" cols=\""%s\"">\n"" % (rows, cols)&#xa;    format = format.replace('%', '')&#xa;    col_formats = []&#xa;&#xa;    get_label = lambda label: str(label) if not isinstance(label, tuple) else '/'.join(map(str, label))&#xa;&#xa;    for col in range(cols):&#xa;        dtype = df.dtypes.iloc[col].kind&#xa;        fmt = format if (dtype == 'f' and format) else array_default_format(dtype)&#xa;        col_formats.append('%' + fmt)&#xa;        bounds = col_bounds[col]&#xa;&#xa;        xml += '<colheader index=\""%s\"" label=\""%s\"" type=\""%s\"" format=\""%s\"" max=\""%s\"" min=\""%s\"" />\n' % \&#xa;               (str(col), get_label(df.axes[1].values[col]), dtype, fmt, bounds[1], bounds[0])&#xa;    for row, label in enumerate(iter(df.axes[0])):&#xa;        xml += ""<rowheader index=\""%s\"" label = \""%s\""/>\n"" % \&#xa;               (str(row), get_label(label))&#xa;    xml += ""</headerdata>\n""&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>\n"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>\n"" % str(row)&#xa;        for col in range(cols):&#xa;            value = df.iat[row, col]&#xa;            value = col_formats[col] % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;"
862173|"#!/usr/bin/env python&#xa;'''This is a quick and dirty way to provide developers a way to download&#xa;resources needed to build and test components of the open tree software.&#xa;&#xa;environmental variables:&#xa;    OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_LEVEL=debug for more info&#xa;    &#xa;    OPEN_TREE_DOWNLOAD_DEV_RESOURCE_CFG or OPEN_TREE_USER_SETTINGS_DIR to change the &#xa;        default location for config file that stores local filepaths&#xa;&#xa;'''&#xa;import os&#xa;import sys&#xa;import logging&#xa;from os.path import basename&#xa;from urlparse import urlsplit&#xa;import urllib2&#xa;import urllib&#xa;import shutil&#xa;import subprocess&#xa;import zipfile&#xa;from open_tree_env import get_otol_build_env, put_otol_build_env_into_env, abbreviate_path&#xa;&#xa;&#xa;&#xa;# Global dictionary of resources grouped by 'category' Note that dict is filled&#xa;#   as a side effect of creating an OpenTreeResource&#xa;ALL_RESOURCES = {}&#xa;RESOURCE_CATEGORIES = {'taxonomy' : 'OPEN_TREE_TAXONOMY_DIR',&#xa;                       'dependency' : 'OPEN_TREE_DEPENDENCY_DIR'&#xa;                       }&#xa;for key in RESOURCE_CATEGORIES.keys():&#xa;    ALL_RESOURCES[key] = []&#xa;SUPPORTED_PROTOCOLS = ['http', 'svn']&#xa;&#xa;# global config file parser&#xa;_CFG_PARSER = None&#xa;_ACTION_LOG_FILE = None&#xa;_LAST_LOGGED_DIR = None&#xa;def _get_action_log_file():&#xa;    global _ACTION_LOG_FILE&#xa;    if _ACTION_LOG_FILE is None:&#xa;        fp = os.path.join(get_otol_build_env('OPEN_TREE_DEPENDENCY_DIR'), 'log-bootstrap.txt')&#xa;        _LOG.debug('Opening ""%s"" as action log' % fp)&#xa;        fo = open(fp, 'a')&#xa;        _ACTION_LOG_FILE = fo&#xa;    return _ACTION_LOG_FILE&#xa;        &#xa;def log_action(message):&#xa;    global _LAST_LOGGED_DIR&#xa;    op = os.path.abspath(os.curdir)&#xa;    action_log_file = _get_action_log_file()&#xa;    if (_LAST_LOGGED_DIR is None) or (_LAST_LOGGED_DIR != op):&#xa;        _LAST_LOGGED_DIR = op&#xa;        abbrev = abbreviate_path(op)&#xa;        action_log_file.write('cd ""%s""\n' % abbrev)        &#xa;    action_log_file.write(message + '\n')&#xa;    action_log_file.flush()&#xa;    _LOG.debug(""ACTION: "" + message)&#xa;    &#xa;&#xa;################################################################################&#xa;# The following is from http://stackoverflow.com/questions/862173/how-to-download-a-file-using-python-in-a-smarter-way&#xa;############################&#xa;&#xa;def url2name(url):&#xa;    return basename(urllib.unquote(urlsplit(url)[2]))&#xa;&#xa;def download_http(url, localFileName=None):&#xa;    localName = url2name(url)&#xa;    req = urllib2.Request(url)&#xa;    r = urllib2.urlopen(req)&#xa;    if r.info().has_key('Content-Disposition'):&#xa;        # If the response has Content-Disposition, we take file name from it&#xa;        localName = r.info()['Content-Disposition'].split('filename=')[1]&#xa;        dq = (localName[0] == '""' and localName[-1] == '""')&#xa;        sq = (localName[0] == ""'"" and localName[-1] == ""'"")&#xa;        if sq or dq:&#xa;            localName = localName[1:-1]&#xa;    elif r.url != url: &#xa;        # if we were redirected, the real file name we take from the final URL&#xa;        localName = url2name(r.url)&#xa;    if localFileName: &#xa;        # we can force to save the file as specified name&#xa;        localName = localFileName&#xa;    f = open(localName, 'wb')&#xa;    d = os.path.abspath(os.curdir)&#xa;    _LOG.info('Downloading ""%s"" to ""%s""...\n' % (localName, d))&#xa;    shutil.copyfileobj(r, f)&#xa;    f.close()&#xa;    r.close()&#xa;    log_action('wget ""%s""' % (url))&#xa;    return localName&#xa;&#xa;def _my_makedirs(dir):&#xa;    '''Calls os.makedirs (if needed), and logs the action.'''&#xa;    if not os.path.exists(dir):&#xa;        os.makedirs(dir)&#xa;        log_action('mkdir -p ""%s""' % os.path.abspath(dir))&#xa;def _my_chdir(dir):&#xa;    '''Calls os.chdir, and logs the action.'''&#xa;    if not os.path.exists(dir):&#xa;        _my_makedirs(dir)&#xa;    os.chdir(dir)&#xa;&#xa;&#xa;############################&#xa;# End code snippet from stackoverflow&#xa;################################################################################&#xa;## The following is modified from: http://code.activestate.com/recipes/252508/ (r2)&#xa;# unzip.py&#xa;#     Version: 1.1&#xa;# &#xa;#     Extract a zipfile to the directory provided&#xa;#     It first creates the directory structure to house the files&#xa;#     then it extracts the files to it.&#xa;#     &#xa;# &#xa;#     By Doug Tolton&#xa;&#xa;&#xa;class unzip:&#xa;        &#xa;    def extract(self, file, dir):&#xa;        if not dir.endswith(':'):&#xa;            _my_makedirs(dir)&#xa;    &#xa;        zf = zipfile.ZipFile(file)&#xa;&#xa;        # create directory structure to house files&#xa;        self._createstructure(file, dir)&#xa;&#xa;        # extract files to directory structure&#xa;        for i, name in enumerate(zf.namelist()):&#xa;            if not name.endswith('/'):&#xa;                outfile = open(os.path.join(dir, name), 'wb')&#xa;                outfile.write(zf.read(name))&#xa;                outfile.flush()&#xa;                outfile.close()&#xa;        log_action('unzip ""%s""' % (file))&#xa;&#xa;    def _createstructure(self, file, dir):&#xa;        self._makedirs(self._listdirs(file), dir)&#xa;&#xa;&#xa;    def _makedirs(self, directories, basedir):&#xa;        """""" Create any directories that don't currently exist """"""&#xa;        for dir in directories:&#xa;            curdir = os.path.join(basedir, dir)&#xa;            _my_makedirs(curdir)&#xa;&#xa;    def _listdirs(self, file):&#xa;        """""" Grabs all the directories in the zip structure&#xa;        This is necessary to create the structure before trying&#xa;        to extract the file to it. """"""&#xa;        zf = zipfile.ZipFile(file)&#xa;&#xa;        dirs = []&#xa;&#xa;        for name in zf.namelist():&#xa;            if name.endswith('/'):&#xa;                dirs.append(name)&#xa;&#xa;        dirs.sort()&#xa;        return dirs&#xa;&#xa;def unzip_file(file, dest_parent):&#xa;    u = unzip()&#xa;    u.extract(file, dest_parent)&#xa;## end of http://code.activestate.com/recipes/252508/ }}}&#xa;&#xa;def untar_gz(file, dest_parent):&#xa;    system_call(['tar', 'xfvz', os.path.abspath(file)], wd=dest_parent)&#xa;&#xa;class RESOURCE_STATUS_CODE:&#xa;    SKIPPED, MISSING, DOWNLOADED, INSTALLED = range(4)&#xa;&#xa;&#xa;def system_call(invoc, wd=None):&#xa;    prev_dir = None&#xa;    if wd is not None:&#xa;        prev_dir = os.path.abspath(os.curdir)&#xa;        wd = os.path.expandvars(wd)&#xa;        _my_makedirs(wd)&#xa;    else:&#xa;        wd = os.curdir&#xa;    try:&#xa;        wl = []&#xa;        for word in invoc:&#xa;            if (len(word.split()) > 1) or (len(word.split('$')) > 1):&#xa;                wl.append('""%s""' % word)&#xa;            else:&#xa;                wl.append(word)&#xa;        m = ' '.join(wl)&#xa;        d = os.path.abspath(wd)&#xa;        _my_chdir(d)&#xa;        log_action(m)&#xa;        rc = subprocess.call(invoc, shell=True)&#xa;        if rc != 0:&#xa;            message = 'The command:\n""%s""\nexecuted from %s failed with returncode %d\n' % (m, d, rc)&#xa;            raise RuntimeError(message)&#xa;    finally:&#xa;        if prev_dir is not None:&#xa;            _my_chdir(prev_dir)&#xa;        &#xa;class OpenTreeResource(object):&#xa;    '''A bundle of information about a resource. Attributes:&#xa;        `name` - name resource when downloaded and unpacked&#xa;        `url` - location of the downloadable resource&#xa;        `protocol` = http, git, svn&#xa;        `compression` - compression protocol (empty or zip)&#xa;        `min_version` - tuple of 3 integers (major, minor, revision)&#xa;        `category` - name of the type of resource (taxonomy, dependency...)&#xa;        `compressed_name` - if different from the default for the compression type.&#xa;                should be identical to the file name created&#xa;        `description` - string describing the resource&#xa;        `install_steps` is a list of dicts. each one has a wd key (working directory)&#xa;                and a list of commands to be run from that directory&#xa;        `install_parent` the parent directory of the installed products&#xa;        `install_sub` a path to be joined to `install_parent` to verify that &#xa;                the install step worked.&#xa;    '''&#xa;    UNPACKING_PROTOCOLS = ['', 'zip', 'tar.gz']&#xa;    def __init__(self,&#xa;                 name,&#xa;                 url,&#xa;                 protocol='http',&#xa;                 min_version=(0,0,0),&#xa;                 category=None,&#xa;                 compression='',&#xa;                 compressed_name=None,&#xa;                 contact='',&#xa;                 description='',&#xa;                 install_steps=None,&#xa;                 install_parent='',&#xa;                 install_sub='',&#xa;                 requires=None):&#xa;        self.name = name&#xa;        self.url = url&#xa;        self.protocol = protocol.lower()&#xa;        if self.protocol not in SUPPORTED_PROTOCOLS:&#xa;            raise ValueError('Protocol ' + protocol + ' unrecognized')&#xa;        self.category = category.lower()&#xa;        if self.category not in RESOURCE_CATEGORIES.keys():&#xa;            raise ValueError('Category ' + category + ' unrecognized')&#xa;        self.compression = compression.lower()&#xa;        if self.compression not in OpenTreeResource.UNPACKING_PROTOCOLS:&#xa;            raise ValueError('Compression method ' + compression + ' unrecognized')&#xa;        if compressed_name is None:&#xa;            if self.compression == '':&#xa;                self.compressed_name = self.name&#xa;            elif self.compression == 'zip':&#xa;                self.compressed_name = self.name + '.zip'&#xa;            elif self.compression == 'tar.gz':&#xa;                self.compressed_name = self.name + '.tar.gz'&#xa;            else:&#xa;                assert False&#xa;        else:&#xa;            self.compressed_name = compressed_name&#xa;        self.contact = contact&#xa;        self.description = description&#xa;        self.path = None&#xa;        self.installed_path = None&#xa;        self.status = None&#xa;        self.install_steps = install_steps&#xa;        self.install_parent = install_parent&#xa;        self.install_sub = install_sub&#xa;        self.requires = requires&#xa;        # here is the hack in which we add resource to the global list&#xa;        ALL_RESOURCES[self.category].append(self)&#xa;&#xa;    def listing(self, opts):&#xa;        return '%s = %s' % (self.name, self.description)&#xa;&#xa;    def do_install(self, cfg_path, opts):&#xa;        if self.status < RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;            self.do_download()&#xa;        if self.install_steps:&#xa;            final_path = os.path.join(os.path.expandvars(self.install_parent), &#xa;                                      os.path.expandvars(self.install_sub))&#xa;        else:&#xa;            final_path = self.path&#xa;        if os.path.exists(final_path):&#xa;            self.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;            self.installed_path = final_path&#xa;            return&#xa;        if self.requires:&#xa;            for requirement in self.requires:&#xa;                install_command(requirement, cfg_path, opts)&#xa;        cwd = os.path.abspath(os.getcwd())&#xa;        try:&#xa;            &#xa;            parent_var = os.path.expandvars(self.install_parent)&#xa;            _my_makedirs(parent_var)&#xa;            download_parent = os.path.dirname(os.path.abspath(self.path))&#xa;            _my_chdir(self.path)&#xa;            for step_dict in self.install_steps:&#xa;                wd = step_dict.get('wd')&#xa;                cmd_list = step_dict.get('commands')&#xa;                for cmd in cmd_list:&#xa;                    system_call(cmd, wd)&#xa;        finally:&#xa;            _my_chdir(cwd)&#xa;        if not os.path.exists(final_path):&#xa;            raise RuntimeError('Installation steps completed, but installation products were not found at ""%s""' % final_path)&#xa;        self.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;        self.installed_path = final_path&#xa;            &#xa;    def do_download(self):&#xa;        parent_var = RESOURCE_CATEGORIES[self.category]&#xa;        parent_dir = get_otol_build_env(parent_var)&#xa;        assert parent_dir&#xa;        _my_makedirs(parent_dir)&#xa;        cwd = os.path.abspath(os.getcwd())&#xa;        try:&#xa;            parent_dir = os.path.abspath(parent_dir)&#xa;            expected_path = os.path.join(parent_dir, self.compressed_name)&#xa;            _my_chdir(parent_dir)&#xa;            if self.protocol == 'http':&#xa;                if os.path.exists(expected_path):&#xa;                    _LOG.warn('Path ""%s"" already exists. It will not be downloaded again...\n' % expected_path)&#xa;                    fp = expected_path&#xa;                else:&#xa;                    p = download_http(self.url)&#xa;                    fp = os.path.join(parent_dir, p)&#xa;                self.compressed_path = fp&#xa;&#xa;                expected_path = os.path.join(parent_dir, self.name)&#xa;                if os.path.exists(expected_path):&#xa;                    if expected_path != fp:&#xa;                        _LOG.warn('Path ""%s"" already exists. It will not be unpacked again...\n' % expected_path)&#xa;                    fp = expected_path&#xa;                else:&#xa;                    if self.compression == 'zip':&#xa;                        _LOG.info('Unzipping ""%s""...\n""' % fp)&#xa;                        unzip_file(fp, parent_dir)&#xa;                        fp = os.path.join(parent_dir, self.name)&#xa;                    elif self.compression == 'tar.gz':&#xa;                        _LOG.info('Unpacking ""%s""...\n""' % fp)&#xa;                        untar_gz(fp, parent_dir)&#xa;                        fp = os.path.join(parent_dir, self.name)&#xa;                    _LOG.info('Obtained ""%s""...\n""' % fp)&#xa;                self.path = fp&#xa;                return fp&#xa;            elif self.protocol == 'svn':&#xa;                if os.path.exists(expected_path):&#xa;                    _LOG.warn('Path ""%s"" already exists. It will not be downloaded again...\n' % fp)&#xa;                else:&#xa;                    system_call(['svn', 'checkout', self.url, self.name])&#xa;                self.path = expected_path&#xa;                self.compressed_path = self.path&#xa;            else:&#xa;                assert False&#xa;            self.status = RESOURCE_STATUS_CODE.DOWNLOADED&#xa;&#xa;        finally:&#xa;            _my_chdir(cwd)&#xa;&#xa;def get_resource_obj(name=''):&#xa;    nl = name.lower()&#xa;    for row in ALL_RESOURCES.itervalues():&#xa;        for resource in row:&#xa;            if resource.name.lower() == nl:&#xa;                return resource&#xa;    return None&#xa;&#xa;def get_flattened_resource_list():&#xa;    r = []&#xa;    for key in RESOURCE_CATEGORIES.keys():&#xa;        r.extend(ALL_RESOURCES[key])&#xa;    return r&#xa;&#xa;def list_command(opts):&#xa;    '''Lists all the known resources to stdout.'''&#xa;    out = sys.stdout&#xa;    for resource in get_flattened_resource_list():&#xa;        out.write(resource.listing(opts))&#xa;        out.write('\n')&#xa;&#xa;&#xa;def get_cfg_interface(cfg_path):&#xa;    global _CFG_PARSER&#xa;    if _CFG_PARSER is None:&#xa;        from ConfigParser import RawConfigParser&#xa;        c = RawConfigParser()&#xa;        for resource in get_flattened_resource_list():&#xa;            c.add_section(resource.name.lower())&#xa;        if cfg_path and os.path.exists(cfg_path):&#xa;            co = open(cfg_path)&#xa;            c.readfp(co)&#xa;            co.close()&#xa;        _CFG_PARSER = c&#xa;    return _CFG_PARSER&#xa;        &#xa;&#xa;def get_resource_status_code(res_id, cfg_path, opts):&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if resource is None:&#xa;        raise ValueError('Resource ""%s"" not recognized' % res_id)&#xa;    resource_path = None&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    try:&#xa;        p = cfg_interface.get(res_id.lower(), 'path')&#xa;    except:&#xa;        p = None&#xa;    resource.status = RESOURCE_STATUS_CODE.MISSING&#xa;    if p and os.path.exists(p):&#xa;        resource.path = p&#xa;        resource.status = RESOURCE_STATUS_CODE.DOWNLOADED&#xa;    try:&#xa;        p = cfg_interface.get(res_id.lower(), 'installed_path')&#xa;    except:&#xa;        p = None&#xa;    if p and os.path.exists(p):&#xa;        resource.installed_path = p&#xa;        resource.status = RESOURCE_STATUS_CODE.INSTALLED&#xa;    return resource.status&#xa;        &#xa;    &#xa;    &#xa;def status_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('""%s"" is INSTALLED at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        _LOG.info('""%s"" is DOWNLOADED at ""%s""' % (res_id, resource.path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.MISSING:&#xa;        _LOG.info('""%s"" is MISSING' % (res_id))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.SKIPPED:&#xa;        _LOG.info('""%s"" is SKIPPED' % (res_id))&#xa;        return&#xa;    assert False&#xa;    &#xa;    &#xa;def get_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('Resource ""%s"" already installed at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s == RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        _LOG.info('Resource ""%s"" already obtained at ""%s""' % (res_id, resource.path))&#xa;        return&#xa;    p = resource.do_download()&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    cfg_interface.set(res_id.lower(), 'path', resource.path)&#xa;    cfg_path_par = os.path.dirname(cfg_path)&#xa;    _my_makedirs(cfg_path_par)&#xa;    with open(cfg_path, 'wb') as cfg_fileobj:&#xa;        cfg_interface.write(cfg_fileobj)&#xa;&#xa;def install_command(res_id, cfg_path, opts):&#xa;    '''Downloads and installs the resource with name `res_id` if it is not &#xa;    already installed.&#xa;    '''&#xa;    s = get_resource_status_code(res_id, cfg_path, opts)&#xa;    resource = get_resource_obj(name=res_id)&#xa;    if s == RESOURCE_STATUS_CODE.INSTALLED:&#xa;        _LOG.info('Resource ""%s"" already installed at ""%s""' % (res_id, resource.installed_path))&#xa;        return&#xa;    if s != RESOURCE_STATUS_CODE.DOWNLOADED:&#xa;        get_command(res_id, cfg_path, opts)&#xa;        &#xa;    p = resource.do_install(cfg_path, opts)&#xa;    cfg_interface = get_cfg_interface(cfg_path)&#xa;    cfg_interface.set(res_id.lower(), 'installed_path', resource.installed_path)&#xa;    cfg_path_par = os.path.dirname(cfg_path)&#xa;    _my_makedirs(cfg_path_par)&#xa;    with open(cfg_path, 'wb') as cfg_fileobj:&#xa;        cfg_interface.write(cfg_fileobj)&#xa;    &#xa;_LOGGING_LEVEL_ENVAR=""OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_LEVEL""&#xa;_LOGGING_FORMAT_ENVAR=""OPEN_TREE_DOWNLOAD_DEV_RESOURCE_LOGGING_FORMAT""&#xa;&#xa;def get_logging_level():&#xa;    if _LOGGING_LEVEL_ENVAR in os.environ:&#xa;        if os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""NOTSET"":&#xa;            level = logging.NOTSET&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""DEBUG"":&#xa;            level = logging.DEBUG&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""INFO"":&#xa;            level = logging.INFO&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""WARNING"":&#xa;            level = logging.WARNING&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""ERROR"":&#xa;            level = logging.ERROR&#xa;        elif os.environ[_LOGGING_LEVEL_ENVAR].upper() == ""CRITICAL"":&#xa;            level = logging.CRITICAL&#xa;        else:&#xa;            level = logging.NOTSET&#xa;    else:&#xa;        level = logging.INFO&#xa;    return level&#xa;&#xa;def get_logger(name=""download_dev_resource""):&#xa;    """"""&#xa;    Returns a logger with name set as given, and configured&#xa;    to the level given by the environment variable _LOGGING_LEVEL_ENVAR.&#xa;    """"""&#xa;&#xa;#     package_dir = os.path.dirname(module_path)&#xa;#     config_filepath = os.path.join(package_dir, _LOGGING_CONFIG_FILE)&#xa;#     if os.path.exists(config_filepath):&#xa;#         try:&#xa;#             logging.config.fileConfig(config_filepath)&#xa;#             logger_set = True&#xa;#         except:&#xa;#             logger_set = False&#xa;    logger = logging.getLogger(name)&#xa;    if not hasattr(logger, 'is_configured'):&#xa;        logger.is_configured = False&#xa;    if not logger.is_configured:&#xa;        level = get_logging_level()&#xa;        rich_formatter = logging.Formatter(""[%(asctime)s] %(filename)s (%(lineno)d): %(levelname) 8s: %(message)s"")&#xa;        simple_formatter = logging.Formatter(""%(levelname) 8s: %(message)s"")&#xa;        raw_formatter = logging.Formatter(""%(message)s"")&#xa;        default_formatter = None&#xa;        logging_formatter = default_formatter&#xa;        if _LOGGING_FORMAT_ENVAR in os.environ:&#xa;            if os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""RICH"":&#xa;                logging_formatter = rich_formatter&#xa;            elif os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""SIMPLE"":&#xa;                logging_formatter = simple_formatter&#xa;            elif os.environ[_LOGGING_FORMAT_ENVAR].upper() == ""NONE"":&#xa;                logging_formatter = None&#xa;            else:&#xa;                logging_formatter = default_formatter&#xa;        else:&#xa;            logging_formatter = default_formatter&#xa;        if logging_formatter is not None:&#xa;            logging_formatter.datefmt='%H:%M:%S'&#xa;        logger.setLevel(level)&#xa;        ch = logging.StreamHandler()&#xa;        ch.setLevel(level)&#xa;        ch.setFormatter(logging_formatter)&#xa;        logger.addHandler(ch)&#xa;        logger.is_configured = True&#xa;    return logger&#xa;_LOG = get_logger()&#xa;        &#xa;            &#xa;&#xa;################################################################################&#xa;# Script by Mark T. Holder available for use under the terms of either the &#xa;# BSD or GPL (v3) license. (see BSDLicense.txt GPL.txt)&#xa;#&#xa;################################################################################&#xa;"
263890|# -*- coding: utf-8 -*-&#xa;# vim: sw=4 ts=4 fenc=utf-8&#xa;'''&#xa;getTerminalSize()&#xa; - get width and height of console&#xa; - works on linux,os x,windows,cygwin(windows)&#xa; - taken from http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python&#xa;'''&#xa;&#xa;# Import python libs&#xa;from __future__ import absolute_import, print_function&#xa;import os&#xa;import platform&#xa;import struct&#xa;import ctypes&#xa;import subprocess&#xa;import fcntl&#xa;import termios&#xa;&#xa;__all__ = ['getTerminalSize']&#xa;&#xa;&#xa;def getTerminalSize():&#xa;    current_os = platform.system()&#xa;    tuple_xy = None&#xa;    if current_os == 'Windows':&#xa;        tuple_xy = _getTerminalSize_windows()&#xa;        if tuple_xy is None:&#xa;            tuple_xy = _getTerminalSize_tput()&#xa;            # needed for window's python in cygwin's xterm!&#xa;    if current_os == 'Linux' or current_os == 'Darwin' or \&#xa;            current_os.startswith('CYGWIN'):&#xa;        tuple_xy = _getTerminalSize_linux()&#xa;    if tuple_xy is None:&#xa;        tuple_xy = (80, 25)      # default value&#xa;    return tuple_xy&#xa;&#xa;&#xa;def _getTerminalSize_windows():&#xa;    res = None&#xa;    try:&#xa;        # stdin handle is -10&#xa;        # stdout handle is -11&#xa;        # stderr handle is -12&#xa;&#xa;        h = ctypes.windll.kernel32.GetStdHandle(-12)&#xa;        csbi = ctypes.create_string_buffer(22)&#xa;        res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)&#xa;    except Exception:&#xa;        return None&#xa;    if res:&#xa;        (bufx, bufy, curx, cury, wattr,&#xa;         left, top, right, bottom, maxx, maxy) = struct.unpack(&#xa;             b'hhhhHhhhhhh', csbi.raw)&#xa;        sizex = right - left + 1&#xa;        sizey = bottom - top + 1&#xa;        return sizex, sizey&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;def _getTerminalSize_tput():&#xa;    # get terminal width&#xa;    # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window&#xa;    try:&#xa;        proc = subprocess.Popen(&#xa;            ['tput', 'cols'], stdin=subprocess.PIPE, stdout=subprocess.PIPE&#xa;        )&#xa;        output = proc.communicate(input=None)&#xa;        cols = int(output[0])&#xa;        proc = subprocess.Popen(&#xa;            ['tput', 'lines'], stdin=subprocess.PIPE, stdout=subprocess.PIPE&#xa;        )&#xa;        output = proc.communicate(input=None)&#xa;        rows = int(output[0])&#xa;        return (cols, rows)&#xa;    except Exception:&#xa;        return None&#xa;&#xa;&#xa;def _getTerminalSize_linux():&#xa;    def ioctl_GWINSZ(fd):&#xa;        try:&#xa;            cr = struct.unpack(&#xa;                b'hh', fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234')&#xa;            )&#xa;        except Exception:&#xa;            return None&#xa;        return cr&#xa;    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)&#xa;    if not cr:&#xa;        try:&#xa;            fd = os.open(os.ctermid(), os.O_RDONLY)&#xa;            cr = ioctl_GWINSZ(fd)&#xa;            os.close(fd)&#xa;        except Exception:&#xa;            pass&#xa;    if not cr:&#xa;        try:&#xa;            cr = (os.environ['LINES'], os.environ['COLUMNS'])&#xa;        except Exception:&#xa;            return None&#xa;    return int(cr[1]), int(cr[0])&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    sizex, sizey = getTerminalSize()&#xa;    print('width = {0}  height = {1}'.format(sizex, sizey))&#xa;
566746|"#!/usr/bin/env python&#xa;&#xa;# ROOT command line tools module: cmdLineUtils&#xa;# Author: Julien Ripoche&#xa;# Mail: julien.ripoche@u-psud.fr&#xa;# Date: 20/08/15&#xa;&#xa;""""""Contain utils for ROOT command line tools""""""&#xa;&#xa;##########&#xa;# Stream redirect functions&#xa;# The original code of the these functions can be found here :&#xa;# http://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python/22434262#22434262&#xa;# Thanks J.F. Sebastian !!&#xa;&#xa;from contextlib import contextmanager&#xa;import os&#xa;import sys&#xa;&#xa;def fileno(file_or_fd):&#xa;    """"""&#xa;    Look for 'fileno' attribute.&#xa;    """"""&#xa;    fd = getattr(file_or_fd, 'fileno', lambda: file_or_fd)()&#xa;    if not isinstance(fd, int):&#xa;        raise ValueError(""Expected a file (`.fileno()`) or a file descriptor"")&#xa;    return fd&#xa;&#xa;@contextmanager&#xa;def streamRedirected(source=sys.stdout, destination=os.devnull):&#xa;    """"""&#xa;    Redirect the output from source to destination.&#xa;    """"""&#xa;    stdout_fd = fileno(source)&#xa;    # copy stdout_fd before it is overwritten&#xa;    #NOTE: `copied` is inheritable on Windows when duplicating a standard stream&#xa;    with os.fdopen(os.dup(stdout_fd), 'wb') as copied:&#xa;        source.flush()  # flush library buffers that dup2 knows nothing about&#xa;        try:&#xa;            os.dup2(fileno(destination), stdout_fd)  # $ exec >&destination&#xa;        except ValueError:  # filename&#xa;            with open(destination, 'wb') as destination_file:&#xa;                os.dup2(destination_file.fileno(), stdout_fd)  # $ exec > destination&#xa;        try:&#xa;            yield source # allow code to be run with the redirected stream&#xa;        finally:&#xa;            # restore source to its previous value&#xa;            #NOTE: dup2 makes stdout_fd inheritable unconditionally&#xa;            source.flush()&#xa;            os.dup2(copied.fileno(), stdout_fd)  # $ exec >&copied&#xa;&#xa;def stdoutRedirected():&#xa;    """"""&#xa;    Redirect the output from sys.stdout to os.devnull.&#xa;    """"""&#xa;    return streamRedirected(sys.stdout, os.devnull)&#xa;&#xa;def stderrRedirected():&#xa;    """"""&#xa;    Redirect the output from sys.stderr to os.devnull.&#xa;    """"""&#xa;    return streamRedirected(sys.stderr, os.devnull)&#xa;&#xa;# The end of streamRedirected functions&#xa;##########&#xa;&#xa;##########&#xa;# Imports&#xa;&#xa;##&#xa;# redirect output (escape characters during ROOT importation...)&#xa;# The gymnastic with sys argv  is necessary to workaround for ROOT-7577&#xa;argvTmp = sys.argv[:]&#xa;sys.argv = []&#xa;with stdoutRedirected():&#xa;    import ROOT&#xa;ROOT.gROOT.GetVersion()&#xa;sys.argv = argvTmp&#xa;&#xa;import argparse&#xa;import glob&#xa;import fnmatch&#xa;import logging&#xa;&#xa;# The end of imports&#xa;##########&#xa;&#xa;##########&#xa;# Different functions to get a parser of arguments and options&#xa;&#xa;def _getParser(theHelp, theEpilog):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils.&#xa;   """"""&#xa;   return argparse.ArgumentParser(description=theHelp,&#xa;                                  formatter_class=argparse.RawDescriptionHelpFormatter,&#xa;                                  epilog = theEpilog)&#xa;&#xa;def getParserSingleFile(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils and a&#xa;   source file or not.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""FILE"", nargs='?', help=""Input file"")&#xa;   return parser&#xa;&#xa;def getParserFile(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils and a&#xa;   list of source files.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""FILE"", nargs='+', help=""Input file"")&#xa;   return parser&#xa;&#xa;def getParserSourceDest(theHelp, theEpilog=""""):&#xa;   """"""&#xa;   Get a commandline parser with the defaults of the commandline utils,&#xa;   a list of source files and a destination file.&#xa;   """"""&#xa;   parser = _getParser(theHelp, theEpilog)&#xa;   parser.add_argument(""SOURCE"", nargs='+', help=""Source file"")&#xa;   parser.add_argument(""DEST"", help=""Destination file"")&#xa;   return parser&#xa;&#xa;# The end of get parser functions&#xa;##########&#xa;&#xa;##########&#xa;# Several utils&#xa;&#xa;@contextmanager&#xa;def _setIgnoreLevel(level):&#xa;    originalLevel = ROOT.gErrorIgnoreLevel&#xa;    ROOT.gErrorIgnoreLevel = level&#xa;    yield&#xa;    ROOT.gErrorIgnoreLevel = originalLevel&#xa;&#xa;def changeDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Change the current directory (ROOT.gDirectory) by the corresponding (rootFile,pathSplit)&#xa;    """"""&#xa;    rootFile.cd()&#xa;    for directoryName in pathSplit:&#xa;        theDir = ROOT.gDirectory.Get(directoryName)&#xa;        if not theDir:&#xa;            logging.warning(""Directory %s does not exist."" %directoryName)&#xa;            return 1&#xa;        else:&#xa;            theDir.cd()&#xa;    return 0&#xa;&#xa;def createDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Add a directory named 'pathSplit[-1]' in (rootFile,pathSplit[:-1])&#xa;    """"""&#xa;    retcode = changeDirectory(rootFile,pathSplit[:-1])&#xa;    if retcode == 0: ROOT.gDirectory.mkdir(pathSplit[-1])&#xa;    return retcode&#xa;&#xa;def getFromDirectory(objName):&#xa;    """"""&#xa;    Get the object objName from the current directory&#xa;    """"""&#xa;    return ROOT.gDirectory.Get(objName)&#xa;&#xa;def isExisting(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), exits&#xa;    """"""&#xa;    changeDirectory(rootFile,pathSplit[:-1])&#xa;    return ROOT.gDirectory.GetListOfKeys().Contains(pathSplit[-1])&#xa;&#xa;def isDirectoryKey(key):&#xa;    """"""&#xa;    Return True if the object, corresponding to the key, inherits from TDirectory&#xa;    """"""&#xa;    classname = key.GetClassName()&#xa;    cl = ROOT.gROOT.GetClass(classname)&#xa;    return cl.InheritsFrom(ROOT.TDirectory.Class())&#xa;&#xa;def isTreeKey(key):&#xa;    """"""&#xa;    Return True if the object, corresponding to the key, inherits from TTree&#xa;    """"""&#xa;    classname = key.GetClassName()&#xa;    cl = ROOT.gROOT.GetClass(classname)&#xa;    return cl.InheritsFrom(ROOT.TTree.Class())&#xa;&#xa;def getKey(rootFile,pathSplit):&#xa;    """"""&#xa;    Get the key of the corresponding object (rootFile,pathSplit)&#xa;    """"""&#xa;    changeDirectory(rootFile,pathSplit[:-1])&#xa;    return ROOT.gDirectory.GetKey(pathSplit[-1])&#xa;&#xa;def isDirectory(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), inherits from TDirectory&#xa;    """"""&#xa;    if pathSplit == []: return True # the object is the rootFile itself&#xa;    else: return isDirectoryKey(getKey(rootFile,pathSplit))&#xa;&#xa;def isTree(rootFile,pathSplit):&#xa;    """"""&#xa;    Return True if the object, corresponding to (rootFile,pathSplit), inherits from TTree&#xa;    """"""&#xa;    if pathSplit == []: return False # the object is the rootFile itself&#xa;    else: return isTreeKey(getKey(rootFile,pathSplit))&#xa;&#xa;def getKeyList(rootFile,pathSplit):&#xa;    """"""&#xa;    Get the list of keys of the directory (rootFile,pathSplit),&#xa;    if (rootFile,pathSplit) is not a directory then get the key in a list&#xa;    """"""&#xa;    if isDirectory(rootFile,pathSplit):&#xa;        changeDirectory(rootFile,pathSplit)&#xa;        return ROOT.gDirectory.GetListOfKeys()&#xa;    else: return [getKey(rootFile,pathSplit)]&#xa;&#xa;def keyListSort(keyList):&#xa;    """"""&#xa;    Sort list of keys by their names ignoring the case&#xa;    """"""&#xa;    keyList.sort(key=lambda x: x.GetName().lower())&#xa;&#xa;def tupleListSort(tupleList):&#xa;    """"""&#xa;    Sort list of tuples by their first elements ignoring the case&#xa;    """"""&#xa;    tupleList.sort(key=lambda x: x[0].lower())&#xa;&#xa;def dirListSort(dirList):&#xa;    """"""&#xa;    Sort list of directories by their names ignoring the case&#xa;    """"""&#xa;    dirList.sort(key=lambda x: [n.lower() for n in x])&#xa;&#xa;def keyClassSpliter(rootFile,pathSplitList):&#xa;    """"""&#xa;    Return a list of directories and a list of keys corresponding&#xa;    to the other objects, for rootLs and rooprint use&#xa;    """"""&#xa;    keyList = []&#xa;    dirList = []&#xa;    for pathSplit in pathSplitList:&#xa;        if pathSplit == []: dirList.append(pathSplit)&#xa;        elif isDirectory(rootFile,pathSplit): dirList.append(pathSplit)&#xa;        else: keyList.append(getKey(rootFile,pathSplit))&#xa;    keyListSort(keyList)&#xa;    dirListSort(dirList)&#xa;    return keyList,dirList&#xa;&#xa;def openROOTFile(fileName, mode=""read""):&#xa;    """"""&#xa;    Open the ROOT file corresponding to fileName in the corresponding mode,&#xa;    redirecting the output not to see missing dictionnaries&#xa;    """"""&#xa;    #with stderrRedirected():&#xa;    with _setIgnoreLevel(ROOT.kError):&#xa;        theFile = ROOT.TFile.Open(fileName, mode)&#xa;    if not theFile:&#xa;        logging.warning(""File %s does not exist"", fileName)&#xa;    return theFile&#xa;&#xa;def openROOTFileCompress(fileName, compress, recreate):&#xa;    """"""&#xa;    Open a ROOT file (like openROOTFile) with the possibility&#xa;    to change compression settings&#xa;    """"""&#xa;    if compress != None and os.path.isfile(fileName):&#xa;        logging.warning(""can't change compression settings on existing file"")&#xa;        return None&#xa;    mode = ""recreate"" if recreate else ""update""&#xa;    theFile = openROOTFile(fileName, mode)&#xa;    if compress != None: theFile.SetCompressionSettings(compress)&#xa;    return theFile&#xa;&#xa;def joinPathSplit(pathSplit):&#xa;    """"""&#xa;    Join the pathSplit with '/'&#xa;    """"""&#xa;    return ""/"".join(pathSplit)&#xa;&#xa;MANY_OCCURENCE_WARNING = ""Same name objects aren't supported: '{0}' of '{1}' won't be processed""&#xa;&#xa;def manyOccurenceRemove(pathSplitList,fileName):&#xa;    """"""&#xa;    Search for double occurence of the same pathSplit and remove them&#xa;    """"""&#xa;    if len(pathSplitList) > 1:&#xa;        for n in pathSplitList:&#xa;            if pathSplitList.count(n) != 1:&#xa;                logging.warning(MANY_OCCURENCE_WARNING.format(joinPathSplit(n),fileName))&#xa;                while n in pathSplitList: pathSplitList.remove(n)&#xa;&#xa;def patternToPathSplitList(fileName,pattern):&#xa;    """"""&#xa;    Get the list of pathSplit of objects in the ROOT file&#xa;    corresponding to fileName that match with the pattern&#xa;    """"""&#xa;    # Open ROOT file&#xa;    rootFile = openROOTFile(fileName)&#xa;    if not rootFile: return []&#xa;&#xa;    # Split pattern avoiding multiple slash problem&#xa;    patternSplit = [n for n in pattern.split(""/"") if n != """"]&#xa;&#xa;    # Main loop&#xa;    pathSplitList = [[]]&#xa;    for patternPiece in patternSplit:&#xa;        newPathSplitList = []&#xa;        for pathSplit in pathSplitList:&#xa;            if isDirectory(rootFile,pathSplit):&#xa;                changeDirectory(rootFile,pathSplit)&#xa;                newPathSplitList.extend( \&#xa;                    [pathSplit + [key.GetName()] \&#xa;                    for key in ROOT.gDirectory.GetListOfKeys() \&#xa;                    if fnmatch.fnmatch(key.GetName(),patternPiece)])&#xa;        pathSplitList = newPathSplitList&#xa;&#xa;    # No match&#xa;    if pathSplitList == []:&#xa;        logging.warning(""can't find {0} in {1}"".format(pattern,fileName))&#xa;&#xa;    # Same match (remove double occurences from the list)&#xa;    manyOccurenceRemove(pathSplitList,fileName)&#xa;&#xa;    return pathSplitList&#xa;&#xa;def fileNameListMatch(filePattern,wildcards):&#xa;    """"""&#xa;    Get the list of fileName that match with objPattern&#xa;    """"""&#xa;    if wildcards: return [os.path.expandvars(os.path.expanduser(i)) for i in glob.iglob(filePattern)]&#xa;    else: return [os.path.expandvars(os.path.expanduser(filePattern))]&#xa;&#xa;def pathSplitListMatch(fileName,objPattern,wildcards):&#xa;    """"""&#xa;    Get the list of pathSplit that match with objPattern&#xa;    """"""&#xa;    if wildcards: return patternToPathSplitList(fileName,objPattern)&#xa;    else: return [[n for n in objPattern.split(""/"") if n != """"]]&#xa;&#xa;def patternToFileNameAndPathSplitList(pattern,wildcards = True):&#xa;    """"""&#xa;    Get the list of tuple containing both :&#xa;    - ROOT file name&#xa;    - list of splited path (in the corresponding file) of objects that matche&#xa;    Use unix wildcards by default&#xa;    """"""&#xa;    rootFilePattern = ""*.root""&#xa;    rootObjPattern = rootFilePattern+"":*""&#xa;    httpRootFilePattern = ""htt*://*.root""&#xa;    httpRootObjPattern = httpRootFilePattern+"":*""&#xa;    xrootdRootFilePattern = ""root://*.root""&#xa;    xrootdRootObjPattern = xrootdRootFilePattern+"":*""&#xa;    s3RootFilePattern = ""s3://*.root""&#xa;    s3RootObjPattern = s3RootFilePattern+"":*""&#xa;    gsRootFilePattern = ""gs://*.root""&#xa;    gsRootObjPattern = gsRootFilePattern+"":*""&#xa;    rfioRootFilePattern = ""rfio://*.root""&#xa;    rfioRootObjPattern = rfioRootFilePattern+"":*""&#xa;    pcmFilePattern = ""*.pcm""&#xa;    pcmObjPattern = pcmFilePattern+"":*""&#xa;&#xa;    if fnmatch.fnmatch(pattern,httpRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,xrootdRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,s3RootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,gsRootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,rfioRootObjPattern):&#xa;        patternSplit = pattern.rsplit("":"", 1)&#xa;        fileName = patternSplit[0]&#xa;        objPattern = patternSplit[1]&#xa;        pathSplitList = pathSplitListMatch(fileName,objPattern,wildcards)&#xa;        return [(fileName,pathSplitList)]&#xa;&#xa;    if fnmatch.fnmatch(pattern,httpRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,xrootdRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,s3RootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,gsRootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,rfioRootFilePattern):&#xa;        fileName = pattern&#xa;        pathSplitList = [[]]&#xa;        return [(fileName,pathSplitList)]&#xa;&#xa;    if fnmatch.fnmatch(pattern,rootObjPattern) or \&#xa;       fnmatch.fnmatch(pattern,pcmObjPattern):&#xa;        patternSplit = pattern.split("":"")&#xa;        filePattern = patternSplit[0]&#xa;        objPattern = patternSplit[1]&#xa;        fileNameList = fileNameListMatch(filePattern,wildcards)&#xa;        return [(fileName,pathSplitListMatch(fileName,objPattern,wildcards)) for fileName in fileNameList]&#xa;&#xa;    if fnmatch.fnmatch(pattern,rootFilePattern) or \&#xa;       fnmatch.fnmatch(pattern,pcmFilePattern):&#xa;        filePattern = pattern&#xa;        fileNameList = fileNameListMatch(filePattern,wildcards)&#xa;        pathSplitList = [[]]&#xa;        return [(fileName,pathSplitList) for fileName in fileNameList]&#xa;&#xa;    logging.warning(""{0}: No such file (or extension not supported)"".format(pattern))&#xa;    return []&#xa;&#xa;# End of utils&#xa;##########&#xa;&#xa;##########&#xa;# Set of functions to put the arguments in shape&#xa;&#xa;def getArgs(parser):&#xa;   """"""&#xa;   Get arguments corresponding to parser.&#xa;   """"""&#xa;   return parser.parse_args()&#xa;&#xa;def getSourceListArgs(parser, wildcards = True):&#xa;   """"""&#xa;   Create a list of tuples that contain source ROOT file names&#xa;   and lists of path in these files as well as the original arguments&#xa;   """"""&#xa;   args = getArgs(parser)&#xa;   inputFiles = []&#xa;   try:&#xa;      inputFiles = args.FILE&#xa;   except:&#xa;      inputFiles = args.SOURCE&#xa;   sourceList = \&#xa;      [tup for pattern in inputFiles \&#xa;      for tup in patternToFileNameAndPathSplitList(pattern,wildcards)]&#xa;   return sourceList, args&#xa;&#xa;def getSourceListOptDict(parser, wildcards = True):&#xa;    """"""&#xa;    Get the list of tuples and the dictionary with options&#xa;    """"""&#xa;    sourceList, args = getSourceListArgs(parser, wildcards)&#xa;    return sourceList, vars(args)&#xa;&#xa;def getSourceDestListOptDict(parser, wildcards = True):&#xa;    """"""&#xa;    Get the list of tuples of sources, create destination name, destination pathSplit&#xa;    and the dictionary with options&#xa;    """"""&#xa;    sourceList, args = getSourceListArgs(parser, wildcards)&#xa;    destList = \&#xa;        patternToFileNameAndPathSplitList( \&#xa;        args.DEST,wildcards=False)&#xa;    if destList != []:&#xa;        destFileName,destPathSplitList = destList[0]&#xa;        destPathSplit = destPathSplitList[0]&#xa;    else:&#xa;        destFileName = """"&#xa;        destPathSplit = []&#xa;    return sourceList, destFileName, destPathSplit, vars(args)&#xa;&#xa;# The end of the set of functions to put the arguments in shape&#xa;##########&#xa;&#xa;##########&#xa;# Several functions shared by roocp, roomv and roorm&#xa;&#xa;TARGET_ERROR = ""target '{0}' is not a directory""&#xa;OMITTING_FILE_ERROR = ""omitting file '{0}'""&#xa;OMITTING_DIRECTORY_ERROR = ""omitting directory '{0}'""&#xa;OVERWRITE_ERROR = ""cannot overwrite non-directory '{0}' with directory '{1}'""&#xa;&#xa;def copyRootObject(sourceFile,sourcePathSplit,destFile,destPathSplit,oneSource,recursive,replace):&#xa;    """"""&#xa;    Initialize the recursive function 'copyRootObjectRecursive', written to be as unix-like as possible&#xa;    """"""&#xa;    retcode = 0&#xa;    isMultipleInput = not (oneSource and sourcePathSplit != [])&#xa;    recursiveOption = recursive&#xa;    # Multiple input and unexisting or non-directory destination&#xa;    # TARGET_ERROR&#xa;    if isMultipleInput and destPathSplit != [] \&#xa;        and not (isExisting(destFile,destPathSplit) \&#xa;        and isDirectory(destFile,destPathSplit)):&#xa;        logging.warning(TARGET_ERROR.format(destPathSplit[-1]))&#xa;        retcode += 1&#xa;    # Entire ROOT file or directory in input omitting ""-r"" option&#xa;    # OMITTING_FILE_ERROR or OMITTING_DIRECTORY_ERROR&#xa;    if not recursiveOption:&#xa;        if sourcePathSplit == []:&#xa;            logging.warning(OMITTING_FILE_ERROR.format( \&#xa;                sourceFile.GetName()))&#xa;            retcode += 1&#xa;        elif isDirectory(sourceFile,sourcePathSplit):&#xa;            logging.warning(OMITTING_DIRECTORY_ERROR.format( \&#xa;                sourcePathSplit[-1]))&#xa;            retcode += 1&#xa;    # Run copyRootObjectRecursive function with the wish&#xa;    # to follow the unix copy behaviour&#xa;    if sourcePathSplit == []:&#xa;        retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;            destFile,destPathSplit,replace)&#xa;    else:&#xa;        setName = """"&#xa;        if not isMultipleInput and (destPathSplit != [] \&#xa;            and not isExisting(destFile,destPathSplit)):&#xa;            setName = destPathSplit[-1]&#xa;        objectName = sourcePathSplit[-1]&#xa;        if isDirectory(sourceFile,sourcePathSplit):&#xa;            if setName != """":&#xa;                createDirectory(destFile,destPathSplit[:-1]+[setName])&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1]+[setName],replace)&#xa;            elif isDirectory(destFile,destPathSplit):&#xa;                if not isExisting(destFile,destPathSplit+[objectName]):&#xa;                    createDirectory(destFile,destPathSplit+[objectName])&#xa;                if isDirectory(destFile,destPathSplit+[objectName]):&#xa;                    retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                        destFile,destPathSplit+[objectName],replace)&#xa;                else:&#xa;                    logging.warning(OVERWRITE_ERROR.format( \&#xa;                        objectName,objectName))&#xa;                    retcode += 1&#xa;            else:&#xa;                logging.warning(OVERWRITE_ERROR.format( \&#xa;                    destPathSplit[-1],objectName))&#xa;                retcode += 1&#xa;        else:&#xa;            if setName != """":&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1],replace,setName)&#xa;            elif isDirectory(destFile,destPathSplit):&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit,replace)&#xa;            else:&#xa;                setName = destPathSplit[-1]&#xa;                retcode += copyRootObjectRecursive(sourceFile,sourcePathSplit, \&#xa;                    destFile,destPathSplit[:-1],replace,setName)&#xa;    return retcode&#xa;&#xa;DELETE_ERROR = ""object {0} was not existing, so it is not deleted""&#xa;&#xa;def deleteObject(rootFile,pathSplit):&#xa;    """"""&#xa;    Delete the object 'pathSplit[-1]' from (rootFile,pathSplit[:-1])&#xa;    """"""&#xa;    retcode = changeDirectory(rootFile,pathSplit[:-1])&#xa;    if retcode == 0:&#xa;        fileName = pathSplit[-1]&#xa;        if isExisting(rootFile,pathSplit):&#xa;            ROOT.gDirectory.Delete(fileName+"";*"")&#xa;        else:&#xa;            logging.warning(DELETE_ERROR.format(fileName))&#xa;            retcode += 1&#xa;    return retcode&#xa;&#xa;def copyRootObjectRecursive(sourceFile,sourcePathSplit,destFile,destPathSplit,replace,setName=""""):&#xa;    """"""&#xa;    Copy objects from a file or directory (sourceFile,sourcePathSplit)&#xa;    to an other file or directory (destFile,destPathSplit)&#xa;    - Has the will to be unix-like&#xa;    - that's a recursive function&#xa;    - Python adaptation of a root input/output tutorial :&#xa;      $ROOTSYS/tutorials/io/copyFiles.C&#xa;    """"""&#xa;    retcode = 0&#xa;    replaceOption = replace&#xa;    for key in getKeyList(sourceFile,sourcePathSplit):&#xa;        objectName = key.GetName()&#xa;        if isDirectoryKey(key):&#xa;            if not isExisting(destFile,destPathSplit+[objectName]):&#xa;                createDirectory(destFile,destPathSplit+[objectName])&#xa;            if isDirectory(destFile,destPathSplit+[objectName]):&#xa;                retcode +=copyRootObjectRecursive(sourceFile, \&#xa;                    sourcePathSplit+[objectName], \&#xa;                    destFile,destPathSplit+[objectName],replace)&#xa;            else:&#xa;                logging.warning(OVERWRITE_ERROR.format( \&#xa;                    objectName,objectName))&#xa;                retcode += 1&#xa;        elif isTreeKey(key):&#xa;            T = key.GetMotherDir().Get(objectName+"";""+str(key.GetCycle()))&#xa;            if replaceOption and isExisting(destFile,destPathSplit+[T.GetName()]):&#xa;                retcodeTemp = deleteObject(destFile,destPathSplit+[T.GetName()])&#xa;                if retcodeTemp:&#xa;                    retcode += retcodeTemp&#xa;                    continue&#xa;            changeDirectory(destFile,destPathSplit)&#xa;            newT = T.CloneTree(-1,""fast"")&#xa;            if setName != """":&#xa;                newT.SetName(setName)&#xa;            newT.Write()&#xa;        else:&#xa;            obj = key.ReadObj()&#xa;            if replaceOption and isExisting(destFile,destPathSplit+[setName]):&#xa;                changeDirectory(destFile,destPathSplit)&#xa;                otherObj = getFromDirectory(setName)&#xa;                if not otherObj == obj:&#xa;                    retcodeTemp = deleteObject(destFile,destPathSplit+[setName])&#xa;                    if retcodeTemp:&#xa;                        retcode += retcodeTemp&#xa;                        continue&#xa;                    else:&#xa;                        obj.SetName(setName)&#xa;                        changeDirectory(destFile,destPathSplit)&#xa;                        obj.Write()&#xa;                else:&#xa;                    obj.SetName(setName)&#xa;                    changeDirectory(destFile,destPathSplit)&#xa;                    obj.Write()&#xa;            else:&#xa;                if setName != """":&#xa;                    obj.SetName(setName)&#xa;                changeDirectory(destFile,destPathSplit)&#xa;                obj.Write()&#xa;            obj.Delete()&#xa;    changeDirectory(destFile,destPathSplit)&#xa;    ROOT.gDirectory.SaveSelf(ROOT.kTRUE)&#xa;    return retcode&#xa;&#xa;FILE_REMOVE_ERROR = ""cannot remove '{0}': Is a ROOT file""&#xa;DIRECTORY_REMOVE_ERROR = ""cannot remove '{0}': Is a directory""&#xa;ASK_FILE_REMOVE = ""remove '{0}' ? (y/n) : ""&#xa;ASK_OBJECT_REMOVE = ""remove '{0}' from '{1}' ? (y/n) : ""&#xa;&#xa;def deleteRootObject(rootFile, pathSplit, interactive, recursive):&#xa;    """"""&#xa;    Remove the object (rootFile,pathSplit)&#xa;    -interactive : prompt before every removal&#xa;    -recursive : allow directory, and ROOT file, removal&#xa;    """"""&#xa;    retcode = 0&#xa;    if not recursive and isDirectory(rootFile,pathSplit):&#xa;        if pathSplit == []:&#xa;            logging.warning(FILE_REMOVE_ERROR.format(rootFile.GetName()))&#xa;            retcode += 1&#xa;        else:&#xa;            logging.warning(DIRECTORY_REMOVE_ERROR.format(pathSplit[-1]))&#xa;            retcode += 1&#xa;    else:&#xa;        if interactive:&#xa;            if pathSplit != []:&#xa;                answer = raw_input(ASK_OBJECT_REMOVE \&#xa;                    .format(""/"".join(pathSplit),rootFile.GetName()))&#xa;            else:&#xa;                answer = raw_input(ASK_FILE_REMOVE \&#xa;                    .format(rootFile.GetName()))&#xa;            remove = answer.lower() == 'y'&#xa;        else:&#xa;            remove = True&#xa;        if remove:&#xa;            if pathSplit != []:&#xa;                retcode += deleteObject(rootFile,pathSplit)&#xa;            else:&#xa;                rootFile.Close()&#xa;                os.remove(rootFile.GetName())&#xa;    return retcode&#xa;&#xa;# End of functions shared by roocp, roomv and roorm&#xa;##########&#xa;&#xa;##########&#xa;# Help strings for ROOT command line tools&#xa;&#xa;# Arguments&#xa;SOURCE_HELP = ""path of the source.""&#xa;SOURCES_HELP = ""path of the source(s).""&#xa;DEST_HELP = ""path of the destination.""&#xa;&#xa;# Options&#xa;COMPRESS_HELP = \&#xa;""""""change the compression settings of the&#xa;destination file (if not already existing).""""""&#xa;INTERACTIVE_HELP = ""prompt before every removal.""&#xa;RECREATE_HELP = ""recreate the destination file.""&#xa;RECURSIVE_HELP = ""recurse inside directories""&#xa;REPLACE_HELP = ""replace object if already existing""&#xa;&#xa;# End of help strings&#xa;##########&#xa;&#xa;##########&#xa;# ROOTBROWSE&#xa;&#xa;def _openBrowser(rootFile=None):&#xa;    browser = ROOT.TBrowser()&#xa;    if rootFile: rootFile.Browse(browser)&#xa;    ROOT.PyROOT.TPyROOTApplication.Run(ROOT.gApplication)&#xa;&#xa;def rootBrowse(fileName=None):&#xa;    if fileName:&#xa;        rootFile = openROOTFile(fileName)&#xa;        if not rootFile: return 1&#xa;        _openBrowser(rootFile)&#xa;        rootFile.Close()&#xa;    else:&#xa;        _openBrowser()&#xa;    return 0&#xa;&#xa;# End of ROOTBROWSE&#xa;##########&#xa;&#xa;##########&#xa;# ROOTCP&#xa;&#xa;def _copyObjects(fileName, pathSplitList, destFile, destPathSplit, oneFile, \&#xa;                 recursive, replace):&#xa;    retcode = 0&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName) \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(rootFile) # Fast copy necessity&#xa;    for pathSplit in pathSplitList:&#xa;        oneSource = oneFile and len(pathSplitList)==1&#xa;        retcode += copyRootObject(rootFile, pathSplit, destFile, destPathSplit, \&#xa;                                  oneSource, recursive, replace)&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootCp(sourceList, destFileName, destPathSplit, \&#xa;           compress=None, recreate=False, recursive=False, replace=False):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in [n[0] for n in sourceList]:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName, compress, recreate)&#xa;    if not destFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(destFile) # Fast copy necessity&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _copyObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                len(sourceList)==1, recursive, replace)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTCP&#xa;##########&#xa;&#xa;##########&#xa;# ROOTEVENTSELECTOR&#xa;&#xa;def _copyTreeSubset(sourceFile,sourcePathSplit,destFile,destPathSplit,firstEvent,lastEvent):&#xa;    """"""Copy a subset of the tree from (sourceFile,sourcePathSplit)&#xa;    to (destFile,destPathSplit) according to options in optDict""""""&#xa;    retcode = changeDirectory(sourceFile,sourcePathSplit[:-1])&#xa;    if retcode != 0: return retcode&#xa;    bigTree = getFromDirectory(sourcePathSplit[-1])&#xa;    nbrEntries = bigTree.GetEntries()&#xa;    # changeDirectory for the small tree not to be memory-resident&#xa;    retcode = changeDirectory(destFile,destPathSplit)&#xa;    if retcode != 0: return retcode&#xa;    smallTree = bigTree.CloneTree(0)&#xa;    if lastEvent == -1:&#xa;        lastEvent = nbrEntries-1&#xa;    isNtuple = bigTree.InheritsFrom(ROOT.TNtuple.Class())&#xa;    for i in xrange(firstEvent, lastEvent+1):&#xa;        bigTree.GetEntry(i)&#xa;        if isNtuple:&#xa;            super(ROOT.TNtuple,smallTree).Fill()&#xa;        else:&#xa;            smallTree.Fill()&#xa;    smallTree.Write()&#xa;    return retcode&#xa;&#xa;def _copyTreeSubsets(fileName, pathSplitList, destFile, destPathSplit, first, last):&#xa;    retcode = 0&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName) \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        if isTree(rootFile,pathSplit):&#xa;            retcode += _copyTreeSubset(rootFile,pathSplit, \&#xa;            destFile,destPathSplit,first,last)&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootEventselector(sourceList, destFileName, destPathSplit, \&#xa;                      compress=None, recreate=False, first=0, last=-1):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in sourceList:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName, compress, recreate)&#xa;    if not destFile: return 1&#xa;&#xa;    # Loop on the root file&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _copyTreeSubsets(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                    first, last)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTEVENTSELECTOR&#xa;##########&#xa;&#xa;##########&#xa;# ROOTLS&#xa;&#xa;# Ansi characters&#xa;ANSI_BOLD = ""\x1B[1m""&#xa;ANSI_BLUE = ""\x1B[34m""&#xa;ANSI_GREEN = ""\x1B[32m""&#xa;ANSI_END = ""\x1B[0m""&#xa;&#xa;# Needed for column width calculation&#xa;ANSI_BOLD_LENGTH = len(ANSI_BOLD+ANSI_END)&#xa;ANSI_BLUE_LENGTH = len(ANSI_BLUE+ANSI_END)&#xa;ANSI_GREEN_LENGTH = len(ANSI_GREEN+ANSI_END)&#xa;&#xa;# Terminal and platform booleans&#xa;IS_TERMINAL = sys.stdout.isatty()&#xa;IS_WIN32 = sys.platform == 'win32'&#xa;&#xa;def isSpecial(ansiCode,string):&#xa;    """"""Use ansi code on 'string' if the output is the&#xa;    terminal of a not Windows platform""""""&#xa;    if IS_TERMINAL and not IS_WIN32: return ansiCode+string+ANSI_END&#xa;    else: return string&#xa;&#xa;def write(string,indent=0,end=""""):&#xa;    """"""Use sys.stdout.write to write the string with an indentation&#xa;    equal to indent and specifying the end character""""""&#xa;    sys.stdout.write("" ""*indent+string+end)&#xa;&#xa;TREE_TEMPLATE = ""{0:{nameWidth}}""+""{1:{titleWidth}}{2:{memoryWidth}}""&#xa;&#xa;def _recursifTreePrinter(tree,indent):&#xa;    """"""Print recursively tree informations""""""&#xa;    listOfBranches = tree.GetListOfBranches()&#xa;    if len(listOfBranches) > 0: # Width informations&#xa;        maxCharName = max([len(branch.GetName()) \&#xa;            for branch in listOfBranches])&#xa;        maxCharTitle = max([len(branch.GetTitle()) \&#xa;            for branch in listOfBranches])&#xa;        dic = { \&#xa;            ""nameWidth"":maxCharName+2, \&#xa;            ""titleWidth"":maxCharTitle+4, \&#xa;            ""memoryWidth"":1}&#xa;    for branch in listOfBranches: # Print loop&#xa;        rec = \&#xa;            [branch.GetName(), \&#xa;            ""\""""+branch.GetTitle()+""\"""", \&#xa;            str(branch.GetTotBytes())]&#xa;        write(TREE_TEMPLATE.format(*rec,**dic),indent,end=""\n"")&#xa;        _recursifTreePrinter(branch,indent+2)&#xa;&#xa;def _prepareTime(time):&#xa;    """"""Get time in the proper shape&#xa;    ex : 174512 for 17h 45m 12s&#xa;    ex : 094023 for 09h 40m 23s""""""&#xa;    time = str(time)&#xa;    time = '000000'+time&#xa;    time = time[len(time)-6:]&#xa;    return time&#xa;&#xa;MONTH = {1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'Jun', \&#xa;         7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}&#xa;LONG_TEMPLATE = \&#xa;    isSpecial(ANSI_BOLD,""{0:{classWidth}}"")+""{1:{timeWidth}}"" + \&#xa;    ""{2:{nameWidth}}{3:{titleWidth}}""&#xa;&#xa;def _rootLsPrintLongLs(keyList,indent,treeListing):&#xa;    """"""Print a list of Tkey in columns&#xa;    pattern : classname, datetime, name and title""""""&#xa;    if len(keyList) > 0: # Width informations&#xa;        maxCharClass = max([len(key.GetClassName()) for key in keyList])&#xa;        maxCharTime = 12&#xa;        maxCharName = max([len(key.GetName()) for key in keyList])&#xa;        dic = { \&#xa;            ""classWidth"":maxCharClass+2, \&#xa;            ""timeWidth"":maxCharTime+2, \&#xa;            ""nameWidth"":maxCharName+2, \&#xa;            ""titleWidth"":1}&#xa;    date = ROOT.Long(0)&#xa;    for key in keyList:&#xa;        time = ROOT.Long(0)&#xa;        datime = key.GetDatime()&#xa;        datime.GetDateTime(datime.Get(),date,time)&#xa;        time = _prepareTime(time)&#xa;        rec = \&#xa;            [key.GetClassName(), \&#xa;            MONTH[int(str(date)[4:6])]+"" "" +str(date)[6:]+ \&#xa;            "" ""+time[:2]+"":""+time[2:4], \&#xa;            key.GetName(), \&#xa;            ""\""""+key.GetTitle()+""\""""]&#xa;        write(LONG_TEMPLATE.format(*rec,**dic),indent,end=""\n"")&#xa;        if treeListing and isTreeKey(key):&#xa;            tree = key.ReadObj()&#xa;            _recursifTreePrinter(tree,indent+2)&#xa;&#xa;##&#xa;# The code of the getTerminalSize function can be found here :&#xa;# https://gist.github.com/jtriley/1108174&#xa;# Thanks jtriley !!&#xa;&#xa;import os&#xa;import shlex&#xa;import struct&#xa;import platform&#xa;import subprocess&#xa;&#xa;def getTerminalSize():&#xa;    """""" getTerminalSize()&#xa;     - get width and height of console&#xa;     - works on linux,os x,windows,cygwin(windows)&#xa;     originally retrieved from:&#xa;     http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python""""""&#xa;    current_os = platform.system()&#xa;    tuple_xy = None&#xa;    if current_os == 'Windows':&#xa;        tuple_xy = _get_terminal_size_windows()&#xa;        if tuple_xy is None:&#xa;            tuple_xy = _get_terminal_size_tput()&#xa;            # needed for window's python in cygwin's xterm!&#xa;    if current_os in ['Linux', 'Darwin'] or current_os.startswith('CYGWIN'):&#xa;        tuple_xy = _get_terminal_size_linux()&#xa;    if tuple_xy is None:&#xa;        #print ""default""&#xa;        #_get_terminal_size_windows() or _get_terminal_size_tput don't work&#xa;        tuple_xy = (80, 25)      # default value&#xa;    return tuple_xy&#xa;&#xa;def _get_terminal_size_windows():&#xa;    try:&#xa;        from ctypes import windll, create_string_buffer&#xa;        # stdin handle is -10&#xa;        # stdout handle is -11&#xa;        # stderr handle is -12&#xa;        h = windll.kernel32.GetStdHandle(-12)&#xa;        csbi = create_string_buffer(22)&#xa;        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)&#xa;        if res:&#xa;            (bufx, bufy, curx, cury, wattr,&#xa;             left, top, right, bottom,&#xa;             maxx, maxy) = struct.unpack(""hhhhHhhhhhh"", csbi.raw)&#xa;            sizex = right - left + 1&#xa;            sizey = bottom - top + 1&#xa;            return sizex, sizey&#xa;    except:&#xa;        pass&#xa;&#xa;def _get_terminal_size_tput():&#xa;    # get terminal width&#xa;    # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window&#xa;    try:&#xa;        cols = int(subprocess.check_call(shlex.split('tput cols')))&#xa;        rows = int(subprocess.check_call(shlex.split('tput lines')))&#xa;        return (cols, rows)&#xa;    except:&#xa;        pass&#xa;&#xa;def _get_terminal_size_linux():&#xa;    def ioctl_GWINSZ(fd):&#xa;        try:&#xa;            import fcntl&#xa;            import termios&#xa;            cr = struct.unpack('hh',&#xa;                               fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))&#xa;            return cr&#xa;        except:&#xa;            pass&#xa;    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)&#xa;    if not cr:&#xa;        try:&#xa;            fd = os.open(os.ctermid(), os.O_RDONLY)&#xa;            cr = ioctl_GWINSZ(fd)&#xa;            os.close(fd)&#xa;        except:&#xa;            pass&#xa;    if not cr:&#xa;        try:&#xa;            cr = (os.environ['LINES'], os.environ['COLUMNS'])&#xa;        except:&#xa;            return None&#xa;    return int(cr[1]), int(cr[0])&#xa;&#xa;# End of getTerminalSize code&#xa;##&#xa;&#xa;def _rootLsPrintSimpleLs(keyList,indent,oneColumn):&#xa;    """"""Print list of strings in columns&#xa;    - blue for directories&#xa;    - green for trees""""""&#xa;    # This code is adaptated from the pprint_list function here :&#xa;    # http://stackoverflow.com/questions/25026556/output-list-like-ls&#xa;    # Thanks hawkjo !!&#xa;    if len(keyList) == 0: return&#xa;    (term_width, term_height) = getTerminalSize()&#xa;    term_width = term_width - indent&#xa;    min_chars_between = 2&#xa;    min_element_width = min( len(key.GetName()) for key in keyList ) \&#xa;                        + min_chars_between&#xa;    max_element_width = max( len(key.GetName()) for key in keyList ) \&#xa;                        + min_chars_between&#xa;    if max_element_width >= term_width: ncol,col_widths = 1,[1]&#xa;    else:&#xa;        # Start with max possible number of columns and reduce until it fits&#xa;        ncol = 1 if oneColumn else min( len(keyList), term_width / min_element_width  )&#xa;        while True:&#xa;            col_widths = \&#xa;                [ max( len(key.GetName()) + min_chars_between \&#xa;                for j, key in enumerate(keyList) if j % ncol == i ) \&#xa;                for i in range(ncol) ]&#xa;            if sum( col_widths ) <= term_width: break&#xa;            else: ncol -= 1&#xa;&#xa;    for i, key in enumerate(keyList):&#xa;        if i%ncol == 0: write("""",indent) # indentation&#xa;        # Don't add spaces after the last element of the line or of the list&#xa;        if (i+1)%ncol != 0 and i != len(keyList)-1:&#xa;            if not IS_TERMINAL: write( \&#xa;                key.GetName().ljust(col_widths[i%ncol]))&#xa;            elif isDirectoryKey(keyList[i]): write( \&#xa;                isSpecial(ANSI_BLUE,key.GetName()).ljust( \&#xa;                    col_widths[i%ncol] + ANSI_BLUE_LENGTH))&#xa;            elif isTreeKey(keyList[i]): write( \&#xa;                isSpecial(ANSI_GREEN,key.GetName()).ljust( \&#xa;                    col_widths[i%ncol] + ANSI_GREEN_LENGTH))&#xa;            else: write(key.GetName().ljust(col_widths[i%ncol]))&#xa;        else: # No spaces after the last element of the line or of the list&#xa;            if not IS_TERMINAL: write(key.GetName())&#xa;            elif isDirectoryKey(keyList[i]):&#xa;                write(isSpecial(ANSI_BLUE, key.GetName()))&#xa;            elif isTreeKey(keyList[i]):&#xa;                write(isSpecial(ANSI_GREEN, key.GetName()))&#xa;            else: write(key.GetName())&#xa;            write('\n')&#xa;&#xa;def _rootLsPrint(keyList, indent, oneColumn, \&#xa;                 longListing, treeListing):&#xa;    """"""Print informations given by keyList with a rootLs&#xa;    style choosen with the options""""""&#xa;    if longListing or treeListing: \&#xa;       _rootLsPrintLongLs(keyList, indent, treeListing)&#xa;    else:&#xa;       _rootLsPrintSimpleLs(keyList, indent, oneColumn)&#xa;&#xa;def _rootLsProcessFile(fileName, pathSplitList, manySources, indent, \&#xa;                       oneColumn, longListing, treeListing):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName)&#xa;    if not rootFile: return 1&#xa;&#xa;    keyList,dirList = keyClassSpliter(rootFile,pathSplitList)&#xa;    if manySources: write(""{0} :"".format(fileName)+""\n"")&#xa;    _rootLsPrint(keyList, indent, oneColumn, longListing, treeListing)&#xa;&#xa;    # Loop on the directories&#xa;    manyPathSplits = len(pathSplitList) > 1&#xa;    indentDir = 2 if manyPathSplits else 0&#xa;    for pathSplit in dirList:&#xa;        keyList = getKeyList(rootFile,pathSplit)&#xa;        keyListSort(keyList)&#xa;        if manyPathSplits: write(""{0} :"".format(""/"".join(pathSplit)),indent,end=""\n"")&#xa;        _rootLsPrint(keyList, indent+indentDir, oneColumn, longListing, treeListing)&#xa;&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootLs(sourceList, oneColumn=False, longListing=False, treeListing=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;    tupleListSort(sourceList)&#xa;&#xa;    # Loop on the ROOT files&#xa;    retcode = 0&#xa;    manySources = len(sourceList) > 1&#xa;    indent = 2 if manySources else 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _rootLsProcessFile(fileName, pathSplitList, manySources, indent, \&#xa;                                      oneColumn, longListing, treeListing)&#xa;    return retcode&#xa;&#xa;# End of ROOTLS&#xa;##########&#xa;&#xa;##########&#xa;# ROOTMKDIR&#xa;&#xa;MKDIR_ERROR = ""cannot create directory '{0}'""&#xa;&#xa;def _createDirectories(rootFile,pathSplit,parents):&#xa;    """"""Same behaviour as createDirectory but allows the possibility&#xa;    to build an whole path recursively with the option \""parents\"" """"""&#xa;    retcode = 0&#xa;    lenPathSplit = len(pathSplit)&#xa;    if lenPathSplit == 0:&#xa;        pass&#xa;    elif parents:&#xa;        for i in xrange(lenPathSplit):&#xa;            currentPathSplit = pathSplit[:i+1]&#xa;            if not (isExisting(rootFile,currentPathSplit) \&#xa;                and isDirectory(rootFile,currentPathSplit)):&#xa;                retcode += createDirectory(rootFile,currentPathSplit)&#xa;    else:&#xa;        doMkdir = True&#xa;        for i in xrange(lenPathSplit-1):&#xa;            currentPathSplit = pathSplit[:i+1]&#xa;            if not (isExisting(rootFile,currentPathSplit) \&#xa;                and isDirectory(rootFile,currentPathSplit)):&#xa;                doMkdir = False&#xa;                break&#xa;        if doMkdir:&#xa;            retcode += createDirectory(rootFile,pathSplit)&#xa;        else:&#xa;            logging.warning(MKDIR_ERROR.format(""/"".join(pathSplit)))&#xa;            retcode += 1&#xa;    return retcode&#xa;&#xa;def _rootMkdirProcessFile(fileName, pathSplitList, parents):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName,""update"")&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        retcode+=_createDirectories(rootFile,pathSplit,parents)&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootMkdir(sourceList, parents=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;&#xa;    # Loop on the ROOT files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _rootMkdirProcessFile(fileName, pathSplitList, parents)&#xa;    return retcode&#xa;&#xa;# End of ROOTMKDIR&#xa;##########&#xa;&#xa;##########&#xa;# ROOTMV&#xa;&#xa;MOVE_ERROR = ""error during copy of {0}, it is not removed from {1}""&#xa;&#xa;def _moveObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                 oneFile, interactive):&#xa;    retcode = 0&#xa;    recursive = True&#xa;    replace = True&#xa;    destFileName = destFile.GetName()&#xa;    rootFile = openROOTFile(fileName,""update"") \&#xa;        if fileName != destFileName else \&#xa;        destFile&#xa;    if not rootFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(rootFile) # Fast copy necessity&#xa;    for pathSplit in pathSplitList:&#xa;        oneSource = oneFile and len(pathSplitList)==1&#xa;        retcodeTemp = copyRootObject(rootFile,pathSplit, \&#xa;            destFile,destPathSplit,oneSource,recursive,replace)&#xa;        if not retcodeTemp:&#xa;            retcode += deleteRootObject(rootFile, pathSplit, interactive, recursive)&#xa;        else:&#xa;            logging.warning(MOVE_ERROR.format(""/"".join(pathSplit),rootFile.GetName()))&#xa;            retcode += retcodeTemp&#xa;    if fileName != destFileName: rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootMv(sourceList, destFileName, destPathSplit, compress=None, \&#xa;           interactive=False, recreate=False):&#xa;    # Check arguments&#xa;    if sourceList == [] or destFileName == """": return 1&#xa;    if recreate and destFileName in sourceList:&#xa;        logging.error(""cannot recreate destination file if this is also a source file"")&#xa;        return 1&#xa;&#xa;    # Open destination file&#xa;    destFile = openROOTFileCompress(destFileName,compress,recreate)&#xa;    if not destFile: return 1&#xa;    ROOT.gROOT.GetListOfFiles().Remove(destFile) # Fast copy necessity&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _moveObjects(fileName, pathSplitList, destFile, destPathSplit, \&#xa;                                len(sourceList)==1, interactive)&#xa;    destFile.Close()&#xa;    return retcode&#xa;&#xa;# End of ROOTMV&#xa;##########&#xa;&#xa;##########&#xa;# ROOTPRINT&#xa;&#xa;def _keyListExtended(rootFile,pathSplitList):&#xa;    keyList,dirList = keyClassSpliter(rootFile,pathSplitList)&#xa;    for pathSplit in dirList: keyList.extend(getKeyList(rootFile,pathSplit))&#xa;    keyList = [key for key in keyList if not isDirectoryKey(key)]&#xa;    keyListSort(keyList)&#xa;    return keyList&#xa;&#xa;def rootPrint(sourceList, directoryOption = None, divideOption = None, drawOption = """", formatOption = None, \&#xa;              outputOption = None, sizeOption = None, styleOption = None, verboseOption = False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;    tupleListSort(sourceList)&#xa;&#xa;    # Don't open windows&#xa;    ROOT.gROOT.SetBatch()&#xa;&#xa;    # (Style option)&#xa;    if styleOption: ROOT.gInterpreter.ProcessLine("".x {0}"".format(styleOption))&#xa;&#xa;    # (Verbose option)&#xa;    if not verboseOption: ROOT.gErrorIgnoreLevel = 9999&#xa;&#xa;    # Initialize the canvas (Size option)&#xa;    if sizeOption:&#xa;        try:&#xa;            width,height = sizeOption.split(""x"")&#xa;            width = int(width)&#xa;            height = int(height)&#xa;        except ValueError:&#xa;            logging.warning(""canvas size is on a wrong format"")&#xa;            return 1&#xa;        canvas = ROOT.TCanvas(""canvas"",""canvas"",width,height)&#xa;    else:&#xa;        canvas = ROOT.TCanvas(""canvas"")&#xa;&#xa;    # Divide the canvas (Divide option)&#xa;    if divideOption:&#xa;        try:&#xa;            x,y = divideOption.split("","")&#xa;            x = int(x)&#xa;            y = int(y)&#xa;        except ValueError:&#xa;            logging.warning(""divide is on a wrong format"")&#xa;            return 1&#xa;        canvas.Divide(x,y)&#xa;        caseNumber = x*y&#xa;&#xa;    # Take the format of the output file (formatOutput option)&#xa;    if not formatOption and outputOption:&#xa;        fileName = outputOption&#xa;        fileFormat = fileName.split(""."")[-1]&#xa;        formatOption = fileFormat&#xa;&#xa;    # Use pdf as default format&#xa;    if not formatOption: formatOption = ""pdf""&#xa;&#xa;    # Create the output directory (directory option)&#xa;    if directoryOption:&#xa;        if not os.path.isdir(os.path.join(os.getcwd(),directoryOption)):&#xa;            os.mkdir(directoryOption)&#xa;&#xa;    # Make the output name, begin to print (output option)&#xa;    if outputOption:&#xa;        if formatOption in ['ps','pdf']:&#xa;            outputFileName = outputOption&#xa;            if directoryOption: outputFileName = \&#xa;                directoryOption + ""/"" + outputFileName&#xa;            canvas.Print(outputFileName+""["",formatOption)&#xa;        else:&#xa;            logging.warning(""can't merge pictures, only postscript or pdf files"")&#xa;            return 1&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    objDrawnNumber = 0&#xa;    openRootFiles = []&#xa;    for fileName, pathSplitList in sourceList:&#xa;        rootFile = openROOTFile(fileName)&#xa;        if not rootFile:&#xa;            retcode += 1&#xa;            continue&#xa;        openRootFiles.append(rootFile)&#xa;        # Fill the key list (almost the same as in rools)&#xa;        keyList = _keyListExtended(rootFile,pathSplitList)&#xa;        for key in keyList:&#xa;            if isTreeKey(key):&#xa;                pass&#xa;            else:&#xa;                if divideOption:&#xa;                    canvas.cd(objDrawnNumber%caseNumber + 1)&#xa;                    objDrawnNumber += 1&#xa;                obj = key.ReadObj()&#xa;                obj.Draw(drawOption)&#xa;                if divideOption:&#xa;                    if objDrawnNumber%caseNumber == 0:&#xa;                        if not outputOption:&#xa;                            outputFileName = str(objDrawnNumber//caseNumber)+"".""+formatOption&#xa;                            if directoryOption:&#xa;                                outputFileName = os.path.join( \&#xa;                                    directoryOption,outputFileName)&#xa;                        canvas.Print(outputFileName,formatOption)&#xa;                        canvas.Clear()&#xa;                        canvas.Divide(x,y)&#xa;                else:&#xa;                    if not outputOption:&#xa;                        outputFileName = key.GetName() + ""."" +formatOption&#xa;                        if directoryOption:&#xa;                            outputFileName = os.path.join( \&#xa;                                directoryOption,outputFileName)&#xa;                    if outputOption or formatOption == 'pdf':&#xa;                        objTitle = ""Title:""+key.GetClassName()+"" : ""+key.GetTitle()&#xa;                        canvas.Print(outputFileName,objTitle)&#xa;                    else:&#xa;                        canvas.Print(outputFileName,formatOption)&#xa;&#xa;    # Last page (divideOption)&#xa;    if divideOption:&#xa;        if objDrawnNumber%caseNumber != 0:&#xa;            if not outputOption:&#xa;                outputFileName = str(objDrawnNumber//caseNumber + 1)+"".""+formatOption&#xa;                if directoryOption:&#xa;                    outputFileName = os.path.join(directoryOption,outputFileName)&#xa;            canvas.Print(outputFileName,formatOption)&#xa;&#xa;    # End to print (output option)&#xa;    if outputOption:&#xa;        if not divideOption:&#xa;            canvas.Print(outputFileName+""]"",objTitle)&#xa;        else:&#xa;            canvas.Print(outputFileName+""]"")&#xa;&#xa;    # Close ROOT files&#xa;    map(lambda rootFile: rootFile.Close(),openRootFiles)&#xa;&#xa;    return retcode&#xa;&#xa;# End of ROOTPRINT&#xa;##########&#xa;&#xa;##########&#xa;# ROOTRM&#xa;&#xa;def _removeObjects(fileName, pathSplitList, interactive=False, recursive=False):&#xa;    retcode = 0&#xa;    rootFile = openROOTFile(fileName,""update"")&#xa;    if not rootFile: return 1&#xa;    for pathSplit in pathSplitList:&#xa;        retcode += deleteRootObject(rootFile, pathSplit, interactive, recursive)&#xa;    rootFile.Close()&#xa;    return retcode&#xa;&#xa;def rootRm(sourceList, interactive=False, recursive=False):&#xa;    # Check arguments&#xa;    if sourceList == []: return 1&#xa;&#xa;    # Loop on the root files&#xa;    retcode = 0&#xa;    for fileName, pathSplitList in sourceList:&#xa;        retcode += _removeObjects(fileName, pathSplitList, interactive, recursive)&#xa;    return retcode&#xa;&#xa;# End of ROOTRM&#xa;##########&#xa;"
3071415|"&#xa;import argparse&#xa;import commands&#xa;import getpass&#xa;import itertools&#xa;import numpy&#xa;import os&#xa;import os.path&#xa;import sys&#xa;import time&#xa;&#xa;from env import gidgetConfigVars&#xa;import miscIO&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;debugFlag = 0&#xa;&#xa;# order returns the order of each element in x as a list&#xa;&#xa;&#xa;def order(x):&#xa;    if (debugFlag):&#xa;        print "" in FUNC order() ... ""&#xa;    L = len(x)&#xa;    rangeL = range(L)&#xa;    z = itertools.izip(x, rangeL)&#xa;    z = itertools.izip(z, rangeL)    # avoid problems with duplicates&#xa;    D = sorted(z)&#xa;    return [d[1] for d in D]&#xa;&#xa;# rank returns the rankings of the elements in x as a list&#xa;&#xa;&#xa;def rank(x):&#xa;    if (debugFlag):&#xa;        print "" in FUNC rank() ... ""&#xa;    L = len(x)&#xa;    ordering = order(x)&#xa;    ranks = [0] * L&#xa;    for i in range(L):&#xa;        ranks[ordering[i]] = i&#xa;    return ranks&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# new 28-jan-2013 from&#xa;# http://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python&#xa;&#xa;&#xa;def rank_simple(vector):&#xa;    return sorted(range(len(vector)), key=vector.__getitem__)&#xa;&#xa;&#xa;def rankdata(a):&#xa;    n = len(a)&#xa;    ivec = rank_simple(a)&#xa;    svec = [a[rank] for rank in ivec]&#xa;    sumranks = 0&#xa;    dupcount = 0&#xa;    newarray = [0] * n&#xa;    for i in xrange(n):&#xa;        sumranks += i&#xa;        dupcount += 1&#xa;        if i == n - 1 or svec[i] != svec[i + 1]:&#xa;            averank = sumranks / float(dupcount) + 1&#xa;            for j in xrange(i - dupcount + 1, i + 1):&#xa;                newarray[ivec[j]] = averank&#xa;            sumranks = 0&#xa;            dupcount = 0&#xa;&#xa;    print "" returning from rankdata ... "", min(newarray), max(newarray)&#xa;&#xa;    return newarray&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def countSamples(tokenList):&#xa;&#xa;    numSamp = 0&#xa;    for aTok in tokenList:&#xa;        if (aTok != ""NA""):&#xa;            numSamp += 1&#xa;&#xa;    return (numSamp)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getClinSampFeat(featureMatrixFile, min_samples):&#xa;&#xa;    fh = file(featureMatrixFile)&#xa;&#xa;    indexList = []&#xa;    featList = []&#xa;&#xa;    lineNo = 0&#xa;    for aLine in fh:&#xa;        aLine = aLine.strip()&#xa;        if (lineNo > 0):&#xa;            tokenList = aLine.split('\t')&#xa;            featName = tokenList[0]&#xa;            if (featName.find("":CLIN:"") > 0):&#xa;                numSamp = countSamples(tokenList)&#xa;                if (numSamp >= min_samples):&#xa;                    indexList += [lineNo - 1]&#xa;                    featList += [featName]&#xa;                else:&#xa;                    print "" skipping feature <%s> due to low counts (%d) "" % (featName, numSamp)&#xa;            elif (featName.find("":SAMP:"") > 0):&#xa;                numSamp = countSamples(tokenList)&#xa;                if (numSamp >= min_samples):&#xa;                    indexList += [lineNo - 1]&#xa;                    featList += [featName]&#xa;                else:&#xa;                    print "" skipping feature <%s> due to low counts (%d) "" % (featName, numSamp)&#xa;        lineNo += 1&#xa;&#xa;    fh.close()&#xa;&#xa;    return (indexList, featList)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getNumSamples(featureMatrixFile):&#xa;&#xa;    fh = file(featureMatrixFile)&#xa;    numCols = miscIO.num_cols(fh, '\t')&#xa;    numSamples = numCols - 1&#xa;    fh.close()&#xa;&#xa;    return (numSamples)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;# input file is assumed to end in .tsv&#xa;# this function checks to see if the binFile exists and is up to date&#xa;# with respect to the tsvFile ... if necessary, it will call prep4pairwise&#xa;# to create the bin file&#xa;&#xa;&#xa;def preProcessTSV(tsvFile):&#xa;&#xa;    tsvTime = os.path.getmtime(tsvFile)&#xa;    # print tsvTime&#xa;&#xa;    binFile = tsvFile[:-4] + "".bin""&#xa;    catFile = tsvFile[:-4] + "".cat""&#xa;    try:&#xa;        binTime = os.path.getmtime(binFile)&#xa;        # print binTime&#xa;    except:&#xa;        binTime = 0&#xa;&#xa;    if (tsvTime > binTime):&#xa;&#xa;        # just to be sure, delete the *.bin and *.cat files ...&#xa;        cmdString = ""rm -fr %s"" % binFile&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        cmdString = ""rm -fr %s"" % catFile&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;&#xa;        print "" creating bin file ""&#xa;        cmdString = ""%s %s/prep4pairwise.py %s"" % (gidgetConfigVars['TCGAFMP_PYTHON3'], gidgetConfigVars['TCGAFMP_PAIRWISE_ROOT'], tsvFile)&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        if (status != 0):&#xa;            print "" ERROR ??? failed to execute command ??? ""&#xa;            print cmdString&#xa;            sys.exit(-1)&#xa;    else:&#xa;        print "" bin file already up to date ""&#xa;&#xa;    return (binFile)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;&#xa;def getTypeRanks(aType, tmpDir, indexList, featList):&#xa;&#xa;    print "" ""&#xa;    print "" in getTypeRanks ... "", aType&#xa;&#xa;    outNames = []&#xa;    dTypeScores = []&#xa;    maxNum = 50000&#xa;&#xa;    pCounts = {}&#xa;&#xa;    for kk in range(len(indexList)):&#xa;&#xa;        index = indexList[kk]&#xa;        featName = featList[kk]&#xa;&#xa;        # first, open the previously generated pairwise file for this index&#xa;        # (feature)&#xa;        pwFile = tmpDir + ""/%d.pw"" % index&#xa;        try:&#xa;            fh = file(pwFile, 'r')&#xa;        except:&#xa;            continue&#xa;&#xa;        # print ""         --> reading input file <%s> "" % pwFile&#xa;&#xa;        # initialze number of p-values&#xa;        pCounts[featName] = 0&#xa;&#xa;        # allocate a vector of zeros&#xa;        dTypeVec = numpy.zeros(maxNum)&#xa;        iG = 0&#xa;&#xa;        # read through the input file, line by line, and keep only&#xa;        # those pairs that involve the current data type&#xa;&#xa;        # each line should look something like this:&#xa;        # <featA> <featB> <??>  +0.753   186   +9.236   0   -0.000   0   -0.000   <*>&#xa;&#xa;        for aLine in fh:&#xa;            if (aLine.startswith(""##"")):&#xa;                continue&#xa;            if (aLine.find(aType) >= 0):&#xa;                tokenList = aLine.split('\t')&#xa;                # check that the # of samples involved in the pairwise test was&#xa;                # at least 20 ...&#xa;                if (int(tokenList[4]) < 20):&#xa;                    continue&#xa;&#xa;                # and then grab the -log(p)&#xa;                pValue = float(tokenList[5])&#xa;                dTypeVec[iG] = pValue&#xa;                iG += 1&#xa;&#xa;        # close the file ...&#xa;        fh.close()&#xa;&#xa;        outNames += [featName]&#xa;&#xa;        if (iG > 0):&#xa;            # print ""         --> got %d p-values "" % iG&#xa;            pCounts[featName] += iG&#xa;            try:&#xa;                dTypeVec = dTypeVec[:iG]&#xa;                dTypeVec.sort()&#xa;                aScore = 0.&#xa;                for p in [0.80, 0.85, 0.90, 0.95]:&#xa;                    iG = int(p * len(dTypeVec))&#xa;                    aScore += dTypeVec[iG]&#xa;                # print index, featName, ""GEXP"", aScore, len(dTypeVec)&#xa;                dTypeScores += [aScore]&#xa;            except:&#xa;                dTypeScores += [0]&#xa;        else:&#xa;            # print ""         --> did NOT get any p-values ""&#xa;            dTypeScores += [0]&#xa;&#xa;    print "" ""&#xa;    print "" got this far ... returning from getTypeRanks ""&#xa;    print ""     names   : "", outNames[:5], outNames[-5:]&#xa;    print ""     scores  : "", dTypeScores[:5], dTypeScores[-5:]&#xa;    if (0):&#xa;        dTypeRanks = rank(dTypeScores)&#xa;    else:&#xa;        dTypeRanks = rankdata(dTypeScores)&#xa;    print ""     ranks   : "", dTypeRanks[:5], dTypeRanks[-5:]&#xa;&#xa;    print "" ""&#xa;    pKeys = pCounts.keys()&#xa;    for ii in range(min(10, len(pKeys))):&#xa;        print ""         pCounts[%s] = %d "" % (pKeys[ii], pCounts[pKeys[ii]])&#xa;    print "" ""&#xa;&#xa;    return (outNames, dTypeScores, dTypeRanks, pCounts)&#xa;&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;&#xa;def makeOutFileName(tsvFile):&#xa;&#xa;    ii = len(tsvFile) - 1&#xa;    while (tsvFile[ii] != '/'):&#xa;        ii -= 1&#xa;&#xa;    # the output file name will look just like the input tsv file name,&#xa;    # but it will start with ""featScores_"" and end with "".txt"" rather&#xa;    # than "".tsv""&#xa;    outName = tsvFile[:ii] + '/' + ""featScores_"" + tsvFile[ii + 1:-3] + ""txt""&#xa;    return (outName)&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;#&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    # ALL necessary inputs should be handled using this ArgumentParser ... there shouldn't&#xa;    # be any 'left-over' arguments ... any unrecognized command-line inputs will result&#xa;    # in an error like:&#xa;    # rkpw_list_gen.py: error: unrecognized arguments: abc def&#xa;&#xa;    parser = argparse.ArgumentParser(&#xa;        description='Create runlist for pairwise')&#xa;    parser.add_argument('--min-ct-cell', '-minct',&#xa;                        action='store', default=5, type=int)&#xa;    parser.add_argument('--min-mx-cell', '-minmx',&#xa;                        action='store', default=5, type=int)&#xa;    parser.add_argument('--min-samples', '-M',&#xa;                        action='store', default=30, type=int)&#xa;    parser.add_argument('--verbosity', '-v',&#xa;                        action='store', default=0, type=int)&#xa;    parser.add_argument('--tsvFile', '-f', action='store', required=True)&#xa;    ## parser.add_argument ( '--runFile', '-r', action='store', required=True )&#xa;&#xa;    args = parser.parse_args()&#xa;    print args&#xa;&#xa;    # force user to be using the 'cncrreg' group to run this ...&#xa;    if (0):&#xa;        cmdString = ""newgrp cncrreg""&#xa;        print "" trying to force group ... ""&#xa;        (status, output) = commands.getstatusoutput(cmdString)&#xa;        print "" back from that ... ""&#xa;&#xa;    # at this point we should have a Namespace called 'args' that looks something like this:&#xa;    # Namespace ( tsvFile=['test.tsv'],&#xa;    # runFile=['test.run'],&#xa;    ##		   byname=False, input=None,&#xa;    # min_ct_cell=5,&#xa;    # tail=0, verbosity=0 )&#xa;&#xa;    # get the tsv feature matrix file and also the number of features it&#xa;    # contains&#xa;    tsvFile = args.tsvFile&#xa;    print "" input tsv file name <%s> "" % tsvFile&#xa;    if (not os.path.exists(tsvFile)):&#xa;        print "" <%s> is not a valid file, exiting ... "" % tsvFile&#xa;        sys.exit(-1)&#xa;    if (not tsvFile.endswith("".tsv"")):&#xa;        print "" <%s> input file should be a TSV file "" % tsvFile&#xa;        sys.exit(-1)&#xa;    if (tsvFile[0] != ""/""):&#xa;        print "" absolute path name for input file <%s> is required "" % tsvFile&#xa;        sys.exit(-1)&#xa;    (indexList, featList) = getClinSampFeat(tsvFile, args.min_samples)&#xa;&#xa;    if (len(indexList) < 5):&#xa;        print "" ERROR ... does not seem worth continuing ... ""&#xa;        print indexList&#xa;        print featList&#xa;        sys.exit(-1)&#xa;&#xa;    print "" --> number of features : "", len(indexList)&#xa;    numSamples = getNumSamples(tsvFile)&#xa;    print "" --> number of samples  : "", numSamples&#xa;&#xa;    # we need to pre-process the tsv file (unless it appears to have already&#xa;    # been done)&#xa;    binFile = preProcessTSV(tsvFile)&#xa;&#xa;    # create a random name for this particular run ...&#xa;    # and then make a subdirectory for the outputs ...&#xa;    curJobName = miscIO.make_random_fname()&#xa;    print "" ""&#xa;    print "" randomly generated job name : <%s> "" % curJobName&#xa;    print "" ""&#xa;&#xa;    tmpDir = ""%s/%s"" % (gidgetConfigVars['TCGAFMP_CLUSTER_SCRATCH'], curJobName)&#xa;    cmdString = ""mkdir %s"" % tmpDir&#xa;    (status, output) = commands.getstatusoutput(cmdString)&#xa;    if (not os.path.exists(tmpDir)):&#xa;        print "" mkdir command failed ??? ""&#xa;        print cmdString&#xa;        sys.exit(-1)&#xa;&#xa;    # write the jobInfo file ...&#xa;    jobFile = tmpDir + ""/jobInfo.txt""&#xa;    try:&#xa;        fh = file(jobFile, 'w')&#xa;    except:&#xa;        print "" failed to open output file <%s>, exiting ... "" % jobFile&#xa;        sys.exit(-1)&#xa;    fh.write(""tsvFile = %s\n"" % args.tsvFile)&#xa;    fh.close()&#xa;&#xa;    # open the runFile ...&#xa;    runFile = tmpDir + ""/runList.txt""&#xa;    try:&#xa;        fh = file(runFile, 'w')&#xa;    except:&#xa;        print "" failed to open output file <%s>, exiting ... "" % runFile&#xa;        sys.exit(-1)&#xa;&#xa;    pythonbin = sys.executable&#xa;&#xa;    golempwd = ""PASSWD_HERE""&#xa;    fhC = file ( gidgetConfigVars['TCGAFMP_CLUSTER_SCRATCH'] + ""/config"", 'r' )&#xa;    aLine = fhC.readline()&#xa;    fhC.close()&#xa;    aLine = aLine.strip()&#xa;    golempwd = aLine&#xa;    print "" got this ... <%s> "" % golempwd&#xa;&#xa;    numJobs = 0&#xa;    for index in indexList:&#xa;        outName = tmpDir + ""/"" + str(index) + "".pw""&#xa;        cmdString = ""1 "" + gidgetConfigVars['TCGAFMP_PAIRWISE_ROOT'] + ""/pairwise-1.1.2""&#xa;        cmdString += "" --pvalue 1. --min-ct-cell %d --min-mx-cell %d --min-samples %d"" \&#xa;            % (args.min_ct_cell, args.min_mx_cell, args.min_samples)&#xa;        cmdString += "" --outer %d:%d:1 --inner 0::1  %s  %s "" \&#xa;            % (index, index + 1, binFile, outName)&#xa;        fh.write(""%s\n"" % cmdString)&#xa;        numJobs += 1&#xa;&#xa;    fh.close()&#xa;&#xa;    # ok, now we want to actually launch the jobs ...&#xa;    cmdString = ""python "" + gidgetConfigVars['TCGAFMP_ROOT_DIR'] + ""/main/golem.py ""&#xa;    cmdString += ""http://glados.systemsbiology.net:7083 -p "" + golempwd + "" ""&#xa;    cmdString += ""-L scoreCatFeat -u ""&#xa;    cmdString += getpass.getuser() + "" ""&#xa;    cmdString += ""runlist "" + runFile&#xa;    print cmdString&#xa;    (status, output) = commands.getstatusoutput(cmdString)&#xa;    print status&#xa;    print output&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" --------------- ""&#xa;&#xa;    done = 0&#xa;    lastCheck = -1&#xa;    noChange = 0&#xa;    while not done:&#xa;&#xa;        ## count up the number of output files ...&#xa;        numOutFiles = 0&#xa;        for aName in os.listdir(tmpDir):&#xa;            if (aName.endswith("".pw"")):&#xa;                numOutFiles += 1&#xa;        print numOutFiles&#xa;&#xa;        ## if the number of output files matches the&#xa;        ## number of jobs, we're good to go&#xa;        if (numOutFiles == numJobs): done = 1&#xa;&#xa;        ## if this count has not changed in a while,&#xa;        ## they we probably want to bail ...&#xa;        if ( lastCheck == numOutFiles ):&#xa;            noChange += 1&#xa;        if ( noChange > 5 ): done = 1&#xa;        lastCheck = numOutFiles&#xa;&#xa;        time.sleep(10)&#xa;&#xa;    print "" should be done !!! "", numOutFiles, numJobs&#xa;&#xa;    # now we are ready for the post-processing ...&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    featScores = {}&#xa;    pCounts = {}&#xa;&#xa;    # we are looking for associations with 5 different molecular data types:&#xa;    typeList = [""N:GEXP:"", ""N:RPPA:"", ""N:METH:"", ""N:MIRN:"", ""N:CNVR:""]&#xa;    for aType in typeList:&#xa;        print "" aType : "", aType&#xa;&#xa;        outNames = []&#xa;&#xa;        (outNames, typeScores, typeRanks, pTmp) = getTypeRanks (aType, tmpDir, indexList, featList)&#xa;&#xa;        # at this point we have:&#xa;        # names : vector of feature names&#xa;        # typeScores : vector of scores (a higher score is better)&#xa;        # typeRanks  : vector of ranks (a higher rank is better)&#xa;        # pTmp : # of p-values considered for each feature name (this is a&#xa;        # dictionary with the feature name as the key)&#xa;&#xa;# for iR in range(len(typeRanks)):&#xa;####	    jR = len(typeRanks) - iR - 1&#xa;####	    kR = typeRanks.index(jR)&#xa;# print iR, jR, kR, outNames[kR], typeScores[kR]&#xa;# if ( outNames[kR] in featScores.keys() ):&#xa;####	        featScores[outNames[kR]] += [ jR ]&#xa;####		pCounts[outNames[kR]] += pTmp[outNames[kR]]&#xa;# else:&#xa;####		featScores[outNames[kR]] = [ jR ]&#xa;####		pCounts[outNames[kR]] = pTmp[outNames[kR]]&#xa;&#xa;        for iR in range(len(outNames)):&#xa;            if (outNames[iR] in featScores.keys()):&#xa;                featScores[outNames[iR]] += [typeRanks[iR]]&#xa;                pCounts[outNames[iR]] += pTmp[outNames[iR]]&#xa;            else:&#xa;                featScores[outNames[iR]] = [typeRanks[iR]]&#xa;                pCounts[outNames[iR]] = pTmp[outNames[iR]]&#xa;&#xa;        # now we have&#xa;        # featScores{} : this is now based on the *rank* (rather than the *score* which was based on p-values)&#xa;        # pCounts{}    : this keeps track of how many p-values contributed to&#xa;        # this rank/score&#xa;&#xa;        print "" ""&#xa;        keyList = featScores.keys()&#xa;        for ii in range(min(10, len(keyList))):&#xa;            print keyList[ii], featScores[keyList[ii]], pCounts[keyList[ii]]&#xa;        print "" ""&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" FINISHED looping over typeList ... ""&#xa;&#xa;    if (1):&#xa;        print "" ""&#xa;        print "" ""&#xa;        for aName in featScores.keys():&#xa;            print aName, featScores[aName], pCounts[aName]&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    # sum up the scores ...&#xa;    nameList = featScores.keys()&#xa;    sumScores = []&#xa;    for aName in nameList:&#xa;        aSum = 0&#xa;        for aScore in featScores[aName]:&#xa;            aSum += aScore&#xa;        sumScores += [aSum]&#xa;&#xa;    # also, find the median pCounts value, and reset the scores for those to 0&#xa;    # ...&#xa;    pList = []&#xa;    for aName in nameList:&#xa;        pList += [pCounts[aName]]&#xa;    pList.sort()&#xa;    nHalf = len(pList) / 2&#xa;    pMedian = pList[nHalf]&#xa;    pMin = pMedian / 2&#xa;    if (pMin == 0):&#xa;        pMin = 1&#xa;    print "" pMin = "", pMin&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    if (1):&#xa;        scoreRanks = rank(sumScores)&#xa;    else:&#xa;        scoreRanks = rankdata(sumScores)&#xa;    newScores = [0] * len(sumScores)&#xa;    maxScore = len(typeList) * len(nameList)&#xa;    for iR in range(len(sumScores)):&#xa;        jR = len(sumScores) - iR - 1&#xa;        kR = scoreRanks.index(jR)&#xa;        aName = nameList[kR]&#xa;        # print iR, jR, kR, nameList[kR], sumScores[kR], (&#xa;        # float(sumScores[kR])/float(maxScore) )&#xa;        if (pCounts[aName] >= pMin):&#xa;            # this line looks like this:&#xa;            # 57 48724 B_CLIN_person_neoplasm_cancer_status 338 0.140248962656&#xa;            print jR, pCounts[aName], aName, sumScores[kR], (float(sumScores[kR]) / float(maxScore))&#xa;            newScores[kR] = sumScores[kR]&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;&#xa;    ## outFile = tmpDir + ""/featScores.tsv""&#xa;    outFile = makeOutFileName(tsvFile)&#xa;    print "" --> opening output file <%s> "" % outFile&#xa;    fh = file(outFile, 'w')&#xa;&#xa;    # finally we want to pretty print the output ...&#xa;    numOut = 0&#xa;    for iR in range(len(sumScores)):&#xa;        jR = len(sumScores) - iR - 1&#xa;        kR = scoreRanks.index(jR)&#xa;        aName = nameList[kR]&#xa;        nameTokens = aName.split(':')&#xa;        shortName = nameTokens[2]&#xa;        featName = aName&#xa;        typeName = aName[2:6]&#xa;        if (newScores[kR] > 0):&#xa;            fh.write(""%s\t%s\t%s\t%.3f\n"" %&#xa;                     (shortName, featName, typeName, newScores[kR]))&#xa;            numOut += 1&#xa;&#xa;    if (numOut == 0):&#xa;        fh.write(&#xa;            ""## there were no significant associations between clinical/sample features and molecular features \n"")&#xa;    fh.close()&#xa;&#xa;    # and we can delete the individual *.pw files ...&#xa;    cmdString = ""rm -fr %s"" % tmpDir&#xa;    print cmdString&#xa;    ## ( status, output ) = commands.getstatusoutput ( cmdString )&#xa;&#xa;    print "" ""&#xa;    print "" ""&#xa;    print "" FINISHED ""&#xa;    print "" ""&#xa;&#xa;&#xa;&#xa;&#xa;# -#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#&#xa;"
132058|"# -*- coding: utf-8 -*-&#xa;##############################################################################&#xa;#&#xa;#    OpenERP, Open Source Management Solution&#xa;#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).&#xa;#&#xa;#    This program is free software: you can redistribute it and/or modify&#xa;#    it under the terms of the GNU Affero General Public License as&#xa;#    published by the Free Software Foundation, either version 3 of the&#xa;#    License, or (at your option) any later version.&#xa;#&#xa;#    This program is distributed in the hope that it will be useful,&#xa;#    but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;#    GNU Affero General Public License for more details.&#xa;#&#xa;#    You should have received a copy of the GNU Affero General Public License&#xa;#    along with this program.  If not, see <http://www.gnu.org/licenses/>.&#xa;#&#xa;##############################################################################&#xa;&#xa;""""""&#xa;OpenERP - Server&#xa;OpenERP is an ERP+CRM program for small and medium businesses.&#xa;&#xa;The whole source code is distributed under the terms of the&#xa;GNU Public Licence.&#xa;&#xa;(c) 2003-TODAY, Fabien Pinckaers - OpenERP SA&#xa;""""""&#xa;&#xa;import logging&#xa;import os&#xa;import signal&#xa;import sys&#xa;import threading&#xa;import traceback&#xa;import time&#xa;&#xa;import openerp&#xa;&#xa;from . import Command&#xa;&#xa;__author__ = openerp.release.author&#xa;__version__ = openerp.release.version&#xa;&#xa;# Also use the `openerp` logger for the main script.&#xa;_logger = logging.getLogger('openerp')&#xa;&#xa;def check_root_user():&#xa;    """""" Exit if the process's user is 'root' (on POSIX system).""""""&#xa;    if os.name == 'posix':&#xa;        import pwd&#xa;        if pwd.getpwuid(os.getuid())[0] == 'root' :&#xa;            sys.stderr.write(""Running as user 'root' is a security risk, aborting.\n"")&#xa;            sys.exit(1)&#xa;&#xa;def check_postgres_user():&#xa;    """""" Exit if the configured database user is 'postgres'.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    if config['db_user'] == 'postgres':&#xa;        sys.stderr.write(""Using the database user 'postgres' is a security risk, aborting."")&#xa;        sys.exit(1)&#xa;&#xa;def report_configuration():&#xa;    """""" Log the server version and some configuration values.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    _logger.info(""OpenERP version %s"", __version__)&#xa;    for name, value in [('addons paths', config['addons_path']),&#xa;                        ('database hostname', config['db_host'] or 'localhost'),&#xa;                        ('database port', config['db_port'] or '5432'),&#xa;                        ('database user', config['db_user'])]:&#xa;        _logger.info(""%s: %s"", name, value)&#xa;&#xa;def setup_pid_file():&#xa;    """""" Create a file with the process id written in it.&#xa;&#xa;    This function assumes the configuration has been initialized.&#xa;    """"""&#xa;    config = openerp.tools.config&#xa;    if config['pidfile']:&#xa;        fd = open(config['pidfile'], 'w')&#xa;        pidtext = ""%d"" % (os.getpid())&#xa;        fd.write(pidtext)&#xa;        fd.close()&#xa;&#xa;def preload_registry(dbname):&#xa;    """""" Preload a registry, and start the cron.""""""&#xa;    try:&#xa;        update_module = True if openerp.tools.config['init'] or openerp.tools.config['update'] else False&#xa;        db, registry = openerp.pooler.get_db_and_pool(dbname,update_module=update_module)&#xa;    except Exception:&#xa;        _logger.exception('Failed to initialize database `%s`.', dbname)&#xa;&#xa;def run_test_file(dbname, test_file):&#xa;    """""" Preload a registry, possibly run a test file, and start the cron.""""""&#xa;    try:&#xa;        config = openerp.tools.config&#xa;        db, registry = openerp.pooler.get_db_and_pool(dbname, update_module=config['init'] or config['update'])&#xa;        cr = db.cursor()&#xa;        _logger.info('loading test file %s', test_file)&#xa;        openerp.tools.convert_yaml_import(cr, 'base', file(test_file), 'test', {}, 'test', True)&#xa;        cr.rollback()&#xa;        cr.close()&#xa;    except Exception:&#xa;        _logger.exception('Failed to initialize database `%s` and run test file `%s`.', dbname, test_file)&#xa;&#xa;def export_translation():&#xa;    config = openerp.tools.config&#xa;    dbname = config['db_name']&#xa;&#xa;    if config[""language""]:&#xa;        msg = ""language %s"" % (config[""language""],)&#xa;    else:&#xa;        msg = ""new language""&#xa;    _logger.info('writing translation file for %s to %s', msg,&#xa;        config[""translate_out""])&#xa;&#xa;    fileformat = os.path.splitext(config[""translate_out""])[-1][1:].lower()&#xa;    buf = file(config[""translate_out""], ""w"")&#xa;    cr = openerp.pooler.get_db(dbname).cursor()&#xa;    openerp.tools.trans_export(config[""language""],&#xa;        config[""translate_modules""] or [""all""], buf, fileformat, cr)&#xa;    cr.close()&#xa;    buf.close()&#xa;&#xa;    _logger.info('translation file written successfully')&#xa;&#xa;def import_translation():&#xa;    config = openerp.tools.config&#xa;    context = {'overwrite': config[""overwrite_existing_translations""]}&#xa;    dbname = config['db_name']&#xa;&#xa;    cr = openerp.pooler.get_db(dbname).cursor()&#xa;    openerp.tools.trans_load( cr, config[""translate_in""], config[""language""],&#xa;        context=context)&#xa;    cr.commit()&#xa;    cr.close()&#xa;&#xa;# Variable keeping track of the number of calls to the signal handler defined&#xa;# below. This variable is monitored by ``quit_on_signals()``.&#xa;quit_signals_received = 0&#xa;&#xa;def signal_handler(sig, frame):&#xa;    """""" Signal handler: exit ungracefully on the second handled signal.&#xa;&#xa;    :param sig: the signal number&#xa;    :param frame: the interrupted stack frame or None&#xa;    """"""&#xa;    global quit_signals_received&#xa;    quit_signals_received += 1&#xa;    if quit_signals_received > 1:&#xa;        # logging.shutdown was already called at this point.&#xa;        sys.stderr.write(""Forced shutdown.\n"")&#xa;        os._exit(0)&#xa;&#xa;def dumpstacks(sig, frame):&#xa;    """""" Signal handler: dump a stack trace for each existing thread.""""""&#xa;    # code from http://stackoverflow.com/questions/132058/getting-stack-trace-from-a-running-python-application#answer-2569696&#xa;    # modified for python 2.5 compatibility&#xa;    threads_info = dict([(th.ident, {'name': th.name,&#xa;                                    'uid': getattr(th,'uid','n/a')})&#xa;                                for th in threading.enumerate()])&#xa;    code = []&#xa;    for threadId, stack in sys._current_frames().items():&#xa;        thread_info = threads_info.get(threadId)&#xa;        code.append(""\n# Thread: %s (id:%s) (uid:%s)"" % \&#xa;                    (thread_info and thread_info['name'] or 'n/a',&#xa;                     threadId,&#xa;                     thread_info and thread_info['uid'] or 'n/a'))&#xa;        for filename, lineno, name, line in traceback.extract_stack(stack):&#xa;            code.append('File: ""%s"", line %d, in %s' % (filename, lineno, name))&#xa;            if line:&#xa;                code.append(""  %s"" % (line.strip()))&#xa;    _logger.info(""\n"".join(code))&#xa;&#xa;def setup_signal_handlers():&#xa;    """""" Register the signal handler defined above. """"""&#xa;    SIGNALS = map(lambda x: getattr(signal, ""SIG%s"" % x), ""INT TERM"".split())&#xa;    if os.name == 'posix':&#xa;        map(lambda sig: signal.signal(sig, signal_handler), SIGNALS)&#xa;        signal.signal(signal.SIGQUIT, dumpstacks)&#xa;    elif os.name == 'nt':&#xa;        import win32api&#xa;        win32api.SetConsoleCtrlHandler(lambda sig: signal_handler(sig, None), 1)&#xa;&#xa;def quit_on_signals():&#xa;    """""" Wait for one or two signals then shutdown the server.&#xa;&#xa;    The first SIGINT or SIGTERM signal will initiate a graceful shutdown while&#xa;    a second one if any will force an immediate exit.&#xa;&#xa;    """"""&#xa;    # Wait for a first signal to be handled. (time.sleep will be interrupted&#xa;    # by the signal handler.) The try/except is for the win32 case.&#xa;    try:&#xa;        while quit_signals_received == 0:&#xa;            time.sleep(60)&#xa;    except KeyboardInterrupt:&#xa;        pass&#xa;&#xa;    config = openerp.tools.config&#xa;    openerp.service.stop_services()&#xa;&#xa;    if getattr(openerp, 'phoenix', False):&#xa;        # like the phoenix, reborn from ashes...&#xa;        openerp.service._reexec()&#xa;        return&#xa;&#xa;    if config['pidfile']:&#xa;        os.unlink(config['pidfile'])&#xa;    sys.exit(0)&#xa;&#xa;def main(args):&#xa;    check_root_user()&#xa;    openerp.tools.config.parse_config(args)&#xa;&#xa;    check_postgres_user()&#xa;    openerp.netsvc.init_logger()&#xa;    report_configuration()&#xa;&#xa;    config = openerp.tools.config&#xa;&#xa;    setup_signal_handlers()&#xa;&#xa;    if config[""test_file""]:&#xa;        run_test_file(config['db_name'], config['test_file'])&#xa;        sys.exit(0)&#xa;&#xa;    if config[""translate_out""]:&#xa;        export_translation()&#xa;        sys.exit(0)&#xa;&#xa;    if config[""translate_in""]:&#xa;        import_translation()&#xa;        sys.exit(0)&#xa;&#xa;    if not config[""stop_after_init""]:&#xa;        setup_pid_file()&#xa;        # Some module register themselves when they are loaded so we need the&#xa;        # services to be running before loading any registry.&#xa;        if config['workers']:&#xa;            openerp.service.start_services_workers()&#xa;        else:&#xa;            openerp.service.start_services()&#xa;&#xa;    if config['db_name']:&#xa;        for dbname in config['db_name'].split(','):&#xa;            preload_registry(dbname)&#xa;&#xa;    if config[""stop_after_init""]:&#xa;        sys.exit(0)&#xa;&#xa;    _logger.info('OpenERP server is running, waiting for connections...')&#xa;    quit_on_signals()&#xa;&#xa;class Server(Command):&#xa;    def run(self, args):&#xa;        main(args)&#xa;&#xa;# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:&#xa;"
279237|"#!/usr/bin/env python&#xa;&#xa;# Test whether a client sends a correct SUBSCRIBE to a topic with QoS 0.&#xa;&#xa;# The client should connect to port 1888 with keepalive=60, clean session set,&#xa;# and client id subscribe-qos0-test&#xa;# The test will send a CONNACK message to the client with rc=0. Upon receiving&#xa;# the CONNACK and verifying that rc=0, the client should send a SUBSCRIBE&#xa;# message to subscribe to topic ""qos0/test"" with QoS=0. If rc!=0, the client&#xa;# should exit with an error.&#xa;# Upon receiving the correct SUBSCRIBE message, the test will reply with a&#xa;# SUBACK message with the accepted QoS set to 0. On receiving the SUBACK&#xa;# message, the client should send a DISCONNECT message.&#xa;&#xa;import inspect&#xa;import os&#xa;import subprocess&#xa;import socket&#xa;import sys&#xa;import time&#xa;&#xa;# From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder&#xa;cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],"".."")))&#xa;if cmd_subfolder not in sys.path:&#xa;    sys.path.insert(0, cmd_subfolder)&#xa;&#xa;import mosq_test&#xa;&#xa;rc = 1&#xa;keepalive = 60&#xa;connect_packet = mosq_test.gen_connect(""subscribe-qos0-test"", keepalive=keepalive)&#xa;connack_packet = mosq_test.gen_connack(rc=0)&#xa;&#xa;disconnect_packet = mosq_test.gen_disconnect()&#xa;&#xa;mid = 1&#xa;subscribe_packet = mosq_test.gen_subscribe(mid, ""qos0/test"", 0)&#xa;suback_packet = mosq_test.gen_suback(mid, 0)&#xa;&#xa;sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&#xa;sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)&#xa;sock.settimeout(10)&#xa;sock.bind(('', 1888))&#xa;sock.listen(5)&#xa;&#xa;client_args = sys.argv[1:]&#xa;env = dict(os.environ)&#xa;env['LD_LIBRARY_PATH'] = '../../lib:../../lib/cpp'&#xa;try:&#xa;    pp = env['PYTHONPATH']&#xa;except KeyError:&#xa;    pp = ''&#xa;env['PYTHONPATH'] = '../../lib/python:'+pp&#xa;client = mosq_test.start_client(filename=sys.argv[1].replace('/', '-'), cmd=client_args, env=env)&#xa;&#xa;try:&#xa;    (conn, address) = sock.accept()&#xa;    conn.settimeout(10)&#xa;&#xa;    if mosq_test.expect_packet(conn, ""connect"", connect_packet):&#xa;        conn.send(connack_packet)&#xa;&#xa;        if mosq_test.expect_packet(conn, ""subscribe"", subscribe_packet):&#xa;            conn.send(suback_packet)&#xa;        &#xa;            if mosq_test.expect_packet(conn, ""disconnect"", disconnect_packet):&#xa;                rc = 0&#xa;        &#xa;    conn.close()&#xa;finally:&#xa;    client.terminate()&#xa;    client.wait()&#xa;    sock.close()&#xa;&#xa;exit(rc)&#xa;"
898669|"#!/usr/bin/env python&#xa;&#xa;# Copyright 2015 The Kubernetes Authors.&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;&#xa;from __future__ import print_function&#xa;&#xa;import argparse&#xa;import os&#xa;import re&#xa;import sys&#xa;&#xa;parser = argparse.ArgumentParser()&#xa;parser.add_argument(""filenames"", help=""list of files to check, all files if unspecified"", nargs='*')&#xa;args = parser.parse_args()&#xa;&#xa;# Cargo culted from http://stackoverflow.com/questions/898669/how-can-i-detect-if-a-file-is-binary-non-text-in-python&#xa;def is_binary(pathname):&#xa;    """"""Return true if the given filename is binary.&#xa;    @raise EnvironmentError: if the file does not exist or cannot be accessed.&#xa;    @attention: found @ http://bytes.com/topic/python/answers/21222-determine-file-type-binary-text on 6/08/2010&#xa;    @author: Trent Mick <TrentM@ActiveState.com>&#xa;    @author: Jorge Orpinel <jorge@orpinel.com>""""""&#xa;    try:&#xa;        with open(pathname, 'r') as f:&#xa;            CHUNKSIZE = 1024&#xa;            while 1:&#xa;                chunk = f.read(CHUNKSIZE)&#xa;                if '\0' in chunk: # found null byte&#xa;                    return True&#xa;                if len(chunk) < CHUNKSIZE:&#xa;                    break # done&#xa;    except:&#xa;        return True&#xa;&#xa;    return False&#xa;&#xa;def get_all_files(rootdir):&#xa;    all_files = []&#xa;    for root, dirs, files in os.walk(rootdir):&#xa;        # don't visit certain dirs&#xa;        if 'vendor' in dirs:&#xa;            dirs.remove('vendor')&#xa;        if 'staging' in dirs:&#xa;            dirs.remove('staging')&#xa;        if '_output' in dirs:&#xa;            dirs.remove('_output')&#xa;        if '_gopath' in dirs:&#xa;            dirs.remove('_gopath')&#xa;        if 'third_party' in dirs:&#xa;            dirs.remove('third_party')&#xa;        if '.git' in dirs:&#xa;            dirs.remove('.git')&#xa;        if '.make' in dirs:&#xa;            dirs.remove('.make')&#xa;        if 'BUILD' in files:&#xa;           files.remove('BUILD')&#xa;&#xa;        for name in files:&#xa;            pathname = os.path.join(root, name)&#xa;            if is_binary(pathname):&#xa;                continue&#xa;            all_files.append(pathname)&#xa;    return all_files&#xa;&#xa;# Collects all the flags used in golang files and verifies the flags do&#xa;# not contain underscore. If any flag needs to be excluded from this check,&#xa;# need to add that flag in hack/verify-flags/excluded-flags.txt.&#xa;def check_underscore_in_flags(rootdir, files):&#xa;    # preload the 'known' flags which don't follow the - standard&#xa;    pathname = os.path.join(rootdir, ""hack/verify-flags/excluded-flags.txt"")&#xa;    f = open(pathname, 'r')&#xa;    excluded_flags = set(f.read().splitlines())&#xa;    f.close()&#xa;&#xa;    regexs = [ re.compile('Var[P]?\([^,]*, ""([^""]*)""'),&#xa;               re.compile('.String[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Int[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Bool[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Duration[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.StringSlice[P]?\(""([^""]*)"",[^,]+,[^)]+\)') ]&#xa;&#xa;    new_excluded_flags = set()&#xa;    # walk all the files looking for any flags being declared&#xa;    for pathname in files:&#xa;        if not pathname.endswith("".go""):&#xa;            continue&#xa;        f = open(pathname, 'r')&#xa;        data = f.read()&#xa;        f.close()&#xa;        matches = []&#xa;        for regex in regexs:&#xa;            matches = matches + regex.findall(data)&#xa;        for flag in matches:&#xa;            if any(x in flag for x in excluded_flags):&#xa;                continue&#xa;            if ""_"" in flag:&#xa;                new_excluded_flags.add(flag)&#xa;    if len(new_excluded_flags) != 0:&#xa;        print(""Found a flag declared with an _ but which is not explicitly listed as a valid flag name in hack/verify-flags/excluded-flags.txt"")&#xa;        print(""Are you certain this flag should not have been declared with an - instead?"")&#xa;        l = list(new_excluded_flags)&#xa;        l.sort()&#xa;        print(""%s"" % ""\n"".join(l))&#xa;        sys.exit(1)&#xa;&#xa;def main():&#xa;    rootdir = os.path.dirname(__file__) + ""/../""&#xa;    rootdir = os.path.abspath(rootdir)&#xa;&#xa;    if len(args.filenames) > 0:&#xa;        files = args.filenames&#xa;    else:&#xa;        files = get_all_files(rootdir)&#xa;&#xa;    check_underscore_in_flags(rootdir, files)&#xa;&#xa;if __name__ == ""__main__"":&#xa;  sys.exit(main())&#xa;"
19331550|"import csv&#xa;import datetime&#xa;import shutil&#xa;import sqlite3&#xa;import time&#xa;import enum&#xa;import re&#xa;&#xa;import bwb.bwbglobal&#xa;&#xa;#################&#xa;#&#xa;# Model&#xa;#&#xa;# This module contains everything related to the model for the application:&#xa;# * The db schema&#xa;# * The db connection&#xa;# * Data structure classes (each of which contains functions for reading and writing to the db)&#xa;# * Database creation and setup&#xa;# * Various functions (for backing up the db etc)&#xa;#&#xa;# Notes:&#xa;# * When inserting vales, it's best to use ""VALUES (?, ?)"" because then the sqlite3 module will take care of&#xa;#   escaping strings for us&#xa;#&#xa;#################&#xa;&#xa;SQLITE_FALSE = 0&#xa;SQLITE_TRUE = 1&#xa;SQLITE_NULL = ""NULL""&#xa;TIME_NOT_SET = -1&#xa;NO_REFERENCE = -1&#xa;&#xa;&#xa;class QuestionSetupEnum(enum.Enum):&#xa;    # -only used at setup&#xa;    practice = 1&#xa;    gratitude = 2&#xa;    sharing = 3&#xa;    contribution = 4&#xa;    study = 5&#xa;    self_compassion = 6&#xa;&#xa;&#xa;class MoveDirectionEnum(enum.Enum):&#xa;    up = 1&#xa;    down = 2&#xa;&#xa;&#xa;def get_schema_version(i_db_conn):&#xa;    t_cursor = i_db_conn.execute(""PRAGMA user_version"")&#xa;    return t_cursor.fetchone()[0]&#xa;&#xa;&#xa;def set_schema_version(i_db_conn, i_version_it):&#xa;    i_db_conn.execute(""PRAGMA user_version={:d}"".format(i_version_it))&#xa;&#xa;&#xa;def initial_schema_and_setup(i_db_conn):&#xa;    """"""Auto-increment is not needed in our case: https://www.sqlite.org/autoinc.html&#xa;    """"""&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.QuestionTable.name + ""(""&#xa;        + DbSchemaM.QuestionTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.QuestionTable.Cols.sort_order + "" INTEGER NOT NULL, ""&#xa;        + DbSchemaM.QuestionTable.Cols.title + "" TEXT NOT NULL, ""&#xa;        + DbSchemaM.QuestionTable.Cols.question + "" TEXT NOT NULL DEFAULT '', ""&#xa;        + DbSchemaM.QuestionTable.Cols.archived + "" INTEGER DEFAULT "" + str(SQLITE_FALSE)&#xa;        + "")""&#xa;    )&#xa;&#xa;    i_db_conn.execute(&#xa;        ""INSERT INTO "" + DbSchemaM.QuestionTable.name + ""(""&#xa;        + DbSchemaM.QuestionTable.Cols.id + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.sort_order + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.title + "", ""&#xa;        + DbSchemaM.QuestionTable.Cols.question&#xa;        + "") VALUES (?, ?, ?, ?)"", (bwb.bwbglobal.NO_ACTIVE_QUESTION_INT, -1, ""<i>no question</i>"", """")&#xa;    )&#xa;&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.date_added + "" INTEGER, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.favorite + "" INTEGER NOT NULL DEFAULT '""&#xa;        + str(SQLITE_FALSE) + ""', ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "" TEXT, ""&#xa;        + DbSchemaM.DiaryEntryTable.Cols.question_ref&#xa;        + "" INTEGER REFERENCES "" + DbSchemaM.QuestionTable.name&#xa;        + ""("" + DbSchemaM.QuestionTable.Cols.id + "")""&#xa;        + "" NOT NULL DEFAULT '"" + str(bwb.bwbglobal.NO_ACTIVE_QUESTION_INT) + ""'""&#xa;        + "")""&#xa;    )&#xa;&#xa;    # + "" NOT NULL DEFAULT '"" + str(bwb.bwbglobal.NO_ACTIVE_QUESTION_INT) + ""'""&#xa;&#xa;    """"""&#xa;    i_db_conn.execute(&#xa;        ""CREATE INDEX "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;        + "")""&#xa;    )&#xa;    """"""&#xa;&#xa;    i_db_conn.execute(&#xa;        ""CREATE TABLE "" + DbSchemaM.ReminderTable.name + ""(""&#xa;        + DbSchemaM.ReminderTable.Cols.id + "" INTEGER PRIMARY KEY, ""&#xa;        + DbSchemaM.ReminderTable.Cols.title + "" TEXT DEFAULT '', ""&#xa;        + DbSchemaM.ReminderTable.Cols.reminder + "" TEXT DEFAULT ''""&#xa;        + "")""&#xa;    )&#xa;&#xa;    if not bwb.bwbglobal.persistent_bool:&#xa;        populate_db_with_test_data()&#xa;&#xa;&#xa;""""""&#xa;Example of db upgrade code:&#xa;def upgrade_1_2(i_db_conn):&#xa;    backup_db_file()&#xa;    i_db_conn.execute(&#xa;        ""ALTER TABLE "" + DbSchemaM.ObservancesTable.name + "" ADD COLUMN ""&#xa;        + DbSchemaM.ObservancesTable.Cols.user_text + "" TEXT DEFAULT ''""&#xa;    )&#xa;""""""&#xa;&#xa;&#xa;upgrade_steps = {&#xa;    1: initial_schema_and_setup,&#xa;}&#xa;&#xa;&#xa;class DbHelperM(object):&#xa;    __db_connection = None  # ""Static""&#xa;&#xa;    # noinspection PyTypeChecker&#xa;    @staticmethod&#xa;    def get_db_connection():&#xa;        if DbHelperM.__db_connection is None:&#xa;            DbHelperM.__db_connection = sqlite3.connect(bwb.bwbglobal.get_database_filename())&#xa;&#xa;            # Upgrading the database&#xa;            # Very good upgrade explanation:&#xa;            # http://stackoverflow.com/questions/19331550/database-change-with-software-update&#xa;            # More info here: https://www.sqlite.org/pragma.html#pragma_schema_version&#xa;            current_db_ver_it = get_schema_version(DbHelperM.__db_connection)&#xa;            target_db_ver_it = max(upgrade_steps)&#xa;            for upgrade_step_it in range(current_db_ver_it + 1, target_db_ver_it + 1):&#xa;                if upgrade_step_it in upgrade_steps:&#xa;                    upgrade_steps[upgrade_step_it](DbHelperM.__db_connection)&#xa;                    set_schema_version(DbHelperM.__db_connection, upgrade_step_it)&#xa;            DbHelperM.__db_connection.commit()&#xa;&#xa;            # TODO: Where do we close the db connection? (Do we need to close it?)&#xa;            # http://stackoverflow.com/questions/3850261/doing-something-before-program-exit&#xa;&#xa;        return DbHelperM.__db_connection&#xa;&#xa;&#xa;class DbSchemaM:&#xa;    class QuestionTable:&#xa;        name = ""question""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            sort_order = ""sort_order""&#xa;            title = ""title""&#xa;            question = ""question""&#xa;            archived = ""archived""&#xa;&#xa;    class DiaryEntryTable:&#xa;        name = ""diary_entry""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            date_added = ""date_added""&#xa;            favorite = ""favorite""&#xa;            diary_entry = ""diary_entry""&#xa;            question_ref = ""question_ref""&#xa;&#xa;    class ReminderTable:&#xa;        name = ""reminder""&#xa;&#xa;        class Cols:&#xa;            id = ""id""  # key&#xa;            title = ""title""&#xa;            reminder = ""reminder""&#xa;&#xa;&#xa;class QuestionM:&#xa;    def __init__(self, i_id: int, i_order: int, i_title: str, i_question: str, i_archived: bool=False) -> None:&#xa;        self.id_int = i_id&#xa;        self.sort_order_int = i_order&#xa;        self.title_str = i_title&#xa;        self.question_str = i_question&#xa;        self.archived_bl = i_archived&#xa;&#xa;    @staticmethod&#xa;    def add(i_question_id_int: int, i_title_str: str, i_question_str: str) -> None:&#xa;        sort_order = len(QuestionM.get_all())&#xa;        print(""sort_order = "" + str(sort_order))&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.QuestionTable.name + ""(""&#xa;            + DbSchemaM.QuestionTable.Cols.id + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.sort_order + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.title + "", ""&#xa;            + DbSchemaM.QuestionTable.Cols.question&#xa;            + "") VALUES (?, ?, ?, ?)"",&#xa;            (i_question_id_int, sort_order, i_title_str, i_question_str)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        journal_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return QuestionM(*journal_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_active_by_sort_order(i_sort_order: int, i_move_direction: MoveDirectionEnum):&#xa;&#xa;        direction_as_lt_gt_str = "">""&#xa;        sort_direction_str = ""DESC""&#xa;        if i_move_direction == MoveDirectionEnum.up:&#xa;            direction_as_lt_gt_str = ""<""&#xa;            sort_direction_str = ""DESC""&#xa;        elif i_move_direction == MoveDirectionEnum.down:&#xa;            direction_as_lt_gt_str = "">""&#xa;            sort_direction_str = ""ASC""&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.sort_order + direction_as_lt_gt_str + str(i_sort_order)&#xa;            + "" AND "" + DbSchemaM.QuestionTable.Cols.archived + ""="" + str(SQLITE_FALSE)&#xa;            + "" ORDER BY "" + DbSchemaM.QuestionTable.Cols.sort_order + "" "" + sort_direction_str&#xa;        )&#xa;        journal_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return QuestionM(*journal_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all(i_show_archived_questions_bool = False):&#xa;        if i_show_archived_questions_bool:&#xa;            show_archived_questions_bool_as_int = SQLITE_TRUE&#xa;        else:&#xa;            show_archived_questions_bool_as_int = SQLITE_FALSE&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.archived + ""="" + str(show_archived_questions_bool_as_int)&#xa;            + "" ORDER BY "" + DbSchemaM.QuestionTable.Cols.sort_order&#xa;        )&#xa;        journal_db_te_list = db_cursor_result.fetchall()&#xa;        db_connection.commit()&#xa;&#xa;        return [QuestionM(*journal_db_te) for journal_db_te in journal_db_te_list]&#xa;&#xa;    @staticmethod&#xa;    def update_active_sort_order_move_up_down(i_id: int, i_move_direction: MoveDirectionEnum) -> None:&#xa;        """"""&#xa;        There is a swap of the sort order value with the question above or below which means that the&#xa;        archived questions can keep their sort orders (the previous solution was to inc/dec the sort&#xa;        order value)&#xa;        """"""&#xa;        main_id_int = i_id&#xa;        main_sort_order_int = QuestionM.get(i_id).sort_order_int&#xa;        if i_move_direction == MoveDirectionEnum.up:&#xa;            if main_sort_order_int == 0 or main_sort_order_int > len(QuestionM.get_all()):&#xa;                return&#xa;        elif i_move_direction == MoveDirectionEnum.down:&#xa;            if main_sort_order_int < 0 or main_sort_order_int >= len(QuestionM.get_all()):&#xa;                return&#xa;        else:&#xa;            pass&#xa;        other = QuestionM.get_active_by_sort_order(main_sort_order_int, i_move_direction)&#xa;        other_id_int = other.id_int&#xa;        other_sort_order_int = other.sort_order_int&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.sort_order + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(other_sort_order_int), str(main_id_int))&#xa;        )&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.sort_order + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(main_sort_order_int), str(other_id_int))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_title(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.title + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_description(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.question + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_archived(i_id_it, i_archived_bool):&#xa;&#xa;        archived_bool_as_int = SQLITE_FALSE&#xa;        if i_archived_bool:&#xa;            archived_bool_as_int = SQLITE_TRUE&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.QuestionTable.name&#xa;            + "" SET "" + DbSchemaM.QuestionTable.Cols.archived + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + "" = ?"",&#xa;            (str(archived_bool_as_int), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_it):&#xa;&#xa;        if i_id_it == bwb.bwbglobal.NO_ACTIVE_QUESTION_INT:&#xa;            raise Exception(""This cannot be removed"")&#xa;            return&#xa;&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.QuestionTable.name&#xa;            + "" WHERE "" + DbSchemaM.QuestionTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;&#xa;&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + SQLITE_NULL&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_id_it)&#xa;        )&#xa;&#xa;&#xa;&#xa;        db_connection.commit()&#xa;&#xa;&#xa;class DiaryEntryM:&#xa;    def __init__(self, i_id, i_date_added_it, i_favorite_it, i_diary_text, i_question_ref_it):&#xa;        self.id = i_id&#xa;        self.date_added_it = i_date_added_it&#xa;        self.favorite_it = i_favorite_it&#xa;        self.diary_text = i_diary_text&#xa;        self.question_ref_it = i_question_ref_it&#xa;&#xa;    @staticmethod&#xa;    def add(i_date_added_it, i_favorite_it, i_diary_text, i_journal_ref_it: int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.DiaryEntryTable.name + ""(""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.date_added + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.favorite + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "", ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.question_ref&#xa;            + "") VALUES (?, ?, ?, ?)"",&#xa;            (i_date_added_it, i_favorite_it, i_diary_text, i_journal_ref_it)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;        # t_diary_id = db_cursor.lastrowid&#xa;&#xa;    @staticmethod&#xa;    def update_note(i_id_it, i_new_text_sg):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (i_new_text_sg, str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_question(i_id_it, i_question_ref_id_int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_question_ref_id_int), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def clear_question(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (None, str(i_id_it))&#xa;            # -Please note: We cannot use ""SQLITE_NULL"" (which is the string ""null"", instead we use None&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_date(i_id_it, i_new_time_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_new_time_it), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def update_favorite(i_id_it, i_favorite_bool_as_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""UPDATE "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" SET "" + DbSchemaM.DiaryEntryTable.Cols.favorite + "" = ?""&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + "" = ?"",&#xa;            (str(i_favorite_bool_as_it), str(i_id_it))&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_it):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name + "" WHERE ""&#xa;            + DbSchemaM.DiaryEntryTable.Cols.id + ""="" + str(i_id_it)&#xa;        )&#xa;        diary_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return DiaryEntryM(*diary_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all(i_reverse_bl = False):&#xa;        t_direction_sg = ""ASC""&#xa;        if i_reverse_bl:&#xa;            t_direction_sg = ""DESC""&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" "" + t_direction_sg&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_search_term(i_search_term_str: str, i_page_number_int: int):&#xa;        t_direction_sg = ""ASC""&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry&#xa;            + "" LIKE "" + '""%' + i_search_term_str + '%""'&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "" "" + t_direction_sg&#xa;            + "" LIMIT "" + str(bwb.bwbglobal.diary_entries_per_page_int)&#xa;            + "" OFFSET "" + str(i_page_number_int * bwb.bwbglobal.diary_entries_per_page_int)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_question_and_month(&#xa;            i_question_id_it, i_start_of_month_as_unix_time_it,&#xa;            i_number_of_days_in_month_it, i_reverse_bl=False):&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(i_start_of_month_as_unix_time_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<""&#xa;            + str(i_start_of_month_as_unix_time_it + 24 * 3600 * i_number_of_days_in_month_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_question_id_it)&#xa;            + "" ORDER BY "" + DbSchemaM.DiaryEntryTable.Cols.date_added&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        if i_reverse_bl:&#xa;            ret_diary_list.reverse()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_for_active_day(i_reverse_bl=False):&#xa;        start_of_day_datetime = datetime.datetime(&#xa;            year=bwb.bwbglobal.active_date_qdate.year(),&#xa;            month=bwb.bwbglobal.active_date_qdate.month(),&#xa;            day=bwb.bwbglobal.active_date_qdate.day()&#xa;        )&#xa;        start_of_day_unixtime_it = int(start_of_day_datetime.timestamp())&#xa;&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(start_of_day_unixtime_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<"" + str(start_of_day_unixtime_it + 24 * 3600)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        if i_reverse_bl:&#xa;            ret_diary_list.reverse()&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_for_question_and_active_day(i_question_id: int) -> QuestionM:&#xa;        start_of_day_datetime = datetime.datetime(&#xa;            year=bwb.bwbglobal.active_date_qdate.year(),&#xa;            month=bwb.bwbglobal.active_date_qdate.month(),&#xa;            day=bwb.bwbglobal.active_date_qdate.day()&#xa;        )&#xa;        start_of_day_unixtime_it = int(start_of_day_datetime.timestamp())&#xa;&#xa;        ret_diary_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.date_added + "">="" + str(start_of_day_unixtime_it)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.date_added + ""<"" + str(start_of_day_unixtime_it + 24 * 3600)&#xa;            + "" AND "" + DbSchemaM.DiaryEntryTable.Cols.question_ref + ""="" + str(i_question_id)&#xa;        )&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            ret_diary_list.append(DiaryEntryM(*diary_db_te))&#xa;        db_connection.commit()&#xa;&#xa;        return ret_diary_list&#xa;&#xa;    @staticmethod&#xa;    def get_all_tags_or_friends(i_special_char_str: str) -> list:&#xa;        ret_tag_tuple_list_list = []&#xa;        # ret_tag_tuple_list_list: [(""#tag1"", [id1, id2, ___]), (""#tag2"", [id1, id3, ___]), ___]&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.DiaryEntryTable.name&#xa;            + "" WHERE "" + DbSchemaM.DiaryEntryTable.Cols.diary_entry&#xa;            + "" LIKE "" + '""%' + i_special_char_str + '%""'&#xa;        )&#xa;        # -http://sqlite.org/lang_expr.html#like&#xa;        diary_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in diary_db_te_list:&#xa;            diary_entry = DiaryEntryM(*diary_db_te)&#xa;            string_with_hashtag_str = diary_entry.diary_text&#xa;            t_diary_id_int = diary_entry.id&#xa;            regexp_pattern_obj = re.compile(""\\"" + i_special_char_str + r""\w+"")&#xa;            # Please note: we need to escape the caret (""^"") character becase this is a&#xa;            # special character (""literal"")&#xa;            regexp_search_result_list = regexp_pattern_obj.findall(string_with_hashtag_str)&#xa;            # https://docs.python.org/3/library/re.html&#xa;&#xa;            for t_re_tag_str in regexp_search_result_list:&#xa;                # -regexp_search_result_list: [""#tag1"", ""#tag2"", ___]&#xa;                flag_boolean = False&#xa;                for (t_ret_tag_str, t_ret_diary_id_list) in ret_tag_tuple_list_list:&#xa;                    if t_re_tag_str == t_ret_tag_str:&#xa;                        t_ret_diary_id_list.append(t_diary_id_int)&#xa;                        flag_boolean = True&#xa;                        break&#xa;                if flag_boolean:&#xa;                    break&#xa;                else:&#xa;                    ret_tag_tuple_list_list.append((t_re_tag_str, [t_diary_id_int]))&#xa;&#xa;        db_connection.commit()&#xa;&#xa;        # TODO: Removing duplicates&#xa;&#xa;        return ret_tag_tuple_list_list&#xa;&#xa;&#xa;class ReminderM:&#xa;    def __init__(self, i_id_int: int, i_title_str: str, i_reminder_str: str) -> None:&#xa;        self.id_int = i_id_int&#xa;        self.title_str = i_title_str&#xa;        self.reminder_str = i_reminder_str&#xa;&#xa;    @staticmethod&#xa;    def add(i_title_str: str, i_reminder_str: str) -> None:&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""INSERT INTO "" + DbSchemaM.ReminderTable.name + ""(""&#xa;            + DbSchemaM.ReminderTable.Cols.title + "", ""&#xa;            + DbSchemaM.ReminderTable.Cols.reminder&#xa;            + "") VALUES (?, ?)"", (i_title_str, i_reminder_str)&#xa;        )&#xa;&#xa;        db_connection.commit()&#xa;&#xa;    @staticmethod&#xa;    def get(i_id_int: int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.ReminderTable.name&#xa;            + "" WHERE "" + DbSchemaM.ReminderTable.Cols.id + ""="" + str(i_id_int)&#xa;        )&#xa;        reminder_db_te = db_cursor_result.fetchone()&#xa;        db_connection.commit()&#xa;&#xa;        return ReminderM(*reminder_db_te)&#xa;&#xa;    @staticmethod&#xa;    def get_all():&#xa;        ret_reminder_list = []&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor_result = db_cursor.execute(&#xa;            ""SELECT * FROM "" + DbSchemaM.ReminderTable.name&#xa;        )&#xa;        reminder_db_te_list = db_cursor_result.fetchall()&#xa;        for diary_db_te in reminder_db_te_list:&#xa;            ret_reminder_list.append(ReminderM(*diary_db_te))&#xa;        db_connection.commit()&#xa;        return ret_reminder_list&#xa;&#xa;    @staticmethod&#xa;    def remove(i_id_int):&#xa;        db_connection = DbHelperM.get_db_connection()&#xa;        db_cursor = db_connection.cursor()&#xa;        db_cursor.execute(&#xa;            ""DELETE FROM "" + DbSchemaM.ReminderTable.name&#xa;            + "" WHERE "" + DbSchemaM.ReminderTable.Cols.id + ""="" + str(i_id_int)&#xa;        )&#xa;        db_connection.commit()&#xa;&#xa;&#xa;def export_all():&#xa;    csv_writer = csv.writer(open(""exported.csv"", ""w""))&#xa;    for diary_item in DiaryEntryM.get_all():&#xa;        time_datetime = datetime.date.fromtimestamp(diary_item.date_added_it)&#xa;        date_str = time_datetime.strftime(""%Y-%m-%d"")&#xa;        csv_writer.writerow((date_str, diary_item.diary_text))&#xa;&#xa;&#xa;def backup_db_file():&#xa;    date_sg = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")&#xa;    new_file_name_sg = bwb.bwbglobal.get_database_filename() + ""_"" + date_sg&#xa;    shutil.copyfile(bwb.bwbglobal.get_database_filename(), new_file_name_sg)&#xa;    return&#xa;&#xa;&#xa;def populate_db_with_test_data():&#xa;    delta_day_it = 24 * 60 * 60&#xa;&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.gratitude.value,&#xa;        QuestionSetupEnum.gratitude.name.capitalize(),&#xa;        ""What did I do to water the seeds of joy in myself today? What posivite things came my way today?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.practice.value,&#xa;        QuestionSetupEnum.practice.name.capitalize(),&#xa;        ""What practices did I do today? Sitting meditation ? Walking meditation? Gathas?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.sharing.value,&#xa;        QuestionSetupEnum.sharing.name.capitalize(),&#xa;        ""Did I share my happiness with others? Did I enjoy the happiness of others?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.contribution.value,&#xa;        QuestionSetupEnum.contribution.name.capitalize(),&#xa;        ""How did I contribute to the well-being on others? Did I share my joy with my friends and family?"")&#xa;    QuestionM.add(&#xa;        QuestionSetupEnum.study.value,&#xa;        QuestionSetupEnum.study.name.capitalize(),&#xa;        ""What did I read and listen to today and learn? Dharma talks? Lectures? Books? Sutras?"")&#xa;&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dear Buddha, today i was #practicing #sitting meditation before meeting a friend of mine to be able to be more present during our meeting"",&#xa;        bwb.bwbglobal.NO_ACTIVE_QUESTION_INT)&#xa;&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dear Buddha, i'm #grateful for being able to breathe!"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(time.time() - delta_day_it, SQLITE_FALSE,&#xa;        ""Most difficult today was my negative thinking, #practicing with this by changing the peg from negative thoughts to positive thinking"",&#xa;        QuestionSetupEnum.practice.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 7 * delta_day_it, SQLITE_FALSE,&#xa;        ""Grateful for having a place to live, a roof over my head, food to eat, and people to care for"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 7 * delta_day_it, SQLITE_FALSE,&#xa;        ""Grateful for the blue sky and the white clouds"",&#xa;        QuestionSetupEnum.gratitude.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 3 * delta_day_it, SQLITE_FALSE,&#xa;        ""Dear Buddha, today i read about the four foundations of mindfulness. Some important parts: 1. Body 2. Feelings 3. Mind 4. Objects of mind"",&#xa;        QuestionSetupEnum.study.value)&#xa;    DiaryEntryM.add(&#xa;        time.time() - 4 * delta_day_it, SQLITE_FALSE,&#xa;        ""Programming and working on the application. Using Python and Qt. Cooperating with @John and @Emma"",&#xa;        QuestionSetupEnum.contribution.value)&#xa;    DiaryEntryM.add(&#xa;        time.time(), SQLITE_FALSE,&#xa;        ""Dharma talk from ^Plum-Village"",&#xa;        QuestionSetupEnum.practice.value)&#xa;&#xa;    ReminderM.add(""Inter-being"",&#xa;        ""All things in the universe inter-are, our suffering and happiness inter-is with the suffernig and happiness of others"")&#xa;    ReminderM.add(""No Mud, no lotus"",&#xa;        ""A lotus flower cannot grow on marble!"")&#xa;&#xa;"
5658622|"import os&#xa;import re&#xa;import sys&#xa;from distutils import log&#xa;import xml.dom.pulldom&#xa;import shlex&#xa;import locale&#xa;import codecs&#xa;import unicodedata&#xa;import warnings&#xa;from setuptools.compat import unicode&#xa;from xml.sax.saxutils import unescape&#xa;&#xa;try:&#xa;    import urlparse&#xa;except ImportError:&#xa;    import urllib.parse as urlparse&#xa;&#xa;from subprocess import Popen as _Popen, PIPE as _PIPE&#xa;&#xa;#NOTE: Use of the command line options require SVN 1.3 or newer (December 2005)&#xa;#      and SVN 1.3 hasn't been supported by the developers since mid 2008.&#xa;&#xa;#subprocess is called several times with shell=(sys.platform=='win32')&#xa;#see the follow for more information:&#xa;#       http://bugs.python.org/issue8557&#xa;#       http://stackoverflow.com/questions/5658622/&#xa;#              python-subprocess-popen-environment-path&#xa;&#xa;&#xa;def _run_command(args, stdout=_PIPE, stderr=_PIPE, encoding=None, stream=0):&#xa;    #regarding the shell argument, see: http://bugs.python.org/issue8557&#xa;    try:&#xa;        proc = _Popen(args, stdout=stdout, stderr=stderr,&#xa;                      shell=(sys.platform == 'win32'))&#xa;&#xa;        data = proc.communicate()[stream]&#xa;    except OSError:&#xa;        return 1, ''&#xa;&#xa;    #doubled checked and&#xa;    data = decode_as_string(data, encoding)&#xa;&#xa;    #communciate calls wait()&#xa;    return proc.returncode, data&#xa;&#xa;&#xa;def _get_entry_schedule(entry):&#xa;    schedule = entry.getElementsByTagName('schedule')[0]&#xa;    return """".join([t.nodeValue&#xa;                    for t in schedule.childNodes&#xa;                    if t.nodeType == t.TEXT_NODE])&#xa;&#xa;&#xa;def _get_target_property(target):&#xa;    property_text = target.getElementsByTagName('property')[0]&#xa;    return """".join([t.nodeValue&#xa;                    for t in property_text.childNodes&#xa;                    if t.nodeType == t.TEXT_NODE])&#xa;&#xa;&#xa;def _get_xml_data(decoded_str):&#xa;    if sys.version_info < (3, 0):&#xa;        #old versions want an encoded string&#xa;        data = decoded_str.encode('utf-8')&#xa;    else:&#xa;        data = decoded_str&#xa;    return data&#xa;&#xa;&#xa;def joinpath(prefix, *suffix):&#xa;    if not prefix or prefix == '.':&#xa;        return os.path.join(*suffix)&#xa;    return os.path.join(prefix, *suffix)&#xa;&#xa;def determine_console_encoding():&#xa;    try:&#xa;        #try for the preferred encoding&#xa;        encoding = locale.getpreferredencoding()&#xa;&#xa;        #see if the locale.getdefaultlocale returns null&#xa;        #some versions of python\platforms return US-ASCII&#xa;        #when it cannot determine an encoding&#xa;        if not encoding or encoding == ""US-ASCII"":&#xa;            encoding = locale.getdefaultlocale()[1]&#xa;&#xa;        if encoding:&#xa;            codecs.lookup(encoding)  # make sure a lookup error is not made&#xa;&#xa;    except (locale.Error, LookupError):&#xa;        encoding = None&#xa;&#xa;    is_osx = sys.platform == ""darwin""&#xa;    if not encoding:&#xa;        return [""US-ASCII"", ""utf-8""][is_osx]&#xa;    elif encoding.startswith(""mac-"") and is_osx:&#xa;        #certain versions of python would return mac-roman as default&#xa;        #OSX as a left over of earlier mac versions.&#xa;        return ""utf-8""&#xa;    else:&#xa;        return encoding&#xa;&#xa;_console_encoding = determine_console_encoding()&#xa;&#xa;def decode_as_string(text, encoding=None):&#xa;    """"""&#xa;    Decode the console or file output explicitly using getpreferredencoding.&#xa;    The text paraemeter should be a encoded string, if not no decode occurs&#xa;    If no encoding is given, getpreferredencoding is used.  If encoding is&#xa;    specified, that is used instead.  This would be needed for SVN --xml&#xa;    output.  Unicode is explicitly put in composed NFC form.&#xa;&#xa;    --xml should be UTF-8 (SVN Issue 2938) the discussion on the Subversion&#xa;    DEV List from 2007 seems to indicate the same.&#xa;    """"""&#xa;    #text should be a byte string&#xa;&#xa;    if encoding is None:&#xa;        encoding = _console_encoding&#xa;&#xa;    if not isinstance(text, unicode):&#xa;        text = text.decode(encoding)&#xa;&#xa;    text = unicodedata.normalize('NFC', text)&#xa;&#xa;    return text&#xa;&#xa;&#xa;def parse_dir_entries(decoded_str):&#xa;    '''Parse the entries from a recursive info xml'''&#xa;    doc = xml.dom.pulldom.parseString(_get_xml_data(decoded_str))&#xa;    entries = list()&#xa;&#xa;    for event, node in doc:&#xa;        if event == 'START_ELEMENT' and node.nodeName == 'entry':&#xa;            doc.expandNode(node)&#xa;            if not _get_entry_schedule(node).startswith('delete'):&#xa;                entries.append((node.getAttribute('path'),&#xa;                                node.getAttribute('kind')))&#xa;&#xa;    return entries[1:]  # do not want the root directory&#xa;&#xa;&#xa;def parse_externals_xml(decoded_str, prefix=''):&#xa;    '''Parse a propget svn:externals xml'''&#xa;    prefix = os.path.normpath(prefix)&#xa;    prefix = os.path.normcase(prefix)&#xa;&#xa;    doc = xml.dom.pulldom.parseString(_get_xml_data(decoded_str))&#xa;    externals = list()&#xa;&#xa;    for event, node in doc:&#xa;        if event == 'START_ELEMENT' and node.nodeName == 'target':&#xa;            doc.expandNode(node)&#xa;            path = os.path.normpath(node.getAttribute('path'))&#xa;&#xa;            if os.path.normcase(path).startswith(prefix):&#xa;                path = path[len(prefix)+1:]&#xa;&#xa;            data = _get_target_property(node)&#xa;            #data should be decoded already&#xa;            for external in parse_external_prop(data):&#xa;                externals.append(joinpath(path, external))&#xa;&#xa;    return externals  # do not want the root directory&#xa;&#xa;&#xa;def parse_external_prop(lines):&#xa;    """"""&#xa;    Parse the value of a retrieved svn:externals entry.&#xa;&#xa;    possible token setups (with quotng and backscaping in laters versions)&#xa;        URL[@#] EXT_FOLDERNAME&#xa;        [-r#] URL EXT_FOLDERNAME&#xa;        EXT_FOLDERNAME [-r#] URL&#xa;    """"""&#xa;    externals = []&#xa;    for line in lines.splitlines():&#xa;        line = line.lstrip()  # there might be a ""\ ""&#xa;        if not line:&#xa;            continue&#xa;&#xa;        if sys.version_info < (3, 0):&#xa;            #shlex handles NULLs just fine and shlex in 2.7 tries to encode&#xa;            #as ascii automatiically&#xa;            line = line.encode('utf-8')&#xa;        line = shlex.split(line)&#xa;        if sys.version_info < (3, 0):&#xa;            line = [x.decode('utf-8') for x in line]&#xa;&#xa;        #EXT_FOLDERNAME is either the first or last depending on where&#xa;        #the URL falls&#xa;        if urlparse.urlsplit(line[-1])[0]:&#xa;            external = line[0]&#xa;        else:&#xa;            external = line[-1]&#xa;&#xa;        external = decode_as_string(external, encoding=""utf-8"")&#xa;        externals.append(os.path.normpath(external))&#xa;&#xa;    return externals&#xa;&#xa;&#xa;def parse_prop_file(filename, key):&#xa;    found = False&#xa;    f = open(filename, 'rt')&#xa;    data = ''&#xa;    try:&#xa;        for line in iter(f.readline, ''):    # can't use direct iter!&#xa;            parts = line.split()&#xa;            if len(parts) == 2:&#xa;                kind, length = parts&#xa;                data = f.read(int(length))&#xa;                if kind == 'K' and data == key:&#xa;                    found = True&#xa;                elif kind == 'V' and found:&#xa;                    break&#xa;    finally:&#xa;        f.close()&#xa;&#xa;    return data&#xa;&#xa;&#xa;class SvnInfo(object):&#xa;    '''&#xa;    Generic svn_info object.  No has little knowledge of how to extract&#xa;    information.  Use cls.load to instatiate according svn version.&#xa;&#xa;    Paths are not filesystem encoded.&#xa;    '''&#xa;&#xa;    @staticmethod&#xa;    def get_svn_version():&#xa;        code, data = _run_command(['svn', '--version', '--quiet'])&#xa;        if code == 0 and data:&#xa;            return data.strip()&#xa;        else:&#xa;            return ''&#xa;&#xa;    #svnversion return values (previous implementations return max revision)&#xa;    #   4123:4168     mixed revision working copy&#xa;    #   4168M         modified working copy&#xa;    #   4123S         switched working copy&#xa;    #   4123:4168MS   mixed revision, modified, switched working copy&#xa;    revision_re = re.compile(r'(?:([\-0-9]+):)?(\d+)([a-z]*)\s*$', re.I)&#xa;&#xa;    @classmethod&#xa;    def load(cls, dirname=''):&#xa;        normdir = os.path.normpath(dirname)&#xa;        code, data = _run_command(['svn', 'info', normdir])&#xa;        # Must check for some contents, as some use empty directories&#xa;        # in testcases&#xa;        svn_dir = os.path.join(normdir, '.svn')&#xa;        has_svn = (os.path.isfile(os.path.join(svn_dir, 'entries')) or&#xa;                   os.path.isfile(os.path.join(svn_dir, 'dir-props')) or&#xa;                   os.path.isfile(os.path.join(svn_dir, 'dir-prop-base')))&#xa;&#xa;        svn_version = tuple(cls.get_svn_version().split('.'))&#xa;&#xa;        try:&#xa;            base_svn_version = tuple(int(x) for x in svn_version[:2])&#xa;        except ValueError:&#xa;            base_svn_version = tuple()&#xa;&#xa;        if not has_svn:&#xa;            return SvnInfo(dirname)&#xa;&#xa;        if code or not base_svn_version or base_svn_version < (1, 3):&#xa;            warnings.warn((""No SVN 1.3+ command found: falling back ""&#xa;                           ""on pre 1.7 .svn parsing""), DeprecationWarning)&#xa;            return SvnFileInfo(dirname)&#xa;&#xa;        if base_svn_version < (1, 5):&#xa;            return Svn13Info(dirname)&#xa;&#xa;        return Svn15Info(dirname)&#xa;&#xa;    def __init__(self, path=''):&#xa;        self.path = path&#xa;        self._entries = None&#xa;        self._externals = None&#xa;&#xa;    def get_revision(self):&#xa;        'Retrieve the directory revision informatino using svnversion'&#xa;        code, data = _run_command(['svnversion', '-c', self.path])&#xa;        if code:&#xa;            log.warn(""svnversion failed"")&#xa;            return 0&#xa;&#xa;        parsed = self.revision_re.match(data)&#xa;        if parsed:&#xa;            return int(parsed.group(2))&#xa;        else:&#xa;            return 0&#xa;&#xa;    @property&#xa;    def entries(self):&#xa;        if self._entries is None:&#xa;            self._entries = self.get_entries()&#xa;        return self._entries&#xa;&#xa;    @property&#xa;    def externals(self):&#xa;        if self._externals is None:&#xa;            self._externals = self.get_externals()&#xa;        return self._externals&#xa;&#xa;    def iter_externals(self):&#xa;        '''&#xa;        Iterate over the svn:external references in the repository path.&#xa;        '''&#xa;        for item in self.externals:&#xa;            yield item&#xa;&#xa;    def iter_files(self):&#xa;        '''&#xa;        Iterate over the non-deleted file entries in the repository path&#xa;        '''&#xa;        for item, kind in self.entries:&#xa;            if kind.lower() == 'file':&#xa;                yield item&#xa;&#xa;    def iter_dirs(self, include_root=True):&#xa;        '''&#xa;        Iterate over the non-deleted file entries in the repository path&#xa;        '''&#xa;        if include_root:&#xa;            yield self.path&#xa;        for item, kind in self.entries:&#xa;            if kind.lower() == 'dir':&#xa;                yield item&#xa;&#xa;    def get_entries(self):&#xa;        return []&#xa;&#xa;    def get_externals(self):&#xa;        return []&#xa;&#xa;&#xa;class Svn13Info(SvnInfo):&#xa;    def get_entries(self):&#xa;        code, data = _run_command(['svn', 'info', '-R', '--xml', self.path],&#xa;                                  encoding=""utf-8"")&#xa;&#xa;        if code:&#xa;            log.debug(""svn info failed"")&#xa;            return []&#xa;&#xa;        return parse_dir_entries(data)&#xa;&#xa;    def get_externals(self):&#xa;        #Previous to 1.5 --xml was not supported for svn propget and the -R&#xa;        #output format breaks the shlex compatible semantics.&#xa;        cmd = ['svn', 'propget', 'svn:externals']&#xa;        result = []&#xa;        for folder in self.iter_dirs():&#xa;            code, lines = _run_command(cmd + [folder], encoding=""utf-8"")&#xa;            if code != 0:&#xa;                log.warn(""svn propget failed"")&#xa;                return []&#xa;            #lines should a str&#xa;            for external in parse_external_prop(lines):&#xa;                if folder:&#xa;                    external = os.path.join(folder, external)&#xa;                result.append(os.path.normpath(external))&#xa;&#xa;        return result&#xa;&#xa;&#xa;class Svn15Info(Svn13Info):&#xa;    def get_externals(self):&#xa;        cmd = ['svn', 'propget', 'svn:externals', self.path, '-R', '--xml']&#xa;        code, lines = _run_command(cmd, encoding=""utf-8"")&#xa;        if code:&#xa;            log.debug(""svn propget failed"")&#xa;            return []&#xa;        return parse_externals_xml(lines, prefix=os.path.abspath(self.path))&#xa;&#xa;&#xa;class SvnFileInfo(SvnInfo):&#xa;&#xa;    def __init__(self, path=''):&#xa;        super(SvnFileInfo, self).__init__(path)&#xa;        self._directories = None&#xa;        self._revision = None&#xa;&#xa;    def _walk_svn(self, base):&#xa;        entry_file = joinpath(base, '.svn', 'entries')&#xa;        if os.path.isfile(entry_file):&#xa;            entries = SVNEntriesFile.load(base)&#xa;            yield (base, False, entries.parse_revision())&#xa;            for path in entries.get_undeleted_records():&#xa;                path = decode_as_string(path)&#xa;                path = joinpath(base, path)&#xa;                if os.path.isfile(path):&#xa;                    yield (path, True, None)&#xa;                elif os.path.isdir(path):&#xa;                    for item in self._walk_svn(path):&#xa;                        yield item&#xa;&#xa;    def _build_entries(self):&#xa;        entries = list()&#xa;&#xa;        rev = 0&#xa;        for path, isfile, dir_rev in self._walk_svn(self.path):&#xa;            if isfile:&#xa;                entries.append((path, 'file'))&#xa;            else:&#xa;                entries.append((path, 'dir'))&#xa;                rev = max(rev, dir_rev)&#xa;&#xa;        self._entries = entries&#xa;        self._revision = rev&#xa;&#xa;    def get_entries(self):&#xa;        if self._entries is None:&#xa;            self._build_entries()&#xa;        return self._entries&#xa;&#xa;    def get_revision(self):&#xa;        if self._revision is None:&#xa;            self._build_entries()&#xa;        return self._revision&#xa;&#xa;    def get_externals(self):&#xa;        prop_files = [['.svn', 'dir-prop-base'],&#xa;                      ['.svn', 'dir-props']]&#xa;        externals = []&#xa;&#xa;        for dirname in self.iter_dirs():&#xa;            prop_file = None&#xa;            for rel_parts in prop_files:&#xa;                filename = joinpath(dirname, *rel_parts)&#xa;                if os.path.isfile(filename):&#xa;                    prop_file = filename&#xa;&#xa;            if prop_file is not None:&#xa;                ext_prop = parse_prop_file(prop_file, 'svn:externals')&#xa;                #ext_prop should be utf-8 coming from svn:externals&#xa;                ext_prop = decode_as_string(ext_prop, encoding=""utf-8"")&#xa;                externals.extend(parse_external_prop(ext_prop))&#xa;&#xa;        return externals&#xa;&#xa;&#xa;def svn_finder(dirname=''):&#xa;    #combined externals due to common interface&#xa;    #combined externals and entries due to lack of dir_props in 1.7&#xa;    info = SvnInfo.load(dirname)&#xa;    for path in info.iter_files():&#xa;        yield path&#xa;&#xa;    for path in info.iter_externals():&#xa;        sub_info = SvnInfo.load(path)&#xa;        for sub_path in sub_info.iter_files():&#xa;            yield sub_path&#xa;&#xa;&#xa;class SVNEntriesFile(object):&#xa;    def __init__(self, data):&#xa;        self.data = data&#xa;&#xa;    @classmethod&#xa;    def load(class_, base):&#xa;        filename = os.path.join(base, '.svn', 'entries')&#xa;        f = open(filename)&#xa;        try:&#xa;            result = SVNEntriesFile.read(f)&#xa;        finally:&#xa;            f.close()&#xa;        return result&#xa;&#xa;    @classmethod&#xa;    def read(class_, fileobj):&#xa;        data = fileobj.read()&#xa;        is_xml = data.startswith('<?xml')&#xa;        class_ = [SVNEntriesFileText, SVNEntriesFileXML][is_xml]&#xa;        return class_(data)&#xa;&#xa;    def parse_revision(self):&#xa;        all_revs = self.parse_revision_numbers() + [0]&#xa;        return max(all_revs)&#xa;&#xa;&#xa;class SVNEntriesFileText(SVNEntriesFile):&#xa;    known_svn_versions = {&#xa;        '1.4.x': 8,&#xa;        '1.5.x': 9,&#xa;        '1.6.x': 10,&#xa;    }&#xa;&#xa;    def __get_cached_sections(self):&#xa;        return self.sections&#xa;&#xa;    def get_sections(self):&#xa;        SECTION_DIVIDER = '\f\n'&#xa;        sections = self.data.split(SECTION_DIVIDER)&#xa;        sections = [x for x in map(str.splitlines, sections)]&#xa;        try:&#xa;            # remove the SVN version number from the first line&#xa;            svn_version = int(sections[0].pop(0))&#xa;            if not svn_version in self.known_svn_versions.values():&#xa;                log.warn(""Unknown subversion verson %d"", svn_version)&#xa;        except ValueError:&#xa;            return&#xa;        self.sections = sections&#xa;        self.get_sections = self.__get_cached_sections&#xa;        return self.sections&#xa;&#xa;    def is_valid(self):&#xa;        return bool(self.get_sections())&#xa;&#xa;    def get_url(self):&#xa;        return self.get_sections()[0][4]&#xa;&#xa;    def parse_revision_numbers(self):&#xa;        revision_line_number = 9&#xa;        rev_numbers = [&#xa;            int(section[revision_line_number])&#xa;            for section in self.get_sections()&#xa;            if (len(section) > revision_line_number&#xa;                and section[revision_line_number])&#xa;        ]&#xa;        return rev_numbers&#xa;&#xa;    def get_undeleted_records(self):&#xa;        undeleted = lambda s: s and s[0] and (len(s) < 6 or s[5] != 'delete')&#xa;        result = [&#xa;            section[0]&#xa;            for section in self.get_sections()&#xa;            if undeleted(section)&#xa;        ]&#xa;        return result&#xa;&#xa;&#xa;class SVNEntriesFileXML(SVNEntriesFile):&#xa;    def is_valid(self):&#xa;        return True&#xa;&#xa;    def get_url(self):&#xa;        ""Get repository URL""&#xa;        urlre = re.compile('url=""([^""]+)""')&#xa;        return urlre.search(self.data).group(1)&#xa;&#xa;    def parse_revision_numbers(self):&#xa;        revre = re.compile(r'committed-rev=""(\d+)""')&#xa;        return [&#xa;            int(m.group(1))&#xa;            for m in revre.finditer(self.data)&#xa;        ]&#xa;&#xa;    def get_undeleted_records(self):&#xa;        entries_pattern = \&#xa;            re.compile(r'name=""([^""]+)""(?![^>]+deleted=""true"")', re.I)&#xa;        results = [&#xa;            unescape(match.group(1))&#xa;            for match in entries_pattern.finditer(self.data)&#xa;        ]&#xa;        return results&#xa;&#xa;&#xa;if __name__ == '__main__':&#xa;    for name in svn_finder(sys.argv[1]):&#xa;        print(name)&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = ln -f ""$<"" ""$@"" 2>/dev/null || (rm -rf ""$@"" && cp -af ""$<"" ""$@"")&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;    if (self.type == 'shared_library' and&#xa;        (self.flavor != 'mac' or self.toolset != 'target')):&#xa;      # Install all shared libs into a common directory (per toolset) for&#xa;      # convenient access with LD_LIBRARY_PATH.&#xa;      return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host',), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CPPFLAGS) $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CPPFLAGS) $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?= $(CPPFLAGS_host) $(CFLAGS_host)&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?= $(CPPFLAGS_host) $(CXXFLAGS_host)&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = rm -rf ""$@"" && cp %(copy_archive_args)s ""$<"" ""$@""&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'netbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;&#xa;    # XXX(TooTallNate): disabling this code since we don't want this behavior...&#xa;    #if (self.type == 'shared_library' and&#xa;    #    (self.flavor != 'mac' or self.toolset != 'target')):&#xa;    #  # Install all shared libs into a common directory (per toolset) for&#xa;    #  # convenient access with LD_LIBRARY_PATH.&#xa;    #  return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  copy_archive_arguments = '-af'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;      'copy_archive_args': copy_archive_arguments,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'openbsd':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host', 'CC'), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host', 'AR'), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host', 'CXX'), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host', 'LINK'), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
4124220|"from __future__ import absolute_import, unicode_literals&#xa;&#xa;import re&#xa;from collections import OrderedDict&#xa;&#xa;from django import template&#xa;from django.template import loader&#xa;from django.utils import six&#xa;from django.utils.encoding import force_text, iri_to_uri&#xa;from django.utils.html import escape, format_html, smart_urlquote&#xa;from django.utils.safestring import SafeData, mark_safe&#xa;&#xa;from rest_framework.compat import (&#xa;    NoReverseMatch, markdown, pygments_highlight, reverse, template_render&#xa;)&#xa;from rest_framework.renderers import HTMLFormRenderer&#xa;from rest_framework.utils.urls import replace_query_param&#xa;&#xa;register = template.Library()&#xa;&#xa;# Regex for adding classes to html snippets&#xa;class_re = re.compile(r'(?<=class=[""\'])(.*)(?=[""\'])')&#xa;&#xa;&#xa;@register.tag(name='code')&#xa;def highlight_code(parser, token):&#xa;    code = token.split_contents()[-1]&#xa;    nodelist = parser.parse(('endcode',))&#xa;    parser.delete_first_token()&#xa;    return CodeNode(code, nodelist)&#xa;&#xa;&#xa;class CodeNode(template.Node):&#xa;    style = 'emacs'&#xa;&#xa;    def __init__(self, lang, code):&#xa;        self.lang = lang&#xa;        self.nodelist = code&#xa;&#xa;    def render(self, context):&#xa;        text = self.nodelist.render(context)&#xa;        return pygments_highlight(text, self.lang, self.style)&#xa;&#xa;&#xa;@register.filter()&#xa;def with_location(fields, location):&#xa;    return [&#xa;        field for field in fields&#xa;        if field.location == location&#xa;    ]&#xa;&#xa;&#xa;@register.simple_tag&#xa;def form_for_link(link):&#xa;    import coreschema&#xa;    properties = OrderedDict([&#xa;        (field.name, field.schema or coreschema.String())&#xa;        for field in link.fields&#xa;    ])&#xa;    required = [&#xa;        field.name&#xa;        for field in link.fields&#xa;        if field.required&#xa;    ]&#xa;    schema = coreschema.Object(properties=properties, required=required)&#xa;    return mark_safe(coreschema.render_to_form(schema))&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_markdown(markdown_text):&#xa;    if not markdown:&#xa;        return markdown_text&#xa;    return mark_safe(markdown.markdown(markdown_text))&#xa;&#xa;&#xa;@register.simple_tag&#xa;def get_pagination_html(pager):&#xa;    return pager.to_html()&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_form(serializer, template_pack=None):&#xa;    style = {'template_pack': template_pack} if template_pack else {}&#xa;    renderer = HTMLFormRenderer()&#xa;    return renderer.render(serializer.data, None, {'style': style})&#xa;&#xa;&#xa;@register.simple_tag&#xa;def render_field(field, style):&#xa;    renderer = style.get('renderer', HTMLFormRenderer())&#xa;    return renderer.render_field(field, style)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_login(request):&#xa;    """"""&#xa;    Include a login snippet if REST framework's login view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        login_url = reverse('rest_framework:login')&#xa;    except NoReverseMatch:&#xa;        return ''&#xa;&#xa;    snippet = ""<li><a href='{href}?next={next}'>Log in</a></li>""&#xa;    snippet = format_html(snippet, href=login_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_docs_login(request):&#xa;    """"""&#xa;    Include a login snippet if REST framework's login view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        login_url = reverse('rest_framework:login')&#xa;    except NoReverseMatch:&#xa;        return 'log in'&#xa;&#xa;    snippet = ""<a href='{href}?next={next}'>log in</a>""&#xa;    snippet = format_html(snippet, href=login_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def optional_logout(request, user):&#xa;    """"""&#xa;    Include a logout snippet if REST framework's logout view is in the URLconf.&#xa;    """"""&#xa;    try:&#xa;        logout_url = reverse('rest_framework:logout')&#xa;    except NoReverseMatch:&#xa;        snippet = format_html('<li class=""navbar-text"">{user}</li>', user=escape(user))&#xa;        return mark_safe(snippet)&#xa;&#xa;    snippet = """"""<li class=""dropdown"">&#xa;        <a href=""#"" class=""dropdown-toggle"" data-toggle=""dropdown"">&#xa;            {user}&#xa;            <b class=""caret""></b>&#xa;        </a>&#xa;        <ul class=""dropdown-menu"">&#xa;            <li><a href='{href}?next={next}'>Log out</a></li>&#xa;        </ul>&#xa;    </li>""""""&#xa;    snippet = format_html(snippet, user=escape(user), href=logout_url, next=escape(request.path))&#xa;&#xa;    return mark_safe(snippet)&#xa;&#xa;&#xa;@register.simple_tag&#xa;def add_query_param(request, key, val):&#xa;    """"""&#xa;    Add a query parameter to the current request url, and return the new url.&#xa;    """"""&#xa;    iri = request.get_full_path()&#xa;    uri = iri_to_uri(iri)&#xa;    return escape(replace_query_param(uri, key, val))&#xa;&#xa;&#xa;@register.filter&#xa;def as_string(value):&#xa;    if value is None:&#xa;        return ''&#xa;    return '%s' % value&#xa;&#xa;&#xa;@register.filter&#xa;def as_list_of_strings(value):&#xa;    return [&#xa;        '' if (item is None) else ('%s' % item)&#xa;        for item in value&#xa;    ]&#xa;&#xa;&#xa;@register.filter&#xa;def add_class(value, css_class):&#xa;    """"""&#xa;    http://stackoverflow.com/questions/4124220/django-adding-css-classes-when-rendering-form-fields-in-a-template&#xa;&#xa;    Inserts classes into template variables that contain HTML tags,&#xa;    useful for modifying forms without needing to change the Form objects.&#xa;&#xa;    Usage:&#xa;&#xa;        {{ field.label_tag|add_class:""control-label"" }}&#xa;&#xa;    In the case of REST Framework, the filter is used to add Bootstrap-specific&#xa;    classes to the forms.&#xa;    """"""&#xa;    html = six.text_type(value)&#xa;    match = class_re.search(html)&#xa;    if match:&#xa;        m = re.search(r'^%s$|^%s\s|\s%s\s|\s%s$' % (css_class, css_class,&#xa;                                                    css_class, css_class),&#xa;                      match.group(1))&#xa;        if not m:&#xa;            return mark_safe(class_re.sub(match.group(1) + "" "" + css_class,&#xa;                                          html))&#xa;    else:&#xa;        return mark_safe(html.replace('>', ' class=""%s"">' % css_class, 1))&#xa;    return value&#xa;&#xa;&#xa;@register.filter&#xa;def format_value(value):&#xa;    if getattr(value, 'is_hyperlink', False):&#xa;        name = six.text_type(value.obj)&#xa;        return mark_safe('<a href=%s>%s</a>' % (value, escape(name)))&#xa;    if value is None or isinstance(value, bool):&#xa;        return mark_safe('<code>%s</code>' % {True: 'true', False: 'false', None: 'null'}[value])&#xa;    elif isinstance(value, list):&#xa;        if any([isinstance(item, (list, dict)) for item in value]):&#xa;            template = loader.get_template('rest_framework/admin/list_value.html')&#xa;        else:&#xa;            template = loader.get_template('rest_framework/admin/simple_list_value.html')&#xa;        context = {'value': value}&#xa;        return template_render(template, context)&#xa;    elif isinstance(value, dict):&#xa;        template = loader.get_template('rest_framework/admin/dict_value.html')&#xa;        context = {'value': value}&#xa;        return template_render(template, context)&#xa;    elif isinstance(value, six.string_types):&#xa;        if (&#xa;            (value.startswith('http:') or value.startswith('https:')) and not&#xa;            re.search(r'\s', value)&#xa;        ):&#xa;            return mark_safe('<a href=""{value}"">{value}</a>'.format(value=escape(value)))&#xa;        elif '@' in value and not re.search(r'\s', value):&#xa;            return mark_safe('<a href=""mailto:{value}"">{value}</a>'.format(value=escape(value)))&#xa;        elif '\n' in value:&#xa;            return mark_safe('<pre>%s</pre>' % escape(value))&#xa;    return six.text_type(value)&#xa;&#xa;&#xa;@register.filter&#xa;def items(value):&#xa;    """"""&#xa;    Simple filter to return the items of the dict. Useful when the dict may&#xa;    have a key 'items' which is resolved first in Django tempalte dot-notation&#xa;    lookup.  See issue #4931&#xa;    Also see: https://stackoverflow.com/questions/15416662/django-template-loop-over-dictionary-items-with-items-as-key&#xa;    """"""&#xa;    return value.items()&#xa;&#xa;&#xa;@register.filter&#xa;def add_nested_class(value):&#xa;    if isinstance(value, dict):&#xa;        return 'class=nested'&#xa;    if isinstance(value, list) and any([isinstance(item, (list, dict)) for item in value]):&#xa;        return 'class=nested'&#xa;    return ''&#xa;&#xa;&#xa;# Bunch of stuff cloned from urlize&#xa;TRAILING_PUNCTUATION = ['.', ',', ':', ';', '.)', '""', ""']"", ""'}"", ""'""]&#xa;WRAPPING_PUNCTUATION = [('(', ')'), ('<', '>'), ('[', ']'), ('&lt;', '&gt;'),&#xa;                        ('""', '""'), (""'"", ""'"")]&#xa;word_split_re = re.compile(r'(\s+)')&#xa;simple_url_re = re.compile(r'^https?://\[?\w', re.IGNORECASE)&#xa;simple_url_2_re = re.compile(r'^www\.|^(?!http)\w[^@]+\.(com|edu|gov|int|mil|net|org)$', re.IGNORECASE)&#xa;simple_email_re = re.compile(r'^\S+@\S+\.\S+$')&#xa;&#xa;&#xa;def smart_urlquote_wrapper(matched_url):&#xa;    """"""&#xa;    Simple wrapper for smart_urlquote. ValueError(""Invalid IPv6 URL"") can&#xa;    be raised here, see issue #1386&#xa;    """"""&#xa;    try:&#xa;        return smart_urlquote(matched_url)&#xa;    except ValueError:&#xa;        return None&#xa;&#xa;&#xa;@register.filter&#xa;def urlize_quoted_links(text, trim_url_limit=None, nofollow=True, autoescape=True):&#xa;    """"""&#xa;    Converts any URLs in text into clickable links.&#xa;&#xa;    Works on http://, https://, www. links, and also on links ending in one of&#xa;    the original seven gTLDs (.com, .edu, .gov, .int, .mil, .net, and .org).&#xa;    Links can have trailing punctuation (periods, commas, close-parens) and&#xa;    leading punctuation (opening parens) and it'll still do the right thing.&#xa;&#xa;    If trim_url_limit is not None, the URLs in link text longer than this limit&#xa;    will truncated to trim_url_limit-3 characters and appended with an ellipsis.&#xa;&#xa;    If nofollow is True, the URLs in link text will get a rel=""nofollow""&#xa;    attribute.&#xa;&#xa;    If autoescape is True, the link text and URLs will get autoescaped.&#xa;    """"""&#xa;    def trim_url(x, limit=trim_url_limit):&#xa;        return limit is not None and (len(x) > limit and ('%s...' % x[:max(0, limit - 3)])) or x&#xa;&#xa;    safe_input = isinstance(text, SafeData)&#xa;    words = word_split_re.split(force_text(text))&#xa;    for i, word in enumerate(words):&#xa;        if '.' in word or '@' in word or ':' in word:&#xa;            # Deal with punctuation.&#xa;            lead, middle, trail = '', word, ''&#xa;            for punctuation in TRAILING_PUNCTUATION:&#xa;                if middle.endswith(punctuation):&#xa;                    middle = middle[:-len(punctuation)]&#xa;                    trail = punctuation + trail&#xa;            for opening, closing in WRAPPING_PUNCTUATION:&#xa;                if middle.startswith(opening):&#xa;                    middle = middle[len(opening):]&#xa;                    lead = lead + opening&#xa;                # Keep parentheses at the end only if they're balanced.&#xa;                if (&#xa;                    middle.endswith(closing) and&#xa;                    middle.count(closing) == middle.count(opening) + 1&#xa;                ):&#xa;                    middle = middle[:-len(closing)]&#xa;                    trail = closing + trail&#xa;&#xa;            # Make URL we want to point to.&#xa;            url = None&#xa;            nofollow_attr = ' rel=""nofollow""' if nofollow else ''&#xa;            if simple_url_re.match(middle):&#xa;                url = smart_urlquote_wrapper(middle)&#xa;            elif simple_url_2_re.match(middle):&#xa;                url = smart_urlquote_wrapper('http://%s' % middle)&#xa;            elif ':' not in middle and simple_email_re.match(middle):&#xa;                local, domain = middle.rsplit('@', 1)&#xa;                try:&#xa;                    domain = domain.encode('idna').decode('ascii')&#xa;                except UnicodeError:&#xa;                    continue&#xa;                url = 'mailto:%s@%s' % (local, domain)&#xa;                nofollow_attr = ''&#xa;&#xa;            # Make link.&#xa;            if url:&#xa;                trimmed = trim_url(middle)&#xa;                if autoescape and not safe_input:&#xa;                    lead, trail = escape(lead), escape(trail)&#xa;                    url, trimmed = escape(url), escape(trimmed)&#xa;                middle = '<a href=""%s""%s>%s</a>' % (url, nofollow_attr, trimmed)&#xa;                words[i] = mark_safe('%s%s%s' % (lead, middle, trail))&#xa;            else:&#xa;                if safe_input:&#xa;                    words[i] = mark_safe(word)&#xa;                elif autoescape:&#xa;                    words[i] = escape(word)&#xa;        elif safe_input:&#xa;            words[i] = mark_safe(word)&#xa;        elif autoescape:&#xa;            words[i] = escape(word)&#xa;    return ''.join(words)&#xa;&#xa;&#xa;@register.filter&#xa;def break_long_headers(header):&#xa;    """"""&#xa;    Breaks headers longer than 160 characters (~page length)&#xa;    when possible (are comma separated)&#xa;    """"""&#xa;    if len(header) > 160 and ',' in header:&#xa;        header = mark_safe('<br> ' + ', <br>'.join(header.split(',')))&#xa;    return header&#xa;"
6796492|"#!/usr/bin/env python&#xa;&#xa;## \file redirect.py&#xa;#  \brief python package for file redirection &#xa;#  \author T. Lukaczyk, F. Palacios&#xa;#  \version 5.0.0 ""Raven""&#xa;#&#xa;# SU2 Original Developers: Dr. Francisco D. Palacios.&#xa;#                          Dr. Thomas D. Economon.&#xa;#&#xa;# SU2 Developers: Prof. Juan J. Alonso's group at Stanford University.&#xa;#                 Prof. Piero Colonna's group at Delft University of Technology.&#xa;#                 Prof. Nicolas R. Gauger's group at Kaiserslautern University of Technology.&#xa;#                 Prof. Alberto Guardone's group at Polytechnic University of Milan.&#xa;#                 Prof. Rafael Palacios' group at Imperial College London.&#xa;#                 Prof. Edwin van der Weide's group at the University of Twente.&#xa;#                 Prof. Vincent Terrapon's group at the University of Liege.&#xa;#&#xa;# Copyright (C) 2012-2017 SU2, the open-source CFD code.&#xa;#&#xa;# SU2 is free software; you can redistribute it and/or&#xa;# modify it under the terms of the GNU Lesser General Public&#xa;# License as published by the Free Software Foundation; either&#xa;# version 2.1 of the License, or (at your option) any later version.&#xa;#&#xa;# SU2 is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU&#xa;# Lesser General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU Lesser General Public&#xa;# License along with SU2. If not, see <http://www.gnu.org/licenses/>.&#xa;&#xa;# ----------------------------------------------------------------------&#xa;#  Imports&#xa;# ----------------------------------------------------------------------&#xa;&#xa;import os, sys, shutil, copy, glob&#xa;from .tools import add_suffix, make_link, expand_part&#xa;&#xa;# -------------------------------------------------------------------&#xa;#  Output Redirection &#xa;# -------------------------------------------------------------------&#xa;# original source: http://stackoverflow.com/questions/6796492/python-temporarily-redirect-stdout-stderr&#xa;class output(object):&#xa;    ''' with SU2.io.redirect_output(stdout,stderr)&#xa;    &#xa;        Temporarily redirects sys.stdout and sys.stderr when used in&#xa;        a 'with' contextmanager&#xa;        &#xa;        Example:&#xa;        with SU2.io.redirect_output('stdout.txt','stderr.txt'):&#xa;            sys.stdout.write(""standard out"")&#xa;            sys.stderr.write(""stanrard error"")&#xa;            # code&#xa;        #: with output redirection&#xa;        &#xa;        Inputs:&#xa;            stdout - None, a filename, or a file stream&#xa;            stderr - None, a filename, or a file stream&#xa;        None will not redirect outptu&#xa;        &#xa;    '''&#xa;    def __init__(self, stdout=None, stderr=None):&#xa;        &#xa;        _newout = False&#xa;        _newerr = False&#xa;        &#xa;        if isinstance(stdout,str):&#xa;            stdout = open(stdout,'a')&#xa;            _newout = True            &#xa;        if isinstance(stderr,str):&#xa;            stderr = open(stderr,'a')&#xa;            _newerr = True                   &#xa;                &#xa;        self._stdout = stdout or sys.stdout&#xa;        self._stderr = stderr or sys.stderr&#xa;        self._newout = _newout&#xa;        self._newerr = _newerr&#xa;&#xa;    def __enter__(self):&#xa;        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr&#xa;        self.old_stdout.flush(); self.old_stderr.flush()&#xa;        sys.stdout, sys.stderr = self._stdout, self._stderr&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        self._stdout.flush(); self._stderr.flush()&#xa;        sys.stdout = self.old_stdout&#xa;        sys.stderr = self.old_stderr&#xa;        &#xa;        if self._newout:&#xa;            self._stdout.close()&#xa;        if self._newerr:&#xa;            self._stderr.close()           &#xa;&#xa;#: class output()&#xa;&#xa;&#xa;# -------------------------------------------------------------------&#xa;#  Folder Redirection &#xa;# -------------------------------------------------------------------&#xa;class folder(object):&#xa;    ''' with SU2.io.redirect_folder(folder,pull,link,force) as push&#xa;    &#xa;        Temporarily redirects to a working folder, pulling &#xa;        and pushing needed files&#xa;        &#xa;        Example:&#xa;        &#xa;        folder = 'temp'                    &#xa;        pull   = ['file1.txt','file2.txt'] &#xa;        link   = ['file3.big']             &#xa;        force  = True                      &#xa;        &#xa;        # original path&#xa;        import os&#xa;        print os.getcwd()&#xa;        &#xa;        # enter folder&#xa;        with SU2.io.redirect_folder(folder,pull,link,force) as push:&#xa;            print os.getcwd()&#xa;            # code&#xa;            push.append('file4.txt')&#xa;        #: with folder redirection&#xa;        &#xa;        # returned to original path&#xa;        print os.getcwd()&#xa;        &#xa;        Inputs:&#xa;            folder - working folder, relative or absolute&#xa;            pull   - list of files to pull (copy to working folder)&#xa;            link   - list of files to link (symbolic link in working folder)&#xa;            force  - True/False overwrite existing files in working folder&#xa;        &#xa;        Targets:&#xa;            push   - list of files to push (copy to originating path)&#xa;        &#xa;        Notes:&#xa;            push must be appended or extended, not overwritten&#xa;            links in Windows not supported, will simply copy&#xa;    '''&#xa;    &#xa;    def __init__(self, folder, pull=None, link=None, force=True ):&#xa;        ''' folder redirection initialization&#xa;            see help( folder ) for more info&#xa;        '''&#xa;        &#xa;        if pull is None: pull = []&#xa;        if link is None: link = []&#xa;        &#xa;        if not isinstance(pull,list) : pull = [pull]&#xa;        if not isinstance(link,list) : link = [link]&#xa;        &#xa;        origin = os.getcwd()&#xa;        origin = os.path.abspath(origin).rstrip('/')+'/'&#xa;        folder = os.path.abspath(folder).rstrip('/')+'/'&#xa;        &#xa;        self.origin = origin&#xa;        self.folder = folder&#xa;        self.pull   = copy.deepcopy(pull)&#xa;        self.push   = []&#xa;        self.link   = copy.deepcopy(link)&#xa;        self.force  = force&#xa;&#xa;    def __enter__(self): &#xa;        &#xa;        origin = self.origin  # absolute path&#xa;        folder = self.folder  # absolute path&#xa;        pull   = self.pull&#xa;        push   = self.push&#xa;        link   = self.link&#xa;        force  = self.force&#xa;        &#xa;        # check for no folder change&#xa;        if folder == origin:&#xa;            return []&#xa;        &#xa;        # relative folder path&#xa;        #relative = os.path.relpath(folder,origin)&#xa;        &#xa;        # check, make folder&#xa;        if not os.path.exists(folder):&#xa;            os.makedirs(folder)&#xa;        &#xa;        # copy pull files&#xa;        for name in pull:&#xa;            old_name = os.path.abspath(name)&#xa;            new_name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(folder,new_name)&#xa;            if old_name == new_name: continue&#xa;            if os.path.exists( new_name ): &#xa;                if force: os.remove( new_name )&#xa;                else: continue&#xa;            shutil.copy(old_name,new_name)&#xa;&#xa;        # make links&#xa;        for name in link:&#xa;            old_name = os.path.abspath(name)&#xa;            new_name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(folder,new_name)&#xa;            if old_name == new_name: continue&#xa;            if os.path.exists( new_name ): &#xa;                if force: os.remove( new_name )&#xa;                else: continue&#xa;            make_link(old_name,new_name)&#xa;            &#xa;        # change directory&#xa;        os.chdir(folder)&#xa;        &#xa;        # return empty list to append with files to push to super folder&#xa;        return push&#xa;&#xa;    def __exit__(self, exc_type, exc_value, traceback):&#xa;        &#xa;        origin = self.origin&#xa;        folder = self.folder&#xa;        push   = self.push&#xa;        force  = self.force&#xa;        &#xa;        # check for no folder change&#xa;        if folder == origin:&#xa;            return&#xa;        &#xa;        # move assets&#xa;        for name in push:&#xa;            &#xa;            old_name = os.path.abspath(name)&#xa;            name = os.path.split(name)[-1]&#xa;            new_name = os.path.join(origin,name)&#xa;            &#xa;            # links&#xa;            if os.path.islink(old_name):&#xa;                source = os.path.realpath(old_name)&#xa;                if source == new_name: continue&#xa;                if os.path.exists( new_name ):&#xa;                    if force: os.remove( new_name )&#xa;                    else: continue&#xa;                make_link(source,new_name)&#xa;            &#xa;            # moves&#xa;            else:&#xa;                if old_name == new_name: continue&#xa;                if os.path.exists( new_name ):&#xa;                    if force: os.remove( new_name )&#xa;                    else: continue&#xa;                shutil.move(old_name,new_name)&#xa;            &#xa;        # change directory&#xa;        os.chdir(origin)&#xa;        &#xa;#: class folder()&#xa;"
3401428|"#!/usr/bin/env python&#xa;# -*- coding: utf-8 -*-&#xa;from wsgiref.simple_server import make_server&#xa;import sys&#xa;import json&#xa;import traceback&#xa;import datetime&#xa;from multiprocessing import Process&#xa;from getopt import getopt, GetoptError&#xa;from jsonrpcbase import JSONRPCService, InvalidParamsError, KeywordError,\&#xa;    JSONRPCError, InvalidRequestError&#xa;from jsonrpcbase import ServerError as JSONServerError&#xa;from os import environ&#xa;from ConfigParser import ConfigParser&#xa;from biokbase import log&#xa;import requests as _requests&#xa;import random as _random&#xa;import os&#xa;from biokbase.RNASeq.authclient import KBaseAuth as _KBaseAuth&#xa;&#xa;DEPLOY = 'KB_DEPLOYMENT_CONFIG'&#xa;SERVICE = 'KB_SERVICE_NAME'&#xa;AUTH = 'auth-server-url'&#xa;&#xa;# Note that the error fields do not match the 2.0 JSONRPC spec&#xa;&#xa;&#xa;def get_config_file():&#xa;    return environ.get(DEPLOY, None)&#xa;&#xa;&#xa;def get_service_name():&#xa;    return environ.get(SERVICE, None)&#xa;&#xa;&#xa;def get_config():&#xa;    if not get_config_file():&#xa;        return None&#xa;    retconfig = {}&#xa;    config = ConfigParser()&#xa;    config.read(get_config_file())&#xa;    for nameval in config.items(get_service_name() or 'KBaseRNASeq'):&#xa;        retconfig[nameval[0]] = nameval[1]&#xa;    return retconfig&#xa;&#xa;config = get_config()&#xa;&#xa;from biokbase.RNASeq.KBaseRNASeqImpl import KBaseRNASeq  # @IgnorePep8&#xa;impl_KBaseRNASeq = KBaseRNASeq(config)&#xa;&#xa;&#xa;class JSONObjectEncoder(json.JSONEncoder):&#xa;&#xa;    def default(self, obj):&#xa;        if isinstance(obj, set):&#xa;            return list(obj)&#xa;        if isinstance(obj, frozenset):&#xa;            return list(obj)&#xa;        if hasattr(obj, 'toJSONable'):&#xa;            return obj.toJSONable()&#xa;        return json.JSONEncoder.default(self, obj)&#xa;&#xa;&#xa;class JSONRPCServiceCustom(JSONRPCService):&#xa;&#xa;    def call(self, ctx, jsondata):&#xa;        """"""&#xa;        Calls jsonrpc service's method and returns its return value in a JSON&#xa;        string or None if there is none.&#xa;&#xa;        Arguments:&#xa;        jsondata -- remote method call in jsonrpc format&#xa;        """"""&#xa;        result = self.call_py(ctx, jsondata)&#xa;        if result is not None:&#xa;            return json.dumps(result, cls=JSONObjectEncoder)&#xa;&#xa;        return None&#xa;&#xa;    def _call_method(self, ctx, request):&#xa;        """"""Calls given method with given params and returns it value.""""""&#xa;        method = self.method_data[request['method']]['method']&#xa;        params = request['params']&#xa;        result = None&#xa;        try:&#xa;            if isinstance(params, list):&#xa;                # Does it have enough arguments?&#xa;                if len(params) < self._man_args(method) - 1:&#xa;                    raise InvalidParamsError('not enough arguments')&#xa;                # Does it have too many arguments?&#xa;                if(not self._vargs(method) and len(params) >&#xa;                        self._max_args(method) - 1):&#xa;                    raise InvalidParamsError('too many arguments')&#xa;&#xa;                result = method(ctx, *params)&#xa;            elif isinstance(params, dict):&#xa;                # Do not accept keyword arguments if the jsonrpc version is&#xa;                # not >=1.1.&#xa;                if request['jsonrpc'] < 11:&#xa;                    raise KeywordError&#xa;&#xa;                result = method(ctx, **params)&#xa;            else:  # No params&#xa;                result = method(ctx)&#xa;        except JSONRPCError:&#xa;            raise&#xa;        except Exception as e:&#xa;            # log.exception('method %s threw an exception' % request['method'])&#xa;            # Exception was raised inside the method.&#xa;            newerr = JSONServerError()&#xa;            newerr.trace = traceback.format_exc()&#xa;            newerr.data = e.message&#xa;            raise newerr&#xa;        return result&#xa;&#xa;    def call_py(self, ctx, jsondata):&#xa;        """"""&#xa;        Calls jsonrpc service's method and returns its return value in python&#xa;        object format or None if there is none.&#xa;&#xa;        This method is same as call() except the return value is a python&#xa;        object instead of JSON string. This method is mainly only useful for&#xa;        debugging purposes.&#xa;        """"""&#xa;        rdata = jsondata&#xa;        # we already deserialize the json string earlier in the server code, no&#xa;        # need to do it again&#xa;#        try:&#xa;#            rdata = json.loads(jsondata)&#xa;#        except ValueError:&#xa;#            raise ParseError&#xa;&#xa;        # set some default values for error handling&#xa;        request = self._get_default_vals()&#xa;&#xa;        if isinstance(rdata, dict) and rdata:&#xa;            # It's a single request.&#xa;            self._fill_request(request, rdata)&#xa;            respond = self._handle_request(ctx, request)&#xa;&#xa;            # Don't respond to notifications&#xa;            if respond is None:&#xa;                return None&#xa;&#xa;            return respond&#xa;        elif isinstance(rdata, list) and rdata:&#xa;            # It's a batch.&#xa;            requests = []&#xa;            responds = []&#xa;&#xa;            for rdata_ in rdata:&#xa;                # set some default values for error handling&#xa;                request_ = self._get_default_vals()&#xa;                self._fill_request(request_, rdata_)&#xa;                requests.append(request_)&#xa;&#xa;            for request_ in requests:&#xa;                respond = self._handle_request(ctx, request_)&#xa;                # Don't respond to notifications&#xa;                if respond is not None:&#xa;                    responds.append(respond)&#xa;&#xa;            if responds:&#xa;                return responds&#xa;&#xa;            # Nothing to respond.&#xa;            return None&#xa;        else:&#xa;            # empty dict, list or wrong type&#xa;            raise InvalidRequestError&#xa;&#xa;    def _handle_request(self, ctx, request):&#xa;        """"""Handles given request and returns its response.""""""&#xa;        if self.method_data[request['method']].has_key('types'): # @IgnorePep8&#xa;            self._validate_params_types(request['method'], request['params'])&#xa;&#xa;        result = self._call_method(ctx, request)&#xa;&#xa;        # Do not respond to notifications.&#xa;        if request['id'] is None:&#xa;            return None&#xa;&#xa;        respond = {}&#xa;        self._fill_ver(request['jsonrpc'], respond)&#xa;        respond['result'] = result&#xa;        respond['id'] = request['id']&#xa;&#xa;        return respond&#xa;&#xa;&#xa;class MethodContext(dict):&#xa;&#xa;    def __init__(self, logger):&#xa;        self['client_ip'] = None&#xa;        self['user_id'] = None&#xa;        self['authenticated'] = None&#xa;        self['token'] = None&#xa;        self['module'] = None&#xa;        self['method'] = None&#xa;        self['call_id'] = None&#xa;        self['rpc_context'] = None&#xa;        self['provenance'] = None&#xa;        self._debug_levels = set([7, 8, 9, 'DEBUG', 'DEBUG2', 'DEBUG3'])&#xa;        self._logger = logger&#xa;&#xa;    def log_err(self, message):&#xa;        self._log(log.ERR, message)&#xa;&#xa;    def log_info(self, message):&#xa;        self._log(log.INFO, message)&#xa;&#xa;    def log_debug(self, message, level=1):&#xa;        if level in self._debug_levels:&#xa;            pass&#xa;        else:&#xa;            level = int(level)&#xa;            if level < 1 or level > 3:&#xa;                raise ValueError(""Illegal log level: "" + str(level))&#xa;            level = level + 6&#xa;        self._log(level, message)&#xa;&#xa;    def set_log_level(self, level):&#xa;        self._logger.set_log_level(level)&#xa;&#xa;    def get_log_level(self):&#xa;        return self._logger.get_log_level()&#xa;&#xa;    def clear_log_level(self):&#xa;        self._logger.clear_user_log_level()&#xa;&#xa;    def _log(self, level, message):&#xa;        self._logger.log_message(level, message, self['client_ip'],&#xa;                                 self['user_id'], self['module'],&#xa;                                 self['method'], self['call_id'])&#xa;&#xa;    def provenance(self):&#xa;        callbackURL = os.environ.get('SDK_CALLBACK_URL')&#xa;        if callbackURL:&#xa;            # OK, there's a callback server from which we can get provenance&#xa;            arg_hash = {'method': 'CallbackServer.get_provenance',&#xa;                        'params': [],&#xa;                        'version': '1.1',&#xa;                        'id': str(_random.random())[2:]&#xa;                        }&#xa;            body = json.dumps(arg_hash)&#xa;            response = _requests.post(callbackURL, data=body,&#xa;                                      timeout=60)&#xa;            response.encoding = 'utf-8'&#xa;            if response.status_code == 500:&#xa;                if ('content-type' in response.headers and&#xa;                        response.headers['content-type'] ==&#xa;                        'application/json'):&#xa;                    err = response.json()&#xa;                    if 'error' in err:&#xa;                        raise ServerError(**err['error'])&#xa;                    else:&#xa;                        raise ServerError('Unknown', 0, response.text)&#xa;                else:&#xa;                    raise ServerError('Unknown', 0, response.text)&#xa;            if not response.ok:&#xa;                response.raise_for_status()&#xa;            resp = response.json()&#xa;            if 'result' not in resp:&#xa;                raise ServerError('Unknown', 0,&#xa;                                  'An unknown server error occurred')&#xa;            return resp['result'][0]&#xa;        else:&#xa;            return self.get('provenance')&#xa;&#xa;&#xa;class ServerError(Exception):&#xa;    '''&#xa;    The call returned an error. Fields:&#xa;    name - the name of the error.&#xa;    code - the error code.&#xa;    message - a human readable error message.&#xa;    data - the server side stacktrace.&#xa;    '''&#xa;&#xa;    def __init__(self, name, code, message, data=None, error=None):&#xa;        super(Exception, self).__init__(message)&#xa;        self.name = name&#xa;        self.code = code&#xa;        self.message = message if message else ''&#xa;        self.data = data or error or ''&#xa;        # data = JSON RPC 2.0, error = 1.1&#xa;&#xa;    def __str__(self):&#xa;        return self.name + ': ' + str(self.code) + '. ' + self.message + \&#xa;            '\n' + self.data&#xa;&#xa;&#xa;def getIPAddress(environ):&#xa;    xFF = environ.get('HTTP_X_FORWARDED_FOR')&#xa;    realIP = environ.get('HTTP_X_REAL_IP')&#xa;    trustXHeaders = config is None or \&#xa;        config.get('dont_trust_x_ip_headers') != 'true'&#xa;&#xa;    if (trustXHeaders):&#xa;        if (xFF):&#xa;            return xFF.split(',')[0].strip()&#xa;        if (realIP):&#xa;            return realIP.strip()&#xa;    return environ.get('REMOTE_ADDR')&#xa;&#xa;&#xa;class Application(object):&#xa;    # Wrap the wsgi handler in a class definition so that we can&#xa;    # do some initialization and avoid regenerating stuff over&#xa;    # and over&#xa;&#xa;    def logcallback(self):&#xa;        self.serverlog.set_log_file(self.userlog.get_log_file())&#xa;&#xa;    def log(self, level, context, message):&#xa;        self.serverlog.log_message(level, message, context['client_ip'],&#xa;                                   context['user_id'], context['module'],&#xa;                                   context['method'], context['call_id'])&#xa;&#xa;    def __init__(self):&#xa;        submod = get_service_name() or 'KBaseRNASeq'&#xa;        self.userlog = log.log(&#xa;            submod, ip_address=True, authuser=True, module=True, method=True,&#xa;            call_id=True, changecallback=self.logcallback,&#xa;            config=get_config_file())&#xa;        self.serverlog = log.log(&#xa;            submod, ip_address=True, authuser=True, module=True, method=True,&#xa;            call_id=True, logfile=self.userlog.get_log_file())&#xa;        self.serverlog.set_log_level(6)&#xa;        self.rpc_service = JSONRPCServiceCustom()&#xa;        self.method_authentication = dict()&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CreateRNASeqSampleSet,&#xa;                             name='KBaseRNASeq.CreateRNASeqSampleSet',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CreateRNASeqSampleSet'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.BuildBowtie2Index,&#xa;                             name='KBaseRNASeq.BuildBowtie2Index',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.BuildBowtie2Index'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.GetFeaturesToGTF,&#xa;                             name='KBaseRNASeq.GetFeaturesToGTF',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.GetFeaturesToGTF'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Bowtie2Call,&#xa;                             name='KBaseRNASeq.Bowtie2Call',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Bowtie2Call'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Hisat2Call,&#xa;                             name='KBaseRNASeq.Hisat2Call',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Hisat2Call'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.TophatCall,&#xa;                             name='KBaseRNASeq.TophatCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.TophatCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.StringTieCall,&#xa;                             name='KBaseRNASeq.StringTieCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.StringTieCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.Hisat2StringTieCall,&#xa;                             name='KBaseRNASeq.Hisat2StringTieCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.Hisat2StringTieCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CufflinksCall,&#xa;                             name='KBaseRNASeq.CufflinksCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CufflinksCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.CuffdiffCall,&#xa;                             name='KBaseRNASeq.CuffdiffCall',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.CuffdiffCall'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.DiffExpCallforBallgown,&#xa;                             name='KBaseRNASeq.DiffExpCallforBallgown',&#xa;                             types=[dict])&#xa;        self.method_authentication['KBaseRNASeq.DiffExpCallforBallgown'] = 'required'&#xa;        self.rpc_service.add(impl_KBaseRNASeq.status,&#xa;                             name='KBaseRNASeq.status',&#xa;                             types=[dict])&#xa;        authurl = config.get(AUTH) if config else None&#xa;        self.auth_client = _KBaseAuth(authurl)&#xa;&#xa;    def __call__(self, environ, start_response):&#xa;        # Context object, equivalent to the perl impl CallContext&#xa;        ctx = MethodContext(self.userlog)&#xa;        ctx['client_ip'] = getIPAddress(environ)&#xa;        status = '500 Internal Server Error'&#xa;&#xa;        try:&#xa;            body_size = int(environ.get('CONTENT_LENGTH', 0))&#xa;        except (ValueError):&#xa;            body_size = 0&#xa;        if environ['REQUEST_METHOD'] == 'OPTIONS':&#xa;            # we basically do nothing and just return headers&#xa;            status = '200 OK'&#xa;            rpc_result = """"&#xa;        else:&#xa;            request_body = environ['wsgi.input'].read(body_size)&#xa;            try:&#xa;                req = json.loads(request_body)&#xa;            except ValueError as ve:&#xa;                err = {'error': {'code': -32700,&#xa;                                 'name': ""Parse error"",&#xa;                                 'message': str(ve),&#xa;                                 }&#xa;                       }&#xa;                rpc_result = self.process_error(err, ctx, {'version': '1.1'})&#xa;            else:&#xa;                ctx['module'], ctx['method'] = req['method'].split('.')&#xa;                ctx['call_id'] = req['id']&#xa;                ctx['rpc_context'] = {&#xa;                    'call_stack': [{'time': self.now_in_utc(),&#xa;                                    'method': req['method']}&#xa;                                   ]&#xa;                }&#xa;                prov_action = {'service': ctx['module'],&#xa;                               'method': ctx['method'],&#xa;                               'method_params': req['params']&#xa;                               }&#xa;                ctx['provenance'] = [prov_action]&#xa;                try:&#xa;                    token = environ.get('HTTP_AUTHORIZATION')&#xa;                    # parse out the method being requested and check if it&#xa;                    # has an authentication requirement&#xa;                    method_name = req['method']&#xa;                    auth_req = self.method_authentication.get(&#xa;                        method_name, 'none')&#xa;                    if auth_req != 'none':&#xa;                        if token is None and auth_req == 'required':&#xa;                            err = JSONServerError()&#xa;                            err.data = (&#xa;                                'Authentication required for KBaseRNASeq ' +&#xa;                                'but no authentication header was passed')&#xa;                            raise err&#xa;                        elif token is None and auth_req == 'optional':&#xa;                            pass&#xa;                        else:&#xa;                            try:&#xa;                                user = self.auth_client.get_user(token)&#xa;                                ctx['user_id'] = user&#xa;                                ctx['authenticated'] = 1&#xa;                                ctx['token'] = token&#xa;                            except Exception, e:&#xa;                                if auth_req == 'required':&#xa;                                    err = JSONServerError()&#xa;                                    err.data = \&#xa;                                        ""Token validation failed: %s"" % e&#xa;                                    raise err&#xa;                    if (environ.get('HTTP_X_FORWARDED_FOR')):&#xa;                        self.log(log.INFO, ctx, 'X-Forwarded-For: ' +&#xa;                                 environ.get('HTTP_X_FORWARDED_FOR'))&#xa;                    self.log(log.INFO, ctx, 'start method')&#xa;                    rpc_result = self.rpc_service.call(ctx, req)&#xa;                    self.log(log.INFO, ctx, 'end method')&#xa;                    status = '200 OK'&#xa;                except JSONRPCError as jre:&#xa;                    err = {'error': {'code': jre.code,&#xa;                                     'name': jre.message,&#xa;                                     'message': jre.data&#xa;                                     }&#xa;                           }&#xa;                    trace = jre.trace if hasattr(jre, 'trace') else None&#xa;                    rpc_result = self.process_error(err, ctx, req, trace)&#xa;                except Exception, e:&#xa;                    err = {'error': {'code': 0,&#xa;                                     'name': 'Unexpected Server Error',&#xa;                                     'message': 'An unexpected server error ' +&#xa;                                                'occurred',&#xa;                                     }&#xa;                           }&#xa;                    rpc_result = self.process_error(err, ctx, req,&#xa;                                                    traceback.format_exc())&#xa;&#xa;        # print 'The request method was %s\n' % environ['REQUEST_METHOD']&#xa;        # print 'The environment dictionary is:\n%s\n' % pprint.pformat(environ) @IgnorePep8&#xa;        # print 'The request body was: %s' % request_body&#xa;        # print 'The result from the method call is:\n%s\n' % \&#xa;        #    pprint.pformat(rpc_result)&#xa;&#xa;        if rpc_result:&#xa;            response_body = rpc_result&#xa;        else:&#xa;            response_body = ''&#xa;&#xa;        response_headers = [&#xa;            ('Access-Control-Allow-Origin', '*'),&#xa;            ('Access-Control-Allow-Headers', environ.get(&#xa;                'HTTP_ACCESS_CONTROL_REQUEST_HEADERS', 'authorization')),&#xa;            ('content-type', 'application/json'),&#xa;            ('content-length', str(len(response_body)))]&#xa;        start_response(status, response_headers)&#xa;        return [response_body]&#xa;&#xa;    def process_error(self, error, context, request, trace=None):&#xa;        if trace:&#xa;            self.log(log.ERR, context, trace.split('\n')[0:-1])&#xa;        if 'id' in request:&#xa;            error['id'] = request['id']&#xa;        if 'version' in request:&#xa;            error['version'] = request['version']&#xa;            e = error['error'].get('error')&#xa;            if not e:&#xa;                error['error']['error'] = trace&#xa;        elif 'jsonrpc' in request:&#xa;            error['jsonrpc'] = request['jsonrpc']&#xa;            error['error']['data'] = trace&#xa;        else:&#xa;            error['version'] = '1.0'&#xa;            error['error']['error'] = trace&#xa;        return json.dumps(error)&#xa;&#xa;    def now_in_utc(self):&#xa;        # Taken from http://stackoverflow.com/questions/3401428/how-to-get-an-isoformat-datetime-string-including-the-default-timezone @IgnorePep8&#xa;        dtnow = datetime.datetime.now()&#xa;        dtutcnow = datetime.datetime.utcnow()&#xa;        delta = dtnow - dtutcnow&#xa;        hh, mm = divmod((delta.days * 24*60*60 + delta.seconds + 30) // 60, 60)&#xa;        return ""%s%+02d:%02d"" % (dtnow.isoformat(), hh, mm)&#xa;&#xa;application = Application()&#xa;&#xa;# This is the uwsgi application dictionary. On startup uwsgi will look&#xa;# for this dict and pull its configuration from here.&#xa;# This simply lists where to ""mount"" the application in the URL path&#xa;#&#xa;# This uwsgi module ""magically"" appears when running the app within&#xa;# uwsgi and is not available otherwise, so wrap an exception handler&#xa;# around it&#xa;#&#xa;# To run this server in uwsgi with 4 workers listening on port 9999 use:&#xa;# uwsgi -M -p 4 --http :9999 --wsgi-file _this_file_&#xa;# To run a using the single threaded python BaseHTTP service&#xa;# listening on port 9999 by default execute this file&#xa;#&#xa;try:&#xa;    import uwsgi&#xa;# Before we do anything with the application, see if the&#xa;# configs specify patching all std routines to be asynch&#xa;# *ONLY* use this if you are going to wrap the service in&#xa;# a wsgi container that has enabled gevent, such as&#xa;# uwsgi with the --gevent option&#xa;    if config is not None and config.get('gevent_monkeypatch_all', False):&#xa;        print ""Monkeypatching std libraries for async""&#xa;        from gevent import monkey&#xa;        monkey.patch_all()&#xa;    uwsgi.applications = {&#xa;        '': application&#xa;        }&#xa;except ImportError:&#xa;    # Not available outside of wsgi, ignore&#xa;    pass&#xa;&#xa;_proc = None&#xa;&#xa;&#xa;def start_server(host='localhost', port=0, newprocess=False):&#xa;    '''&#xa;    By default, will start the server on localhost on a system assigned port&#xa;    in the main thread. Excecution of the main thread will stay in the server&#xa;    main loop until interrupted. To run the server in a separate process, and&#xa;    thus allow the stop_server method to be called, set newprocess = True. This&#xa;    will also allow returning of the port number.'''&#xa;&#xa;    global _proc&#xa;    if _proc:&#xa;        raise RuntimeError('server is already running')&#xa;    httpd = make_server(host, port, application)&#xa;    port = httpd.server_address[1]&#xa;    print ""Listening on port %s"" % port&#xa;    if newprocess:&#xa;        _proc = Process(target=httpd.serve_forever)&#xa;        _proc.daemon = True&#xa;        _proc.start()&#xa;    else:&#xa;        httpd.serve_forever()&#xa;    return port&#xa;&#xa;&#xa;def stop_server():&#xa;    global _proc&#xa;    _proc.terminate()&#xa;    _proc = None&#xa;&#xa;&#xa;def process_async_cli(input_file_path, output_file_path, token):&#xa;    exit_code = 0&#xa;    with open(input_file_path) as data_file:&#xa;        req = json.load(data_file)&#xa;    if 'version' not in req:&#xa;        req['version'] = '1.1'&#xa;    if 'id' not in req:&#xa;        req['id'] = str(_random.random())[2:]&#xa;    ctx = MethodContext(application.userlog)&#xa;    if token:&#xa;        user = application.auth_client.get_user(token)&#xa;        ctx['user_id'] = user&#xa;        ctx['authenticated'] = 1&#xa;        ctx['token'] = token&#xa;    if 'context' in req:&#xa;        ctx['rpc_context'] = req['context']&#xa;    ctx['CLI'] = 1&#xa;    ctx['module'], ctx['method'] = req['method'].split('.')&#xa;    prov_action = {'service': ctx['module'], 'method': ctx['method'],&#xa;                   'method_params': req['params']}&#xa;    ctx['provenance'] = [prov_action]&#xa;    resp = None&#xa;    try:&#xa;        resp = application.rpc_service.call_py(ctx, req)&#xa;    except JSONRPCError as jre:&#xa;        trace = jre.trace if hasattr(jre, 'trace') else None&#xa;        resp = {'id': req['id'],&#xa;                'version': req['version'],&#xa;                'error': {'code': jre.code,&#xa;                          'name': jre.message,&#xa;                          'message': jre.data,&#xa;                          'error': trace}&#xa;                }&#xa;    except Exception:&#xa;        trace = traceback.format_exc()&#xa;        resp = {'id': req['id'],&#xa;                'version': req['version'],&#xa;                'error': {'code': 0,&#xa;                          'name': 'Unexpected Server Error',&#xa;                          'message': 'An unexpected server error occurred',&#xa;                          'error': trace}&#xa;                }&#xa;    if 'error' in resp:&#xa;        exit_code = 500&#xa;    with open(output_file_path, ""w"") as f:&#xa;        f.write(json.dumps(resp, cls=JSONObjectEncoder))&#xa;    return exit_code&#xa;&#xa;if __name__ == ""__main__"":&#xa;    if (len(sys.argv) >= 3 and len(sys.argv) <= 4 and&#xa;            os.path.isfile(sys.argv[1])):&#xa;        token = None&#xa;        if len(sys.argv) == 4:&#xa;            if os.path.isfile(sys.argv[3]):&#xa;                with open(sys.argv[3]) as token_file:&#xa;                    token = token_file.read()&#xa;            else:&#xa;                token = sys.argv[3]&#xa;        sys.exit(process_async_cli(sys.argv[1], sys.argv[2], token))&#xa;    try:&#xa;        opts, args = getopt(sys.argv[1:], """", [""port="", ""host=""])&#xa;    except GetoptError as err:&#xa;        # print help information and exit:&#xa;        print str(err)  # will print something like ""option -a not recognized""&#xa;        sys.exit(2)&#xa;    port = 9999&#xa;    host = 'localhost'&#xa;    for o, a in opts:&#xa;        if o == '--port':&#xa;            port = int(a)&#xa;        elif o == '--host':&#xa;            host = a&#xa;            print ""Host set to %s"" % host&#xa;        else:&#xa;            assert False, ""unhandled option""&#xa;&#xa;    start_server(host=host, port=port)&#xa;#    print ""Listening on port %s"" % port&#xa;#    httpd = make_server( host, port, application)&#xa;#&#xa;#    httpd.serve_forever()&#xa;"
12816941|"# -*- coding: utf-8 -*-&#xa;# pylint: disable=E1103,C0103&#xa;""""""&#xa;    Copyright 2013-2014 Olivier Corts <oc@1flow.io>&#xa;&#xa;    This file is part of the 1flow project.&#xa;&#xa;    1flow is free software: you can redistribute it and/or modify&#xa;    it under the terms of the GNU Affero General Public License as&#xa;    published by the Free Software Foundation, either version 3 of&#xa;    the License, or (at your option) any later version.&#xa;&#xa;    1flow is distributed in the hope that it will be useful,&#xa;    but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;    GNU Affero General Public License for more details.&#xa;&#xa;    You should have received a copy of the GNU Affero General Public&#xa;    License along with 1flow.  If not, see http://www.gnu.org/licenses/&#xa;&#xa;""""""&#xa;&#xa;import logging&#xa;&#xa;from django.core import mail&#xa;#from django.conf import settings&#xa;from django.test import TransactionTestCase&#xa;from django.test.client import Client&#xa;from django.test.utils import override_settings&#xa;from django.core.urlresolvers import reverse&#xa;from django.utils.translation import activate&#xa;&#xa;&#xa;LOGGER = logging.getLogger(__file__)&#xa;&#xa;&#xa;#http://stackoverflow.com/questions/12816941/unit-testing-with-django-pipeline&#xa;@override_settings(STATICFILES_STORAGE=&#xa;                   'pipeline.storage.NonPackagingPipelineStorage',&#xa;                   CELERY_EAGER_PROPAGATES_EXCEPTIONS=True,&#xa;                   CELERY_ALWAYS_EAGER=True,&#xa;                   BROKER_BACKEND='memory',)&#xa;class LandingTests(TransactionTestCase):&#xa;&#xa;    fixtures = ['base_2013-05-14_final-before-beta-opening',&#xa;                'landing_2013-05-14_final-before-beta-opening']&#xa;&#xa;    def setUp(self):&#xa;        self.client = Client()&#xa;        self.http_headers = {&#xa;            #""HTTP_ACCEPT_LANGUAGE"": ""fr-FR,fr;q=0.8,en-US;q=0.5,en;q=0.3"",&#xa;            ""HTTP_ACCEPT_LANGUAGE"": ""fr"",&#xa;            ""HTTP_USER_AGENT"": ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.8; ""&#xa;            ""rv:21.0) Gecko/20100101 Firefox/21.0""}&#xa;        self.test_email = 'test-ocE3f6VQqFaaAZ@1flow.io'&#xa;&#xa;    def test_no_empty_mail(self):&#xa;        """""" """"""&#xa;&#xa;        response = self.client.post(&#xa;            reverse('landing_home'), {'email': ''},&#xa;            follow=True&#xa;        )&#xa;        self.assertEqual(response.status_code, 200)&#xa;        self.assertContains(response, u'This field is required')&#xa;&#xa;    def test_request_invite_nolang(self):&#xa;        """""" This should send a mail. """"""&#xa;&#xa;        response = self.client.post(&#xa;            reverse('landing_home'), {'email': self.test_email},&#xa;            follow=True&#xa;        )&#xa;        self.assertEqual(response.status_code, 200)&#xa;&#xa;        self.assertEqual(mail.outbox[0].subject,&#xa;                         'Your boarding card for the 1flow flight')&#xa;        # TODO: assertTrue( """" in mail.outbox[0].body)&#xa;&#xa;    def test_request_invite_then_unsubscribe(self):&#xa;        """""" This should send a mail, and we should able to unsubscribe. """"""&#xa;&#xa;        response = self.client.post(&#xa;            reverse('landing_home'), {'email': self.test_email},&#xa;            follow=True&#xa;        )&#xa;        self.assertEqual(response.status_code, 200)&#xa;&#xa;        sent_email = mail.outbox[0]&#xa;&#xa;        self.assertEqual(sent_email.subject,&#xa;                         'Your boarding card for the 1flow flight')&#xa;        # TODO: assertTrue( """" in mail.outbox[0].body)&#xa;&#xa;        # TODO: follow unsubscribe link and&#xa;        # check in the DB that email_announcements are False&#xa;&#xa;    def test_request_invite_lang_fr(self):&#xa;        """""" This should send a mail in French. """"""&#xa;&#xa;        # cf. https://code.djangoproject.com/ticket/15143&#xa;&#xa;        # THIS works, but it's not what we want. Bummer!&#xa;        activate('fr')&#xa;&#xa;        # THIS doesn't work, but I would have liked if it did.&#xa;        #&#xa;        # response = self.client.post(reverse('set_language'),&#xa;        #                             data={'language': 'fr'},&#xa;        #                             follow=True)&#xa;        # self.assertEqual(response.status_code, 200)&#xa;&#xa;        # THIS doesn't work either, but I would have liked if it did.&#xa;        #self.client.cookies.load({settings.LANGUAGE_COOKIE_NAME: 'fr'})&#xa;&#xa;        response = self.client.post(&#xa;            reverse('landing_home'), {'email': self.test_email},&#xa;            follow=True)&#xa;&#xa;        # THIS doesn't work neither, but I would have liked if it did.&#xa;        # , **self.http_headers)&#xa;        self.assertEqual(response.status_code, 200)&#xa;&#xa;        self.assertEqual(mail.outbox[0].subject,&#xa;                         ""Votre carte d'embarquement pour le vol 1flow"")&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = ln -f ""$<"" ""$@"" 2>/dev/null || (rm -rf ""$@"" && cp -af ""$<"" ""$@"")&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;    if (self.type == 'shared_library' and&#xa;        (self.flavor != 'mac' or self.toolset != 'target')):&#xa;      # Install all shared libs into a common directory (per toolset) for&#xa;      # convenient access with LD_LIBRARY_PATH.&#xa;      return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host',), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
6527633|"#&#xa;# Licensed to the Apache Software Foundation (ASF) under one or more&#xa;# contributor license agreements.  See the NOTICE file distributed with&#xa;# this work for additional information regarding copyright ownership.&#xa;# The ASF licenses this file to You under the Apache License, Version 2.0&#xa;# (the ""License""); you may not use this file except in compliance with&#xa;# the License.  You may obtain a copy of the License at&#xa;#&#xa;#    http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;#&#xa;&#xa;""""""&#xa;PySpark is the Python API for Spark.&#xa;&#xa;Public classes:&#xa;&#xa;  - :class:`SparkContext`:&#xa;      Main entry point for Spark functionality.&#xa;  - :class:`RDD`:&#xa;      A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.&#xa;  - :class:`Broadcast`:&#xa;      A broadcast variable that gets reused across tasks.&#xa;  - :class:`Accumulator`:&#xa;      An ""add-only"" shared variable that tasks can only add values to.&#xa;  - :class:`SparkConf`:&#xa;      For configuring Spark.&#xa;  - :class:`SparkFiles`:&#xa;      Access files shipped with jobs.&#xa;  - :class:`StorageLevel`:&#xa;      Finer-grained cache persistence levels.&#xa;  - :class:`TaskContext`:&#xa;      Information about the current running task, avaialble on the workers and experimental.&#xa;&#xa;""""""&#xa;&#xa;from functools import wraps&#xa;import types&#xa;&#xa;from pyspark.conf import SparkConf&#xa;from pyspark.context import SparkContext&#xa;from pyspark.rdd import RDD&#xa;from pyspark.files import SparkFiles&#xa;from pyspark.storagelevel import StorageLevel&#xa;from pyspark.accumulators import Accumulator, AccumulatorParam&#xa;from pyspark.broadcast import Broadcast&#xa;from pyspark.serializers import MarshalSerializer, PickleSerializer&#xa;from pyspark.status import *&#xa;from pyspark.taskcontext import TaskContext&#xa;from pyspark.profiler import Profiler, BasicProfiler&#xa;from pyspark.version import __version__&#xa;&#xa;&#xa;def since(version):&#xa;    """"""&#xa;    A decorator that annotates a function to append the version of Spark the function was added.&#xa;    """"""&#xa;    import re&#xa;    indent_p = re.compile(r'\n( +)')&#xa;&#xa;    def deco(f):&#xa;        indents = indent_p.findall(f.__doc__)&#xa;        indent = ' ' * (min(len(m) for m in indents) if indents else 0)&#xa;        f.__doc__ = f.__doc__.rstrip() + ""\n\n%s.. versionadded:: %s"" % (indent, version)&#xa;        return f&#xa;    return deco&#xa;&#xa;&#xa;def copy_func(f, name=None, sinceversion=None, doc=None):&#xa;    """"""&#xa;    Returns a function with same code, globals, defaults, closure, and&#xa;    name (or provide a new name).&#xa;    """"""&#xa;    # See&#xa;    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python&#xa;    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,&#xa;                            f.__closure__)&#xa;    # in case f was given attrs (note this dict is a shallow copy):&#xa;    fn.__dict__.update(f.__dict__)&#xa;    if doc is not None:&#xa;        fn.__doc__ = doc&#xa;    if sinceversion is not None:&#xa;        fn = since(sinceversion)(fn)&#xa;    return fn&#xa;&#xa;&#xa;def keyword_only(func):&#xa;    """"""&#xa;    A decorator that forces keyword arguments in the wrapped method&#xa;    and saves actual input keyword arguments in `_input_kwargs`.&#xa;&#xa;    .. note:: Should only be used to wrap a method where first arg is `self`&#xa;    """"""&#xa;    @wraps(func)&#xa;    def wrapper(self, *args, **kwargs):&#xa;        if len(args) > 0:&#xa;            raise TypeError(""Method %s forces keyword arguments."" % func.__name__)&#xa;        self._input_kwargs = kwargs&#xa;        return func(self, **kwargs)&#xa;    return wrapper&#xa;&#xa;&#xa;# for back compatibility&#xa;from pyspark.sql import SQLContext, HiveContext, Row&#xa;&#xa;__all__ = [&#xa;    ""SparkConf"", ""SparkContext"", ""SparkFiles"", ""RDD"", ""StorageLevel"", ""Broadcast"",&#xa;    ""Accumulator"", ""AccumulatorParam"", ""MarshalSerializer"", ""PickleSerializer"",&#xa;    ""StatusTracker"", ""SparkJobInfo"", ""SparkStageInfo"", ""Profiler"", ""BasicProfiler"", ""TaskContext"",&#xa;]&#xa;"
1010381|"""""""&#xa;Integer factorization&#xa;""""""&#xa;from __future__ import print_function, division&#xa;&#xa;import random&#xa;import math&#xa;&#xa;from .primetest import isprime&#xa;from .generate import sieve, primerange, nextprime&#xa;from sympy.core import sympify&#xa;from sympy.core.evalf import bitcount&#xa;from sympy.core.logic import fuzzy_and&#xa;from sympy.core.numbers import igcd, Rational&#xa;from sympy.core.power import integer_nthroot, Pow&#xa;from sympy.core.mul import Mul&#xa;from sympy.core.compatibility import as_int, SYMPY_INTS, range&#xa;from sympy.core.singleton import S&#xa;from sympy.core.function import Function&#xa;&#xa;small_trailing = [i and max(int(not i % 2**j) and j for j in range(1, 8))&#xa;    for i in range(256)]&#xa;&#xa;&#xa;def smoothness(n):&#xa;    """"""&#xa;    Return the B-smooth and B-power smooth values of n.&#xa;&#xa;    The smoothness of n is the largest prime factor of n; the power-&#xa;    smoothness is the largest divisor raised to its multiplicity.&#xa;&#xa;    >>> from sympy.ntheory.factor_ import smoothness&#xa;    >>> smoothness(2**7*3**2)&#xa;    (3, 128)&#xa;    >>> smoothness(2**4*13)&#xa;    (13, 16)&#xa;    >>> smoothness(2)&#xa;    (2, 2)&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint, smoothness_p&#xa;    """"""&#xa;&#xa;    if n == 1:&#xa;        return (1, 1)  # not prime, but otherwise this causes headaches&#xa;    facs = factorint(n)&#xa;    return max(facs), max(m**facs[m] for m in facs)&#xa;&#xa;&#xa;def smoothness_p(n, m=-1, power=0, visual=None):&#xa;    """"""&#xa;    Return a list of [m, (p, (M, sm(p + m), psm(p + m)))...]&#xa;    where:&#xa;&#xa;    1. p**M is the base-p divisor of n&#xa;    2. sm(p + m) is the smoothness of p + m (m = -1 by default)&#xa;    3. psm(p + m) is the power smoothness of p + m&#xa;&#xa;    The list is sorted according to smoothness (default) or by power smoothness&#xa;    if power=1.&#xa;&#xa;    The smoothness of the numbers to the left (m = -1) or right (m = 1) of a&#xa;    factor govern the results that are obtained from the p +/- 1 type factoring&#xa;    methods.&#xa;&#xa;        >>> from sympy.ntheory.factor_ import smoothness_p, factorint&#xa;        >>> smoothness_p(10431, m=1)&#xa;        (1, [(3, (2, 2, 4)), (19, (1, 5, 5)), (61, (1, 31, 31))])&#xa;        >>> smoothness_p(10431)&#xa;        (-1, [(3, (2, 2, 2)), (19, (1, 3, 9)), (61, (1, 5, 5))])&#xa;        >>> smoothness_p(10431, power=1)&#xa;        (-1, [(3, (2, 2, 2)), (61, (1, 5, 5)), (19, (1, 3, 9))])&#xa;&#xa;    If visual=True then an annotated string will be returned:&#xa;&#xa;        >>> print(smoothness_p(21477639576571, visual=1))&#xa;        p**i=4410317**1 has p-1 B=1787, B-pow=1787&#xa;        p**i=4869863**1 has p-1 B=2434931, B-pow=2434931&#xa;&#xa;    This string can also be generated directly from a factorization dictionary&#xa;    and vice versa:&#xa;&#xa;        >>> factorint(17*9)&#xa;        {3: 2, 17: 1}&#xa;        >>> smoothness_p(_)&#xa;        'p**i=3**2 has p-1 B=2, B-pow=2\\np**i=17**1 has p-1 B=2, B-pow=16'&#xa;        >>> smoothness_p(_)&#xa;        {3: 2, 17: 1}&#xa;&#xa;    The table of the output logic is:&#xa;&#xa;        ====== ====== ======= =======&#xa;        |              Visual&#xa;        ------ ----------------------&#xa;        Input  True   False   other&#xa;        ====== ====== ======= =======&#xa;        dict    str    tuple   str&#xa;        str     str    tuple   dict&#xa;        tuple   str    tuple   str&#xa;        n       str    tuple   tuple&#xa;        mul     str    tuple   tuple&#xa;        ====== ====== ======= =======&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint, smoothness&#xa;    """"""&#xa;    from sympy.utilities import flatten&#xa;&#xa;    # visual must be True, False or other (stored as None)&#xa;    if visual in (1, 0):&#xa;        visual = bool(visual)&#xa;    elif visual not in (True, False):&#xa;        visual = None&#xa;&#xa;    if type(n) is str:&#xa;        if visual:&#xa;            return n&#xa;        d = {}&#xa;        for li in n.splitlines():&#xa;            k, v = [int(i) for i in&#xa;                    li.split('has')[0].split('=')[1].split('**')]&#xa;            d[k] = v&#xa;        if visual is not True and visual is not False:&#xa;            return d&#xa;        return smoothness_p(d, visual=False)&#xa;    elif type(n) is not tuple:&#xa;        facs = factorint(n, visual=False)&#xa;&#xa;    if power:&#xa;        k = -1&#xa;    else:&#xa;        k = 1&#xa;    if type(n) is not tuple:&#xa;        rv = (m, sorted([(f,&#xa;                          tuple([M] + list(smoothness(f + m))))&#xa;                         for f, M in [i for i in facs.items()]],&#xa;                        key=lambda x: (x[1][k], x[0])))&#xa;    else:&#xa;        rv = n&#xa;&#xa;    if visual is False or (visual is not True) and (type(n) in [int, Mul]):&#xa;        return rv&#xa;    lines = []&#xa;    for dat in rv[1]:&#xa;        dat = flatten(dat)&#xa;        dat.insert(2, m)&#xa;        lines.append('p**i=%i**%i has p%+i B=%i, B-pow=%i' % tuple(dat))&#xa;    return '\n'.join(lines)&#xa;&#xa;&#xa;def trailing(n):&#xa;    """"""Count the number of trailing zero digits in the binary&#xa;    representation of n, i.e. determine the largest power of 2&#xa;    that divides n.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy import trailing&#xa;    >>> trailing(128)&#xa;    7&#xa;    >>> trailing(63)&#xa;    0&#xa;    """"""&#xa;    n = int(n)&#xa;    if not n:&#xa;        return 0&#xa;    low_byte = n & 0xff&#xa;    if low_byte:&#xa;        return small_trailing[low_byte]&#xa;&#xa;    # 2**m is quick for z up through 2**30&#xa;    z = bitcount(n) - 1&#xa;    if isinstance(z, SYMPY_INTS):&#xa;        if n == 1 << z:&#xa;            return z&#xa;&#xa;    t = 0&#xa;    p = 8&#xa;    while not n & 1:&#xa;        while not n & ((1 << p) - 1):&#xa;            n >>= p&#xa;            t += p&#xa;            p *= 2&#xa;        p //= 2&#xa;    return t&#xa;&#xa;&#xa;def multiplicity(p, n):&#xa;    """"""&#xa;    Find the greatest integer m such that p**m divides n.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory import multiplicity&#xa;    >>> from sympy.core.numbers import Rational as R&#xa;    >>> [multiplicity(5, n) for n in [8, 5, 25, 125, 250]]&#xa;    [0, 1, 2, 3, 3]&#xa;    >>> multiplicity(3, R(1, 9))&#xa;    -2&#xa;&#xa;    """"""&#xa;    try:&#xa;        p, n = as_int(p), as_int(n)&#xa;    except ValueError:&#xa;        if all(isinstance(i, (SYMPY_INTS, Rational)) for i in (p, n)):&#xa;            try:&#xa;                p = Rational(p)&#xa;                n = Rational(n)&#xa;                if p.q == 1:&#xa;                    if n.p == 1:&#xa;                        return -multiplicity(p.p, n.q)&#xa;                    return S.Zero&#xa;                elif p.p == 1:&#xa;                    return multiplicity(p.q, n.q)&#xa;                else:&#xa;                    like = min(&#xa;                        multiplicity(p.p, n.p),&#xa;                        multiplicity(p.q, n.q))&#xa;                    cross = min(&#xa;                        multiplicity(p.q, n.p),&#xa;                        multiplicity(p.p, n.q))&#xa;                    return like - cross&#xa;            except AttributeError:&#xa;                pass&#xa;        raise ValueError('expecting ints or fractions, got %s and %s' % (p, n))&#xa;&#xa;    if n == 0:&#xa;        raise ValueError('no such integer exists: multiplicity of %s is not-defined' %(n))&#xa;    if p == 2:&#xa;        return trailing(n)&#xa;    if p < 2:&#xa;        raise ValueError('p must be an integer, 2 or larger, but got %s' % p)&#xa;    if p == n:&#xa;        return 1&#xa;&#xa;    m = 0&#xa;    n, rem = divmod(n, p)&#xa;    while not rem:&#xa;        m += 1&#xa;        if m > 5:&#xa;            # The multiplicity could be very large. Better&#xa;            # to increment in powers of two&#xa;            e = 2&#xa;            while 1:&#xa;                ppow = p**e&#xa;                if ppow < n:&#xa;                    nnew, rem = divmod(n, ppow)&#xa;                    if not rem:&#xa;                        m += e&#xa;                        e *= 2&#xa;                        n = nnew&#xa;                        continue&#xa;                return m + multiplicity(p, n)&#xa;        n, rem = divmod(n, p)&#xa;    return m&#xa;&#xa;&#xa;def perfect_power(n, candidates=None, big=True, factor=True):&#xa;    """"""&#xa;    Return ``(b, e)`` such that ``n`` == ``b**e`` if ``n`` is a&#xa;    perfect power; otherwise return ``False``.&#xa;&#xa;    By default, the base is recursively decomposed and the exponents&#xa;    collected so the largest possible ``e`` is sought. If ``big=False``&#xa;    then the smallest possible ``e`` (thus prime) will be chosen.&#xa;&#xa;    If ``candidates`` for exponents are given, they are assumed to be sorted&#xa;    and the first one that is larger than the computed maximum will signal&#xa;    failure for the routine.&#xa;&#xa;    If ``factor=True`` then simultaneous factorization of n is attempted&#xa;    since finding a factor indicates the only possible root for n. This&#xa;    is True by default since only a few small factors will be tested in&#xa;    the course of searching for the perfect power.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy import perfect_power&#xa;    >>> perfect_power(16)&#xa;    (2, 4)&#xa;    >>> perfect_power(16, big = False)&#xa;    (4, 2)&#xa;    """"""&#xa;    n = int(n)&#xa;    if n < 3:&#xa;        return False&#xa;    logn = math.log(n, 2)&#xa;    max_possible = int(logn) + 2  # only check values less than this&#xa;    not_square = n % 10 in [2, 3, 7, 8]  # squares cannot end in 2, 3, 7, 8&#xa;    if not candidates:&#xa;        candidates = primerange(2 + not_square, max_possible)&#xa;&#xa;    afactor = 2 + n % 2&#xa;    for e in candidates:&#xa;        if e < 3:&#xa;            if e == 1 or e == 2 and not_square:&#xa;                continue&#xa;        if e > max_possible:&#xa;            return False&#xa;&#xa;        # see if there is a factor present&#xa;        if factor:&#xa;            if n % afactor == 0:&#xa;                # find what the potential power is&#xa;                if afactor == 2:&#xa;                    e = trailing(n)&#xa;                else:&#xa;                    e = multiplicity(afactor, n)&#xa;                # if it's a trivial power we are done&#xa;                if e == 1:&#xa;                    return False&#xa;&#xa;                # maybe the bth root of n is exact&#xa;                r, exact = integer_nthroot(n, e)&#xa;                if not exact:&#xa;                    # then remove this factor and check to see if&#xa;                    # any of e's factors are a common exponent; if&#xa;                    # not then it's not a perfect power&#xa;                    n //= afactor**e&#xa;                    m = perfect_power(n, candidates=primefactors(e), big=big)&#xa;                    if m is False:&#xa;                        return False&#xa;                    else:&#xa;                        r, m = m&#xa;                        # adjust the two exponents so the bases can&#xa;                        # be combined&#xa;                        g = igcd(m, e)&#xa;                        if g == 1:&#xa;                            return False&#xa;                        m //= g&#xa;                        e //= g&#xa;                        r, e = r**m*afactor**e, g&#xa;                if not big:&#xa;                    e0 = primefactors(e)&#xa;                    if len(e0) > 1 or e0[0] != e:&#xa;                        e0 = e0[0]&#xa;                        r, e = r**(e//e0), e0&#xa;                return r, e&#xa;            else:&#xa;                # get the next factor ready for the next pass through the loop&#xa;                afactor = nextprime(afactor)&#xa;&#xa;        # Weed out downright impossible candidates&#xa;        if logn/e < 40:&#xa;            b = 2.0**(logn/e)&#xa;            if abs(int(b + 0.5) - b) > 0.01:&#xa;                continue&#xa;&#xa;        # now see if the plausible e makes a perfect power&#xa;        r, exact = integer_nthroot(n, e)&#xa;        if exact:&#xa;            if big:&#xa;                m = perfect_power(r, big=big, factor=factor)&#xa;                if m is not False:&#xa;                    r, e = m[0], e*m[1]&#xa;            return int(r), e&#xa;    else:&#xa;        return False&#xa;&#xa;&#xa;def pollard_rho(n, s=2, a=1, retries=5, seed=1234, max_steps=None, F=None):&#xa;    r""""""&#xa;    Use Pollard's rho method to try to extract a nontrivial factor&#xa;    of ``n``. The returned factor may be a composite number. If no&#xa;    factor is found, ``None`` is returned.&#xa;&#xa;    The algorithm generates pseudo-random values of x with a generator&#xa;    function, replacing x with F(x). If F is not supplied then the&#xa;    function x**2 + ``a`` is used. The first value supplied to F(x) is ``s``.&#xa;    Upon failure (if ``retries`` is > 0) a new ``a`` and ``s`` will be&#xa;    supplied; the ``a`` will be ignored if F was supplied.&#xa;&#xa;    The sequence of numbers generated by such functions generally have a&#xa;    a lead-up to some number and then loop around back to that number and&#xa;    begin to repeat the sequence, e.g. 1, 2, 3, 4, 5, 3, 4, 5 -- this leader&#xa;    and loop look a bit like the Greek letter rho, and thus the name, 'rho'.&#xa;&#xa;    For a given function, very different leader-loop values can be obtained&#xa;    so it is a good idea to allow for retries:&#xa;&#xa;    >>> from sympy.ntheory.generate import cycle_length&#xa;    >>> n = 16843009&#xa;    >>> F = lambda x:(2048*pow(x, 2, n) + 32767) % n&#xa;    >>> for s in range(5):&#xa;    ...     print('loop length = %4i; leader length = %3i' % next(cycle_length(F, s)))&#xa;    ...&#xa;    loop length = 2489; leader length =  42&#xa;    loop length =   78; leader length = 120&#xa;    loop length = 1482; leader length =  99&#xa;    loop length = 1482; leader length = 285&#xa;    loop length = 1482; leader length = 100&#xa;&#xa;    Here is an explicit example where there is a two element leadup to&#xa;    a sequence of 3 numbers (11, 14, 4) that then repeat:&#xa;&#xa;    >>> x=2&#xa;    >>> for i in range(9):&#xa;    ...     x=(x**2+12)%17&#xa;    ...     print(x)&#xa;    ...&#xa;    16&#xa;    13&#xa;    11&#xa;    14&#xa;    4&#xa;    11&#xa;    14&#xa;    4&#xa;    11&#xa;    >>> next(cycle_length(lambda x: (x**2+12)%17, 2))&#xa;    (3, 2)&#xa;    >>> list(cycle_length(lambda x: (x**2+12)%17, 2, values=True))&#xa;    [16, 13, 11, 14, 4]&#xa;&#xa;    Instead of checking the differences of all generated values for a gcd&#xa;    with n, only the kth and 2*kth numbers are checked, e.g. 1st and 2nd,&#xa;    2nd and 4th, 3rd and 6th until it has been detected that the loop has been&#xa;    traversed. Loops may be many thousands of steps long before rho finds a&#xa;    factor or reports failure. If ``max_steps`` is specified, the iteration&#xa;    is cancelled with a failure after the specified number of steps.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy import pollard_rho&#xa;    >>> n=16843009&#xa;    >>> F=lambda x:(2048*pow(x,2,n) + 32767) % n&#xa;    >>> pollard_rho(n, F=F)&#xa;    257&#xa;&#xa;    Use the default setting with a bad value of ``a`` and no retries:&#xa;&#xa;    >>> pollard_rho(n, a=n-2, retries=0)&#xa;&#xa;    If retries is > 0 then perhaps the problem will correct itself when&#xa;    new values are generated for a:&#xa;&#xa;    >>> pollard_rho(n, a=n-2, retries=1)&#xa;    257&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    - Richard Crandall & Carl Pomerance (2005), ""Prime Numbers:&#xa;      A Computational Perspective"", Springer, 2nd edition, 229-231&#xa;&#xa;    """"""&#xa;    n = int(n)&#xa;    if n < 5:&#xa;        raise ValueError('pollard_rho should receive n > 4')&#xa;    prng = random.Random(seed + retries)&#xa;    V = s&#xa;    for i in range(retries + 1):&#xa;        U = V&#xa;        if not F:&#xa;            F = lambda x: (pow(x, 2, n) + a) % n&#xa;        j = 0&#xa;        while 1:&#xa;            if max_steps and (j > max_steps):&#xa;                break&#xa;            j += 1&#xa;            U = F(U)&#xa;            V = F(F(V))  # V is 2x further along than U&#xa;            g = igcd(U - V, n)&#xa;            if g == 1:&#xa;                continue&#xa;            if g == n:&#xa;                break&#xa;            return int(g)&#xa;        V = prng.randint(0, n - 1)&#xa;        a = prng.randint(1, n - 3)  # for x**2 + a, a%n should not be 0 or -2&#xa;        F = None&#xa;    return None&#xa;&#xa;&#xa;def pollard_pm1(n, B=10, a=2, retries=0, seed=1234):&#xa;    """"""&#xa;    Use Pollard's p-1 method to try to extract a nontrivial factor&#xa;    of ``n``. Either a divisor (perhaps composite) or ``None`` is returned.&#xa;&#xa;    The value of ``a`` is the base that is used in the test gcd(a**M - 1, n).&#xa;    The default is 2.  If ``retries`` > 0 then if no factor is found after the&#xa;    first attempt, a new ``a`` will be generated randomly (using the ``seed``)&#xa;    and the process repeated.&#xa;&#xa;    Note: the value of M is lcm(1..B) = reduce(ilcm, range(2, B + 1)).&#xa;&#xa;    A search is made for factors next to even numbers having a power smoothness&#xa;    less than ``B``. Choosing a larger B increases the likelihood of finding a&#xa;    larger factor but takes longer. Whether a factor of n is found or not&#xa;    depends on ``a`` and the power smoothness of the even mumber just less than&#xa;    the factor p (hence the name p - 1).&#xa;&#xa;    Although some discussion of what constitutes a good ``a`` some&#xa;    descriptions are hard to interpret. At the modular.math site referenced&#xa;    below it is stated that if gcd(a**M - 1, n) = N then a**M % q**r is 1&#xa;    for every prime power divisor of N. But consider the following:&#xa;&#xa;        >>> from sympy.ntheory.factor_ import smoothness_p, pollard_pm1&#xa;        >>> n=257*1009&#xa;        >>> smoothness_p(n)&#xa;        (-1, [(257, (1, 2, 256)), (1009, (1, 7, 16))])&#xa;&#xa;    So we should (and can) find a root with B=16:&#xa;&#xa;        >>> pollard_pm1(n, B=16, a=3)&#xa;        1009&#xa;&#xa;    If we attempt to increase B to 256 we find that it doesn't work:&#xa;&#xa;        >>> pollard_pm1(n, B=256)&#xa;        >>>&#xa;&#xa;    But if the value of ``a`` is changed we find that only multiples of&#xa;    257 work, e.g.:&#xa;&#xa;        >>> pollard_pm1(n, B=256, a=257)&#xa;        1009&#xa;&#xa;    Checking different ``a`` values shows that all the ones that didn't&#xa;    work had a gcd value not equal to ``n`` but equal to one of the&#xa;    factors:&#xa;&#xa;        >>> from sympy.core.numbers import ilcm, igcd&#xa;        >>> from sympy import factorint, Pow&#xa;        >>> M = 1&#xa;        >>> for i in range(2, 256):&#xa;        ...     M = ilcm(M, i)&#xa;        ...&#xa;        >>> set([igcd(pow(a, M, n) - 1, n) for a in range(2, 256) if&#xa;        ...      igcd(pow(a, M, n) - 1, n) != n])&#xa;        set([1009])&#xa;&#xa;    But does aM % d for every divisor of n give 1?&#xa;&#xa;        >>> aM = pow(255, M, n)&#xa;        >>> [(d, aM%Pow(*d.args)) for d in factorint(n, visual=True).args]&#xa;        [(257**1, 1), (1009**1, 1)]&#xa;&#xa;    No, only one of them. So perhaps the principle is that a root will&#xa;    be found for a given value of B provided that:&#xa;&#xa;    1) the power smoothness of the p - 1 value next to the root&#xa;       does not exceed B&#xa;    2) a**M % p != 1 for any of the divisors of n.&#xa;&#xa;    By trying more than one ``a`` it is possible that one of them&#xa;    will yield a factor.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    With the default smoothness bound, this number can't be cracked:&#xa;&#xa;        >>> from sympy.ntheory import pollard_pm1, primefactors&#xa;        >>> pollard_pm1(21477639576571)&#xa;&#xa;    Increasing the smoothness bound helps:&#xa;&#xa;        >>> pollard_pm1(21477639576571, B=2000)&#xa;        4410317&#xa;&#xa;    Looking at the smoothness of the factors of this number we find:&#xa;&#xa;        >>> from sympy.utilities import flatten&#xa;        >>> from sympy.ntheory.factor_ import smoothness_p, factorint&#xa;        >>> print(smoothness_p(21477639576571, visual=1))&#xa;        p**i=4410317**1 has p-1 B=1787, B-pow=1787&#xa;        p**i=4869863**1 has p-1 B=2434931, B-pow=2434931&#xa;&#xa;    The B and B-pow are the same for the p - 1 factorizations of the divisors&#xa;    because those factorizations had a very large prime factor:&#xa;&#xa;        >>> factorint(4410317 - 1)&#xa;        {2: 2, 617: 1, 1787: 1}&#xa;        >>> factorint(4869863-1)&#xa;        {2: 1, 2434931: 1}&#xa;&#xa;    Note that until B reaches the B-pow value of 1787, the number is not cracked;&#xa;&#xa;        >>> pollard_pm1(21477639576571, B=1786)&#xa;        >>> pollard_pm1(21477639576571, B=1787)&#xa;        4410317&#xa;&#xa;    The B value has to do with the factors of the number next to the divisor,&#xa;    not the divisors themselves. A worst case scenario is that the number next&#xa;    to the factor p has a large prime divisisor or is a perfect power. If these&#xa;    conditions apply then the power-smoothness will be about p/2 or p. The more&#xa;    realistic is that there will be a large prime factor next to p requiring&#xa;    a B value on the order of p/2. Although primes may have been searched for&#xa;    up to this level, the p/2 is a factor of p - 1, something that we don't&#xa;    know. The modular.math reference below states that 15% of numbers in the&#xa;    range of 10**15 to 15**15 + 10**4 are 10**6 power smooth so a B of 10**6&#xa;    will fail 85% of the time in that range. From 10**8 to 10**8 + 10**3 the&#xa;    percentages are nearly reversed...but in that range the simple trial&#xa;    division is quite fast.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    - Richard Crandall & Carl Pomerance (2005), ""Prime Numbers:&#xa;      A Computational Perspective"", Springer, 2nd edition, 236-238&#xa;    - http://modular.math.washington.edu/edu/2007/spring/ent/ent-html/node81.html&#xa;    - http://www.cs.toronto.edu/~yuvalf/Factorization.pdf&#xa;    """"""&#xa;&#xa;    n = int(n)&#xa;    if n < 4 or B < 3:&#xa;        raise ValueError('pollard_pm1 should receive n > 3 and B > 2')&#xa;    prng = random.Random(seed + B)&#xa;&#xa;    # computing a**lcm(1,2,3,..B) % n for B > 2&#xa;    # it looks weird, but it's right: primes run [2, B]&#xa;    # and the answer's not right until the loop is done.&#xa;    for i in range(retries + 1):&#xa;        aM = a&#xa;        for p in sieve.primerange(2, B + 1):&#xa;            e = int(math.log(B, p))&#xa;            aM = pow(aM, pow(p, e), n)&#xa;        g = igcd(aM - 1, n)&#xa;        if 1 < g < n:&#xa;            return int(g)&#xa;&#xa;        # get a new a:&#xa;        # since the exponent, lcm(1..B), is even, if we allow 'a' to be 'n-1'&#xa;        # then (n - 1)**even % n will be 1 which will give a g of 0 and 1 will&#xa;        # give a zero, too, so we set the range as [2, n-2]. Some references&#xa;        # say 'a' should be coprime to n, but either will detect factors.&#xa;        a = prng.randint(2, n - 2)&#xa;&#xa;&#xa;def _trial(factors, n, candidates, verbose=False):&#xa;    """"""&#xa;    Helper function for integer factorization. Trial factors ``n`&#xa;    against all integers given in the sequence ``candidates``&#xa;    and updates the dict ``factors`` in-place. Returns the reduced&#xa;    value of ``n`` and a flag indicating whether any factors were found.&#xa;    """"""&#xa;    if verbose:&#xa;        factors0 = list(factors.keys())&#xa;    nfactors = len(factors)&#xa;    for d in candidates:&#xa;        if n % d == 0:&#xa;            m = multiplicity(d, n)&#xa;            n //= d**m&#xa;            factors[d] = m&#xa;    if verbose:&#xa;        for k in sorted(set(factors).difference(set(factors0))):&#xa;            print(factor_msg % (k, factors[k]))&#xa;    return int(n), len(factors) != nfactors&#xa;&#xa;&#xa;def _check_termination(factors, n, limitp1, use_trial, use_rho, use_pm1,&#xa;                       verbose):&#xa;    """"""&#xa;    Helper function for integer factorization. Checks if ``n``&#xa;    is a prime or a perfect power, and in those cases updates&#xa;    the factorization and raises ``StopIteration``.&#xa;    """"""&#xa;&#xa;    if verbose:&#xa;        print('Check for termination')&#xa;&#xa;    # since we've already been factoring there is no need to do&#xa;    # simultaneous factoring with the power check&#xa;    p = perfect_power(n, factor=False)&#xa;    if p is not False:&#xa;        base, exp = p&#xa;        if limitp1:&#xa;            limit = limitp1 - 1&#xa;        else:&#xa;            limit = limitp1&#xa;        facs = factorint(base, limit, use_trial, use_rho, use_pm1,&#xa;                         verbose=False)&#xa;        for b, e in facs.items():&#xa;            if verbose:&#xa;                print(factor_msg % (b, e))&#xa;            factors[b] = exp*e&#xa;        raise StopIteration&#xa;&#xa;    if isprime(n):&#xa;        factors[int(n)] = 1&#xa;        raise StopIteration&#xa;&#xa;    if n == 1:&#xa;        raise StopIteration&#xa;&#xa;trial_int_msg = ""Trial division with ints [%i ... %i] and fail_max=%i""&#xa;trial_msg = ""Trial division with primes [%i ... %i]""&#xa;rho_msg = ""Pollard's rho with retries %i, max_steps %i and seed %i""&#xa;pm1_msg = ""Pollard's p-1 with smoothness bound %i and seed %i""&#xa;factor_msg = '\t%i ** %i'&#xa;fermat_msg = 'Close factors satisying Fermat condition found.'&#xa;complete_msg = 'Factorization is complete.'&#xa;&#xa;&#xa;def _factorint_small(factors, n, limit, fail_max):&#xa;    """"""&#xa;    Return the value of n and either a 0 (indicating that factorization up&#xa;    to the limit was complete) or else the next near-prime that would have&#xa;    been tested.&#xa;&#xa;    Factoring stops if there are fail_max unsuccessful tests in a row.&#xa;&#xa;    If factors of n were found they will be in the factors dictionary as&#xa;    {factor: multiplicity} and the returned value of n will have had those&#xa;    factors removed. The factors dictionary is modified in-place.&#xa;&#xa;    """"""&#xa;&#xa;    def done(n, d):&#xa;        """"""return n, d if the sqrt(n) wasn't reached yet, else&#xa;           n, 0 indicating that factoring is done.&#xa;        """"""&#xa;        if d*d <= n:&#xa;            return n, d&#xa;        return n, 0&#xa;&#xa;    d = 2&#xa;    m = trailing(n)&#xa;    if m:&#xa;        factors[d] = m&#xa;        n >>= m&#xa;    d = 3&#xa;    if limit < d:&#xa;        if n > 1:&#xa;            factors[n] = 1&#xa;        return done(n, d)&#xa;    # reduce&#xa;    m = 0&#xa;    while n % d == 0:&#xa;        n //= d&#xa;        m += 1&#xa;        if m == 20:&#xa;            mm = multiplicity(d, n)&#xa;            m += mm&#xa;            n //= d**mm&#xa;            break&#xa;    if m:&#xa;        factors[d] = m&#xa;&#xa;    # when d*d exceeds maxx or n we are done; if limit**2 is greater&#xa;    # than n then maxx is set to zero so the value of n will flag the finish&#xa;    if limit*limit > n:&#xa;        maxx = 0&#xa;    else:&#xa;        maxx = limit*limit&#xa;&#xa;    dd = maxx or n&#xa;    d = 5&#xa;    fails = 0&#xa;    while fails < fail_max:&#xa;        if d*d > dd:&#xa;            break&#xa;        # d = 6*i - 1&#xa;        # reduce&#xa;        m = 0&#xa;        while n % d == 0:&#xa;            n //= d&#xa;            m += 1&#xa;            if m == 20:&#xa;                mm = multiplicity(d, n)&#xa;                m += mm&#xa;                n //= d**mm&#xa;                break&#xa;        if m:&#xa;            factors[d] = m&#xa;            dd = maxx or n&#xa;            fails = 0&#xa;        else:&#xa;            fails += 1&#xa;        d += 2&#xa;        if d*d > dd:&#xa;            break&#xa;        # d = 6*i - 1&#xa;        # reduce&#xa;        m = 0&#xa;        while n % d == 0:&#xa;            n //= d&#xa;            m += 1&#xa;            if m == 20:&#xa;                mm = multiplicity(d, n)&#xa;                m += mm&#xa;                n //= d**mm&#xa;                break&#xa;        if m:&#xa;            factors[d] = m&#xa;            dd = maxx or n&#xa;            fails = 0&#xa;        else:&#xa;            fails += 1&#xa;        # d = 6*(i+1) - 1&#xa;        d += 4&#xa;&#xa;    return done(n, d)&#xa;&#xa;&#xa;def factorint(n, limit=None, use_trial=True, use_rho=True, use_pm1=True,&#xa;              verbose=False, visual=None):&#xa;    r""""""&#xa;    Given a positive integer ``n``, ``factorint(n)`` returns a dict containing&#xa;    the prime factors of ``n`` as keys and their respective multiplicities&#xa;    as values. For example:&#xa;&#xa;    >>> from sympy.ntheory import factorint&#xa;    >>> factorint(2000)    # 2000 = (2**4) * (5**3)&#xa;    {2: 4, 5: 3}&#xa;    >>> factorint(65537)   # This number is prime&#xa;    {65537: 1}&#xa;&#xa;    For input less than 2, factorint behaves as follows:&#xa;&#xa;        - ``factorint(1)`` returns the empty factorization, ``{}``&#xa;        - ``factorint(0)`` returns ``{0:1}``&#xa;        - ``factorint(-n)`` adds ``-1:1`` to the factors and then factors ``n``&#xa;&#xa;    Partial Factorization:&#xa;&#xa;    If ``limit`` (> 3) is specified, the search is stopped after performing&#xa;    trial division up to (and including) the limit (or taking a&#xa;    corresponding number of rho/p-1 steps). This is useful if one has&#xa;    a large number and only is interested in finding small factors (if&#xa;    any). Note that setting a limit does not prevent larger factors&#xa;    from being found early; it simply means that the largest factor may&#xa;    be composite. Since checking for perfect power is relatively cheap, it is&#xa;    done regardless of the limit setting.&#xa;&#xa;    This number, for example, has two small factors and a huge&#xa;    semi-prime factor that cannot be reduced easily:&#xa;&#xa;    >>> from sympy.ntheory import isprime&#xa;    >>> from sympy.core.compatibility import long&#xa;    >>> a = 1407633717262338957430697921446883&#xa;    >>> f = factorint(a, limit=10000)&#xa;    >>> f == {991: 1, long(202916782076162456022877024859): 1, 7: 1}&#xa;    True&#xa;    >>> isprime(max(f))&#xa;    False&#xa;&#xa;    This number has a small factor and a residual perfect power whose&#xa;    base is greater than the limit:&#xa;&#xa;    >>> factorint(3*101**7, limit=5)&#xa;    {3: 1, 101: 7}&#xa;&#xa;    Visual Factorization:&#xa;&#xa;    If ``visual`` is set to ``True``, then it will return a visual&#xa;    factorization of the integer.  For example:&#xa;&#xa;    >>> from sympy import pprint&#xa;    >>> pprint(factorint(4200, visual=True))&#xa;     3  1  2  1&#xa;    2 *3 *5 *7&#xa;&#xa;    Note that this is achieved by using the evaluate=False flag in Mul&#xa;    and Pow. If you do other manipulations with an expression where&#xa;    evaluate=False, it may evaluate.  Therefore, you should use the&#xa;    visual option only for visualization, and use the normal dictionary&#xa;    returned by visual=False if you want to perform operations on the&#xa;    factors.&#xa;&#xa;    You can easily switch between the two forms by sending them back to&#xa;    factorint:&#xa;&#xa;    >>> from sympy import Mul, Pow&#xa;    >>> regular = factorint(1764); regular&#xa;    {2: 2, 3: 2, 7: 2}&#xa;    >>> pprint(factorint(regular))&#xa;     2  2  2&#xa;    2 *3 *7&#xa;&#xa;    >>> visual = factorint(1764, visual=True); pprint(visual)&#xa;     2  2  2&#xa;    2 *3 *7&#xa;    >>> print(factorint(visual))&#xa;    {2: 2, 3: 2, 7: 2}&#xa;&#xa;    If you want to send a number to be factored in a partially factored form&#xa;    you can do so with a dictionary or unevaluated expression:&#xa;&#xa;    >>> factorint(factorint({4: 2, 12: 3})) # twice to toggle to dict form&#xa;    {2: 10, 3: 3}&#xa;    >>> factorint(Mul(4, 12, evaluate=False))&#xa;    {2: 4, 3: 1}&#xa;&#xa;    The table of the output logic is:&#xa;&#xa;        ====== ====== ======= =======&#xa;                       Visual&#xa;        ------ ----------------------&#xa;        Input  True   False   other&#xa;        ====== ====== ======= =======&#xa;        dict    mul    dict    mul&#xa;        n       mul    dict    dict&#xa;        mul     mul    dict    dict&#xa;        ====== ====== ======= =======&#xa;&#xa;    Notes&#xa;    =====&#xa;&#xa;    Algorithm:&#xa;&#xa;    The function switches between multiple algorithms. Trial division&#xa;    quickly finds small factors (of the order 1-5 digits), and finds&#xa;    all large factors if given enough time. The Pollard rho and p-1&#xa;    algorithms are used to find large factors ahead of time; they&#xa;    will often find factors of the order of 10 digits within a few&#xa;    seconds:&#xa;&#xa;    >>> factors = factorint(12345678910111213141516)&#xa;    >>> for base, exp in sorted(factors.items()):&#xa;    ...     print('%s %s' % (base, exp))&#xa;    ...&#xa;    2 2&#xa;    2507191691 1&#xa;    1231026625769 1&#xa;&#xa;    Any of these methods can optionally be disabled with the following&#xa;    boolean parameters:&#xa;&#xa;        - ``use_trial``: Toggle use of trial division&#xa;        - ``use_rho``: Toggle use of Pollard's rho method&#xa;        - ``use_pm1``: Toggle use of Pollard's p-1 method&#xa;&#xa;    ``factorint`` also periodically checks if the remaining part is&#xa;    a prime number or a perfect power, and in those cases stops.&#xa;&#xa;&#xa;    If ``verbose`` is set to ``True``, detailed progress is printed.&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    smoothness, smoothness_p, divisors&#xa;&#xa;    """"""&#xa;    factordict = {}&#xa;    if visual and not isinstance(n, Mul) and not isinstance(n, dict):&#xa;        factordict = factorint(n, limit=limit, use_trial=use_trial,&#xa;                               use_rho=use_rho, use_pm1=use_pm1,&#xa;                               verbose=verbose, visual=False)&#xa;    elif isinstance(n, Mul):&#xa;        factordict = dict([(int(k), int(v)) for k, v in&#xa;                           list(n.as_powers_dict().items())])&#xa;    elif isinstance(n, dict):&#xa;        factordict = n&#xa;    if factordict and (isinstance(n, Mul) or isinstance(n, dict)):&#xa;        # check it&#xa;        for k in list(factordict.keys()):&#xa;            if isprime(k):&#xa;                continue&#xa;            e = factordict.pop(k)&#xa;            d = factorint(k, limit=limit, use_trial=use_trial, use_rho=use_rho,&#xa;                          use_pm1=use_pm1, verbose=verbose, visual=False)&#xa;            for k, v in d.items():&#xa;                if k in factordict:&#xa;                    factordict[k] += v*e&#xa;                else:&#xa;                    factordict[k] = v*e&#xa;    if visual or (type(n) is dict and&#xa;                  visual is not True and&#xa;                  visual is not False):&#xa;        if factordict == {}:&#xa;            return S.One&#xa;        if -1 in factordict:&#xa;            factordict.pop(-1)&#xa;            args = [S.NegativeOne]&#xa;        else:&#xa;            args = []&#xa;        args.extend([Pow(*i, evaluate=False)&#xa;                     for i in sorted(factordict.items())])&#xa;        return Mul(*args, evaluate=False)&#xa;    elif isinstance(n, dict) or isinstance(n, Mul):&#xa;        return factordict&#xa;&#xa;    assert use_trial or use_rho or use_pm1&#xa;&#xa;    n = as_int(n)&#xa;    if limit:&#xa;        limit = int(limit)&#xa;&#xa;    # special cases&#xa;    if n < 0:&#xa;        factors = factorint(&#xa;            -n, limit=limit, use_trial=use_trial, use_rho=use_rho,&#xa;            use_pm1=use_pm1, verbose=verbose, visual=False)&#xa;        factors[-1] = 1&#xa;        return factors&#xa;&#xa;    if limit and limit < 2:&#xa;        if n == 1:&#xa;            return {}&#xa;        return {n: 1}&#xa;    elif n < 10:&#xa;        # doing this we are assured of getting a limit > 2&#xa;        # when we have to compute it later&#xa;        return [{0: 1}, {}, {2: 1}, {3: 1}, {2: 2}, {5: 1},&#xa;                {2: 1, 3: 1}, {7: 1}, {2: 3}, {3: 2}][n]&#xa;&#xa;    factors = {}&#xa;&#xa;    # do simplistic factorization&#xa;    if verbose:&#xa;        sn = str(n)&#xa;        if len(sn) > 50:&#xa;            print('Factoring %s' % sn[:5] + \&#xa;                  '..(%i other digits)..' % (len(sn) - 10) + sn[-5:])&#xa;        else:&#xa;            print('Factoring', n)&#xa;&#xa;    if use_trial:&#xa;        # this is the preliminary factorization for small factors&#xa;        small = 2**15&#xa;        fail_max = 600&#xa;        small = min(small, limit or small)&#xa;        if verbose:&#xa;            print(trial_int_msg % (2, small, fail_max))&#xa;        n, next_p = _factorint_small(factors, n, small, fail_max)&#xa;    else:&#xa;        next_p = 2&#xa;    if factors and verbose:&#xa;        for k in sorted(factors):&#xa;            print(factor_msg % (k, factors[k]))&#xa;    if next_p == 0:&#xa;        if n > 1:&#xa;            factors[int(n)] = 1&#xa;        if verbose:&#xa;            print(complete_msg)&#xa;        return factors&#xa;&#xa;    # continue with more advanced factorization methods&#xa;&#xa;    # first check if the simplistic run didn't finish&#xa;    # because of the limit and check for a perfect&#xa;    # power before exiting&#xa;    try:&#xa;        if limit and next_p > limit:&#xa;            if verbose:&#xa;                print('Exceeded limit:', limit)&#xa;&#xa;            _check_termination(factors, n, limit, use_trial, use_rho, use_pm1,&#xa;                               verbose)&#xa;&#xa;            if n > 1:&#xa;                factors[int(n)] = 1&#xa;            return factors&#xa;        else:&#xa;            # Before quitting (or continuing on)...&#xa;&#xa;            # ...do a Fermat test since it's so easy and we need the&#xa;            # square root anyway. Finding 2 factors is easy if they are&#xa;            # ""close enough."" This is the big root equivalent of dividing by&#xa;            # 2, 3, 5.&#xa;            sqrt_n = integer_nthroot(n, 2)[0]&#xa;            a = sqrt_n + 1&#xa;            a2 = a**2&#xa;            b2 = a2 - n&#xa;            for i in range(3):&#xa;                b, fermat = integer_nthroot(b2, 2)&#xa;                if fermat:&#xa;                    break&#xa;                b2 += 2*a + 1  # equiv to (a+1)**2 - n&#xa;                a += 1&#xa;            if fermat:&#xa;                if verbose:&#xa;                    print(fermat_msg)&#xa;                if limit:&#xa;                    limit -= 1&#xa;                for r in [a - b, a + b]:&#xa;                    facs = factorint(r, limit=limit, use_trial=use_trial,&#xa;                                     use_rho=use_rho, use_pm1=use_pm1,&#xa;                                     verbose=verbose)&#xa;                    factors.update(facs)&#xa;                raise StopIteration&#xa;&#xa;            # ...see if factorization can be terminated&#xa;            _check_termination(factors, n, limit, use_trial, use_rho, use_pm1,&#xa;                               verbose)&#xa;&#xa;    except StopIteration:&#xa;        if verbose:&#xa;            print(complete_msg)&#xa;        return factors&#xa;&#xa;    # these are the limits for trial division which will&#xa;    # be attempted in parallel with pollard methods&#xa;    low, high = next_p, 2*next_p&#xa;&#xa;    limit = limit or sqrt_n&#xa;    # add 1 to make sure limit is reached in primerange calls&#xa;    limit += 1&#xa;&#xa;    while 1:&#xa;&#xa;        try:&#xa;            high_ = high&#xa;            if limit < high_:&#xa;                high_ = limit&#xa;&#xa;            # Trial division&#xa;            if use_trial:&#xa;                if verbose:&#xa;                    print(trial_msg % (low, high_))&#xa;                ps = sieve.primerange(low, high_)&#xa;                n, found_trial = _trial(factors, n, ps, verbose)&#xa;                if found_trial:&#xa;                    _check_termination(factors, n, limit, use_trial, use_rho,&#xa;                                       use_pm1, verbose)&#xa;            else:&#xa;                found_trial = False&#xa;&#xa;            if high > limit:&#xa;                if verbose:&#xa;                    print('Exceeded limit:', limit)&#xa;                if n > 1:&#xa;                    factors[int(n)] = 1&#xa;                raise StopIteration&#xa;&#xa;            # Only used advanced methods when no small factors were found&#xa;            if not found_trial:&#xa;                if (use_pm1 or use_rho):&#xa;                    high_root = max(int(math.log(high_**0.7)), low, 3)&#xa;&#xa;                    # Pollard p-1&#xa;                    if use_pm1:&#xa;                        if verbose:&#xa;                            print(pm1_msg % (high_root, high_))&#xa;                        c = pollard_pm1(n, B=high_root, seed=high_)&#xa;                        if c:&#xa;                            # factor it and let _trial do the update&#xa;                            ps = factorint(c, limit=limit - 1,&#xa;                                           use_trial=use_trial,&#xa;                                           use_rho=use_rho,&#xa;                                           use_pm1=use_pm1,&#xa;                                           verbose=verbose)&#xa;                            n, _ = _trial(factors, n, ps, verbose=False)&#xa;                            _check_termination(factors, n, limit, use_trial,&#xa;                                               use_rho, use_pm1, verbose)&#xa;&#xa;                    # Pollard rho&#xa;                    if use_rho:&#xa;                        max_steps = high_root&#xa;                        if verbose:&#xa;                            print(rho_msg % (1, max_steps, high_))&#xa;                        c = pollard_rho(n, retries=1, max_steps=max_steps,&#xa;                                        seed=high_)&#xa;                        if c:&#xa;                            # factor it and let _trial do the update&#xa;                            ps = factorint(c, limit=limit - 1,&#xa;                                           use_trial=use_trial,&#xa;                                           use_rho=use_rho,&#xa;                                           use_pm1=use_pm1,&#xa;                                           verbose=verbose)&#xa;                            n, _ = _trial(factors, n, ps, verbose=False)&#xa;                            _check_termination(factors, n, limit, use_trial,&#xa;                                               use_rho, use_pm1, verbose)&#xa;&#xa;        except StopIteration:&#xa;            if verbose:&#xa;                print(complete_msg)&#xa;            return factors&#xa;&#xa;        low, high = high, high*2&#xa;&#xa;&#xa;def primefactors(n, limit=None, verbose=False):&#xa;    """"""Return a sorted list of n's prime factors, ignoring multiplicity&#xa;    and any composite factor that remains if the limit was set too low&#xa;    for complete factorization. Unlike factorint(), primefactors() does&#xa;    not return -1 or 0.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory import primefactors, factorint, isprime&#xa;    >>> primefactors(6)&#xa;    [2, 3]&#xa;    >>> primefactors(-5)&#xa;    [5]&#xa;&#xa;    >>> sorted(factorint(123456).items())&#xa;    [(2, 6), (3, 1), (643, 1)]&#xa;    >>> primefactors(123456)&#xa;    [2, 3, 643]&#xa;&#xa;    >>> sorted(factorint(10000000001, limit=200).items())&#xa;    [(101, 1), (99009901, 1)]&#xa;    >>> isprime(99009901)&#xa;    False&#xa;    >>> primefactors(10000000001, limit=300)&#xa;    [101]&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    divisors&#xa;    """"""&#xa;    n = int(n)&#xa;    factors = sorted(factorint(n, limit=limit, verbose=verbose).keys())&#xa;    s = [f for f in factors[:-1:] if f not in [-1, 0, 1]]&#xa;    if factors and isprime(factors[-1]):&#xa;        s += [factors[-1]]&#xa;    return s&#xa;&#xa;&#xa;def _divisors(n):&#xa;    """"""Helper function for divisors which generates the divisors.""""""&#xa;&#xa;    factordict = factorint(n)&#xa;    ps = sorted(factordict.keys())&#xa;&#xa;    def rec_gen(n=0):&#xa;        if n == len(ps):&#xa;            yield 1&#xa;        else:&#xa;            pows = [1]&#xa;            for j in range(factordict[ps[n]]):&#xa;                pows.append(pows[-1] * ps[n])&#xa;            for q in rec_gen(n + 1):&#xa;                for p in pows:&#xa;                    yield p * q&#xa;&#xa;    for p in rec_gen():&#xa;        yield p&#xa;&#xa;&#xa;def divisors(n, generator=False):&#xa;    r""""""&#xa;    Return all divisors of n sorted from 1..n by default.&#xa;    If generator is ``True`` an unordered generator is returned.&#xa;&#xa;    The number of divisors of n can be quite large if there are many&#xa;    prime factors (counting repeated factors). If only the number of&#xa;    factors is desired use divisor_count(n).&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy import divisors, divisor_count&#xa;    >>> divisors(24)&#xa;    [1, 2, 3, 4, 6, 8, 12, 24]&#xa;    >>> divisor_count(24)&#xa;    8&#xa;&#xa;    >>> list(divisors(120, generator=True))&#xa;    [1, 2, 4, 8, 3, 6, 12, 24, 5, 10, 20, 40, 15, 30, 60, 120]&#xa;&#xa;    This is a slightly modified version of Tim Peters referenced at:&#xa;    http://stackoverflow.com/questions/1010381/python-factorization&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    primefactors, factorint, divisor_count&#xa;    """"""&#xa;&#xa;    n = as_int(abs(n))&#xa;    if isprime(n):&#xa;        return [1, n]&#xa;    if n == 1:&#xa;        return [1]&#xa;    if n == 0:&#xa;        return []&#xa;    rv = _divisors(n)&#xa;    if not generator:&#xa;        return sorted(rv)&#xa;    return rv&#xa;&#xa;&#xa;def divisor_count(n, modulus=1):&#xa;    """"""&#xa;    Return the number of divisors of ``n``. If ``modulus`` is not 1 then only&#xa;    those that are divisible by ``modulus`` are counted.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    - http://www.mayer.dial.pipex.com/maths/formulae.htm&#xa;&#xa;    >>> from sympy import divisor_count&#xa;    >>> divisor_count(6)&#xa;    4&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint, divisors, totient&#xa;    """"""&#xa;&#xa;    if not modulus:&#xa;        return 0&#xa;    elif modulus != 1:&#xa;        n, r = divmod(n, modulus)&#xa;        if r:&#xa;            return 0&#xa;    if n == 0:&#xa;        return 0&#xa;    return Mul(*[v + 1 for k, v in factorint(n).items() if k > 1])&#xa;&#xa;&#xa;def _udivisors(n):&#xa;    """"""Helper function for udivisors which generates the unitary divisors.""""""&#xa;&#xa;    factorpows = [p**e for p, e in factorint(n).items()]&#xa;    for i in range(2**len(factorpows)):&#xa;        d, j, k = 1, i, 0&#xa;        while j:&#xa;            if (j & 1):&#xa;                d *= factorpows[k]&#xa;            j >>= 1&#xa;            k += 1&#xa;        yield d&#xa;&#xa;&#xa;def udivisors(n, generator=False):&#xa;    r""""""&#xa;    Return all unitary divisors of n sorted from 1..n by default.&#xa;    If generator is ``True`` an unordered generator is returned.&#xa;&#xa;    The number of unitary divisors of n can be quite large if there are many&#xa;    prime factors. If only the number of unitary divisors is desired use&#xa;    udivisor_count(n).&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    - http://en.wikipedia.org/wiki/Unitary_divisor&#xa;    - http://mathworld.wolfram.com/UnitaryDivisor.html&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import udivisors, udivisor_count&#xa;    >>> udivisors(15)&#xa;    [1, 3, 5, 15]&#xa;    >>> udivisor_count(15)&#xa;    4&#xa;&#xa;    >>> sorted(udivisors(120, generator=True))&#xa;    [1, 3, 5, 8, 15, 24, 40, 120]&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    primefactors, factorint, divisors, divisor_count, udivisor_count&#xa;    """"""&#xa;&#xa;    n = as_int(abs(n))&#xa;    if isprime(n):&#xa;        return [1, n]&#xa;    if n == 1:&#xa;        return [1]&#xa;    if n == 0:&#xa;        return []&#xa;    rv = _udivisors(n)&#xa;    if not generator:&#xa;        return sorted(rv)&#xa;    return rv&#xa;&#xa;&#xa;def udivisor_count(n):&#xa;    """"""&#xa;    Return the number of unitary divisors of ``n``.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    - http://mathworld.wolfram.com/UnitaryDivisorFunction.html&#xa;&#xa;    >>> from sympy.ntheory.factor_ import udivisor_count&#xa;    >>> udivisor_count(120)&#xa;    8&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint, divisors, udivisors, divisor_count, totient&#xa;    """"""&#xa;&#xa;    if n == 0:&#xa;        return 0&#xa;    return 2**len([p for p in factorint(n) if p > 1])&#xa;&#xa;&#xa;def _antidivisors(n):&#xa;    """"""Helper function for antidivisors which generates the antidivisors.""""""&#xa;&#xa;    for d in _divisors(n):&#xa;        y = 2*d&#xa;        if n > y and n % y:&#xa;            yield y&#xa;    for d in _divisors(2*n-1):&#xa;        if n > d >= 2 and n % d:&#xa;            yield d&#xa;    for d in _divisors(2*n+1):&#xa;        if n > d >= 2 and n % d:&#xa;            yield d&#xa;&#xa;&#xa;def antidivisors(n, generator=False):&#xa;    r""""""&#xa;    Return all antidivisors of n sorted from 1..n by default.&#xa;&#xa;    Antidivisors [1]_ of n are numbers that do not divide n by the largest&#xa;    possible margin.  If generator is True an unordered generator is returned.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    .. [1] definition is described in http://oeis.org/A066272/a066272a.html&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import antidivisors&#xa;    >>> antidivisors(24)&#xa;    [7, 16]&#xa;&#xa;    >>> sorted(antidivisors(128, generator=True))&#xa;    [3, 5, 15, 17, 51, 85]&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    primefactors, factorint, divisors, divisor_count, antidivisor_count&#xa;    """"""&#xa;&#xa;    n = as_int(abs(n))&#xa;    if n <= 2:&#xa;        return []&#xa;    rv = _antidivisors(n)&#xa;    if not generator:&#xa;        return sorted(rv)&#xa;    return rv&#xa;&#xa;&#xa;def antidivisor_count(n):&#xa;    """"""&#xa;    Return the number of antidivisors [1]_ of ``n``.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    .. [1] formula from https://oeis.org/A066272&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import antidivisor_count&#xa;    >>> antidivisor_count(13)&#xa;    4&#xa;    >>> antidivisor_count(27)&#xa;    5&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint, divisors, antidivisors, divisor_count, totient&#xa;    """"""&#xa;&#xa;    n = as_int(abs(n))&#xa;    if n <= 2:&#xa;        return 0&#xa;    return divisor_count(2*n-1) + divisor_count(2*n+1) + \&#xa;        divisor_count(n) - divisor_count(n, 2) - 5&#xa;&#xa;&#xa;class totient(Function):&#xa;    """"""&#xa;    Calculate the Euler totient function phi(n)&#xa;&#xa;    >>> from sympy.ntheory import totient&#xa;    >>> totient(1)&#xa;    1&#xa;    >>> totient(25)&#xa;    20&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    divisor_count&#xa;    """"""&#xa;    @classmethod&#xa;    def eval(cls, n):&#xa;        n = sympify(n)&#xa;        if n.is_Integer:&#xa;            if n < 1:&#xa;                raise ValueError(""n must be a positive integer"")&#xa;            factors = factorint(n)&#xa;            t = 1&#xa;            for p, k in factors.items():&#xa;                t *= (p - 1) * p**(k - 1)&#xa;            return t&#xa;&#xa;    def _eval_is_integer(self):&#xa;        return fuzzy_and([self.args[0].is_integer, self.args[0].is_positive])&#xa;&#xa;&#xa;class divisor_sigma(Function):&#xa;    """"""&#xa;    Calculate the divisor function `\sigma_k(n)` for positive integer n&#xa;&#xa;    ``divisor_sigma(n, k)`` is equal to ``sum([x**k for x in divisors(n)])``&#xa;&#xa;    If n's prime factorization is:&#xa;&#xa;    .. math ::&#xa;        n = \prod_{i=1}^\omega p_i^{m_i},&#xa;&#xa;    then&#xa;&#xa;    .. math ::&#xa;        \sigma_k(n) = \prod_{i=1}^\omega (1+p_i^k+p_i^{2k}+\cdots&#xa;        + p_i^{m_ik}).&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    k : power of divisors in the sum&#xa;&#xa;        for k = 0, 1:&#xa;        ``divisor_sigma(n, 0)`` is equal to ``divisor_count(n)``&#xa;        ``divisor_sigma(n, 1)`` is equal to ``sum(divisors(n))``&#xa;&#xa;        Default for k is 1.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    .. [1] http://en.wikipedia.org/wiki/Divisor_function&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory import divisor_sigma&#xa;    >>> divisor_sigma(18, 0)&#xa;    6&#xa;    >>> divisor_sigma(39, 1)&#xa;    56&#xa;    >>> divisor_sigma(12, 2)&#xa;    210&#xa;    >>> divisor_sigma(37)&#xa;    38&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    divisor_count, totient, divisors, factorint&#xa;    """"""&#xa;&#xa;    @classmethod&#xa;    def eval(cls, n, k=1):&#xa;        n = sympify(n)&#xa;        k = sympify(k)&#xa;        if n.is_prime:&#xa;            return 1 + n**k&#xa;        if n.is_Integer:&#xa;            if n <= 0:&#xa;                raise ValueError(""n must be a positive integer"")&#xa;            else:&#xa;                return Mul(*[(p**(k*(e + 1)) - 1)/(p**k - 1) if k != 0&#xa;                           else e + 1 for p, e in factorint(n).items()])&#xa;&#xa;&#xa;def core(n, t=2):&#xa;    """"""&#xa;    Calculate core(n,t) = `core_t(n)` of a positive integer n&#xa;&#xa;    ``core_2(n)`` is equal to the squarefree part of n&#xa;&#xa;    If n's prime factorization is:&#xa;&#xa;    .. math ::&#xa;        n = \prod_{i=1}^\omega p_i^{m_i},&#xa;&#xa;    then&#xa;&#xa;    .. math ::&#xa;        core_t(n) = \prod_{i=1}^\omega p_i^{m_i \mod t}.&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    t : core(n,t) calculates the t-th power free part of n&#xa;&#xa;        ``core(n, 2)`` is the squarefree part of ``n``&#xa;        ``core(n, 3)`` is the cubefree part of ``n``&#xa;&#xa;        Default for t is 2.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    .. [1] http://en.wikipedia.org/wiki/Square-free_integer#Squarefree_core&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import core&#xa;    >>> core(24, 2)&#xa;    6&#xa;    >>> core(9424, 3)&#xa;    1178&#xa;    >>> core(379238)&#xa;    379238&#xa;    >>> core(15**11, 10)&#xa;    15&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    factorint&#xa;    """"""&#xa;&#xa;    n = as_int(n)&#xa;    t = as_int(t)&#xa;    if n <= 0:&#xa;        raise ValueError(""n must be a positive integer"")&#xa;    elif t <= 1:&#xa;        raise ValueError(""t must be >= 2"")&#xa;    else:&#xa;        y = 1&#xa;        for p, e in factorint(n).items():&#xa;            y *= p**(e % t)&#xa;        return y&#xa;&#xa;&#xa;def digits(n, b=10):&#xa;    """"""&#xa;    Return a list of the digits of n in base b. The first element in the list&#xa;    is b (or -b if n is negative).&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import digits&#xa;    >>> digits(35)&#xa;    [10, 3, 5]&#xa;    >>> digits(27, 2)&#xa;    [2, 1, 1, 0, 1, 1]&#xa;    >>> digits(65536, 256)&#xa;    [256, 1, 0, 0]&#xa;    >>> digits(-3958, 27)&#xa;    [-27, 5, 11, 16]&#xa;    """"""&#xa;&#xa;    b = as_int(b)&#xa;    n = as_int(n)&#xa;    if b <= 1:&#xa;        raise ValueError(""b must be >= 2"")&#xa;    else:&#xa;        x, y = abs(n), []&#xa;        while x >= b:&#xa;            x, r = divmod(x, b)&#xa;            y.append(r)&#xa;        y.append(x)&#xa;        y.append(-b if n < 0 else b)&#xa;        y.reverse()&#xa;        return y&#xa;&#xa;&#xa;class udivisor_sigma(Function):&#xa;    """"""&#xa;    Calculate the unitary divisor function `\sigma_k^*(n)` for positive integer n&#xa;&#xa;    ``udivisor_sigma(n, k)`` is equal to ``sum([x**k for x in udivisors(n)])``&#xa;&#xa;    If n's prime factorization is:&#xa;&#xa;    .. math ::&#xa;        n = \prod_{i=1}^\omega p_i^{m_i},&#xa;&#xa;    then&#xa;&#xa;    .. math ::&#xa;        \sigma_k^*(n) = \prod_{i=1}^\omega (1+ p_i^{m_ik}).&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    k : power of divisors in the sum&#xa;&#xa;        for k = 0, 1:&#xa;        ``udivisor_sigma(n, 0)`` is equal to ``udivisor_count(n)``&#xa;        ``udivisor_sigma(n, 1)`` is equal to ``sum(udivisors(n))``&#xa;&#xa;        Default for k is 1.&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    .. [1] http://mathworld.wolfram.com/UnitaryDivisorFunction.html&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.ntheory.factor_ import udivisor_sigma&#xa;    >>> udivisor_sigma(18, 0)&#xa;    4&#xa;    >>> udivisor_sigma(74, 1)&#xa;    114&#xa;    >>> udivisor_sigma(36, 3)&#xa;    47450&#xa;    >>> udivisor_sigma(111)&#xa;    152&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    divisor_count, totient, divisors, udivisors, udivisor_count, divisor_sigma,&#xa;    factorint&#xa;    """"""&#xa;&#xa;    @classmethod&#xa;    def eval(cls, n, k=1):&#xa;        n = sympify(n)&#xa;        k = sympify(k)&#xa;        if n.is_prime:&#xa;            return 1 + n**k&#xa;        if n.is_Integer:&#xa;            if n <= 0:&#xa;                raise ValueError(""n must be a positive integer"")&#xa;            else:&#xa;                return Mul(*[1+p**(k*e) for p, e in factorint(n).items()])&#xa;"
2276200|"#!/usr/bin/env python&#xa;# encoding: utf-8&#xa;# ===  IMPORTANT  ====&#xa;# NOTE: In order to support non-ASCII file names,&#xa;#       your system's locale MUST be set to 'utf-8'&#xa;# CAVEAT: DOESN'T work with proxy, the underlying reason being&#xa;#         the 'requests' package used for http communication doesn't seem&#xa;#         to work properly with proxies, reason unclear.&#xa;# NOTE: It seems Baidu doesn't handle MD5 quite right after combining files,&#xa;#       so it may return erroneous MD5s. Perform a rapidupload again may fix the problem.&#xa;#        That's why I changed default behavior to no-verification.&#xa;# NOTE: syncup / upload, syncdown / downdir are partially duplicates&#xa;#       the difference: syncup/down compare and perform actions&#xa;#       while down/up just proceed to download / upload (but still compare during actions)&#xa;#       so roughly the same, except that sync can delete extra files&#xa;#&#xa;# TODO: Dry run?&#xa;# TODO: Use batch functions for better performance&#xa;&#xa;'''&#xa;bypy -- Python client for Baidu Yun&#xa;---&#xa;&#xa;https://github.com/houtianze/bypy&#xa;---&#xa;&#xa;bypy is a Baidu Yun client written in Python (2.7).&#xa;(NOTE: You need to install the 'requests' library by running 'pip install requests')&#xa;&#xa;It offers some file operations like: list, download, upload, syncup, syncdown, etc.&#xa;The main purpose is to utilize Baidu Yun in Linux environment (e.g. Raspberry Pi)&#xa;&#xa;It uses a server for OAuth authorization, to conceal the Application's Secret Key.&#xa;Alternatively, you can create your own App at Baidu and replace the 'ApiKey' and 'SecretKey' with your copies,&#xa;and then, change 'ServerAuth' to 'False'&#xa;---&#xa;@author:     Hou Tianze&#xa;&#xa;@license:    MIT&#xa;&#xa;@contact:    GitHub: houtianze, Twitter: @ibic, G+: +TianzeHou&#xa;'''&#xa;&#xa;# it takes days just to fix you, unicode ...&#xa;# some references&#xa;# https://stackoverflow.com/questions/4374455/how-to-set-sys-stdout-encoding-in-python-3&#xa;# https://stackoverflow.com/questions/492483/setting-the-correct-encoding-when-piping-stdout-in-python&#xa;# http://drj11.wordpress.com/2007/05/14/python-how-is-sysstdoutencoding-chosen/&#xa;# https://stackoverflow.com/questions/11741574/how-to-set-the-default-encoding-to-utf-8-in-python&#xa;# https://stackoverflow.com/questions/2276200/changing-default-encoding-of-python&#xa;from __future__ import unicode_literals&#xa;&#xa;EIncorrectPythonVersion = 1&#xa;import sys&#xa;vi = sys.version_info&#xa;if not hasattr(sys.version_info, 'major') or vi.major != 2 or vi.minor < 7:&#xa;	print(""Error: Incorrect Python version. "" + \&#xa;		""You need 2.7 or above (but not 3)"")&#xa;	sys.exit(EIncorrectPythonVersion)&#xa;&#xa;#reload(sys)&#xa;#sys.setdefaultencoding(SystemEncoding)&#xa;&#xa;import os&#xa;&#xa;import locale&#xa;SystemLanguageCode, SystemEncoding = locale.getdefaultlocale()&#xa;if SystemEncoding and not sys.platform.startswith('win32'):&#xa;	sysenc = SystemEncoding.upper()&#xa;	if sysenc != 'UTF-8' and sysenc != 'UTF8':&#xa;		err = ""You MUST set system locale to 'UTF-8' to support unicode file names.\n"" + \&#xa;			""Current locale is '{}'"".format(SystemEncoding)&#xa;		ex = Exception(err)&#xa;		print(err)&#xa;		raise ex&#xa;&#xa;if not SystemEncoding:&#xa;	# ASSUME UTF-8 encoding, if for whatever reason,&#xa;	# we can't get the default system encoding&#xa;	print(""*WARNING*: Cannot detect the system encoding, assume it's 'UTF-8'"")&#xa;	SystemEncoding = 'utf-8'&#xa;&#xa;import codecs&#xa;# no idea who is the asshole that screws the sys.stdout.encoding&#xa;# the locale is 'UTF-8', sys.stdin.encoding is 'UTF-8',&#xa;# BUT, sys.stdout.encoding is None ...&#xa;if not (sys.stdout.encoding and sys.stdout.encoding.lower() == 'utf-8'):&#xa;	encoding_to_use = sys.stdout.encoding&#xa;	try:&#xa;		codecs.lookup(encoding_to_use)&#xa;		u'\u6c49\u5b57'.encode(encoding_to_use) # u''&#xa;	except: # (LookupError, TypeError, UnicodeEncodeError):&#xa;		encoding_to_use = 'utf-8'&#xa;		sys.exc_clear()&#xa;	sys.stdout = codecs.getwriter(encoding_to_use)(sys.stdout)&#xa;	sys.stderr = codecs.getwriter(encoding_to_use)(sys.stderr)&#xa;import signal&#xa;import time&#xa;import shutil&#xa;import posixpath&#xa;#import types&#xa;import traceback&#xa;import inspect&#xa;import logging&#xa;import httplib&#xa;import urllib&#xa;import json&#xa;import hashlib&#xa;import base64&#xa;import binascii&#xa;import re&#xa;import cPickle as pickle&#xa;import pprint&#xa;import socket&#xa;import math&#xa;#from collections import OrderedDict&#xa;from os.path import expanduser&#xa;from argparse import ArgumentParser&#xa;from argparse import RawDescriptionHelpFormatter&#xa;&#xa;# Defines that should never be changed&#xa;OneK = 1024&#xa;OneM = OneK * OneK&#xa;OneG = OneM * OneK&#xa;OneT = OneG * OneK&#xa;OneP = OneT * OneK&#xa;OneE = OneP * OneK&#xa;OneZ = OneE * OneK&#xa;OneY = OneZ * OneK&#xa;SIPrefixNames = [ '', 'k', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y' ]&#xa;&#xa;SIPrefixTimes = {&#xa;	'K' : OneK,&#xa;	'M' : OneM,&#xa;	'G' : OneG,&#xa;	'T' : OneT,&#xa;	'E' : OneE,&#xa;	'Z' : OneZ,&#xa;	'Y' : OneY }&#xa;&#xa;&#xa;# special variables&#xa;__all__ = []&#xa;__version__ = '1.0.20'&#xa;&#xa;# ByPy default values&#xa;DefaultSliceInMB = 20&#xa;DefaultSliceSize = 20 * OneM&#xa;DefaultDlChunkSize = 20 * OneM&#xa;RetryDelayInSec = 10&#xa;&#xa;# Baidu PCS constants&#xa;MinRapidUploadFileSize = 256 * OneK&#xa;MaxSliceSize = 2 * OneG&#xa;MaxSlicePieces = 1024&#xa;&#xa;# return (error) codes&#xa;ENoError = 0 # plain old OK, fine, no error.&#xa;#EIncorrectPythonVersion = 1&#xa;EApiNotConfigured = 10 # ApiKey, SecretKey and AppPcsPath not properly configured&#xa;EArgument = 10 # invalid program command argument&#xa;EAbort = 20 # aborted&#xa;EException = 30 # unhandled exception occured&#xa;EParameter = 40 # invalid parameter passed to ByPy&#xa;EInvalidJson = 50&#xa;EHashMismatch = 60 # MD5 hashes of the local file and remote file don't match each other&#xa;EFileWrite = 70&#xa;EFileTooBig = 80 # file too big to upload&#xa;EFailToCreateLocalDir = 90&#xa;EFailToCreateLocalFile = 100&#xa;EFailToDeleteDir = 110&#xa;EFailToDeleteFile = 120&#xa;EFileNotFound = 130&#xa;EMaxRetry = 140&#xa;ERequestFailed = 150 # request failed&#xa;ECacheNotLoaded = 160&#xa;EMigrationFailed = 170&#xa;EDownloadCerts = 180&#xa;EFatal = -1 # No way to continue&#xa;&#xa;# internal errors&#xa;IEMD5NotFound = 31079 # File md5 not found, you should use upload API to upload the whole file.&#xa;IEBDUSSExpired = -6&#xa;&#xa;# PCS configuration constants&#xa;# ==== NOTE ====&#xa;# I use server auth, because it's the only possible method to protect the SecretKey.&#xa;# If you don't like that and want to perform local authorization using 'Device' method, you need to:&#xa;# - Change to: ServerAuth = False&#xa;# - Paste your own ApiKey and SecretKey.&#xa;# - Change the AppPcsPath to your own App's directory at Baidu PCS&#xa;# Then you are good to go&#xa;ServerAuth = True # change it to 'False' if you use your own appid&#xa;GaeUrl = 'https://bypyoauth.appspot.com'&#xa;OpenShiftUrl = 'https://bypy-tianze.rhcloud.com'&#xa;HerokuUrl = 'https://bypyoauth.herokuapp.com'&#xa;GaeRedirectUrl = GaeUrl + '/auth'&#xa;GaeRefreshUrl = GaeUrl + '/refresh'&#xa;OpenShiftRedirectUrl = OpenShiftUrl + '/auth'&#xa;OpenShiftRefreshUrl = OpenShiftUrl + '/refresh'&#xa;HerokuRedirectUrl = HerokuUrl + '/auth'&#xa;HerokuRefreshUrl = HerokuUrl + '/refresh'&#xa;&#xa;AuthServerList = [&#xa;	# url, rety?, message&#xa;	(GaeRedirectUrl, False, ""Authorizing with the GAE server ...""),&#xa;	(OpenShiftRedirectUrl, True, ""I think you are WALLed, so let's authorize with the OpenShift server ...""),&#xa;	(HerokuRedirectUrl, True, ""OpenShift also failed. Last resort: authorizing with the Heroku server ...""),&#xa;]&#xa;&#xa;RefreshServerList = [&#xa;	# url, rety?, message&#xa;	(GaeRefreshUrl, False, ""Refreshing with the GAE server ...""),&#xa;	(OpenShiftRefreshUrl, True, ""I think you are WALLed, so let's refresh with the OpenShift server ...""),&#xa;	(HerokuRefreshUrl, True, ""OpenShift also failed. Last resort: refreshing with the Heroku server ...""),&#xa;]&#xa;&#xa;ApiKey = 'q8WE4EpCsau1oS0MplgMKNBn' # replace with your own ApiKey if you use your own appid&#xa;SecretKey = '' # replace with your own SecretKey if you use your own appid&#xa;&#xa;if not SecretKey:&#xa;	ServerAuth = True&#xa;# NOTE: no trailing '/'&#xa;AppPcsPath = '/apps/bypy' # change this to the App's direcotry you specified when creating the app&#xa;# -----experiment begin-----&#xa;if False:&#xa;	ApiKey = ""wcTsGHIo1f7RuAQpu8Uu30fl""&#xa;	SecretKey = ""OkGMfZTzLXUkMStQk9suBKoAqUlPXCpCxx""&#xa;	AppPcsPath = '/apps/clouddrive'&#xa;	ServerAuth = False&#xa;# -----experiment end-----  &#xa;AppPcsPathLen = len(AppPcsPath)&#xa;&#xa;# Program setting constants&#xa;HomeDir = expanduser('~')&#xa;# os.path.join() may not handle unicode well&#xa;ConfigDir = HomeDir + os.sep + '.bypy'&#xa;TokenFilePath = ConfigDir + os.sep + 'bypy.json'&#xa;HashCachePath = ConfigDir + os.sep + 'bypy.pickle'&#xa;BDUSSPath = ConfigDir + os.sep + 'bypy.bduss'&#xa;ByPyCertsFile = 'bypy.cacerts.pem'&#xa;ByPyCertsPath = ConfigDir + os.sep + ByPyCertsFile&#xa;#UserAgent = 'Mozilla/5.0'&#xa;#UserAgent = ""Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)""&#xa;# According to seanlis@github, this User-Agent string affects the download.&#xa;UserAgent = None&#xa;CleanOptionShort= '-c'&#xa;CleanOptionLong= '--clean'&#xa;DisableSslCheckOption = '--disable-ssl-check'&#xa;CaCertsOption = '--cacerts'&#xa;&#xa;# Baidu PCS URLs etc.&#xa;OpenApiUrl = ""https://openapi.baidu.com""&#xa;OpenApiVersion = ""2.0""&#xa;OAuthUrl = OpenApiUrl + ""/oauth/"" + OpenApiVersion&#xa;ServerAuthUrl = OAuthUrl + ""/authorize""&#xa;DeviceAuthUrl = OAuthUrl + ""/device/code""&#xa;TokenUrl = OAuthUrl + ""/token""&#xa;PcsUrl = 'https://pcs.baidu.com/rest/2.0/pcs/'&#xa;CPcsUrl = 'https://c.pcs.baidu.com/rest/2.0/pcs/'&#xa;DPcsUrl = 'https://d.pcs.baidu.com/rest/2.0/pcs/'&#xa;PanAPIUrl = 'http://pan.baidu.com/api/'&#xa;&#xa;# mutable, actual ones used, capital ones are supposed to be immutable&#xa;# this is introduced to support mirrors&#xa;pcsurl  = PcsUrl&#xa;cpcsurl = CPcsUrl&#xa;dpcsurl = DPcsUrl&#xa;&#xa;try:&#xa;	# non-standard python library, needs 'pip install requests'&#xa;	import requests&#xa;except:&#xa;	print(""Fail to import the 'requests' library\n"" + \&#xa;		""You need to install the 'requests' python library\n"" + \&#xa;		""You can install it by running 'pip install requests'"")&#xa;	raise&#xa;&#xa;requests_version = requests.__version__.split('.')&#xa;if int(requests_version[0]) < 1:&#xa;	print(""You Python Requests Library version is to lower than 1.\n"" + \&#xa;		""You can run 'pip install requests' to upgrade it."")&#xa;	raise&#xa;# non-standard python library, needs 'pip install requesocks'&#xa;#import requesocks as requests # if you need socks proxy&#xa;&#xa;# when was your last time flushing a toilet?&#xa;__last_flush = time.time()&#xa;#__last_flush = 0&#xa;PrintFlushPeriodInSec = 5.0&#xa;# save cache if more than 10 minutes passed&#xa;last_cache_save = time.time()&#xa;CacheSavePeriodInSec = 10 * 60.0&#xa;&#xa;# https://stackoverflow.com/questions/287871/print-in-terminal-with-colors-using-python&#xa;# https://en.wikipedia.org/wiki/ANSI_escape_code#Colors&#xa;# 0 - black, 1 - red, 2 - green, 3 - yellow&#xa;# 4 - blue, 5 - magenta, 6 - cyan 7 - white&#xa;class TermColor:&#xa;	NumOfColors = 8&#xa;	Black, Red, Green, Yellow, Blue, Magenta, Cyan, White = range(NumOfColors)&#xa;	Nil = -1&#xa;&#xa;def colorstr(msg, fg, bg):&#xa;	CSI = '\x1b['&#xa;	fgs = ''&#xa;	bgs = ''&#xa;	if fg >=0 and fg <= 7:&#xa;		fgs = str(fg + 30)&#xa;&#xa;	if bg >= 0 and bg <=7:&#xa;		bgs = str(bg + 40)&#xa;&#xa;	cs = ';'.join([fgs, bgs]).strip(';')&#xa;	if cs:&#xa;		return CSI + cs + 'm' + msg + CSI + '0m'&#xa;	else:&#xa;		return msg&#xa;&#xa;def prc(msg):&#xa;	print(msg)&#xa;	# we need to flush the output periodically to see the latest status&#xa;	global __last_flush&#xa;	now = time.time()&#xa;	if now - __last_flush >= PrintFlushPeriodInSec:&#xa;		sys.stdout.flush()&#xa;		__last_flush = now&#xa;&#xa;pr = prc&#xa;&#xa;def prcolorc(msg, fg, bg):&#xa;	if sys.stdout.isatty() and not sys.platform.startswith('win32'):&#xa;		pr(colorstr(msg, fg, bg))&#xa;	else:&#xa;		pr(msg)&#xa;&#xa;prcolor = prcolorc&#xa;&#xa;def plog(tag, msg, showtime = True, showdate = False,&#xa;		prefix = '', suffix = '', fg = TermColor.Nil, bg = TermColor.Nil):&#xa;	if showtime or showdate:&#xa;		now = time.localtime()&#xa;		if showtime:&#xa;			tag += time.strftime(""[%H:%M:%S] "", now)&#xa;		if showdate:&#xa;			tag += time.strftime(""[%Y-%m-%d] "", now)&#xa;&#xa;	if prefix:&#xa;		prcolor(""{}{}"".format(tag, prefix), fg, bg)&#xa;&#xa;	prcolor(""{}{}"".format(tag, msg), fg, bg)&#xa;&#xa;	if suffix:&#xa;		prcolor(""{}{}"".format(tag, suffix), fg, bg)&#xa;&#xa;def perr(msg, showtime = True, showdate = False, prefix = '', suffix = ''):&#xa;	return plog('<E> ', msg, showtime, showdate, prefix, suffix, TermColor.Red)&#xa;&#xa;def pwarn(msg, showtime = True, showdate = False, prefix = '', suffix = ''):&#xa;	return plog('<W> ', msg, showtime, showdate, prefix, suffix, TermColor.Yellow)&#xa;&#xa;def pinfo(msg, showtime = True, showdate = False, prefix = '', suffix = ''):&#xa;	return plog('<I> ', msg, showtime, showdate, prefix, suffix, TermColor.Green)&#xa;&#xa;def pdbg(msg, showtime = True, showdate = False, prefix = '', suffix = ''):&#xa;	return plog('<D> ', msg, showtime, showdate, prefix, suffix, TermColor.Cyan)&#xa;&#xa;def askc(msg, enter = True):&#xa;	pr(msg)&#xa;	if enter:&#xa;		pr('Press [Enter] when you are done')&#xa;	return raw_input()&#xa;&#xa;ask = askc&#xa;&#xa;# print progress&#xa;# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console&#xa;def pprgrc(finish, total, start_time = None, existing = 0,&#xa;		prefix = '', suffix = '', seg = 20):&#xa;	# we don't want this goes to the log, so we use stderr&#xa;	if total > 0:&#xa;		segth = seg * finish // total&#xa;		percent = 100 * finish // total&#xa;	else:&#xa;		segth = seg&#xa;		percent = 100&#xa;	eta = ''&#xa;	now = time.time()&#xa;	if start_time is not None and percent > 5 and finish > 0:&#xa;		finishf = float(finish) - float(existing)&#xa;		totalf = float(total)&#xa;		remainf = totalf - float(finish)&#xa;		elapsed = now - start_time&#xa;		speed = human_speed(finishf / elapsed)&#xa;		eta = 'ETA: ' + human_time_short(elapsed * remainf / finishf) + \&#xa;				' (' + speed + ', ' + \&#xa;				human_time_short(elapsed) + ' gone)'&#xa;	msg = '\r' + prefix + '[' + segth * '=' + (seg - segth) * '_' + ']' + \&#xa;		"" {}% ({}/{})"".format(percent, human_size(finish, 1), human_size(total, 1)) + \&#xa;		' ' + eta + suffix&#xa;	#msg = '\r' + prefix + '[' + segth * '=' + (seg - segth) * '_' + ']' + \&#xa;	#	"" {}% ({}/{})"".format(percent, human_size(finish), human_size(total)) + \&#xa;	#	' ' + eta + suffix&#xa;	sys.stderr.write(msg + ' ') # space is used as a clearer&#xa;	sys.stderr.flush()&#xa;&#xa;pprgr = pprgrc&#xa;&#xa;def remove_backslash(s):&#xa;	return s.replace(r'\/', r'/')&#xa;&#xa;def rb(s):&#xa;	return s.replace(r'\/', r'/')&#xa;&#xa;# marshaling&#xa;def str2bool(s):&#xa;	if isinstance(s, basestring):&#xa;		if s:&#xa;			sc = s.lower()[0]&#xa;			if sc == 't' or sc == 'y' or (sc >= '1' and sc <= '9'):&#xa;				return True&#xa;			else:&#xa;				return False&#xa;		else:&#xa;			return False&#xa;	else:&#xa;		# don't change&#xa;		return s&#xa;&#xa;def str2int(s):&#xa;	if isinstance(s, basestring):&#xa;		return int(s)&#xa;	else:&#xa;		# don't change&#xa;		return s&#xa;&#xa;def str2float(s):&#xa;	if isinstance(s, basestring):&#xa;		return float(s)&#xa;	else:&#xa;		# don't change&#xa;		return s&#xa;&#xa;def human_time(seconds):&#xa;	''' DocTests:&#xa;	>>> human_time(0)&#xa;	u''&#xa;	>>> human_time(122.1)&#xa;	u'2m2s'&#xa;	>>> human_time(133)&#xa;	u'2m13s'&#xa;	>>> human_time(12345678)&#xa;	u'20W2D21h21m18s'&#xa;	'''&#xa;	isec = int(seconds)&#xa;	s = isec % 60&#xa;	m = isec / 60 % 60&#xa;	h = isec / 60 / 60 % 24&#xa;	d = isec / 60 / 60 / 24 % 7&#xa;	w = isec / 60 / 60 / 24 / 7&#xa;&#xa;	result = ''&#xa;	for t in [ ('W', w), ('D', d), ('h', h), ('m', m), ('s', s) ]:&#xa;		if t[1]:&#xa;			result += str(t[1]) + t[0]&#xa;&#xa;	return result&#xa;&#xa;def limit_unit(timestr, num = 2):&#xa;	''' DocTests:&#xa;	>>> limit_unit('1m2s', 1)&#xa;	u'1m'&#xa;	>>> limit_unit('1m2s')&#xa;	u'1m2s'&#xa;	>>> limit_unit('1m2s', 4)&#xa;	u'1m2s'&#xa;	>>> limit_unit('1d2h3m2s')&#xa;	u'1d2h'&#xa;	>>> limit_unit('1d2h3m2s', 1)&#xa;	u'1d'&#xa;	'''&#xa;	l = len(timestr)&#xa;	i = 0&#xa;	p = 0&#xa;	while i < num and p <= l:&#xa;		at = 0&#xa;		while p < l:&#xa;			c = timestr[p]&#xa;			if at == 0:&#xa;				if c.isdigit():&#xa;					p += 1&#xa;				else:&#xa;					at += 1&#xa;			elif at == 1:&#xa;				if not c.isdigit():&#xa;					p += 1&#xa;				else:&#xa;					at += 1&#xa;			else:&#xa;				break&#xa;&#xa;		i += 1&#xa;&#xa;	return timestr[:p]&#xa;&#xa;def human_time_short(seconds):&#xa;	return limit_unit(human_time(seconds))&#xa;&#xa;def interpret_size(si):&#xa;	'''&#xa;	>>> interpret_size(10)&#xa;	10&#xa;	>>> interpret_size('10')&#xa;	10&#xa;	>>> interpret_size('10b')&#xa;	10&#xa;	>>> interpret_size('10k')&#xa;	10240&#xa;	>>> interpret_size('10K')&#xa;	10240&#xa;	>>> interpret_size('10kb')&#xa;	10240&#xa;	>>> interpret_size('10kB')&#xa;	10240&#xa;	>>> interpret_size('a10')&#xa;	Traceback (most recent call last):&#xa;	ValueError&#xa;	>>> interpret_size('10a')&#xa;	Traceback (most recent call last):&#xa;	KeyError: 'A'&#xa;	'''&#xa;	m = re.match(r""\s*(\d+)\s*([ac-z]?)(b?)\s*$"", str(si), re.I)&#xa;	if m:&#xa;		if not m.group(2) and m.group(3):&#xa;			times = 1&#xa;		else:&#xa;			times = SIPrefixTimes[m.group(2).upper()] if m.group(2) else 1&#xa;		return int(m.group(1)) * times&#xa;	else:&#xa;		raise ValueError&#xa;&#xa;def human_num(num, precision = 0, filler = ''):&#xa;	# https://stackoverflow.com/questions/15263597/python-convert-floating-point-number-to-certain-precision-then-copy-to-string/15263885#15263885&#xa;	numfmt = '{{:.{}f}}'.format(precision)&#xa;	exp = math.log(num, OneK) if num > 0 else 0&#xa;	expint = int(math.floor(exp))&#xa;	maxsize = len(SIPrefixNames) - 1&#xa;	if expint > maxsize:&#xa;		pwarn(""Ridiculously large number '{}' pased to 'human_num()'"".format(num))&#xa;		expint = maxsize&#xa;	unit = SIPrefixNames[expint]&#xa;	return numfmt.format(num / float(OneK ** expint)) + filler + unit&#xa;&#xa;def human_size(num, precision = 3):&#xa;	''' DocTests:&#xa;	>>> human_size(1000, 0)&#xa;	u'1000B'&#xa;	>>> human_size(1025)&#xa;	u'1.001kB'&#xa;	'''&#xa;	return human_num(num, precision) + 'B'&#xa;&#xa;def human_speed(speed, precision = 0):&#xa;	return human_num(speed, precision) + 'B/s'&#xa;&#xa;# no leading, trailing '/'&#xa;# remote path rule:&#xa;#  - all public methods of ByPy shall accept remote path as ""partial path""&#xa;#    (before calling get_pcs_path())&#xa;#  - all private methods of ByPy shall accept remote path as ""full path""&#xa;#    (after calling get_pcs_path())&#xa;def get_pcs_path(path):&#xa;	if not path or path == '/' or path == '\\':&#xa;		return AppPcsPath&#xa;&#xa;	return (AppPcsPath + '/' + path.strip('/')).rstrip('/')&#xa;&#xa;# guarantee no-exception&#xa;def copyfile(src, dst):&#xa;	result = ENoError&#xa;	try:&#xa;		shutil.copyfile(src, dst)&#xa;	except (shutil.Error, IOError) as ex:&#xa;		perr(""Fail to copy '{}' to '{}'.\nException:\n{}\nStack:{}\n"".format(&#xa;			src, dst, ex, traceback.format_exc()))&#xa;		result = EFailToCreateLocalFile&#xa;&#xa;	return result&#xa;&#xa;def movefile(src, dst):&#xa;	result = ENoError&#xa;	try:&#xa;		shutil.move(src, dst)&#xa;	except (shutil.Error, OSError) as ex:&#xa;		perr(""Fail to move '{}' to '{}'.\nException:\n{}\nStack:\n{}\n"".format(&#xa;			src, dst, ex, traceback.format_exc()))&#xa;		result = EFailToCreateLocalFile&#xa;&#xa;	return result&#xa;&#xa;def removefile(path, verbose = False):&#xa;	result = ENoError&#xa;	try:&#xa;		if verbose:&#xa;			pr(""Removing local file '{}'"".format(path))&#xa;		if path:&#xa;			os.remove(path)&#xa;	except Exception as ex:&#xa;		perr(""Fail to remove local fle '{}'.\nException:\n{}\nStack:{}\n"".format(&#xa;			path, ex, traceback.format_exc()))&#xa;		result = EFailToDeleteFile&#xa;&#xa;	return result&#xa;&#xa;def removedir(path, verbose = False):&#xa;	result = ENoError&#xa;	try:&#xa;		if verbose:&#xa;			pr(""Removing local directory '{}'"".format(path))&#xa;		if path:&#xa;			shutil.rmtree(path)&#xa;	except Exception as ex:&#xa;		perr(""Fail to remove local directory '{}'.\nException:\n{}\nStack:{}\n"".format(&#xa;			path, ex, traceback.format_exc()))&#xa;		result = EFailToDeleteDir&#xa;&#xa;	return result&#xa;&#xa;def makedir(path, mode = 0o777, verbose = False):&#xa;	result = ENoError&#xa;&#xa;	if verbose:&#xa;		pr(""Creating local directory '{}'"".format(path))&#xa;&#xa;	if path and not os.path.exists(path):&#xa;		try:&#xa;			os.makedirs(path, mode)&#xa;		except os.error as ex:&#xa;			perr(""Failed at creating local dir '{}'.\nException:\n{}\nStack:{}\n"".format(&#xa;				path, ex, traceback.format_exc()))&#xa;			result = EFailToCreateLocalDir&#xa;&#xa;	return result&#xa;&#xa;# guarantee no-exception&#xa;def getfilesize(path):&#xa;	size = -1&#xa;	try:&#xa;		size = os.path.getsize(path)&#xa;	except os.error:&#xa;		perr(""Exception occured while getting size of '{}'. Exception:\n{}"".format(path, traceback.format_exc()))&#xa;&#xa;	return size&#xa;&#xa;# guarantee no-exception&#xa;def getfilemtime(path):&#xa;	mtime = -1&#xa;	try:&#xa;		mtime = os.path.getmtime(path)&#xa;	except os.error:&#xa;		perr(""Exception occured while getting modification time of '{}'. Exception:\n{}"".format(path, traceback.format_exc()))&#xa;&#xa;	return mtime&#xa;&#xa;# seems os.path.join() doesn't handle Unicode well&#xa;def joinpath(first, second, sep = os.sep):&#xa;	head = ''&#xa;	if first:&#xa;		head = first.rstrip(sep) + sep&#xa;&#xa;	tail = ''&#xa;	if second:&#xa;		tail = second.lstrip(sep)&#xa;&#xa;	return head + tail&#xa;&#xa;def donothing():&#xa;	pass&#xa;&#xa;# https://urllib3.readthedocs.org/en/latest/security.html#insecurerequestwarning&#xa;def disable_urllib3_warning():&#xa;	try:&#xa;		import requests.packages.urllib3&#xa;		requests.packages.urllib3.disable_warnings()&#xa;	except:&#xa;		pass&#xa;&#xa;# https://stackoverflow.com/questions/10883399/unable-to-encode-decode-pprint-output&#xa;class MyPrettyPrinter(pprint.PrettyPrinter):&#xa;	def format(self, obj, context, maxlevels, level):&#xa;		if isinstance(obj, unicode):&#xa;			#return (obj.encode('utf8'), True, False)&#xa;			return (obj, True, False)&#xa;		if isinstance(obj, str):&#xa;			convert = False&#xa;			#for c in obj:&#xa;			#	if ord(c) >= 128:&#xa;			#		convert = True&#xa;			#		break&#xa;			try:&#xa;				codecs.decode(obj)&#xa;			except:&#xa;				convert = True&#xa;			if convert:&#xa;				return (""0x{}"".format(binascii.hexlify(obj)), True, False)&#xa;		return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)&#xa;&#xa;# there is room for more space optimization (like using the tree structure),&#xa;# but it's not added at the moment. for now, it's just simple pickle.&#xa;# SQLite might be better for portability&#xa;# NOTE: file names are case-sensitive&#xa;class cached(object):&#xa;	''' simple decorator for hash caching (using pickle) '''&#xa;	usecache = True&#xa;	verbose = False&#xa;	debug = False&#xa;	cache = {}&#xa;	cacheloaded = False&#xa;	dirty = False&#xa;	# we don't do cache loading / unloading here because it's an decorator,&#xa;	# and probably multiple instances are created for md5, crc32, etc&#xa;	# it's a bit complex, and i thus don't have the confidence to do it in ctor/dtor&#xa;	def __init__(self, f):&#xa;		self.f = f&#xa;&#xa;	def __call__(self, *args):&#xa;		assert len(args) > 0&#xa;		result = None&#xa;		path = args[0]&#xa;		dir, file = os.path.split(path) # the 'filename' parameter&#xa;		absdir = os.path.abspath(dir)&#xa;		if absdir in cached.cache:&#xa;			entry = cached.cache[absdir]&#xa;			if file in entry:&#xa;				info = entry[file]&#xa;				if self.f.__name__ in info \&#xa;					and info['size'] == getfilesize(path) \&#xa;					and info['mtime'] == getfilemtime(path) \&#xa;					and self.f.__name__ in info \&#xa;					and cached.usecache:&#xa;					result = info[self.f.__name__]&#xa;					if cached.debug:&#xa;						pdbg(""Cache hit for file '{}',\n{}: {}\nsize: {}\nmtime: {}"".format(&#xa;							path, self.f.__name__,&#xa;							result if isinstance(result, (int, long, float, complex)) else binascii.hexlify(result),&#xa;							info['size'], info['mtime']))&#xa;				else:&#xa;					result = self.f(*args)&#xa;					self.__store(info, path, result)&#xa;			else:&#xa;				result = self.f(*args)&#xa;				entry[file] = {}&#xa;				info = entry[file]&#xa;				self.__store(info, path, result)&#xa;		else:&#xa;			result = self.f(*args)&#xa;			cached.cache[absdir] = {}&#xa;			entry = cached.cache[absdir]&#xa;			entry[file] = {}&#xa;			info = entry[file]&#xa;			self.__store(info, path, result)&#xa;&#xa;		return result&#xa;&#xa;	def __store(self, info, path, value):&#xa;		cached.dirty = True&#xa;		info['size'] = getfilesize(path)&#xa;		info['mtime'] = getfilemtime(path)&#xa;		info[self.f.__name__] = value&#xa;		if cached.debug:&#xa;			situation = ""Storing cache""&#xa;			if cached.usecache:&#xa;				situation = ""Cache miss""&#xa;			pdbg((situation + "" for file '{}',\n{}: {}\nsize: {}\nmtime: {}"").format(&#xa;				path, self.f.__name__,&#xa;				value if isinstance(value, (int, long, float, complex)) else binascii.hexlify(value),&#xa;				info['size'], info['mtime']))&#xa;&#xa;		# periodically save to prevent loss in case of system crash&#xa;		global last_cache_save&#xa;		now = time.time()&#xa;		if now - last_cache_save >= CacheSavePeriodInSec:&#xa;			cached.savecache()&#xa;			last_cache_save = now&#xa;		if cached.debug:&#xa;			pdbg(""Periodically saving Hash Cash"")&#xa;&#xa;	@staticmethod&#xa;	def loadcache():&#xa;		# load cache even we don't use cached hash values,&#xa;		# because we will save (possibly updated) and hash values&#xa;		if not cached.cacheloaded: # no double-loading&#xa;			if cached.verbose:&#xa;				pr(""Loading Hash Cache File '{}'..."".format(HashCachePath))&#xa;&#xa;			if os.path.exists(HashCachePath):&#xa;				try:&#xa;					with open(HashCachePath, 'rb') as f:&#xa;						cached.cache = pickle.load(f)&#xa;					cached.cacheloaded = True&#xa;					if cached.verbose:&#xa;						pr(""Hash Cache File loaded."")&#xa;				except (&#xa;					pickle.PickleError,&#xa;					# the following is for dealing with corrupted cache file&#xa;					EOFError, TypeError, ValueError):&#xa;					perr(""Fail to load the Hash Cache, no caching. Exception:\n{}"".format(traceback.format_exc()))&#xa;					cached.cache = {}&#xa;			else:&#xa;				if cached.verbose:&#xa;					pr(""Hash Cache File not found, no caching"")&#xa;		else:&#xa;			if cached.verbose:&#xa;				pr(""Not loading Hash Cache since 'cacheloaded' is '{}'"".format( cached.cacheloaded))&#xa;&#xa;		return cached.cacheloaded&#xa;&#xa;	@staticmethod&#xa;	def savecache(force_saving = False):&#xa;		saved = False&#xa;		# even if we were unable to load the cache, we still save it.&#xa;		if cached.dirty or force_saving:&#xa;			if cached.verbose:&#xa;				pr(""Saving Hash Cache..."")&#xa;&#xa;			try:&#xa;				with open(HashCachePath, 'wb') as f:&#xa;					pickle.dump(cached.cache, f)&#xa;					f.close()&#xa;				if cached.verbose:&#xa;					pr(""Hash Cache saved."")&#xa;				saved = True&#xa;				cached.dirty = False&#xa;			except Exception:&#xa;				perr(""Failed to save Hash Cache. Exception:\n{}"".format(traceback.format_exc()))&#xa;&#xa;		else:&#xa;			if cached.verbose:&#xa;				pr(""Not saving Hash Cache since 'dirty' is '{}' and 'force_saving' is '{}'"".format(&#xa;					cached.dirty, force_saving))&#xa;&#xa;		return saved&#xa;&#xa;	@staticmethod&#xa;	def cleancache():&#xa;		if cached.loadcache():&#xa;			for absdir in cached.cache.keys():&#xa;				if not os.path.exists(absdir):&#xa;					if cached.verbose:&#xa;						pr(""Directory: '{}' no longer exists, removing the cache entries"".format(absdir))&#xa;					cached.dirty = True&#xa;					del cached.cache[absdir]&#xa;				else:&#xa;					oldfiles = cached.cache[absdir]&#xa;					files = {}&#xa;					needclean = False&#xa;					for f in oldfiles.keys():&#xa;						#p = os.path.join(absdir, f)&#xa;						p = joinpath(absdir, f)&#xa;						if os.path.exists(p):&#xa;							files[f] = oldfiles[f]&#xa;						else:&#xa;							if cached.verbose:&#xa;								needclean = True&#xa;								pr(""File '{}' no longer exists, removing the cache entry"".format(p))&#xa;&#xa;					if needclean:&#xa;						cached.dirty = True&#xa;						cached.cache[absdir] = files&#xa;		cached.savecache()&#xa;&#xa;@cached&#xa;def md5(filename, slice = OneM):&#xa;	m = hashlib.md5()&#xa;	with open(filename, ""rb"") as f:&#xa;		while True:&#xa;			buf = f.read(slice)&#xa;			if buf:&#xa;				m.update(buf)&#xa;			else:&#xa;				break&#xa;&#xa;	return m.digest()&#xa;&#xa;# slice md5 for baidu rapidupload&#xa;@cached&#xa;def slice_md5(filename):&#xa;	m = hashlib.md5()&#xa;	with open(filename, ""rb"") as f:&#xa;		buf = f.read(256 * OneK)&#xa;		m.update(buf)&#xa;&#xa;	return m.digest()&#xa;&#xa;@cached&#xa;def crc32(filename, slice = OneM):&#xa;	with open(filename, ""rb"") as f:&#xa;		buf = f.read(slice)&#xa;		crc = binascii.crc32(buf)&#xa;		while True:&#xa;			buf = f.read(slice)&#xa;			if buf:&#xa;				crc = binascii.crc32(buf, crc)&#xa;			else:&#xa;				break&#xa;&#xa;	return crc & 0xffffffff&#xa;&#xa;def enable_http_logging():&#xa;	httplib.HTTPConnection.debuglevel = 1&#xa;&#xa;	logging.basicConfig() # you need to initialize logging, otherwise you will not see anything from requests&#xa;	logging.getLogger().setLevel(logging.DEBUG)&#xa;	requests_log = logging.getLogger(""requests.packages.urllib3"")&#xa;	requests_log.setLevel(logging.DEBUG)&#xa;	requests_log.propagate = True&#xa;&#xa;def ls_type(isdir):&#xa;	return 'D' if isdir else 'F'&#xa;&#xa;def ls_time(itime):&#xa;	return time.strftime('%Y-%m-%d, %H:%M:%S', time.localtime(itime))&#xa;&#xa;def print_pcs_list(json, foundmsg = ""Found:"", notfoundmsg = ""Nothing found.""):&#xa;	list = json['list']&#xa;	if list:&#xa;		pr(foundmsg)&#xa;		for f in list:&#xa;			pr(""{} {} {} {} {} {}"".format(&#xa;				ls_type(f['isdir']),&#xa;				f['path'],&#xa;				f['size'],&#xa;				ls_time(f['ctime']),&#xa;				ls_time(f['mtime']),&#xa;				f['md5']))&#xa;	else:&#xa;		pr(notfoundmsg)&#xa;&#xa;# tree represented using dictionary, (Obsolete: OrderedDict no longer required)&#xa;# NOTE: No own-name is kept, so the caller needs to keep track of that&#xa;# NOTE: Case-sensitive, as I don't want to waste time wrapping up a case-insensitive one&#xa;# single-linked-list, no backwards travelling capability&#xa;class PathDictTree(dict):&#xa;	def __init__(self, type = 'D', **kwargs):&#xa;		self.type = type&#xa;		self.extra = {}&#xa;		for k, v in kwargs.items():&#xa;			self.extra[k] = v&#xa;		super(PathDictTree, self).__init__()&#xa;&#xa;	def __str__(self):&#xa;		return self.__str('')&#xa;&#xa;	def __str(self, prefix):&#xa;		result = ''&#xa;		for k, v in self.iteritems():&#xa;			result += ""{} - {}/{} - size: {} - md5: {} \n"".format(&#xa;				v.type, prefix, k,&#xa;				v.extra['size'] if 'size' in v.extra else '',&#xa;				binascii.hexlify(v.extra['md5']) if 'md5' in v.extra else '')&#xa;&#xa;		for k, v in self.iteritems():&#xa;			if v.type == 'D':&#xa;				result += v.__str(prefix + '/' + k)&#xa;&#xa;		return result&#xa;&#xa;	def add(self, name, child):&#xa;		self[name] = child&#xa;		return child&#xa;&#xa;	# returns the child tree at the given path&#xa;	# assume that path is only separated by '/', instead of '\\'&#xa;	def get(self, path):&#xa;		place = self&#xa;		if path:&#xa;			# Linux can have file / folder names with '\\'?&#xa;			if sys.platform.startswith('win32'):&#xa;				assert '\\' not in path&#xa;			route = filter(None, path.split('/'))&#xa;			for part in route:&#xa;				if part in place:&#xa;					sub = place[part]&#xa;					assert place.type == 'D' # sanity check&#xa;					place = sub&#xa;				else:&#xa;					return None&#xa;&#xa;		return place&#xa;&#xa;	# return a string list of all 'path's in the tree&#xa;	def allpath(self):&#xa;		result = []&#xa;&#xa;		for k, v in self.items():&#xa;			result.append(k)&#xa;			if v.type == 'D':&#xa;				for p in self.get(k).allpath():&#xa;					result.append(k + '/' + p)&#xa;&#xa;		return result&#xa;&#xa;class ByPy(object):&#xa;	'''The main class of the bypy program'''&#xa;&#xa;	# public static properties&#xa;	HelpMarker = ""Usage:""&#xa;&#xa;	ListFormatDict = {&#xa;		'$t' : (lambda json: ls_type(json['isdir'])),&#xa;		'$f' : (lambda json: json['path'].split('/')[-1]),&#xa;		'$c' : (lambda json: ls_time(json['ctime'])),&#xa;		'$m' : (lambda json: ls_time(json['mtime'])),&#xa;		'$d' : (lambda json: str(json['md5'] if 'md5' in json else '')),&#xa;		'$s' : (lambda json: str(json['size'])),&#xa;		'$i' : (lambda json: str(json['fs_id'])),&#xa;		'$b' : (lambda json: str(json['block_list'] if 'block_list' in json else '')),&#xa;		'$u' : (lambda json: 'HasSubDir' if 'ifhassubdir' in json and json['ifhassubdir'] else 'NoSubDir'),&#xa;		'$$' : (lambda json: '$')&#xa;	}&#xa;&#xa;	# Old setting locations, should be moved to ~/.bypy to be clean&#xa;	OldTokenFilePath = HomeDir + os.sep + '.bypy.json'&#xa;	OldHashCachePath = HomeDir + os.sep + '.bypy.pickle'&#xa;&#xa;	@staticmethod&#xa;	def migratesettings():&#xa;		result = ENoError&#xa;&#xa;		filesToMove = [&#xa;			[ByPy.OldTokenFilePath, TokenFilePath],&#xa;			[ByPy.OldHashCachePath, HashCachePath]&#xa;		]&#xa;&#xa;		result = makedir(ConfigDir, 0o700) and result # make it secretive&#xa;		# this directory must exist&#xa;		if result != ENoError:&#xa;			perr(""Fail to create config directory '{}'"".format(ConfigDir))&#xa;			return result&#xa;&#xa;		for tomove in filesToMove:&#xa;			oldfile = tomove[0]&#xa;			newfile = tomove[1]&#xa;			if os.path.exists(oldfile):&#xa;				dst = newfile&#xa;				if os.path.exists(newfile):&#xa;					dst = TokenFilePath + '.old'&#xa;				result = movefile(oldfile, dst) and result&#xa;&#xa;		return result&#xa;&#xa;	@staticmethod&#xa;	def getcertfile():&#xa;		result = ENoError&#xa;		if not os.path.exists(ByPyCertsPath):&#xa;			if os.path.exists(ByPyCertsFile):&#xa;				result = copyfile(ByPyCertsFile, ByPyCertsPath)&#xa;			else:&#xa;				try:&#xa;					# perform a simple download from github&#xa;					urllib.urlretrieve(&#xa;					'https://raw.githubusercontent.com/houtianze/bypy/master/bypy.cacerts.pem', ByPyCertsPath)&#xa;				except IOError as ex:&#xa;					perr(""Fail download CA Certs to '{}'.\n"" + \&#xa;						""Exception:\n{}\nStack:{}\n"".format(&#xa;						ByPyCertsPath, ex, traceback.format_exc()))&#xa;&#xa;					result = EDownloadCerts	&#xa;&#xa;		return result&#xa;&#xa;	def __init__(self,&#xa;		slice_size = DefaultSliceSize,&#xa;		dl_chunk_size = DefaultDlChunkSize,&#xa;		verify = True,&#xa;		retry = 5, timeout = None,&#xa;		quit_when_fail = False,&#xa;		listfile = None,&#xa;		resumedownload = True,&#xa;		extraupdate = lambda: (),&#xa;		incregex = '',&#xa;		ondup = '',&#xa;		followlink = True,&#xa;		checkssl = True,&#xa;		cacerts = None,&#xa;		rapiduploadonly = False,&#xa;		verbose = 0, debug = False):&#xa;&#xa;		# handle backward compatibility&#xa;		sr = ByPy.migratesettings()&#xa;		if sr != ENoError:&#xa;			# bail out&#xa;			perr(""Failed to migrate old settings."")&#xa;			onexit(EMigrationFailed)&#xa;		# it doesn't matter if it failed, we can disable SSL verification anyway&#xa;		ByPy.getcertfile()&#xa;&#xa;		self.__slice_size = slice_size&#xa;		self.__dl_chunk_size = dl_chunk_size&#xa;		self.__verify = verify&#xa;		self.__retry = retry&#xa;		self.__quit_when_fail = quit_when_fail&#xa;		self.__timeout = timeout&#xa;		self.__listfile = listfile&#xa;		self.__resumedownload = resumedownload&#xa;		self.__extraupdate = extraupdate&#xa;		self.__incregex = incregex&#xa;		self.__incregmo = re.compile(incregex)&#xa;		if ondup and len(ondup) > 0:&#xa;			self.__ondup = ondup[0].upper()&#xa;		else:&#xa;			self.__ondup = 'O' # O - Overwrite* S - Skip P - Prompt&#xa;		# TODO: whether this works is still to be tried out&#xa;		self.__isrev = False&#xa;		self.__followlink = followlink;&#xa;&#xa;		# TODO: properly fix this InsecurePlatformWarning&#xa;		checkssl = False&#xa;		# using a mirror, which has name mismatch SSL error,&#xa;		# so need to disable SSL check&#xa;		if pcsurl != PcsUrl:&#xa;			# TODO: print a warning&#xa;			checkssl = False&#xa;&#xa;		self.__checkssl = checkssl&#xa;		self.__rapiduploadonly = rapiduploadonly&#xa;&#xa;		self.Verbose = verbose&#xa;		self.Debug = debug&#xa;&#xa;		if self.__checkssl:&#xa;			# sort of undocumented by requests&#xa;			# http://stackoverflow.com/questions/10667960/python-requests-throwing-up-sslerror&#xa;			if cacerts is not None:&#xa;				if os.path.isfile(cacerts):&#xa;					self.__checkssl = cacerts&#xa;				else:&#xa;					perr(""Invalid CA Bundle '{}' specified"")&#xa;&#xa;			# falling through here means no customized CA Certs specified&#xa;			if self.__checkssl is True:&#xa;				# use our own CA Bundle if possible&#xa;				if os.path.isfile(ByPyCertsPath):&#xa;					self.__checkssl = ByPyCertsPath&#xa;				else:&#xa;					# Well, disable cert verification&#xa;					pwarn(&#xa;""** SSL Certificate Verification has been disabled **\n\n"" + \&#xa;""If you are confident that your CA Bundle can verify "" + \&#xa;""Baidu PCS's certs, you can run the prog with the '"" + CaCertsOption + \&#xa;"" <your ca cert path>' argument to enable SSL cert verification.\n\n"" + \&#xa;""However, most of the time, you can ignore this warning, "" + \&#xa;""you are going to send sensitive data to the cloud plainly right?"")&#xa;					self.__checkssl = False&#xa;&#xa;		if not checkssl:&#xa;			disable_urllib3_warning()&#xa;&#xa;		# the prophet said: thou shalt initialize&#xa;		self.__existing_size = 0&#xa;		self.__json = {}&#xa;		self.__access_token = ''&#xa;		self.__bduss = ''&#xa;		self.__pancookies = {}&#xa;		self.__remote_json = {}&#xa;		self.__slice_md5s = []&#xa;&#xa;		if self.__listfile and os.path.exists(self.__listfile):&#xa;			with open(self.__listfile, 'r') as f:&#xa;				self.__list_file_contents = f.read()&#xa;		else:&#xa;			self.__list_file_contents = None&#xa;&#xa;		# only if user specifies '-ddd' or more 'd's, the following&#xa;		# debugging information will be shown, as it's very talkative.&#xa;		if self.Debug >= 3:&#xa;			# these two lines enable debugging at httplib level (requests->urllib3->httplib)&#xa;			# you will see the REQUEST, including HEADERS and DATA, and RESPONSE with HEADERS but without DATA.&#xa;			# the only thing missing will be the response.body which is not logged.&#xa;			enable_http_logging()&#xa;&#xa;		if not self.__load_local_json():&#xa;			# no need to call __load_local_json() again as __auth() will load the json & acess token.&#xa;			result = self.__auth()&#xa;			if result != ENoError:&#xa;				perr(""Program authorization FAILED.\n"" + \&#xa;					""You need to authorize this program before using any PCS functions.\n"" + \&#xa;					""Quitting...\n"")&#xa;				onexit(result)&#xa;&#xa;		if not self.__load_local_bduss():&#xa;			self.pv(""BDUSS not found at '{}'."".format(BDUSSPath))&#xa;&#xa;	def pv(self, msg, **kwargs):&#xa;		if self.Verbose:&#xa;			pr(msg)&#xa;&#xa;	def pd(self, msg, level = 1, **kwargs):&#xa;		if self.Debug >= level:&#xa;			pdbg(msg, kwargs)&#xa;&#xa;	def shalloverwrite(self, prompt):&#xa;		if self.__ondup == 'S':&#xa;			return False&#xa;		elif self.__ondup == 'P':&#xa;			ans = ask(prompt, False).upper()&#xa;			if not ans.startswith('Y'):&#xa;				return False&#xa;&#xa;		return True&#xa;&#xa;	def __print_error_json(self, r):&#xa;		try:&#xa;			dj = r.json()&#xa;			if 'error_code' in dj and 'error_msg' in dj:&#xa;				ec = dj['error_code']&#xa;				et = dj['error_msg']&#xa;				msg = ''&#xa;				if ec == IEMD5NotFound:&#xa;					pf = pinfo&#xa;					msg = et&#xa;				else:&#xa;					pf = perr&#xa;					msg = ""Error code: {}\nError Description: {}"".format(ec, et)&#xa;				pf(msg)&#xa;		except Exception:&#xa;			perr('Error parsing JSON Error Code from:\n{}'.format(rb(r.text)))&#xa;			perr('Exception:\n{}'.format(traceback.format_exc()))&#xa;&#xa;	def __dump_exception(self, ex, url, pars, r, act):&#xa;		if self.Debug or self.Verbose:&#xa;			perr(""Error accessing '{}'"".format(url))&#xa;			if ex and isinstance(ex, Exception) and self.Debug:&#xa;				perr(""Exception:\n{}"".format(ex))&#xa;			tb = traceback.format_exc()&#xa;			if tb:&#xa;				pr(tb)&#xa;			perr(""Function: {}"".format(act.__name__))&#xa;			perr(""Website parameters: {}"".format(pars))&#xa;			if hasattr(r, 'status_code'):&#xa;				perr(""HTTP Response Status Code: {}"".format(r.status_code))&#xa;				if (r.status_code != 200 and r.status_code != 206) or (not (pars.has_key('method') and pars['method'] == 'download') and url.find('method=download') == -1 and url.find('baidupcs.com/file/') == -1):&#xa;					self.__print_error_json(r)&#xa;					perr(""Website returned: {}"".format(rb(r.text)))&#xa;&#xa;	# always append / replace the 'access_token' parameter in the https request&#xa;	def __request_work(self, url, pars, act, method, actargs = None, addtoken = True, dumpex = True, **kwargs):&#xa;		result = ENoError&#xa;		r = None&#xa;&#xa;		self.__extraupdate()&#xa;		parsnew = pars.copy()&#xa;		if addtoken:&#xa;			parsnew['access_token'] = self.__access_token&#xa;&#xa;		try:&#xa;			self.pd(method + ' ' + url)&#xa;			self.pd(""actargs: {}"".format(actargs))&#xa;			self.pd(""Params: {}"".format(pars))&#xa;&#xa;			if method.upper() == 'GET':&#xa;				r = requests.get(url,&#xa;					params = parsnew, timeout = self.__timeout, verify = self.__checkssl, **kwargs)&#xa;			elif method.upper() == 'POST':&#xa;				r = requests.post(url,&#xa;					params = parsnew, timeout = self.__timeout, verify = self.__checkssl, **kwargs)&#xa;&#xa;			# BUGFIX: DON'T do this, if we are downloading a big file, the program sticks and dies&#xa;			#self.pd(""Request Headers: {}"".format(&#xa;			#	pprint.pformat(r.request.headers)), 2)&#xa;			sc = r.status_code&#xa;			self.pd(""HTTP Status Code: {}"".format(sc))&#xa;			# BUGFIX: DON'T do this, if we are downloading a big file, the program sticks and dies&#xa;			#self.pd(""Header returned: {}"".format(pprint.pformat(r.headers)), 2)&#xa;			#self.pd(""Website returned: {}"".format(rb(r.text)), 3)&#xa;			if sc == requests.codes.ok or sc == 206: # 206 Partial Content&#xa;				if sc == requests.codes.ok:&#xa;					if pars.get('method') != 'download':&#xa;						try:&#xa;							j = r.json()&#xa;							if j.get('error_code') == 0 and j.get('error_msg') == u'no error': # __walk_remote_dir_act() KeyError: u'list'&#xa;								self.pd(""Unexpected response: {}"".format(j))&#xa;								return ERequestFailed&#xa;						except Exception:&#xa;							sys.exc_clear()&#xa;					self.pd(""Request OK, processing action"")&#xa;				else:&#xa;					self.pd(""206 Partial Content"")&#xa;				result = act(r, actargs)&#xa;				if result == ENoError:&#xa;					self.pd(""Request all goes fine"")&#xa;			elif sc == 404 and r.url.find('http://bcscdn.baidu.com/bcs-cdn/wenxintishi') == 0: # = ""error_code"":31390,""error_msg"":""Illegal File""&#xa;				self.pd(""File is blacklisted ('wenxintishi'). Skipping."")&#xa;				result = EFileNotFound&#xa;			else:&#xa;				ec = 0&#xa;				try:&#xa;					j = r.json()&#xa;					ec = j['error_code']&#xa;					# error print is done in __dump_exception()&#xa;					# self.__print_error_json(r)&#xa;				except ValueError:&#xa;					perr(""Not valid error JSON"")&#xa;&#xa;				#   6 (sc: 403): No permission to access user data&#xa;				# 110 (sc: 401): Access token invalid or no longer valid&#xa;				# 111 (sc: 401): Access token expired&#xa;				if ec == 111 or ec == 110 or ec == 6: # and sc == 401:&#xa;					self.pd(""Need to refresh token, refreshing"")&#xa;					if ENoError == self.__refresh_token(): # refresh the token and re-request&#xa;						# TODO: avoid dead recursive loops&#xa;						# TODO: properly pass retry&#xa;						result = self.__request(url, pars, act, method, actargs, True, addtoken, dumpex, **kwargs)&#xa;					else:&#xa;						result = EFatal&#xa;						perr(""FATAL: Token refreshing failed, can't continue.\nQuitting...\n"")&#xa;						onexit(result)&#xa;				# File md5 not found, you should use upload API to upload the whole file.&#xa;				elif ec == IEMD5NotFound: # and sc == 404:&#xa;					self.pd(""MD5 not found, rapidupload failed"")&#xa;					result = ec&#xa;				# user not exists&#xa;				elif ec == 31045: # and sc == 403:&#xa;					self.pd(""BDUSS has expired"")&#xa;					result = IEBDUSSExpired&#xa;				# superfile create failed&#xa;				elif ec == 31081: # and sc == 404:&#xa;					self.pd(""Failed to combine files from MD5 slices (superfile create failed)"")&#xa;					result = ec&#xa;				# topath already exists&#xa;				elif ec == 31196: # and sc == 403:&#xa;					self.pd(""UnzipCopy destination already exists."")&#xa;					result = act(r, actargs)&#xa;				# file copy failed&#xa;				elif ec == 31197: # and sc == 503:&#xa;					result = act(r, actargs)&#xa;				# file size exceeds limit&#xa;				elif ec == 31199: # and sc == 403:&#xa;					result = act(r, actargs)&#xa;				# errors that make retrying meaningless&#xa;				elif (&#xa;					ec == 31061 or # sc == 400 file already exists&#xa;					ec == 31062 or # sc == 400 file name is invalid&#xa;					ec == 31063 or # sc == 400 file parent path does not exist&#xa;					ec == 31064 or # sc == 403 file is not authorized&#xa;					ec == 31065 or # sc == 400 directory is full&#xa;					ec == 31066): # sc == 403 (indeed 404) file does not exist&#xa;					result = ec&#xa;					if dumpex:&#xa;						self.__dump_exception(None, url, pars, r, act)&#xa;				else:&#xa;					result = ERequestFailed&#xa;					if dumpex:&#xa;						self.__dump_exception(None, url, pars, r, act)&#xa;		except (requests.exceptions.RequestException,&#xa;				socket.error) as ex:&#xa;			# If certificate check failed, no need to continue&#xa;			# but prompt the user for work-around and quit&#xa;			# why so kludge? because requests' SSLError doesn't set&#xa;			# the errno and strerror due to using **kwargs,&#xa;			# so we are forced to use string matching&#xa;			if isinstance(ex, requests.exceptions.SSLError) \&#xa;				and re.match(r'^\[Errno 1\].*error:14090086.*:certificate verify failed$', str(ex), re.I):&#xa;				# [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed&#xa;				result = EFatal&#xa;				self.__dump_exception(ex, url, pars, r, act)&#xa;				perr(""\n\n== Baidu's Certificate Verification Failure ==\n"" + \&#xa;				""We couldn't verify Baidu's SSL Certificate.\n"" + \&#xa;				""It's most likely that the system doesn't have "" + \&#xa;				""the corresponding CA certificate installed.\n"" + \&#xa;				""There are two ways of solving this:\n"" + \&#xa;				""Either) Run this prog with the '"" + CaCertsOption + \&#xa;				"" <path to "" + ByPyCertsPath + ""> argument "" + \&#xa;				""("" + ByPyCertsPath + "" comes along with this prog). "" + \&#xa;				""This is the secure way. "" + \&#xa;				""However, it won't work after 2020-02-08 when "" + \&#xa;				""the certificat expires.\n"" + \&#xa;				""Or) Run this prog with the '"" + DisableSslCheckOption + \&#xa;				""' argument. This supresses the CA cert check "" + \&#xa;				""and always works.\n"")&#xa;				onexit(result)&#xa;&#xa;			# why so kludge? because requests' SSLError doesn't set&#xa;			# the errno and strerror due to using **kwargs,&#xa;			# so we are forced to use string matching&#xa;			if isinstance(ex, requests.exceptions.SSLError) \&#xa;				and re.match(r'^\[Errno 1\].*error:14090086.*:certificate verify failed$', str(ex), re.I):&#xa;				# [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed&#xa;				perr(""\n*** We probably don't have Baidu's CA Certificate ***\n"" + \&#xa;				""This in fact doesn't matter most of the time.\n\n"" + \&#xa;				""However, if you are _really_ concern about it, you can:\n"" + \&#xa;				""Either) Run this prog with the '"" + CaCertsOption + \&#xa;				"" <path to bypy.cacerts.pem>' "" + \&#xa;				""argument. This is the secure way.\n"" + \&#xa;				""Or) Run this prog with the '"" + DisableSslCheckOption + \&#xa;				""' argument. This suppresses the CA cert check.\n"")&#xa;&#xa;			result = ERequestFailed&#xa;			if dumpex:&#xa;				self.__dump_exception(ex, url, pars, r, act)&#xa;&#xa;		except Exception as ex:&#xa;			result = EFatal&#xa;			self.__dump_exception(ex, url, pars, r, act)&#xa;			perr(""Fatal Exception, no way to continue.\nQuitting...\n"")&#xa;			perr(""If the error is reproducible, run the program with `-dv` arguments again to get more info.\n"")&#xa;			onexit(result)&#xa;			# we eat the exception, and use return code as the only&#xa;			# error notification method, we don't want to mix them two&#xa;			#raise # must notify the caller about the failure&#xa;&#xa;		return result&#xa;&#xa;	def __request(self, url, pars, act, method, actargs = None, retry = True, addtoken = True, dumpex = True, **kwargs):&#xa;		tries = 1&#xa;		if retry:&#xa;			tries = self.__retry&#xa;&#xa;		i = 0&#xa;		result = ERequestFailed&#xa;&#xa;		# Change the User-Agent to avoid server fuss&#xa;		kwnew = kwargs.copy()&#xa;		if 'headers' not in kwnew:&#xa;			kwnew['headers'] = { 'User-Agent': UserAgent }&#xa;		else:&#xa;			kwnew['headers']['User-Agent'] = UserAgent&#xa;&#xa;		while True:&#xa;			result = self.__request_work(url, pars, act, method, actargs, addtoken, dumpex, **kwnew)&#xa;			i += 1&#xa;			# only ERequestFailed needs retry, other error still directly return&#xa;			if result == ERequestFailed:&#xa;				if i < tries:&#xa;					# algo changed: delay more after each failure&#xa;					delay = RetryDelayInSec * i&#xa;					perr(""Waiting {} seconds before retrying..."".format(delay))&#xa;					time.sleep(delay)&#xa;					perr(""Request Try #{} / {}"".format(i + 1, tries))&#xa;				else:&#xa;					perr(""Maximum number ({}) of tries failed."".format(tries))&#xa;					if self.__quit_when_fail:&#xa;						onexit(EMaxRetry)&#xa;					break&#xa;			else:&#xa;				break&#xa;&#xa;		return result&#xa;&#xa;	def __get(self, url, pars, act, actargs = None, retry = True, addtoken = True, dumpex = True, **kwargs):&#xa;		return self.__request(url, pars, act, 'GET', actargs, retry, addtoken, dumpex, **kwargs)&#xa;&#xa;	def __post(self, url, pars, act, actargs = None, retry = True, addtoken = True, dumpex = True, **kwargs):&#xa;		return self.__request(url, pars, act, 'POST', actargs, retry, addtoken, dumpex, **kwargs)&#xa;&#xa;	# direction: True - upload, False - download&#xa;	def __shallinclude(self, lpath, rpath, direction):&#xa;		arrow = '==>' if direction else '<=='&#xa;		checkpath = lpath if direction else rpath&#xa;		# TODO: bad practice, see os.access() document for more info&#xa;		if direction: # upload&#xa;			if not os.path.exists(lpath):&#xa;				perr(""'{}' {} '{}' skipped since local path no longer exists"".format(&#xa;					lpath, arrow, rpath));&#xa;				return False&#xa;		else: # download&#xa;			if os.path.exists(lpath) and (not os.access(lpath, os.R_OK)):&#xa;				perr(""'{}' {} '{}' skipped due to permission"".format(&#xa;					lpath, arrow, rpath));&#xa;				return False&#xa;&#xa;		if '\\' in os.path.basename(checkpath):&#xa;			perr(""'{}' {} '{}' skipped due to problemic '\\' in the path"".format(&#xa;				lpath, arrow, rpath));&#xa;			return False&#xa;&#xa;		include = (not self.__incregex) or self.__incregmo.match(checkpath)&#xa;		if not include:&#xa;			self.pv(""'{}' {} '{}' skipped as it's not included in the regex pattern"".format(&#xa;				lpath, arrow, rpath));&#xa;&#xa;		return include&#xa;&#xa;	def __replace_list_format(self, fmt, j):&#xa;		output = fmt&#xa;		for k, v in ByPy.ListFormatDict.iteritems():&#xa;			output = output.replace(k, v(j))&#xa;		return output&#xa;&#xa;	def __load_local_json(self):&#xa;		try:&#xa;			with open(TokenFilePath, 'rb') as infile:&#xa;				self.__json = json.load(infile)&#xa;				self.__access_token = self.__json['access_token']&#xa;				self.pd(""Token loaded:"")&#xa;				self.pd(self.__json)&#xa;				return True&#xa;		except IOError:&#xa;			perr('Error while loading baidu pcs token:')&#xa;			perr(traceback.format_exc())&#xa;			return False&#xa;&#xa;	def __store_json_only(self, j):&#xa;		self.__json = j&#xa;		self.__access_token = self.__json['access_token']&#xa;		self.pd(""access token: "" + self.__access_token)&#xa;		self.pd(""Authorize JSON:"")&#xa;		self.pd(self.__json)&#xa;		tokenmode = 0o600&#xa;		try:&#xa;			with open(TokenFilePath, 'wb') as outfile:&#xa;				json.dump(self.__json, outfile)&#xa;&#xa;			os.chmod(TokenFilePath, tokenmode)&#xa;			return ENoError&#xa;		except Exception:&#xa;			perr(""Exception occured while trying to store access token:\n"" \&#xa;				""Exception:\n{}"".format(traceback.format_exc()))&#xa;			return EFileWrite&#xa;&#xa;	def __store_json(self, r):&#xa;		j = {}&#xa;		try:&#xa;			j = r.json()&#xa;		except Exception:&#xa;			perr(""Failed to decode JSON:\n"" \&#xa;				""Exception:\n{}"".format(traceback.format_exc()))&#xa;			perr(""Error response:\n{}"".format(r.text));&#xa;			pinfo('-' * 64)&#xa;			pinfo(""""""This is most likely caused by authorization errors.&#xa;Possible causes:&#xa; - You didn't run this program for a long time (more than a month).&#xa; - You changed your Baidu password after authorizing this program.&#xa; - You didn't give this program the 'netdisk' access while authorizing.&#xa; - ...&#xa;Possible fixes:&#xa; 1. Remove the authorization token by running with the parameter '{}', and then re-run this program.&#xa; 2. If (1) still doesn't solve the problem, you may have to go to:&#xa;    https://passport.baidu.com/accountbind&#xa;    and remove the authorization of this program, and then re-run this program."""""".format(CleanOptionShort))&#xa;			return EInvalidJson&#xa;		return self.__store_json_only(j)&#xa;&#xa;	def __load_local_bduss(self):&#xa;		try:&#xa;			with open(BDUSSPath, 'rb') as infile:&#xa;				self.__bduss = infile.readline().strip()&#xa;				self.pd(""BDUSS loaded: {}"".format(self.__bduss))&#xa;				self.__pancookies = {'BDUSS': self.__bduss}&#xa;				return True&#xa;		except IOError:&#xa;			self.pd('Error loading BDUSS:')&#xa;			self.pd(traceback.format_exc())&#xa;			return False&#xa;&#xa;	def __server_auth_act(self, r, args):&#xa;		return self.__store_json(r)&#xa;&#xa;	def __server_auth(self):&#xa;		params = {&#xa;			'client_id' : ApiKey,&#xa;			'response_type' : 'code',&#xa;			'redirect_uri' : 'oob',&#xa;			'scope' : 'basic netdisk' }&#xa;		pars = urllib.urlencode(params)&#xa;		msg = 'Please visit:\n{}\nAnd authorize this app'.format(ServerAuthUrl + '?' + pars) + \&#xa;			'\nPaste the Authorization Code here within 10 minutes.'&#xa;		auth_code = ask(msg).strip()&#xa;		self.pd(""auth_code: {}"".format(auth_code))&#xa;		pr('Authorizing, please be patient, it may take up to {} seconds...'.format(self.__timeout))&#xa;&#xa;		pars = {&#xa;			'code' : auth_code,&#xa;			'redirect_uri' : 'oob' }&#xa;&#xa;		result = None&#xa;		for auth in AuthServerList:&#xa;			(url, retry, msg) = auth&#xa;			pr(msg)&#xa;			result = self.__get(url, pars, self.__server_auth_act, retry = retry, addtoken = False)&#xa;			if result == ENoError:&#xa;				break&#xa;&#xa;		if result == ENoError:&#xa;			pr(""Successfully authorized"")&#xa;		else:&#xa;			perr(""Fatal: All server authorizations failed."")&#xa;&#xa;		return result&#xa;&#xa;	def __device_auth_act(self, r, args):&#xa;		dj = r.json()&#xa;		return self.__get_token(dj)&#xa;&#xa;	def __device_auth(self):&#xa;		pars = {&#xa;			'client_id' : ApiKey,&#xa;			'response_type' : 'device_code',&#xa;			'scope' : 'basic'}&#xa;		return self.__get(DeviceAuthUrl, pars, self.__device_auth_act, addtoken = False)&#xa;&#xa;	def __auth(self):&#xa;		if ServerAuth:&#xa;			return self.__server_auth()&#xa;		else:&#xa;			return self.__device_auth()&#xa;&#xa;	def __get_token_act(self, r, args):&#xa;		return self.__store_json(r)&#xa;&#xa;	def __get_token(self, deviceJson):&#xa;		msg = 'Please visit:\n' + deviceJson['verification_url'] + \&#xa;			'\nwithin ' + str(deviceJson['expires_in']) + ' seconds\n' + \&#xa;			'Input the CODE: {}\n'.format(deviceJson['user_code']) + \&#xa;			'and Authorize this little app.\n' + \&#xa;			""Press [Enter] when you've finished\n""&#xa;		ask(msg)&#xa;&#xa;		pars = {&#xa;			'grant_type' : 'device_token',&#xa;			'code' :  deviceJson['device_code'],&#xa;			'client_id' : ApiKey,&#xa;			'client_secret' : SecretKey}&#xa;&#xa;		return self.__get(TokenUrl, pars, self.__get_token_act, addtoken = False)&#xa;&#xa;	def __refresh_token_act(self, r, args):&#xa;		return self.__store_json(r)&#xa;&#xa;	def __refresh_token(self):&#xa;		if ServerAuth:&#xa;			pr('Refreshing, please be patient, it may take upto {} seconds...'.format(self.__timeout))&#xa;&#xa;			pars = {&#xa;				'grant_type' : 'refresh_token',&#xa;				'refresh_token' : self.__json['refresh_token'] }&#xa;&#xa;			result = None&#xa;			for refresh in RefreshServerList:&#xa;				(url, retry, msg) = refresh&#xa;				pr(msg)&#xa;				result = self.__get(url, pars, self.__refresh_token_act, retry = retry, addtoken = False)&#xa;				if result == ENoError:&#xa;					break&#xa;&#xa;			if result == ENoError:&#xa;				pr(""Token successfully refreshed"")&#xa;			else:&#xa;				perr(""Token-refreshing on all the servers failed"")&#xa;&#xa;			return result&#xa;		else:&#xa;			pars = {&#xa;				'grant_type' : 'refresh_token',&#xa;				'refresh_token' : self.__json['refresh_token'],&#xa;				'client_secret' : SecretKey,&#xa;				'client_id' : ApiKey }&#xa;			return self.__post(TokenUrl, pars, self.__refresh_token_act)&#xa;&#xa;	def __quota_act(self, r, args):&#xa;		j = r.json()&#xa;		pr('Quota: ' + human_size(j['quota']))&#xa;		pr('Used: ' + human_size(j['used']))&#xa;		return ENoError&#xa;&#xa;	def help(self, command): # this comes first to make it easy to spot&#xa;		''' Usage: help command - provide some information for the command '''&#xa;		for i, v in ByPy.__dict__.iteritems():&#xa;			if callable(v) and v.__doc__ and v.__name__ == command :&#xa;				help = v.__doc__.strip()&#xa;				pos = help.find(ByPy.HelpMarker)&#xa;				if pos != -1:&#xa;					pr(""Usage: "" + help[pos + len(ByPy.HelpMarker):].strip())&#xa;&#xa;	def refreshtoken(self):&#xa;		''' Usage: refreshtoken - refresh the access token '''&#xa;		return self.__refresh_token()&#xa;&#xa;	def info(self):&#xa;		return self.quota()&#xa;&#xa;	def quota(self):&#xa;		''' Usage: quota/info - displays the quota information '''&#xa;		pars = {&#xa;			'method' : 'info' }&#xa;		return self.__get(pcsurl + 'quota', pars, self.__quota_act)&#xa;&#xa;	# return:&#xa;	#   0: local and remote files are of same size&#xa;	#   1: local file is larger&#xa;	#   2: remote file is larger&#xa;	#  -1: inconclusive (probably invalid remote json)&#xa;	def __compare_size(self, lsize, rjson):&#xa;		if 'size' in rjson:&#xa;			rsize = rjson['size']&#xa;			if lsize == rsize:&#xa;				return 0;&#xa;			elif lsize > rsize:&#xa;				return 1;&#xa;			else:&#xa;				return 2&#xa;		else:&#xa;			return -1&#xa;&#xa;	def __verify_current_file(self, j, gotlmd5):&#xa;		# if we really don't want to verify&#xa;		if self.__current_file == '/dev/null' and not self.__verify:&#xa;			return ENoError&#xa;&#xa;		rsize = 0&#xa;		rmd5 = 0&#xa;&#xa;		# always perform size check even __verify is False&#xa;		if 'size' in j:&#xa;			rsize = j['size']&#xa;		else:&#xa;			perr(""Unable to verify JSON: '{}', as no 'size' entry found"".format(j))&#xa;			return EHashMismatch&#xa;&#xa;		if 'md5' in j:&#xa;			rmd5 = binascii.unhexlify(j['md5'])&#xa;		#elif 'block_list' in j and len(j['block_list']) > 0:&#xa;		#	rmd5 = j['block_list'][0]&#xa;		#else:&#xa;		#	# quick hack for meta's 'block_list' field&#xa;		#	pwarn(""No 'md5' nor 'block_list' found in json:\n{}"".format(j))&#xa;		#	pwarn(""Assuming MD5s match, checking size ONLY."")&#xa;		#	rmd5 = self.__current_file_md5&#xa;		else:&#xa;			perr(""Unable to verify JSON: '{}', as no 'md5' entry found"".format(j))&#xa;			return EHashMismatch&#xa;&#xa;		self.pd(""Comparing local file '{}' and remote file '{}'"".format(&#xa;			self.__current_file, j['path']))&#xa;		self.pd(""Local file size : {}"".format(self.__current_file_size))&#xa;		self.pd(""Remote file size: {}"".format(rsize))&#xa;&#xa;		if self.__current_file_size == rsize:&#xa;			self.pd(""Local file and remote file sizes match"")&#xa;			if self.__verify:&#xa;				if not gotlmd5:&#xa;					self.__current_file_md5 = md5(self.__current_file)&#xa;				self.pd(""Local file MD5 : {}"".format(binascii.hexlify(self.__current_file_md5)))&#xa;				self.pd(""Remote file MD5: {}"".format(binascii.hexlify(rmd5)))&#xa;&#xa;				if self.__current_file_md5 == rmd5:&#xa;					self.pd(""Local file and remote file hashes match"")&#xa;					return ENoError&#xa;				else:&#xa;					pinfo(""Local file and remote file hashes DON'T match"")&#xa;					return EHashMismatch&#xa;			else:&#xa;				return ENoError&#xa;		else:&#xa;			pinfo(""Local file and remote file sizes DON'T match"")&#xa;			return EHashMismatch&#xa;&#xa;	def __get_file_info_act(self, r, args):&#xa;		remotefile = args&#xa;		j = r.json()&#xa;		self.pd(""List json: {}"".format(j))&#xa;		l = j['list']&#xa;		for f in l:&#xa;			if f['path'] == remotefile: # case-sensitive&#xa;				self.__remote_json = f&#xa;				self.pd(""File info json: {}"".format(self.__remote_json))&#xa;				return ENoError;&#xa;&#xa;		return EFileNotFound&#xa;&#xa;	# the 'meta' command sucks, since it doesn't supply MD5 ...&#xa;	# now the JSON is written to self.__remote_json, due to Python call-by-reference chaos&#xa;	# https://stackoverflow.com/questions/986006/python-how-do-i-pass-a-variable-by-reference&#xa;	# as if not enough confusion in Python call-by-reference&#xa;	def __get_file_info(self, remotefile, **kwargs):&#xa;		rdir, rfile = posixpath.split(remotefile)&#xa;		self.pd(""__get_file_info(): rdir : {} | rfile: {}"".format(rdir, rfile))&#xa;		if rdir and rfile:&#xa;			pars = {&#xa;				'method' : 'list',&#xa;				'path' : rdir,&#xa;				'by' : 'name', # sort in case we can use binary-search, etc in the futrue.&#xa;				'order' : 'asc' }&#xa;&#xa;			return self.__get(pcsurl + 'file', pars, self.__get_file_info_act, remotefile, **kwargs)&#xa;		else:&#xa;			perr(""Invalid remotefile '{}' specified."".format(remotefile))&#xa;			return EArgument&#xa;&#xa;	def __list_act(self, r, args):&#xa;		(remotedir, fmt) = args&#xa;		j = r.json()&#xa;		pr(""{} ({}):"".format(remotedir, fmt))&#xa;		for f in j['list']:&#xa;			pr(self.__replace_list_format(fmt, f))&#xa;&#xa;		return ENoError&#xa;&#xa;	def ls(self, remotepath = '',&#xa;		fmt = '$t $f $s $m $d',&#xa;		sort = 'name', order = 'asc'):&#xa;		return self.list(remotepath, fmt, sort, order)&#xa;&#xa;	def list(self, remotepath = '',&#xa;		fmt = '$t $f $s $m $d',&#xa;		sort = 'name', order = 'asc'):&#xa;		''' Usage: list/ls [remotepath] [format] [sort] [order] - list the 'remotepath' directory at Baidu PCS&#xa;    remotepath - the remote path at Baidu PCS. default: root directory '/'&#xa;	format - specifies how the list are displayed&#xa;	  $t - Type: Directory ('D') or File ('F')&#xa;	  $f - File name&#xa;	  $c - Creation time&#xa;	  $m - Modification time&#xa;	  $d - MD5 hash&#xa;	  $s - Size&#xa;	  $$ - The '$' sign&#xa;	  So '$t - $f - $s - $$' will display ""Type - File - Size - $'&#xa;	  Default format: '$t $f $s $m $d'&#xa;    sort - sorting by [name, time, size]. default: 'name'&#xa;    order - sorting order [asc, desc]. default: 'asc'&#xa;		'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;&#xa;		pars = {&#xa;			'method' : 'list',&#xa;			'path' : rpath,&#xa;			'by' : sort,&#xa;			'order' : order }&#xa;&#xa;		return self.__get(pcsurl + 'file', pars, self.__list_act, (rpath, fmt))&#xa;&#xa;	def __meta_act(self, r, args):&#xa;		return self.__list_act(r, args)&#xa;&#xa;	# multi-file meta is not implemented for it's low usage&#xa;	def meta(self, remotepath, fmt = '$t $u $f $s $c $m $i $b'):&#xa;		''' Usage: meta <remotepath> [format] - \&#xa;get information of the given path (dir / file) at Baidu Yun.&#xa;  remotepath - the remote path&#xa;  format - specifies how the list are displayed&#xa;    it supports all the format variables in the 'list' command, and additionally the followings:&#xa;	$i - fs_id&#xa;	$b - MD5 block_list&#xa;	$u - Has sub directory or not&#xa;'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		pars = {&#xa;			'method' : 'meta',&#xa;			'path' : rpath }&#xa;		return self.__get(pcsurl + 'file', pars,&#xa;			self.__meta_act, (rpath, fmt))&#xa;&#xa;	def __combine_file_act(self, r, args):&#xa;		result = self.__verify_current_file(r.json(), False)&#xa;		if result == ENoError:&#xa;			self.pv(""'{}' =C=> '{}' OK."".format(self.__current_file, args))&#xa;		else:&#xa;			perr(""'{}' =C=> '{}' FAILED."".format(self.__current_file, args))&#xa;		# save the md5 list, in case we add in resume function later to this program&#xa;		self.__last_slice_md5s = self.__slice_md5s&#xa;		self.__slice_md5s = []&#xa;&#xa;		return result&#xa;&#xa;	def __combine_file(self, remotepath, ondup = 'overwrite'):&#xa;		pars = {&#xa;			'method' : 'createsuperfile',&#xa;			'path' : remotepath,&#xa;			'ondup' : ondup }&#xa;		if self.__isrev and ondup != 'newcopy':&#xa;			pars['is_revision'] = 1&#xa;&#xa;		# always print this, so that we can use these data to combine file later&#xa;		pr(""Combining the following MD5 slices:"")&#xa;		for m in self.__slice_md5s:&#xa;			pr(m)&#xa;&#xa;		param = { 'block_list' : self.__slice_md5s }&#xa;		return self.__post(pcsurl + 'file',&#xa;				pars, self.__combine_file_act,&#xa;				remotepath,&#xa;				data = { 'param' : json.dumps(param) } )&#xa;&#xa;	def unzip(self, remotepath, subpath = '/', start = 0, limit = 1000):&#xa;		''' Usage: unzip <remotepath> [<subpath> [<start> [<limit>]]]'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		return self.__panapi_unzip_file(rpath, subpath, start, limit);&#xa;&#xa;	def __panapi_unzip_file_act(self, r, args):&#xa;		j = r.json()&#xa;		self.pd(""Unzip response: {}"".format(j))&#xa;		if j['errno'] == 0:&#xa;			if 'time' in j:&#xa;				perr(""Extraction not completed yet: '{}'..."".format(args['path']))&#xa;				return ERequestFailed&#xa;			elif 'list' in j:&#xa;				for e in j['list']:&#xa;					pr(""{}\t{}\t{}"".format(ls_type(e['isdir'] == 1), e['file_name'], e['size']))&#xa;		return ENoError&#xa;&#xa;	def __panapi_unzip_file(self, rpath, subpath, start, limit):&#xa;		pars = {&#xa;			'path' : rpath,&#xa;			'start' : start,&#xa;			'limit' : limit,&#xa;			'subpath' : '/' + subpath.strip('/') }&#xa;&#xa;		self.pd(""Unzip request: {}"".format(pars))&#xa;		return self.__get(PanAPIUrl + 'unzip?app_id=250528',&#xa;				pars, self.__panapi_unzip_file_act, cookies = self.__pancookies, actargs = pars )&#xa;&#xa;	def extract(self, remotepath, subpath, saveaspath = None):&#xa;		''' Usage: extract <remotepath> <subpath> [<saveaspath>]'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		topath = get_pcs_path(saveaspath)&#xa;		if not saveaspath:&#xa;			topath = os.path.dirname(rpath) + '/' + subpath&#xa;		return self.__panapi_unzipcopy_file(rpath, subpath, topath)&#xa;&#xa;	def __panapi_unzipcopy_file_act(self, r, args):&#xa;		j = r.json()&#xa;		self.pd(""UnzipCopy response: {}"".format(j))&#xa;		if 'path' in j:&#xa;			self.pv(""Remote extract: '{}#{}' =xx=> '{}' OK."".format(args['path'], args['subpath'], j[u'path']))&#xa;			return ENoError&#xa;		elif 'error_code' in j:&#xa;			if j['error_code'] == 31196:&#xa;				perr(""Remote extract: '{}#{}' =xx=> '{}' FAILED. File already exists."".format(args['path'], args['subpath'], args['topath']))&#xa;				subresult = self.__delete(args['topath'])&#xa;				if subresult == ENoError:&#xa;					return self.__panapi_unzipcopy_file(args['path'], args['subpath'], args['topath'])&#xa;				else:&#xa;					return ERequestFailed&#xa;			elif j['error_code'] == 31199:&#xa;				perr(""Remote extract: '{}#{}' =xx=> '{}' FAILED. File too large."".format(args['path'], args['subpath'], args['topath']))&#xa;				return EMaxRetry&#xa;			else:&#xa;				perr(""Remote extract: '{}#{}' =xx=> '{}' FAILED. Unknown error {}: {}."".format(args['path'], args['subpath'], args['topath'], j['error_code'], j['error_msg']))&#xa;		return EMaxRetry&#xa;&#xa;	def __panapi_unzipcopy_file(self, rpath, subpath, topath):&#xa;		pars = {&#xa;			'app_id' : 250528,&#xa;			'method' : 'unzipcopy',&#xa;			'path' : rpath,&#xa;			'subpath' : '/' + subpath.strip('/'),&#xa;			'topath' : topath }&#xa;&#xa;		self.pd(""UnzipCopy request: {}"".format(pars))&#xa;		return self.__get(pcsurl + 'file',&#xa;				pars, self.__panapi_unzipcopy_file_act, addtoken = False, cookies = self.__pancookies, actargs = pars )&#xa;&#xa;	def revision(self, remotepath):&#xa;		''' Usage: revision <remotepath> '''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		return self.__panapi_revision_list(rpath)&#xa;&#xa;	def history(self, remotepath):&#xa;		''' Usage: history <remotepath> '''&#xa;		return self.revision(remotepath)&#xa;&#xa;	def __panapi_revision_list_act(self, r, args):&#xa;		j = r.json()&#xa;		self.pd(""RevisionList response: {}"".format(j))&#xa;		if j['errno'] == 0:&#xa;			if 'list' in j:&#xa;				for e in j['list']:&#xa;					pr(""{}\t{}\t{}"".format(e['revision'], e['size'], ls_time(e['revision'] / 1e6)))&#xa;			return ENoError&#xa;		if j['errno'] == -6: # invalid BDUSS&#xa;			pr(""BDUSS has expired."")&#xa;			return IEBDUSSExpired&#xa;		if j['errno'] == -9:&#xa;			pr(""File '{}' not exists."".format(args['path']))&#xa;			return EFileNotFound&#xa;		return ENoError&#xa;&#xa;	def __panapi_revision_list(self, rpath):&#xa;		pars = {&#xa;			'path' : rpath,&#xa;			'desc' : 1 }&#xa;&#xa;		self.pd(""RevisionList request: {}"".format(pars))&#xa;		return self.__post(PanAPIUrl + 'revision/list?app_id=250528',&#xa;				{}, self.__panapi_revision_list_act, pars, data = pars, cookies = self.__pancookies )&#xa;&#xa;	def revert(self, remotepath, revision, dir = None):&#xa;		''' Usage: revert <remotepath> revisionid [dir]'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		dir = get_pcs_path(dir)&#xa;		if not dir:&#xa;			dir = os.path.dirname(rpath)&#xa;		return self.__panapi_revision_revert(rpath, revision, dir)&#xa;&#xa;	def __panapi_revision_revert_act(self, r, args):&#xa;		j = r.json()&#xa;		self.pd(""RevisionRevert response: {}"".format(j))&#xa;		if j['errno'] == 0:&#xa;			self.pv(""Remote revert: '{}#{}' =rr=> '{}' OK."".format(args['path'], args['revision'], j['path']))&#xa;			return ENoError&#xa;		if j['errno'] == -6: # invalid BDUSS&#xa;			pr(""BDUSS has expired."")&#xa;			return IEBDUSSExpired&#xa;		if j['errno'] == -9:&#xa;			pr(""File '{}' not exists."".format(args['path']))&#xa;			return EFileNotFound&#xa;		if j['errno'] == 10:&#xa;			pr(""Reverting '{}' in process..."".format(args['path']))&#xa;			return ERequestFailed&#xa;		return ENoError&#xa;&#xa;	def __panapi_revision_revert(self, rpath, revision, dir = None):&#xa;		if not dir:&#xa;			dir = os.path.dirname(rpath)&#xa;		pars = {&#xa;			'revision' : revision,&#xa;			'path' : rpath,&#xa;			'type' : 2,&#xa;			'dir' : dir }&#xa;&#xa;		self.pd(""RevisionRevert request: {}"".format(pars))&#xa;		return self.__post(PanAPIUrl + 'revision/revert?app_id=250528',&#xa;				{}, self.__panapi_revision_revert_act, pars, data = pars, cookies = self.__pancookies )&#xa;&#xa;	def __upload_slice_act(self, r, args):&#xa;		j = r.json()&#xa;		# slices must be verified and re-upload if MD5s don't match,&#xa;		# otherwise, it makes the uploading slower at the end&#xa;		rsmd5 = j['md5']&#xa;		self.pd(""Uploaded MD5 slice: "" + rsmd5)&#xa;		if self.__current_slice_md5 == binascii.unhexlify(rsmd5):&#xa;			self.__slice_md5s.append(rsmd5)&#xa;			self.pv(""'{}' >>==> '{}' OK."".format(self.__current_file, args))&#xa;			return ENoError&#xa;		else:&#xa;			perr(""'{}' >>==> '{}' FAILED."".format(self.__current_file, args))&#xa;			return EHashMismatch&#xa;&#xa;	def __upload_slice(self, remotepath):&#xa;		pars = {&#xa;			'method' : 'upload',&#xa;			'type' : 'tmpfile'}&#xa;&#xa;		return self.__post(cpcsurl + 'file',&#xa;				pars, self.__upload_slice_act, remotepath,&#xa;				# wants to be proper? properness doesn't work (search this sentence for more occurence)&#xa;				#files = { 'file' : (os.path.basename(self.__current_file), self.__current_slice) } )&#xa;				files = { 'file' : ('file', self.__current_slice) } )&#xa;&#xa;	def __upload_file_slices(self, localpath, remotepath, ondup = 'overwrite'):&#xa;		pieces = MaxSlicePieces&#xa;		slice = self.__slice_size&#xa;		if self.__current_file_size <= self.__slice_size * MaxSlicePieces:&#xa;			# slice them using slice size&#xa;			pieces = (self.__current_file_size + self.__slice_size - 1 ) / self.__slice_size&#xa;		else:&#xa;			# the following comparision is done in the caller:&#xa;			# elif self.__current_file_size <= MaxSliceSize * MaxSlicePieces:&#xa;&#xa;			# no choice, but need to slice them to 'MaxSlicePieces' pieces&#xa;			slice = (self.__current_file_size + MaxSlicePieces - 1) / MaxSlicePieces&#xa;&#xa;		self.pd(""Slice size: {}, Pieces: {}"".format(slice, pieces))&#xa;&#xa;		i = 0&#xa;		ec = ENoError&#xa;		with open(self.__current_file, 'rb') as f:&#xa;			start_time = time.time()&#xa;			while i < pieces:&#xa;				self.__current_slice = f.read(slice)&#xa;				m = hashlib.md5()&#xa;				m.update(self.__current_slice)&#xa;				self.__current_slice_md5 = m.digest()&#xa;				self.pd(""Uploading MD5 slice: {}, #{} / {}"".format(&#xa;					binascii.hexlify(self.__current_slice_md5),&#xa;					i + 1, pieces))&#xa;				j = 0&#xa;				while True:&#xa;					ec = self.__upload_slice(remotepath)&#xa;					if ec == ENoError:&#xa;						self.pd(""Slice MD5 match, continuing next slice"")&#xa;						pprgr(f.tell(), self.__current_file_size, start_time)&#xa;						break&#xa;					elif j < self.__retry:&#xa;						j += 1&#xa;						# TODO: Improve or make it TRY with the __requet retry logic&#xa;						perr(""Slice MD5 mismatch, waiting {} seconds before retrying..."".format(RetryDelayInSec))&#xa;						time.sleep(RetryDelayInSec)&#xa;						perr(""Retrying #{} / {}"".format(j + 1, self.__retry))&#xa;					else:&#xa;						self.__slice_md5s = []&#xa;						break&#xa;				i += 1&#xa;&#xa;		if ec != ENoError:&#xa;			return ec&#xa;		else:&#xa;			#self.pd(""Sleep 2 seconds before combining, just to be safer."")&#xa;			#time.sleep(2)&#xa;			return self.__combine_file(remotepath, ondup = 'overwrite')&#xa;&#xa;	def __rapidupload_file_act(self, r, args):&#xa;		if self.__verify:&#xa;			self.pd(""Not strong-consistent, sleep 1 second before verification"")&#xa;			time.sleep(1)&#xa;			return self.__verify_current_file(r.json(), True)&#xa;		else:&#xa;			return ENoError&#xa;&#xa;	def __rapidupload_file(self, localpath, remotepath, ondup = 'overwrite'):&#xa;		self.__current_file_md5 = md5(self.__current_file)&#xa;		self.__current_file_slice_md5 = slice_md5(self.__current_file)&#xa;		self.__current_file_crc32 = crc32(self.__current_file)&#xa;&#xa;		md5str = binascii.hexlify(self.__current_file_md5)&#xa;		slicemd5str =  binascii.hexlify(self.__current_file_slice_md5)&#xa;		crcstr = hex(self.__current_file_crc32)&#xa;		pars = {&#xa;			'method' : 'rapidupload',&#xa;			'path' : remotepath,&#xa;			'content-length' : self.__current_file_size,&#xa;			'content-md5' : md5str,&#xa;			'slice-md5' : slicemd5str,&#xa;			'content-crc32' : crcstr,&#xa;			'ondup' : ondup }&#xa;		if self.__isrev and ondup != 'newcopy':&#xa;			pars['is_revision'] = 1&#xa;&#xa;		self.pd(""RapidUploading Length: {} MD5: {}, Slice-MD5: {}, CRC: {}"".format(&#xa;			self.__current_file_size, md5str, slicemd5str, crcstr))&#xa;		return self.__post(pcsurl + 'file', pars, self.__rapidupload_file_act)&#xa;&#xa;	def __upload_one_file_act(self, r, args):&#xa;		result = self.__verify_current_file(r.json(), False)&#xa;		if result == ENoError:&#xa;			self.pv(""'{}' ==> '{}' OK."".format(self.__current_file, args))&#xa;		else:&#xa;			perr(""'{}' ==> '{}' FAILED."".format(self.__current_file, args))&#xa;&#xa;		return result&#xa;&#xa;	def __upload_one_file(self, localpath, remotepath, ondup = 'overwrite'):&#xa;		pars = {&#xa;			'method' : 'upload',&#xa;			'path' : remotepath,&#xa;			'ondup' : ondup }&#xa;		if self.__isrev and ondup != 'newcopy':&#xa;			pars['is_revision'] = 1&#xa;&#xa;		with open(localpath, ""rb"") as f:&#xa;			return self.__post(cpcsurl + 'file',&#xa;				pars, self.__upload_one_file_act, remotepath,&#xa;				# wants to be proper? properness doesn't work&#xa;				# there seems to be a bug at Baidu's handling of http text:&#xa;				# Content-Disposition: ...  filename=utf-8''yourfile.ext&#xa;				# (pass '-ddd' to this program to verify this)&#xa;				# when you specify a unicode file name, which will be encoded&#xa;				# using the utf-8'' syntax&#xa;				# so, we put a work-around here: we always call our file 'file'&#xa;				# NOTE: an empty file name '' doesn't seem to work, so we&#xa;				# need to give it a name at will, but empty one.&#xa;				# apperantly, Baidu PCS doesn't use this file name for&#xa;				# checking / verification, so we are probably safe here.&#xa;				#files = { 'file' : (os.path.basename(localpath), f) })&#xa;				files = { 'file' : ('file', f) })&#xa;&#xa;	#TODO: upload empty directories as well?&#xa;	def __walk_upload(self, localpath, remotepath, ondup, walk):&#xa;		(dirpath, dirnames, filenames) = walk&#xa;&#xa;		rdir = os.path.relpath(dirpath, localpath)&#xa;		if rdir == '.':&#xa;			rdir = ''&#xa;		else:&#xa;			rdir = rdir.replace('\\', '/')&#xa;&#xa;		rdir = (remotepath + '/' + rdir).rstrip('/') # '/' bites&#xa;&#xa;		result = ENoError&#xa;		for name in filenames:&#xa;			#lfile = os.path.join(dirpath, name)&#xa;			lfile = joinpath(dirpath, name)&#xa;			self.__current_file = lfile&#xa;			self.__current_file_size = getfilesize(lfile)&#xa;			rfile = rdir + '/' + name.replace('\\', '/')&#xa;			# if the corresponding file matches at Baidu Yun, then don't upload&#xa;			upload = True&#xa;			self.__isrev = False&#xa;			self.__remote_json = {}&#xa;			subresult = self.__get_file_info(rfile, dumpex = False)&#xa;			if subresult == ENoError: # same-name remote file exists&#xa;				self.__isrev = True&#xa;				if ENoError == self.__verify_current_file(self.__remote_json, False):&#xa;					# the two files are the same&#xa;					upload = False&#xa;					self.pv(""Remote file '{}' already exists, skip uploading"".format(rfile))&#xa;				else: # the two files are different&#xa;					if not self.shalloverwrite(""Remote file '{}' exists but is different, "".format(rfile) + \&#xa;							""do you want to overwrite it? [y/N]""):&#xa;						upload = False&#xa;&#xa;			if upload:&#xa;				fileresult = self.__upload_file(lfile, rfile, ondup)&#xa;				if fileresult != ENoError:&#xa;					result = fileresult # we still continue&#xa;			else:&#xa;				pinfo(""Remote file '{}' exists and is the same, skip uploading"".format(rfile))&#xa;				# next / continue&#xa;&#xa;		return result&#xa;&#xa;	def __upload_dir(self, localpath, remotepath, ondup = 'overwrite'):&#xa;		self.pd(""Uploading directory '{}' to '{}'"".format(localpath, remotepath))&#xa;		# it's so minor that we don't care about the return value&#xa;		self.__mkdir(remotepath, dumpex = False)&#xa;		for walk in os.walk(localpath, followlinks=self.__followlink):&#xa;			self.__walk_upload(localpath, remotepath, ondup, walk)&#xa;&#xa;	def __upload_file(self, localpath, remotepath, ondup = 'overwrite'):&#xa;		# TODO: this is a quick patch&#xa;		if not self.__shallinclude(localpath, remotepath, True):&#xa;			# since we are not going to upload it, there is no error&#xa;			return ENoError&#xa;&#xa;		self.__current_file = localpath&#xa;		self.__current_file_size = getfilesize(localpath)&#xa;&#xa;		result = ENoError&#xa;		if self.__current_file_size > MinRapidUploadFileSize:&#xa;			self.pd(""'{}' is being RapidUploaded."".format(self.__current_file))&#xa;			result = self.__rapidupload_file(localpath, remotepath, ondup)&#xa;			if result == ENoError:&#xa;				self.pv(""RapidUpload: '{}' =R=> '{}' OK."".format(localpath, remotepath))&#xa;			else:&#xa;				if not self.__rapiduploadonly:&#xa;					self.pd(""'{}' can't be RapidUploaded, now trying normal uploading."".format(&#xa;						self.__current_file))&#xa;					# rapid upload failed, we have to upload manually&#xa;					if self.__current_file_size <= self.__slice_size:&#xa;						self.pd(""'{}' is being non-slicing uploaded."".format(self.__current_file))&#xa;						# no-slicing upload&#xa;						result = self.__upload_one_file(localpath, remotepath, ondup)&#xa;					elif self.__current_file_size <= MaxSliceSize * MaxSlicePieces:&#xa;						# slice them using slice size&#xa;						self.pd(""'{}' is being slicing uploaded."".format(self.__current_file))&#xa;						result = self.__upload_file_slices(localpath, remotepath, ondup)&#xa;					else:&#xa;						result = EFileTooBig&#xa;						perr(""Error: size of file '{}' - {} is too big"".format(&#xa;							self.__current_file,&#xa;							self.__current_file_size))&#xa;				else:&#xa;					self.pv(""'{}' can't be rapidly uploaded, so it's skipped since we are in the rapid-upload-only mode."".format(localpath))&#xa;&#xa;			return result&#xa;		elif not self.__rapiduploadonly:&#xa;			# very small file, must be uploaded manually and no slicing is needed&#xa;			self.pd(""'{}' is small and being non-slicing uploaded."".format(self.__current_file))&#xa;			return self.__upload_one_file(localpath, remotepath, ondup)&#xa;&#xa;	def upload(self, localpath = u'', remotepath = '', ondup = ""overwrite""):&#xa;		''' Usage: upload [localpath] [remotepath] [ondup] - \&#xa;upload a file or directory (recursively)&#xa;    localpath - local path, is the current directory '.' if not specified&#xa;    remotepath - remote path at Baidu Yun (after app root directory at Baidu Yun)&#xa;    ondup - what to do upon duplication ('overwrite' or 'newcopy'), default: 'overwrite'&#xa;		'''&#xa;		# copying since Python is call-by-reference by default,&#xa;		# so we shall not modify the passed-in parameters&#xa;		lpath = localpath.rstrip('\\/ ') # no trailing slashes&#xa;		lpathbase = os.path.basename(lpath)&#xa;		rpath = remotepath&#xa;		if not lpath:&#xa;			# so, if you don't specify the local path, it will always be the current direcotry&#xa;			# and thus isdir(localpath) is always true&#xa;			lpath = os.path.abspath(""."")&#xa;			self.pd(""localpath not set, set it to current directory '{}'"".format(localpath))&#xa;&#xa;		if os.path.isfile(lpath):&#xa;			self.pd(""Uploading file '{}'"".format(lpath))&#xa;			if not rpath or rpath == '/': # to root we go&#xa;				rpath = lpathbase&#xa;			if rpath[-1] == '/': # user intends to upload to this DIR&#xa;				rpath = get_pcs_path(rpath + lpathbase)&#xa;			else:&#xa;				rpath = get_pcs_path(rpath)&#xa;				# avoid uploading a file and destroy a directory by accident&#xa;				subresult = self.__get_file_info(rpath)&#xa;				if subresult == ENoError: # remove path exists, check is dir or file&#xa;					if self.__remote_json['isdir']: # do this only for dir&#xa;						rpath += '/' + lpathbase # rpath is guaranteed no '/' ended&#xa;					else: # rpath is a file&#xa;						self.__isrev = True&#xa;			self.pd(""remote path is '{}'"".format(rpath))&#xa;			return self.__upload_file(lpath, rpath, ondup)&#xa;		elif os.path.isdir(lpath):&#xa;			self.pd(""Uploading directory '{}' recursively"".format(lpath))&#xa;			rpath = get_pcs_path(rpath)&#xa;			return self.__upload_dir(lpath, rpath, ondup)&#xa;		else:&#xa;			perr(""Error: invalid local path '{}' for uploading specified."".format(localpath))&#xa;			return EParameter&#xa;&#xa;	def combine(self, remotefile, localfile = '', *args):&#xa;		''' Usage: combine <remotefile> [md5s] [localfile] - \&#xa;try to create a file at PCS by combining slices, having MD5s specified&#xa;  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)&#xa;  md5s - MD5 digests of the slices, separated by spaces&#xa;    if not specified, you must specify the 'listfile' using the '-l' or '--list-file' switch in command line. the MD5 digests will be read from the (text) file, which can store the MD5 digest seperate by new-line or spaces&#xa;  localfile - local file for verification, if not specified, no verification is done&#xa;		'''&#xa;		self.__slice_md5s = []&#xa;		if args:&#xa;			for arg in args:&#xa;				self.__slice_md5s.append(arg)&#xa;		elif self.__list_file_contents:&#xa;			digests = filter(None, self.__list_file_contents.split())&#xa;			for d in digests:&#xa;				self.__slice_md5s.append(d)&#xa;		else:&#xa;			perr(""You MUST either provide the MD5s through the command line, ""&#xa;				""or using the '-l' ('--list-file') switch to specify ""&#xa;				""the 'listfile' to read MD5s from"")&#xa;			return EArgument&#xa;&#xa;		verify = self.__verify&#xa;		if localfile:&#xa;			self.__current_file = localfile&#xa;			self.__current_file_size = getfilesize(localfile)&#xa;		else:&#xa;			self.__current_file = '/dev/null' # Force no verify&#xa;			self.__verify = False&#xa;&#xa;		result = self.__combine_file(get_pcs_path(remotefile))&#xa;		self.__verify = verify&#xa;		return result&#xa;&#xa;	# no longer used&#xa;	def __get_meta_act(self, r, args):&#xa;		parse_ok = False&#xa;		j = r.json()&#xa;		if 'list' in j:&#xa;			lj = j['list']&#xa;			if len(lj) > 0:&#xa;				self.__remote_json = lj[0] # TODO: ugly patch&#xa;				# patch for inconsistency between 'list' and 'meta' json&#xa;				#self.__remote_json['md5'] = self.__remote_json['block_list'].strip('[]""')&#xa;				self.pd(""self.__remote_json: {}"".format(self.__remote_json))&#xa;				parse_ok = True&#xa;				return ENoError&#xa;&#xa;		if not parse_ok:&#xa;			self.__remote_json = {}&#xa;			perr(""Invalid JSON: {}\n{}"".format(j, traceback.format_exc()))&#xa;			return EInvalidJson&#xa;&#xa;	# no longer used&#xa;	def __get_meta(self, remotefile):&#xa;		pars = {&#xa;			'method' : 'meta',&#xa;			'path' : remotefile }&#xa;		return self.__get(&#xa;			pcsurl + 'file', pars,&#xa;			self.__get_meta_act)&#xa;&#xa;	# NO LONGER IN USE&#xa;	def __downfile_act(self, r, args):&#xa;		rfile, offset = args&#xa;		with open(self.__current_file, 'r+b' if offset > 0 else 'wb') as f:&#xa;			if offset > 0:&#xa;				f.seek(offset)&#xa;&#xa;			rsize = self.__remote_json['size']&#xa;			start_time = time.time()&#xa;			for chunk in r.iter_content(chunk_size = self.__dl_chunk_size):&#xa;				if chunk: # filter out keep-alive new chunks&#xa;					f.write(chunk)&#xa;					f.flush()&#xa;					pprgr(f.tell(), rsize, start_time)&#xa;					# https://stackoverflow.com/questions/7127075/what-exactly-the-pythons-file-flush-is-doing&#xa;					#os.fsync(f.fileno())&#xa;&#xa;		# No exception above, then everything goes fine&#xa;		result = ENoError&#xa;		if self.__verify:&#xa;			self.__current_file_size = getfilesize(self.__current_file)&#xa;			result = self.__verify_current_file(self.__remote_json, False)&#xa;&#xa;		if result == ENoError:&#xa;			self.pv(""'{}' <== '{}' OK"".format(self.__current_file, rfile))&#xa;		else:&#xa;			perr(""'{}' <== '{}' FAILED"".format(self.__current_file, rfile))&#xa;&#xa;		return result&#xa;&#xa;	def __downchunks_act(self, r, args):&#xa;		rfile, offset, rsize, start_time = args&#xa;&#xa;		expectedBytes = self.__dl_chunk_size&#xa;		if rsize - offset < self.__dl_chunk_size:&#xa;			expectedBytes = rsize - offset&#xa;&#xa;		if len(r.content) != expectedBytes:&#xa;			return ERequestFailed&#xa;		else:&#xa;			with open(self.__current_file, 'r+b' if offset > 0 else 'wb') as f:&#xa;				if offset > 0:&#xa;					f.seek(offset)&#xa;&#xa;				f.write(r.content)&#xa;				pos = f.tell()&#xa;				pprgr(pos, rsize, start_time, existing = self.__existing_size)&#xa;				if pos - offset == expectedBytes:&#xa;					return ENoError&#xa;				else:&#xa;					return EFileWrite&#xa;&#xa;	# requirment: self.__remote_json is already gotten&#xa;	def __downchunks(self, rfile, start):&#xa;		rsize = self.__remote_json['size']&#xa;&#xa;		pars = {&#xa;			'method' : 'download',&#xa;			'path' : rfile }&#xa;&#xa;		offset = start&#xa;		self.__existing_size = offset&#xa;		start_time = time.time()&#xa;		while True:&#xa;			nextoffset = offset + self.__dl_chunk_size&#xa;			if nextoffset < rsize:&#xa;				headers = { ""Range"" : ""bytes={}-{}"".format(&#xa;					offset, nextoffset - 1) }&#xa;			elif offset > 0:&#xa;				headers = { ""Range"" : ""bytes={}-"".format(offset) }&#xa;			elif rsize >= 1: # offset == 0&#xa;				# Fix chunked + gzip response,&#xa;				# seems we need to specify the Range for the first chunk as well:&#xa;				# https://github.com/houtianze/bypy/pull/161&#xa;				#headers = { ""Range"" : ""bytes=0-"".format(rsize - 1) }&#xa;				headers = { ""Range"" : ""bytes=0-{}"".format(rsize - 1) }&#xa;			else:&#xa;				headers = {}&#xa;&#xa;			# this _may_ solve #163: { ""error_code"":31326, ""error_msg"":""anti hotlinking""}&#xa;			if headers.has_key('Range'):&#xa;				self.pd(""headers['Range'][6:]: {} {}"".format(headers['Range'][6:], base64.standard_b64encode(headers['Range'][6:])))&#xa;				pars['ru'] = base64.standard_b64encode(headers['Range'][6:])&#xa;&#xa;			subresult = self.__get(dpcsurl + 'file', pars,&#xa;				self.__downchunks_act, (rfile, offset, rsize, start_time), headers = headers)&#xa;			if subresult != ENoError:&#xa;				return subresult&#xa;&#xa;			if nextoffset < rsize:&#xa;				offset += self.__dl_chunk_size&#xa;			else:&#xa;				break&#xa;&#xa;		# No exception above, then everything goes fine&#xa;		result = ENoError&#xa;		if self.__verify:&#xa;			self.__current_file_size = getfilesize(self.__current_file)&#xa;			result = self.__verify_current_file(self.__remote_json, False)&#xa;&#xa;		if result == ENoError:&#xa;			self.pv(""'{}' <== '{}' OK"".format(self.__current_file, rfile))&#xa;		else:&#xa;			perr(""'{}' <== '{}' FAILED"".format(self.__current_file, rfile))&#xa;&#xa;		return result&#xa;&#xa;	def __downfile(self, remotefile, localfile):&#xa;		# TODO: this is a quick patch&#xa;		if not self.__shallinclude(localfile, remotefile, False):&#xa;			# since we are not going to download it, there is no error&#xa;			return ENoError&#xa;&#xa;		result = ENoError&#xa;		rfile = remotefile&#xa;&#xa;		self.__remote_json = {}&#xa;		self.pd(""Downloading '{}' as '{}'"".format(rfile, localfile))&#xa;		self.__current_file = localfile&#xa;		#if self.__verify or self.__resumedownload:&#xa;		self.pd(""Getting info of remote file '{}' for later verification"".format(rfile))&#xa;		result = self.__get_file_info(rfile)&#xa;		if result != ENoError:&#xa;			return result&#xa;&#xa;		offset = 0&#xa;		self.pd(""Checking if we already have the copy locally"")&#xa;		if os.path.isfile(localfile):&#xa;			self.pd(""Same-name local file '{}' exists, checking if contents match"".format(localfile))&#xa;			self.__current_file_size = getfilesize(self.__current_file)&#xa;			if ENoError == self.__verify_current_file(self.__remote_json, False):&#xa;				self.pd(""Same local file '{}' already exists, skip downloading"".format(localfile))&#xa;				return ENoError&#xa;			else:&#xa;				if not self.shalloverwrite(""Same-name locale file '{}' exists but is different, "".format(localfile) + \&#xa;						""do you want to overwrite it? [y/N]""):&#xa;					pinfo(""Same-name local file '{}' exists but is different, skip downloading"".format(localfile))&#xa;					return ENoError&#xa;&#xa;			if self.__resumedownload and \&#xa;				self.__compare_size(self.__current_file_size, self.__remote_json) == 2:&#xa;				# revert back at least one download chunk&#xa;				pieces = self.__current_file_size // self.__dl_chunk_size&#xa;				if pieces > 1:&#xa;					offset = (pieces - 1) * self.__dl_chunk_size&#xa;		elif os.path.isdir(localfile):&#xa;			if not self.shalloverwrite(""Same-name direcotry '{}' exists, "".format(localfile) + \&#xa;				""do you want to remove it? [y/N]""):&#xa;				pinfo(""Same-name directory '{}' exists, skip downloading"".format(localfile))&#xa;				return ENoError&#xa;&#xa;			self.pv(""Directory with the same name '{}' exists, removing ..."".format(localfile))&#xa;			result = removedir(localfile, self.Verbose)&#xa;			if result == ENoError:&#xa;				self.pv(""Removed"")&#xa;			else:&#xa;				perr(""Error removing the directory '{}'"".format(localfile))&#xa;				return result&#xa;&#xa;		ldir, file = os.path.split(localfile)&#xa;		if ldir and not os.path.exists(ldir):&#xa;			result = makedir(ldir, verbose = self.Verbose)&#xa;			if result != ENoError:&#xa;				perr(""Fail to make directory '{}'"".format(ldir))&#xa;				return result&#xa;&#xa;		return self.__downchunks(rfile, offset)&#xa;&#xa;	def downfile(self, remotefile, localpath = ''):&#xa;		''' Usage: downfile <remotefile> [localpath] - \&#xa;download a remote file.&#xa;  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)&#xa;  localpath - local path.&#xa;    if it ends with '/' or '\\', it specifies the local direcotry&#xa;    if it specifies an existing directory, it is the local direcotry&#xa;    if not specified, the local direcotry is the current directory '.'&#xa;    otherwise, it specifies the local file name&#xa;To stream a file using downfile, you can use the 'mkfifo' trick with omxplayer etc.:&#xa;  mkfifo /tmp/omx&#xa;  bypy.py downfile <remotepath> /tmp/omx &&#xa;  omxplayer /tmp/omx&#xa;		'''&#xa;		localfile = localpath&#xa;		if not localpath:&#xa;			localfile = os.path.basename(remotefile)&#xa;		elif localpath[-1] == '\\' or \&#xa;			localpath[-1] == '/' or \&#xa;			os.path.isdir(localpath):&#xa;			#localfile = os.path.join(localpath, os.path.basename(remotefile))&#xa;			localfile = joinpath(localpath, os.path.basename(remotefile))&#xa;		else:&#xa;			localfile = localpath&#xa;&#xa;		pcsrpath = get_pcs_path(remotefile)&#xa;		return self.__downfile(pcsrpath, localfile)&#xa;&#xa;	def __stream_act_actual(self, r, args):&#xa;		pipe, csize = args&#xa;		with open(pipe, 'wb') as f:&#xa;			for chunk in r.iter_content(chunk_size = csize):&#xa;				if chunk: # filter out keep-alive new chunks&#xa;					f.write(chunk)&#xa;					f.flush()&#xa;					# https://stackoverflow.com/questions/7127075/what-exactly-the-pythons-file-flush-is-doing&#xa;					#os.fsync(f.fileno())&#xa;&#xa;	def __streaming_act(self, r, args):&#xa;		return self.__stream_act_actual(r, args)&#xa;&#xa;	# NOT WORKING YET&#xa;	def streaming(self, remotefile, localpipe, fmt = 'M3U8_480_360', chunk = 4 * OneM):&#xa;		''' Usage: stream <remotefile> <localpipe> [format] [chunk] - \&#xa;stream a video / audio file converted to M3U format at cloud side, to a pipe.&#xa;  remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)&#xa;  localpipe - the local pipe file to write to&#xa;  format - output video format (M3U8_320_240 | M3U8_480_224 | \&#xa;M3U8_480_360 | M3U8_640_480 | M3U8_854_480). Default: M3U8_480_360&#xa;  chunk - chunk (initial buffering) size for streaming (default: 4M)&#xa;To stream a file, you can use the 'mkfifo' trick with omxplayer etc.:&#xa;  mkfifo /tmp/omx&#xa;  bypy.py downfile <remotepath> /tmp/omx &&#xa;  omxplayer /tmp/omx&#xa;  *** NOT WORKING YET ****&#xa;		'''&#xa;		pars = {&#xa;			'method' : 'streaming',&#xa;			'path' : get_pcs_path(remotefile),&#xa;			'type' : fmt }&#xa;&#xa;		return self.__get(pcsurl + 'file', pars,&#xa;			self.__streaming_act, (localpipe, chunk), stream = True)&#xa;&#xa;	def __walk_remote_dir_act(self, r, args):&#xa;		dirjs, filejs = args&#xa;		j = r.json()&#xa;		#self.pd(""Remote path content JSON: {}"".format(j))&#xa;		paths = j['list']&#xa;		for path in paths:&#xa;			if path['isdir']:&#xa;				dirjs.append(path)&#xa;			else:&#xa;				filejs.append(path)&#xa;&#xa;		return ENoError&#xa;&#xa;	def __walk_remote_dir(self, remotepath, proceed, args = None, skip_remote_only_dirs = False):&#xa;		pars = {&#xa;			'method' : 'list',&#xa;			'path' : remotepath,&#xa;			'by' : 'name',&#xa;			'order' : 'asc' }&#xa;&#xa;		# Python parameters are by-reference and mutable, so they are 'out' by default&#xa;		dirjs = []&#xa;		filejs = []&#xa;		result = self.__get(pcsurl + 'file', pars, self.__walk_remote_dir_act, (dirjs, filejs))&#xa;		self.pd(""Remote dirs: {}"".format(dirjs))&#xa;		self.pd(""Remote files: {}"".format(filejs))&#xa;		if result == ENoError:&#xa;			subresult = proceed(remotepath, dirjs, filejs, args)&#xa;			if subresult != ENoError:&#xa;				self.pd(""Error: {} while proceeding remote path'{}'"".format(&#xa;					subresult, remotepath))&#xa;				result = subresult # we continue&#xa;			for dirj in dirjs:&#xa;				crpath = dirj['path'] # crpath - current remote path&#xa;				# TODO: rename 'args' to 'rootremotepath'?&#xa;				if skip_remote_only_dirs and \&#xa;					args != None and isinstance(args, basestring) and args.startswith('/apps') and \&#xa;					self.__local_dir_contents.get(posixpath.relpath(crpath, args)) == None:&#xa;					self.pd(""Skipping remote-only sub-directory '{}'."".format(crpath))&#xa;					continue&#xa;&#xa;				subresult = self.__walk_remote_dir(crpath, proceed, args, skip_remote_only_dirs)&#xa;				if subresult != ENoError:&#xa;					self.pd(""Error: {} while sub-walking remote dirs'{}'"".format(&#xa;						subresult, dirjs))&#xa;					result = subresult&#xa;&#xa;		return result&#xa;&#xa;	def __prepare_local_dir(self, localdir):&#xa;		result = ENoError&#xa;		if os.path.isfile(localdir):&#xa;			result = removefile(localdir, self.Verbose)&#xa;&#xa;		if result == ENoError:&#xa;			if localdir and not os.path.exists(localdir):&#xa;				result = makedir(localdir, verbose = self.Verbose)&#xa;&#xa;		return result&#xa;&#xa;	def __proceed_downdir(self, remotepath, dirjs, filejs, args):&#xa;		result = ENoError&#xa;		rootrpath, localpath = args&#xa;		rlen = len(remotepath) + 1 # '+ 1' for the trailing '/', it bites.&#xa;		rootlen = len(rootrpath) + 1 # ditto&#xa;&#xa;		result = self.__prepare_local_dir(localpath)&#xa;		if result != ENoError:&#xa;			perr(""Fail to create prepare local directory '{}' for downloading, ABORT"".format(localpath))&#xa;			return result&#xa;&#xa;		for dirj in dirjs:&#xa;			reldir = dirj['path'][rlen:]&#xa;			#ldir = os.path.join(localpath, reldir)&#xa;			ldir = joinpath(localpath, reldir)&#xa;			result = self.__prepare_local_dir(ldir)&#xa;			if result != ENoError:&#xa;				perr(""Fail to create prepare local directory '{}' for downloading, ABORT"".format(ldir))&#xa;				return result&#xa;&#xa;		for filej in filejs:&#xa;			rfile = filej['path']&#xa;			relfile = rfile[rootlen:]&#xa;			#lfile = os.path.join(localpath, relfile)&#xa;			lfile = joinpath(localpath, relfile)&#xa;			self.__downfile(rfile, lfile)&#xa;&#xa;		return result&#xa;&#xa;	def downdir(self, remotepath = None, localpath = None):&#xa;		''' Usage: downdir <remotedir> [localdir] - \&#xa;download a remote directory (recursively)&#xa;  remotedir - remote directory at Baidu Yun (after app root directory at Baidu Yun)&#xa;  localdir - local directory. if not specified, it is set to the current direcotry&#xa;		'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		lpath = localpath&#xa;&#xa;		if not lpath:&#xa;			lpath = '' # empty string does it, no need '.'&#xa;&#xa;		lpath = lpath.rstrip('/\\ ')&#xa;&#xa;		return self.__walk_remote_dir(rpath, self.__proceed_downdir, (rpath, lpath))&#xa;&#xa;	def __mkdir_act(self, r, args):&#xa;		if self.Verbose:&#xa;			j = r.json()&#xa;			pr(""path, ctime, mtime, fs_id"")&#xa;			pr(""{path}, {ctime}, {mtime}, {fs_id}"".format(**j))&#xa;&#xa;		return ENoError&#xa;&#xa;	def __mkdir(self, rpath, **kwargs):&#xa;		# TODO: this is a quick patch&#xa;		# the code still works because Baidu Yun doesn't require&#xa;		# parent directory to exist remotely to upload / create a file&#xa;		if not self.__shallinclude('.', rpath, True):&#xa;			return ENoError&#xa;&#xa;		self.pd(""Making remote directory '{}'"".format(rpath))&#xa;&#xa;		pars = {&#xa;			'method' : 'mkdir',&#xa;			'path' : rpath }&#xa;		return self.__post(pcsurl + 'file', pars, self.__mkdir_act, **kwargs)&#xa;&#xa;&#xa;	def mkdir(self, remotepath):&#xa;		''' Usage: mkdir <remotedir> - \&#xa;create a directory at Baidu Yun&#xa;  remotedir - the remote directory&#xa;'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		return self.__mkdir(rpath)&#xa;&#xa;	def __move_act(self, r, args):&#xa;		j = r.json()&#xa;		list = j['extra']['list']&#xa;		fromp = list[0]['from']&#xa;		to = list[0]['to']&#xa;		self.pd(""Remote move: '{}' =mm-> '{}' OK"".format(fromp, to))&#xa;&#xa;	# aliases&#xa;	def mv(self, fromp, to):&#xa;		return self.move(fromp, to)&#xa;&#xa;	def rename(self, fromp, to):&#xa;		return self.move(fromp, to)&#xa;&#xa;	def ren(self, fromp, to):&#xa;		return self.move(fromp, to)&#xa;&#xa;	def move(self, fromp, to):&#xa;		''' Usage: move/mv/rename/ren <from> <to> - \&#xa;move a file / dir remotely at Baidu Yun&#xa;  from - source path (file / dir)&#xa;  to - destination path (file / dir)&#xa;		'''&#xa;		frompp = get_pcs_path(fromp)&#xa;		top = get_pcs_path(to)&#xa;		pars = {&#xa;			'method' : 'move',&#xa;			'from' : frompp,&#xa;			'to' : top }&#xa;&#xa;		self.pd(""Remote moving: '{}' =mm=> '{}'"".format(fromp, to))&#xa;		return self.__post(pcsurl + 'file', pars, self.__move_act)&#xa;&#xa;	def __copy_act(self, r, args):&#xa;		j = r.json()&#xa;		for list in j['extra']['list']:&#xa;			fromp = list['from']&#xa;			to = list['to']&#xa;			self.pd(""Remote copy: '{}' =cc=> '{}' OK"".format(fromp, to))&#xa;&#xa;		return ENoError&#xa;&#xa;	# alias&#xa;	def cp(self, fromp, to):&#xa;		return self.copy(fromp, to)&#xa;&#xa;	def copy(self, fromp, to):&#xa;		''' Usage: copy/cp <from> <to> - \&#xa;copy a file / dir remotely at Baidu Yun&#xa;  from - source path (file / dir)&#xa;  to - destination path (file / dir)&#xa;		'''&#xa;		frompp = get_pcs_path(fromp)&#xa;		top = get_pcs_path(to)&#xa;		pars = {&#xa;			'method' : 'copy',&#xa;			'from' : frompp,&#xa;			'to' : top }&#xa;&#xa;		self.pd(""Remote copying '{}' =cc=> '{}'"".format(frompp, top))&#xa;		return self.__post(pcsurl + 'file', pars, self.__copy_act)&#xa;&#xa;	def __delete_act(self, r, args):&#xa;		rid = r.json()['request_id']&#xa;		if rid:&#xa;			pr(""Deletion request '{}' OK"".format(rid))&#xa;			pr(""Usage 'list' command to confirm"")&#xa;&#xa;			return ENoError&#xa;		else:&#xa;			perr(""Deletion failed"")&#xa;			return EFailToDeleteFile&#xa;&#xa;	def __delete(self, rpath):&#xa;		pars = {&#xa;			'method' : 'delete',&#xa;			'path' : rpath }&#xa;&#xa;		self.pd(""Remote deleting: '{}'"".format(rpath))&#xa;		return self.__post(pcsurl + 'file', pars, self.__delete_act)&#xa;&#xa;	# aliases&#xa;	def remove(self, remotepath):&#xa;		return self.delete(remotepath)&#xa;&#xa;	def rm(self, remotepath):&#xa;		return self.delete(remotepath)&#xa;&#xa;	def delete(self, remotepath):&#xa;		''' Usage: delete/remove/rm <remotepath> - \&#xa;delete a file / dir remotely at Baidu Yun&#xa;  remotepath - destination path (file / dir)&#xa;		'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		return self.__delete(rpath)&#xa;&#xa;	def __search_act(self, r, args):&#xa;		print_pcs_list(r.json())&#xa;		return ENoError&#xa;&#xa;	def search(self, keyword, remotepath = None, recursive = True):&#xa;		''' Usage: search <keyword> [remotepath] [recursive] - \&#xa;search for a file using keyword at Baidu Yun&#xa;  keyword - the keyword to search&#xa;  remotepath - remote path at Baidu Yun, if not specified, it's app's root directory&#xa;  resursive - search recursively or not. default is true&#xa;		'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;&#xa;		pars = {&#xa;			'method' : 'search',&#xa;			'path' : rpath,&#xa;			'wd' : keyword,&#xa;			're' : '1' if str2bool(recursive) else '0'}&#xa;&#xa;		self.pd(""Searching: '{}'"".format(rpath))&#xa;		return self.__get(pcsurl + 'file', pars, self.__search_act)&#xa;&#xa;	def __listrecycle_act(self, r, args):&#xa;		print_pcs_list(r.json())&#xa;		return ENoError&#xa;&#xa;	def listrecycle(self, start = 0, limit = 1000):&#xa;		''' Usage: listrecycle [start] [limit] - \&#xa;list the recycle contents&#xa;  start - starting point, default: 0&#xa;  limit - maximum number of items to display. default: 1000&#xa;		'''&#xa;		pars = {&#xa;			'method' : 'listrecycle',&#xa;			'start' : str2int(start),&#xa;			'limit' : str2int(limit) }&#xa;&#xa;		self.pd(""Listing recycle '{}'"")&#xa;		return self.__get(pcsurl + 'file', pars, self.__listrecycle_act)&#xa;&#xa;	def __restore_act(self, r, args):&#xa;		path = args&#xa;		pr(""'{}' found and restored"".format(path))&#xa;		return ENoError&#xa;&#xa;	def __restore_search_act(self, r, args):&#xa;		path = args&#xa;		flist = r.json()['list']&#xa;		fsid = None&#xa;		for f in flist:&#xa;			if os.path.normpath(f['path'].lower()) == os.path.normpath(path.lower()):&#xa;				fsid = f['fs_id']&#xa;				self.pd(""fs_id for restoring '{}' found"".format(fsid))&#xa;				break&#xa;		if fsid:&#xa;			pars = {&#xa;				'method' : 'restore',&#xa;				'fs_id' : fsid }&#xa;			return self.__post(pcsurl + 'file', pars, self.__restore_act, path)&#xa;		else:&#xa;			perr(""'{}' not found in the recycle bin"".format(path))&#xa;&#xa;	def restore(self, remotepath):&#xa;		''' Usage: restore <remotepath> - \&#xa;restore a file from the recycle bin&#xa;  remotepath - the remote path to restore&#xa;		'''&#xa;		rpath = get_pcs_path(remotepath)&#xa;		# by default, only 1000 items, more than that sounds a bit crazy&#xa;		pars = {&#xa;			'method' : 'listrecycle' }&#xa;&#xa;		self.pd(""Searching for fs_id to restore"")&#xa;		return self.__get(pcsurl + 'file', pars, self.__restore_search_act, rpath)&#xa;&#xa;	def __proceed_local_gather(self, dirlen, walk):&#xa;		#names.sort()&#xa;		(dirpath, dirnames, filenames) = walk&#xa;&#xa;		files = []&#xa;		for name in filenames:&#xa;			#fullname = os.path.join(dirpath, name)&#xa;			fullname = joinpath(dirpath, name)&#xa;			# ignore broken symbolic links&#xa;			if not os.path.exists(fullname):&#xa;				self.pd(""Local path '{}' does not exist (broken symbolic link?)"".format(fullname))&#xa;				continue&#xa;			files.append((name, getfilesize(fullname), md5(fullname)))&#xa;&#xa;		reldir = dirpath[dirlen:].replace('\\', '/')&#xa;		place = self.__local_dir_contents.get(reldir)&#xa;		for dir in dirnames:&#xa;			place.add(dir, PathDictTree('D'))&#xa;		for file in files:&#xa;			place.add(file[0], PathDictTree('F', size = file[1], md5 = file[2]))&#xa;&#xa;		return ENoError&#xa;&#xa;	def __gather_local_dir(self, dir):&#xa;		self.__local_dir_contents = PathDictTree()&#xa;		for walk in os.walk(dir, followlinks=self.__followlink):&#xa;			self.__proceed_local_gather(len(dir), walk)&#xa;		self.pd(self.__local_dir_contents)&#xa;&#xa;	def __proceed_remote_gather(self, remotepath, dirjs, filejs, args = None):&#xa;		# NOTE: the '+ 1' is due to the trailing slash '/'&#xa;		# be careful about the trailing '/', it bit me once, bitterly&#xa;		rootrdir = args&#xa;		rootlen = len(rootrdir)&#xa;		dlen = len(remotepath) + 1&#xa;		for d in dirjs:&#xa;			self.__remote_dir_contents.get(remotepath[rootlen:]).add(&#xa;				d['path'][dlen:], PathDictTree('D', size = d['size'], md5 = binascii.unhexlify(d['md5'])))&#xa;&#xa;		for f in filejs:&#xa;			self.__remote_dir_contents.get(remotepath[rootlen:]).add(&#xa;				f['path'][dlen:], PathDictTree('F', size = f['size'], md5 = binascii.unhexlify(f['md5'])))&#xa;&#xa;		return ENoError&#xa;&#xa;	def __gather_remote_dir(self, rdir, skip_remote_only_dirs = False):&#xa;		self.__remote_dir_contents = PathDictTree()&#xa;		self.__walk_remote_dir(rdir, self.__proceed_remote_gather, rdir, skip_remote_only_dirs)&#xa;		self.pd(""---- Remote Dir Contents ---"")&#xa;		self.pd(self.__remote_dir_contents)&#xa;&#xa;	def __compare(self, remotedir = None, localdir = None, skip_remote_only_dirs = False):&#xa;		if not localdir:&#xa;			localdir = '.'&#xa;&#xa;		self.pv(""Gathering local directory ..."")&#xa;		self.__gather_local_dir(localdir)&#xa;		self.pv(""Done"")&#xa;		self.pv(""Gathering remote directory ..."")&#xa;		self.__gather_remote_dir(remotedir, skip_remote_only_dirs)&#xa;		self.pv(""Done"")&#xa;		self.pv(""Comparing ..."")&#xa;		# list merge, where Python shines&#xa;		commonsame = []&#xa;		commondiff = []&#xa;		localonly = []&#xa;		remoteonly = []&#xa;		# http://stackoverflow.com/questions/1319338/combining-two-lists-and-removing-duplicates-without-removing-duplicates-in-orig&#xa;		lps = self.__local_dir_contents.allpath()&#xa;		rps = self.__remote_dir_contents.allpath()&#xa;		dps = set(rps) - set(lps)&#xa;		allpath = lps + list(dps)&#xa;		for p in allpath:&#xa;			local = self.__local_dir_contents.get(p)&#xa;			remote = self.__remote_dir_contents.get(p)&#xa;			if local is None: # must be in the remote dir, since p is from allpath&#xa;				remoteonly.append((remote.type, p))&#xa;			elif remote is None:&#xa;				localonly.append((local.type, p))&#xa;			else: # all here&#xa;				same = False&#xa;				if local.type == 'D' and remote.type == 'D':&#xa;					type = 'D'&#xa;					same = True&#xa;				elif local.type == 'F' and remote.type == 'F':&#xa;					type = 'F'&#xa;					if local.extra['size'] == remote.extra['size'] and \&#xa;						local.extra['md5'] == remote.extra['md5']:&#xa;						same = True&#xa;					else:&#xa;						same = False&#xa;				else:&#xa;					type = local.type + remote.type&#xa;					same = False&#xa;&#xa;				if same:&#xa;					commonsame.append((type, p))&#xa;				else:&#xa;					commondiff.append((type, p))&#xa;&#xa;		self.pv(""Done"")&#xa;		return commonsame, commondiff, localonly, remoteonly&#xa;&#xa;	def compare(self, remotedir = None, localdir = None, skip_remote_only_dirs = False):&#xa;		''' Usage: compare [remotedir] [localdir] - \&#xa;compare the remote direcotry with the local directory&#xa;  remotedir - the remote directory at Baidu Yun (after app's direcotry). \&#xa;if not specified, it defaults to the root directory.&#xa;  localdir - the local directory, if not specified, it defaults to the current directory.&#xa;  skip_remote_only_dirs - skip remote-only sub-directories (faster if the remote \&#xa;directory is much larger than the local one). it defaults to False.&#xa;		'''&#xa;		same, diff, local, remote = self.__compare(get_pcs_path(remotedir), localdir, str2bool(skip_remote_only_dirs))&#xa;&#xa;		pr(""==== Same files ==="")&#xa;		for c in same:&#xa;			pr(""{} - {}"".format(c[0], c[1]))&#xa;&#xa;		pr(""==== Different files ==="")&#xa;		for d in diff:&#xa;			pr(""{} - {}"".format(d[0], d[1]))&#xa;&#xa;		pr(""==== Local only ===="")&#xa;		for l in local:&#xa;			pr(""{} - {}"".format(l[0], l[1]))&#xa;&#xa;		pr(""==== Remote only ===="")&#xa;		for r in remote:&#xa;			pr(""{} - {}"".format(r[0], r[1]))&#xa;&#xa;		pr(""\nStatistics:"")&#xa;		pr(""--------------------------------"")&#xa;		pr(""Same: {}"".format(len(same)));&#xa;		pr(""Different: {}"".format(len(diff)));&#xa;		pr(""Local only: {}"".format(len(local)));&#xa;		pr(""Remote only: {}"".format(len(remote)));&#xa;&#xa;	def syncdown(self, remotedir = '', localdir = u'', deletelocal = False):&#xa;		''' Usage: syncdown [remotedir] [localdir] [deletelocal] - \&#xa;sync down from the remote direcotry to the local directory&#xa;  remotedir - the remote directory at Baidu Yun (after app's direcotry) to sync from. \&#xa;if not specified, it defaults to the root directory&#xa;  localdir - the local directory to sync to if not specified, it defaults to the current directory.&#xa;  deletelocal - delete local files that are not inside Baidu Yun direcotry, default is False&#xa;		'''&#xa;		result = ENoError&#xa;		rpath = get_pcs_path(remotedir)&#xa;		same, diff, local, remote = self.__compare(rpath, localdir)&#xa;		# clear the way&#xa;		for d in diff:&#xa;			t = d[0]&#xa;			p = d[1]&#xa;			#lcpath = os.path.join(localdir, p) # local complete path&#xa;			lcpath = joinpath(localdir, p) # local complete path&#xa;			rcpath = rpath + '/' + p # remote complete path&#xa;			if t == 'DF':&#xa;				result = removedir(lcpath, self.Verbose)&#xa;				subresult = self.__downfile(rcpath, lcpath)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;			elif t == 'FD':&#xa;				result = removefile(lcpath, self.Verbose)&#xa;				subresult = makedir(lcpath, verbose = self.Verbose)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;			else: # "" t == 'F' "" must be true&#xa;				result = self.__downfile(rcpath, lcpath)&#xa;&#xa;		for r in remote:&#xa;			t = r[0]&#xa;			p = r[1]&#xa;			#lcpath = os.path.join(localdir, p) # local complete path&#xa;			lcpath = joinpath(localdir, p) # local complete path&#xa;			rcpath = rpath + '/' + p # remote complete path&#xa;			if t == 'F':&#xa;				subresult = self.__downfile(rcpath, lcpath)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;			else: # "" t == 'D' "" must be true&#xa;				subresult = makedir(lcpath, verbose = self.Verbose)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;&#xa;		if str2bool(deletelocal):&#xa;			for l in local:&#xa;				# use os.path.isfile()/isdir() instead of l[0], because we need to check file/dir existence.&#xa;				# as we may have removed the parent dir previously during the iteration&#xa;				#p = os.path.join(localdir, l[1])&#xa;				p = joinpath(localdir, l[1])&#xa;				if os.path.isfile(p):&#xa;					subresult = removefile(p, self.Verbose)&#xa;					if subresult != ENoError:&#xa;						result = subresult&#xa;				elif os.path.isdir(p):&#xa;					subresult = removedir(p, self.Verbose)&#xa;					if subresult != ENoError:&#xa;						result = subresult&#xa;&#xa;		return result&#xa;&#xa;	def syncup(self, localdir = u'', remotedir = '', deleteremote = False):&#xa;		''' Usage: syncup [localdir] [remotedir] [deleteremote] - \&#xa;sync up from the local direcotry to the remote directory&#xa;  localdir - the local directory to sync from if not specified, it defaults to the current directory.&#xa;  remotedir - the remote directory at Baidu Yun (after app's direcotry) to sync to. \&#xa;if not specified, it defaults to the root directory&#xa;  deleteremote - delete remote files that are not inside the local direcotry, default is False&#xa;		'''&#xa;		result = ENoError&#xa;		rpath = get_pcs_path(remotedir)&#xa;		#rpartialdir = remotedir.rstrip('/ ')&#xa;		same, diff, local, remote = self.__compare(rpath, localdir, True)&#xa;		# clear the way&#xa;		for d in diff:&#xa;			t = d[0] # type&#xa;			p = d[1] # path&#xa;			#lcpath = os.path.join(localdir, p) # local complete path&#xa;			lcpath = joinpath(localdir, p) # local complete path&#xa;			rcpath = rpath + '/' + p # remote complete path&#xa;			if self.shalloverwrite(""Do you want to overwrite '{}' at Baidu Yun? [y/N]"".format(p)):&#xa;				# this path is before get_pcs_path() since delete() expects so.&#xa;				#result = self.delete(rpartialdir + '/' + p)&#xa;				result = self.__delete(rcpath)&#xa;#				self.pd(""diff type: {}"".format(t))&#xa;#				self.__isrev = True&#xa;#				if t != 'F':&#xa;#					result = self.move(remotedir + '/' + p, remotedir + '/' + p + '.moved_by_bypy.' + time.strftime(""%Y%m%d%H%M%S""))&#xa;#					self.__isrev = False&#xa;				if t == 'F' or t == 'FD':&#xa;					subresult = self.__upload_file(lcpath, rcpath)&#xa;					if subresult != ENoError:&#xa;						result = subresult&#xa;				else: # "" t == 'DF' "" must be true&#xa;					subresult = self.__mkdir(rcpath)&#xa;					if subresult != ENoError:&#xa;						result = subresult&#xa;			else:&#xa;				pinfo(""Uploading '{}' skipped"".format(lcpath))&#xa;&#xa;		for l in local:&#xa;			t = l[0]&#xa;			p = l[1]&#xa;			#lcpath = os.path.join(localdir, p) # local complete path&#xa;			lcpath = joinpath(localdir, p) # local complete path&#xa;			rcpath = rpath + '/' + p # remote complete path&#xa;			self.pd(""local type: {}"".format(t))&#xa;			self.__isrev = False&#xa;			if t == 'F':&#xa;				subresult = self.__upload_file(lcpath, rcpath)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;			else: # "" t == 'D' "" must be true&#xa;				subresult = self.__mkdir(rcpath)&#xa;				if subresult != ENoError:&#xa;					result = subresult&#xa;&#xa;		if str2bool(deleteremote):&#xa;			# i think the list is built top-down, so directories appearing later are either&#xa;			# children or another set of directories&#xa;			pp = '\\' # previous path, setting to '\\' make sure it won't be found in the first step&#xa;			for r in remote:&#xa;				#p = rpartialdir + '/' + r[1]&#xa;				p = rpath + '/' + r[1]&#xa;				if 0 != p.find(pp): # another path&#xa;					#subresult = self.delete(p)&#xa;					subresult = self.__delete(p)&#xa;					if subresult != ENoError:&#xa;						result = subresult&#xa;				pp = p&#xa;&#xa;		return result&#xa;&#xa;	def dumpcache(self):&#xa;		''' Usage: dumpcache - display file hash cache'''&#xa;		if cached.cacheloaded:&#xa;			#pprint.pprint(cached.cache)&#xa;			MyPrettyPrinter().pprint(cached.cache)&#xa;			return ENoError&#xa;		else:&#xa;			perr(""Cache not loaded."")&#xa;			return ECacheNotLoaded&#xa;&#xa;	def cleancache(self):&#xa;		''' Usage: cleancache - remove invalid entries from hash cache file'''&#xa;		if os.path.exists(HashCachePath):&#xa;			try:&#xa;				# backup first&#xa;				backup = HashCachePath + '.lastclean'&#xa;				shutil.copy(HashCachePath, backup)&#xa;				self.pd(""Hash Cache file '{}' backed up as '{}"".format(&#xa;					HashCachePath, backup))&#xa;				cached.cleancache()&#xa;				return ENoError&#xa;			except:&#xa;				perr(""Exception:\n{}"".format(traceback.format_exc()))&#xa;				return EException&#xa;		else:&#xa;			return EFileNotFound&#xa;&#xa;OriginalFloatTime = True&#xa;&#xa;def onexit(retcode = ENoError):&#xa;	# saving is the most important&#xa;	# we save, but don't clean, why?&#xa;	# think about unmount path, moved files,&#xa;	# once we discard the information, they are gone.&#xa;	# so unless the user specifically request a clean,&#xa;	# we don't act too smart.&#xa;	#cached.cleancache()&#xa;	cached.savecache()&#xa;	os.stat_float_times(OriginalFloatTime)&#xa;	# if we flush() on Ctrl-C, we get&#xa;	# IOError: [Errno 32] Broken pipe&#xa;	sys.stdout.flush()&#xa;	sys.exit(retcode)&#xa;&#xa;def sighandler(signum, frame):&#xa;	pr(""Signal {} received, Abort"".format(signum))&#xa;	pr(""Stack:\n"")&#xa;	traceback.print_stack(frame)&#xa;	onexit(EAbort)&#xa;&#xa;def main(argv=None): # IGNORE:C0111&#xa;	''' Main Entry '''&#xa;&#xa;	# *** IMPORTANT ***&#xa;	# We must set this in order for cache to work,&#xa;	# as we need to get integer file mtime, which is used as the key of Hash Cache&#xa;	global OriginalFloatTime&#xa;	OriginalFloatTime = os.stat_float_times()&#xa;	os.stat_float_times(False)&#xa;	# --- IMPORTANT ---&#xa;&#xa;	result = ENoError&#xa;	if argv is None:&#xa;		argv = sys.argv&#xa;	else:&#xa;		sys.argv.extend(argv)&#xa;&#xa;	if sys.platform == 'win32':&#xa;		#signal.signal(signal.CTRL_C_EVENT, sighandler)&#xa;		#signal.signal(signal.CTRL_BREAK_EVENT, sighandler)&#xa;		# bug, see: http://bugs.python.org/issue9524&#xa;		pass&#xa;	else:&#xa;		signal.signal(signal.SIGBUS, sighandler)&#xa;		signal.signal(signal.SIGHUP, sighandler)&#xa;		# https://stackoverflow.com/questions/108183/how-to-prevent-sigpipes-or-handle-them-properly&#xa;		signal.signal(signal.SIGPIPE, signal.SIG_IGN)&#xa;		signal.signal(signal.SIGQUIT, sighandler)&#xa;		signal.signal(signal.SIGSYS, sighandler)&#xa;&#xa;	signal.signal(signal.SIGABRT, sighandler)&#xa;	signal.signal(signal.SIGFPE, sighandler)&#xa;	signal.signal(signal.SIGILL, sighandler)&#xa;	signal.signal(signal.SIGINT, sighandler)&#xa;	signal.signal(signal.SIGSEGV, sighandler)&#xa;	signal.signal(signal.SIGTERM, sighandler)&#xa;&#xa;	#program_name = os.path.basename(sys.argv[0])&#xa;	program_version = ""v%s"" % __version__&#xa;	program_version_message = '%%(prog)s %s' % (program_version )&#xa;	shortdesc = __import__('__main__').__doc__.split(""\n"")[1]&#xa;	shortdesc = program_version_message + ' -- ' + shortdesc.split('--')[1]&#xa;	program_shortdesc = shortdesc&#xa;	program_longdesc = __import__('__main__').__doc__.split(""---\n"")[1]&#xa;&#xa;	try:&#xa;		# +++ DEPRECATED +++&#xa;		# check if ApiKey, SecretKey and AppPcsPath are correctly specified.&#xa;		#if not ApiKey or not SecretKey or not AppPcsPath:&#xa;		if False:&#xa;			ApiNotConfigured = '''&#xa;*** ABORT *** Baidu API not properly configured&#xa;&#xa;- Please go to 'http://developer.baidu.com/' and create an application.&#xa;- Get the ApiKey, SecretKey and configure the App Path (default: '/apps/bypy/')&#xa;- Update the corresponding variables at the beginning of this file, \&#xa;right after the '# PCS configuration constants' comment.&#xa;- Try to run this program again&#xa;&#xa;*** ABORT ***&#xa;'''&#xa;			pr(ApiNotConfigured)&#xa;			return EApiNotConfigured&#xa;		# --- DEPRECATED ---&#xa;&#xa;		# setup argument parser&#xa;		epilog = ""Commands:\n""&#xa;		summary = []&#xa;		for k, v in ByPy.__dict__.items():&#xa;			if callable(v) and v.__doc__:&#xa;				help = v.__doc__.strip()&#xa;				pos = help.find(ByPy.HelpMarker)&#xa;				if pos != -1:&#xa;					pos_body = pos + len(ByPy.HelpMarker)&#xa;					helpbody = help[pos_body:]&#xa;					helpline = helpbody.split('\n')[0].strip() + '\n'&#xa;					if helpline.find('help') == 0:&#xa;						summary.insert(0, helpline)&#xa;					else:&#xa;						summary.append(helpline)&#xa;&#xa;		remaining = summary[1:]&#xa;		remaining.sort()&#xa;		summary = [summary[0]] + remaining&#xa;		epilog += ''.join(summary)&#xa;&#xa;		parser = ArgumentParser(&#xa;			description=program_shortdesc + '\n\n' + program_longdesc,&#xa;			formatter_class=RawDescriptionHelpFormatter, epilog=epilog)&#xa;&#xa;		# special&#xa;		parser.add_argument(""--TESTRUN"", dest=""TESTRUN"", action=""store_true"", help=""Perform python doctest"")&#xa;		parser.add_argument(""--PROFILE"", dest=""PROFILE"", action=""store_true"", help=""Profile the code"")&#xa;&#xa;		# help, version, program information etc&#xa;		parser.add_argument('-V', '--version', action='version', version=program_version_message)&#xa;		#parser.add_argument(dest=""paths"", help=""paths to folder(s) with source file(s) [default: %(default)s]"", metavar=""path"", nargs='+')&#xa;&#xa;		# debug, logging&#xa;		parser.add_argument(""-d"", ""--debug"", dest=""debug"", action=""count"", default=0, help=""enable debugging & logging [default: %(default)s]"")&#xa;		parser.add_argument(""-v"", ""--verbose"", dest=""verbose"", default=0, action=""count"", help=""set verbosity level [default: %(default)s]"")&#xa;&#xa;		# program tunning, configration (those will be passed to class ByPy)&#xa;		parser.add_argument(""-r"", ""--retry"", dest=""retry"", default=5, help=""number of retry attempts on network error [default: %(default)i times]"")&#xa;		parser.add_argument(""-q"", ""--quit-when-fail"", dest=""quit"", default=False, help=""quit when maximum number of retry failed [default: %(default)s]"")&#xa;		parser.add_argument(""-t"", ""--timeout"", dest=""timeout"", default=60, help=""network timeout in seconds [default: %(default)s]"")&#xa;		parser.add_argument(""-s"", ""--slice"", dest=""slice"", default=DefaultSliceSize, help=""size of file upload slice (can use '1024', '2k', '3MB', etc) [default: {} MB]"".format(DefaultSliceInMB))&#xa;		parser.add_argument(""--chunk"", dest=""chunk"", default=DefaultDlChunkSize, help=""size of file download chunk (can use '1024', '2k', '3MB', etc) [default: {} MB]"".format(DefaultDlChunkSize / OneM))&#xa;		parser.add_argument(""-e"", ""--verify"", dest=""verify"", action=""store_true"", default=False, help=""Verify upload / download [default : %(default)s]"")&#xa;		parser.add_argument(""-f"", ""--force-hash"", dest=""forcehash"", action=""store_true"", help=""force file MD5 / CRC32 calculation instead of using cached value"")&#xa;		parser.add_argument(""-l"", ""--list-file"", dest=""listfile"", default=None, help=""input list file (used by some of the commands only [default: %(default)s]"")&#xa;		parser.add_argument(""--resume-download"", dest=""resumedl"", default=True, help=""resume instead of restarting when downloading if local file already exists [default: %(default)s]"")&#xa;		parser.add_argument(""--include-regex"", dest=""incregex"", default='', help=""regular expression of files to include. if not specified (default), everything is included. for download, the regex applies to the remote files; for upload, the regex applies to the local files. to exclude files, think about your regex, some tips here: https://stackoverflow.com/questions/406230/regular-expression-to-match-string-not-containing-a-word [default: %(default)s]"")&#xa;		parser.add_argument(""--on-dup"", dest=""ondup"", default='overwrite', help=""what to do when the same file / folder exists in the destination: 'overwrite', 'skip', 'prompt' [default: %(default)s]"")&#xa;		parser.add_argument(""--no-symlink"", dest=""followlink"", action=""store_false"", help=""DON'T follow symbol links when uploading / syncing up"")&#xa;		parser.add_argument(DisableSslCheckOption, dest=""checkssl"", action=""store_false"", help=""DON'T verify host SSL cerificate"")&#xa;		parser.add_argument(CaCertsOption, dest=""cacerts"", help=""Specify the path for CA Bundle [default: %(default)s]"")&#xa;		parser.add_argument(""--mirror"", dest=""mirror"", default=None, help=""Specify the PCS mirror (e.g. bj.baidupcs.com. Open 'https://pcs.baidu.com/rest/2.0/pcs/manage?method=listhost' to get the list) to use."")&#xa;		parser.add_argument(""--rapid-upload-only"", dest=""rapiduploadonly"", action=""store_true"", help=""Only upload large files that can be rapidly uploaded"")&#xa;&#xa;		# action&#xa;		parser.add_argument(CleanOptionShort, CleanOptionLong, dest=""clean"", action=""count"", default=0, help=""1: clean settings (remove the token file) 2: clean settings and hash cache [default: %(default)s]"")&#xa;&#xa;		# the MAIN parameter - what command to perform&#xa;		parser.add_argument(""command"", nargs='*', help = ""operations (quota / list)"")&#xa;&#xa;		# Process arguments&#xa;		args = parser.parse_args()&#xa;&#xa;		if args.mirror:&#xa;			global pcsurl&#xa;			global cpcsurl&#xa;			global dpcsurl&#xa;			pcsurl = re.sub(r'//.*?/', '//' + args.mirror + '/', pcsurl)&#xa;			cpcsurl = pcsurl&#xa;			dpcsurl = pcsurl&#xa;&#xa;		try:&#xa;			slice_size = interpret_size(args.slice)&#xa;		except (ValueError, KeyError):&#xa;			pr(""Error: Invalid slice size specified '{}'"".format(args.slice))&#xa;			return EArgument&#xa;		try:&#xa;			chunk_size = interpret_size(args.chunk)&#xa;		except (ValueError, KeyError):&#xa;			pr(""Error: Invalid slice size specified '{}'"".format(args.slice))&#xa;			return EArgument&#xa;&#xa;		if args.TESTRUN:&#xa;			return TestRun()&#xa;&#xa;		if args.PROFILE:&#xa;			return Profile()&#xa;&#xa;		pr(""Token file: '{}'"".format(TokenFilePath))&#xa;		pr(""Hash Cache file: '{}'"".format(HashCachePath))&#xa;		pr(""App root path at Baidu Yun '{}'"".format(AppPcsPath))&#xa;		pr(""sys.stdin.encoding = {}"".format(sys.stdin.encoding))&#xa;		pr(""sys.stdout.encoding = {}"".format(sys.stdout.encoding))&#xa;		pr(""sys.stderr.encoding = {}"".format(sys.stderr.encoding))&#xa;&#xa;		if args.verbose > 0:&#xa;			pr(""Verbose level = {}"".format(args.verbose))&#xa;			pr(""Debug = {}"".format(args.debug))&#xa;&#xa;		pr(""----\n"")&#xa;&#xa;		if os.path.exists(HashCachePath):&#xa;			cachesize = getfilesize(HashCachePath)&#xa;			if cachesize > 10 * OneM or cachesize == -1:&#xa;				pr((&#xa;""*** WARNING ***\n""&#xa;""Hash Cache file '{0}' is very large ({1}).\n""&#xa;""This may affect program's performance (high memory consumption).\n""&#xa;""You can first try to run 'bypy.py cleancache' to slim the file.\n""&#xa;""But if the file size won't reduce (this warning persists),""&#xa;"" you may consider deleting / moving the Hash Cache file '{0}'\n""&#xa;""*** WARNING ***\n\n\n"").format(HashCachePath, human_size(cachesize)))&#xa;&#xa;		if args.clean >= 1:&#xa;			result = removefile(TokenFilePath, args.verbose)&#xa;			if result == ENoError:&#xa;				pr(""Token file '{}' removed. You need to re-authorize ""&#xa;					""the application upon next run"".format(TokenFilePath))&#xa;			else:&#xa;				perr(""Failed to remove the token file '{}'"".format(TokenFilePath))&#xa;				perr(""You need to remove it manually"")&#xa;&#xa;			if args.clean >= 2:&#xa;				subresult = os.remove(HashCachePath)&#xa;				if subresult == ENoError:&#xa;					pr(""Hash Cache File '{}' removed."".format(HashCachePath))&#xa;				else:&#xa;					perr(""Failed to remove the Hash Cache File '{}'"".format(HashCachePath))&#xa;					perr(""You need to remove it manually"")&#xa;					result = subresult&#xa;&#xa;			return result&#xa;&#xa;		if len(args.command) <= 0 or \&#xa;			(len(args.command) == 1 and args.command[0].lower() == 'help'):&#xa;			parser.print_help()&#xa;			return EArgument&#xa;		elif args.command[0] in ByPy.__dict__: # dir(ByPy), dir(by)&#xa;			timeout = None&#xa;			if args.timeout:&#xa;				timeout = float(args.timeout)&#xa;&#xa;			cached.usecache = not args.forcehash&#xa;			cached.verbose = args.verbose&#xa;			cached.debug = args.debug&#xa;			cached.loadcache()&#xa;&#xa;			by = ByPy(slice_size = slice_size, dl_chunk_size = chunk_size,&#xa;					verify = args.verify,&#xa;					retry = int(args.retry), timeout = timeout,&#xa;					quit_when_fail = args.quit,&#xa;					listfile = args.listfile,&#xa;					resumedownload = args.resumedl,&#xa;					incregex = args.incregex,&#xa;					ondup = args.ondup,&#xa;					followlink = args.followlink,&#xa;					checkssl = args.checkssl,&#xa;					cacerts = args.cacerts,&#xa;					rapiduploadonly = args.rapiduploadonly,&#xa;					verbose = args.verbose, debug = args.debug)&#xa;			uargs = []&#xa;			for arg in args.command[1:]:&#xa;				uargs.append(unicode(arg, SystemEncoding))&#xa;			result = getattr(by, args.command[0])(*uargs)&#xa;		else:&#xa;			pr(""Error: Command '{}' not available."".format(args.command[0]))&#xa;			parser.print_help()&#xa;			return EParameter&#xa;&#xa;	except KeyboardInterrupt:&#xa;		### handle keyboard interrupt ###&#xa;		pr(""KeyboardInterrupt"")&#xa;		pr(""Abort"")&#xa;	except Exception:&#xa;		perr(""Exception occurred:"")&#xa;		pr(traceback.format_exc())&#xa;		pr(""Abort"")&#xa;		# raise&#xa;&#xa;	onexit(result)&#xa;&#xa;def TestRun():&#xa;	import doctest&#xa;	doctest.testmod()&#xa;	return ENoError&#xa;&#xa;def Profile():&#xa;	import cProfile&#xa;	import pstats&#xa;	profile_filename = 'bypy_profile.txt'&#xa;	cProfile.run('main()', profile_filename)&#xa;	statsfile = open(""profile_stats.txt"", ""wb"")&#xa;	p = pstats.Stats(profile_filename, stream=statsfile)&#xa;	stats = p.strip_dirs().sort_stats('cumulative')&#xa;	stats.print_stats()&#xa;	statsfile.close()&#xa;	sys.exit(ENoError)&#xa;&#xa;def unused():&#xa;	''' just prevent unused warnings '''&#xa;	inspect.stack()&#xa;&#xa;if __name__ == ""__main__"":&#xa;	main()&#xa;&#xa;# vim: tabstop=4 noexpandtab shiftwidth=4 softtabstop=4 ff=unix fileencoding=utf-8&#xa;"
4851463|"#!/usr/bin/python&#xa;from _functools import reduce&#xa;&#xa;import copy&#xa;from bitcoin.main import *&#xa;### Hex to bin converter and vice versa for objects&#xa;&#xa;&#xa;def json_is_base(obj, base):&#xa;    if not is_python2 and isinstance(obj, bytes):&#xa;        return False&#xa;&#xa;    alpha = get_code_string(base)&#xa;    if isinstance(obj, string_types):&#xa;        for i in range(len(obj)):&#xa;            if alpha.find(obj[i]) == -1:&#xa;                return False&#xa;        return True&#xa;    elif isinstance(obj, int_types) or obj is None:&#xa;        return True&#xa;    elif isinstance(obj, list):&#xa;        for i in range(len(obj)):&#xa;            if not json_is_base(obj[i], base):&#xa;                return False&#xa;        return True&#xa;    else:&#xa;        for x in obj:&#xa;            if not json_is_base(obj[x], base):&#xa;                return False&#xa;        return True&#xa;&#xa;&#xa;def json_changebase(obj, changer):&#xa;    if isinstance(obj, string_or_bytes_types):&#xa;        return changer(obj)&#xa;    elif isinstance(obj, int_types) or obj is None:&#xa;        return obj&#xa;    elif isinstance(obj, list):&#xa;        return [json_changebase(x, changer) for x in obj]&#xa;    return dict((x, json_changebase(obj[x], changer)) for x in obj)&#xa;&#xa;# Transaction serialization and deserialization&#xa;&#xa;def deserialize(tx):&#xa;    if is_hexilified(tx):&#xa;        #tx = bytes(bytearray.fromhex(tx))&#xa;        return json_changebase(deserialize(binascii.unhexlify(tx)),&#xa;                              lambda x: safe_hexlify(x))&#xa;    # http://stackoverflow.com/questions/4851463/python-closure-write-to-variable-in-parent-scope&#xa;    # Python's scoping rules are demented, requiring me to make pos an object&#xa;    # so that it is call-by-reference&#xa;    pos = [0]&#xa;&#xa;    def read_as_int(bytez):&#xa;        pos[0] += bytez&#xa;        return decode(tx[pos[0]-bytez:pos[0]][::-1], 256)&#xa;&#xa;    def read_var_int():&#xa;        pos[0] += 1&#xa;&#xa;        val = from_byte_to_int(tx[pos[0]-1])&#xa;        if val < 253:&#xa;            return val&#xa;        return read_as_int(pow(2, val - 252))&#xa;&#xa;    def read_bytes(bytez):&#xa;        pos[0] += bytez&#xa;        return tx[pos[0]-bytez:pos[0]]&#xa;&#xa;    def read_var_string():&#xa;        size = read_var_int()&#xa;        return read_bytes(size)&#xa;&#xa;    obj = {""ins"": [], ""outs"": []}&#xa;    obj[""version""] = read_as_int(4)&#xa;    ins = read_var_int()&#xa;&#xa;    "" begin segwit ""&#xa;    segwit_flag = False&#xa;    if not ins:&#xa;        segwit_flag = read_var_int()&#xa;        ins = read_var_int()&#xa;    "" end segwit ""&#xa;&#xa;    for i in range(ins):&#xa;        obj[""ins""].append({&#xa;            ""outpoint"": {&#xa;                ""hash"": read_bytes(32)[::-1],&#xa;                ""index"": read_as_int(4)&#xa;            },&#xa;            ""script"": read_var_string(),&#xa;            ""sequence"": read_as_int(4)&#xa;        })&#xa;    outs = read_var_int()&#xa;    for i in range(outs):&#xa;        obj[""outs""].append({&#xa;            ""value"": read_as_int(8),&#xa;            ""script"": read_var_string()&#xa;        })&#xa;    if segwit_flag:&#xa;        obj[""segwit""] = True&#xa;        for i in range(ins):&#xa;            howmany = read_var_int()&#xa;            if howmany:&#xa;                obj['ins'][i]['txinwitness'] = [read_var_string() for x in range(0, howmany)]&#xa;&#xa;    obj[""locktime""] = read_as_int(4)&#xa;    return obj&#xa;&#xa;&#xa;def serialize(txobj):&#xa;    SEGWIT_MARKER = b'\x00'&#xa;    SEGWIT_FLAG = b'\x01'&#xa;    o = []&#xa;    if json_is_base(txobj, 16):&#xa;        json_changedbase = json_changebase(txobj, lambda x: binascii.unhexlify(x))&#xa;        return str(binascii.hexlify(serialize(json_changedbase)).decode())&#xa;    o.append(encode(txobj[""version""], 256, 4)[::-1])&#xa;&#xa;    "" begin segwit ""&#xa;    segwit = txobj.get('segwit', False)&#xa;    "" end segwit ""&#xa;&#xa;    o.append(num_to_var_int(len(txobj[""ins""])))&#xa;    for inp in txobj[""ins""]:&#xa;        inp['script'] = inp.get('script', '')&#xa;        "" begin segwit ""&#xa;        segwit = bool(inp.get(""txinwitness"") != None) if not segwit else segwit&#xa;        "" end segwit ""&#xa;        o.append(inp[""outpoint""][""hash""][::-1])&#xa;        o.append(encode(inp[""outpoint""][""index""], 256, 4)[::-1])&#xa;        out_len = num_to_var_int(len(inp[""script""]))&#xa;        script = inp['script'] if inp.get('script') else b''&#xa;        o.append(out_len + script)&#xa;        o.append(encode(inp.get('sequence', 4294967295), 256, 4)[::-1])&#xa;    o.append(num_to_var_int(len(txobj[""outs""])))&#xa;    for out in txobj[""outs""]:&#xa;        out['script'] = out['script']&#xa;        o.append(encode(out[""value""], 256, 8)[::-1])&#xa;        o.append(num_to_var_int(len(out[""script""]))+out[""script""])&#xa;&#xa;    "" begin segwit ""&#xa;    if segwit:&#xa;        o.insert(1, SEGWIT_MARKER + SEGWIT_FLAG)&#xa;        for inp in txobj[""ins""]:&#xa;            if not isinstance(inp.get('txinwitness', None), list):&#xa;                o.append(num_to_var_int(0))&#xa;            else:&#xa;                if len(inp.get('txinwitness')):&#xa;                    o.append(num_to_var_int(len(inp.get('txinwitness'))))&#xa;                    for i in inp.get('txinwitness'):&#xa;                        o.append(num_to_var_int(len(i)))&#xa;                        if i:&#xa;                            o.append(i)&#xa;                else:&#xa;                    o.append(num_to_var_int(2))&#xa;                    o.append(num_to_var_int(0) * 2)&#xa;&#xa;    "" end segwit ""&#xa;&#xa;    o.append(encode(txobj[""locktime""], 256, 4)[::-1])&#xa;    return b''.join(o)&#xa;&#xa;&#xa;def signature_form(tx, i, script, hashcode=SIGHASH_ALL):&#xa;    i, hashcode = int(i), int(hashcode)&#xa;    if isinstance(tx, string_or_bytes_types):&#xa;        sform = signature_form(deserialize(tx), i, script, hashcode)&#xa;        return serialize(sform)&#xa;&#xa;    newtx = copy.deepcopy(tx)&#xa;    for inp in newtx[""ins""]:&#xa;        inp[""script""] =  """"&#xa;    newtx[""ins""][i][""script""] = script&#xa;&#xa;    if hashcode == SIGHASH_NONE:&#xa;        newtx[""outs""] = []&#xa;    elif hashcode == SIGHASH_SINGLE:&#xa;        newtx[""outs""] = newtx[""outs""][:len(newtx[""ins""])]&#xa;        for out in newtx[""outs""][:len(newtx[""ins""]) - 1]:&#xa;            out['value'] = 2**64 - 1&#xa;            out['script'] = """"&#xa;    elif hashcode == SIGHASH_ANYONECANPAY:&#xa;        newtx[""ins""] = [newtx[""ins""][i]]&#xa;    else:&#xa;        pass&#xa;    return newtx&#xa;&#xa;&#xa;# Making the actual signatures&#xa;&#xa;&#xa;def encode_num(n):&#xa;    h = binascii.hexlify(encode(n,256))&#xa;    b = binascii.unhexlify(h)&#xa;    if ord(b[0]) < 0x80:&#xa;        return h&#xa;    else:&#xa;        return '00' + h&#xa;&#xa;def der_encode_sig(v, r, s):&#xa;    b1, b2 = safe_hexlify(encode(r, 256)), safe_hexlify(encode(s, 256))&#xa;    if len(b1) and b1[0] in '89abcdef':&#xa;        b1 = '00' + b1&#xa;    if len(b2) and b2[0] in '89abcdef':&#xa;        b2 = '00' + b2&#xa;    left = '02'+encode(len(b1)//2, 16, 2)+b1&#xa;    right = '02'+encode(len(b2)//2, 16, 2)+b2&#xa;    return '30'+encode(len(left+right)//2, 16, 2)+left+right&#xa;&#xa;def der_decode_sig(sig):&#xa;    leftlenbytes = decode(sig[6:8], 16)&#xa;    leftlen = leftlenbytes*2&#xa;    left = sig[8:8+leftlen]&#xa;    rightlenbytes = decode(sig[10+leftlen:12+leftlen], 16)&#xa;    rightlen = rightlenbytes*2&#xa;    right = sig[12+leftlen:12+leftlen+rightlen]&#xa;    return (leftlenbytes, decode(left, 16), rightlenbytes, decode(right, 16))&#xa;&#xa;def is_bip66(sig):&#xa;    """"""Checks hex DER sig for BIP66 consistency""""""&#xa;    #https://raw.githubusercontent.com/bitcoin/bips/master/bip-0066.mediawiki&#xa;    #0x30  [total-len]  0x02  [R-len]  [R]  0x02  [S-len]  [S]  [sighash]&#xa;    sig = bytearray.fromhex(sig) if re.match('^[0-9a-fA-F]*$', sig) else bytearray(sig)&#xa;    if (sig[0] == 0x30) and (sig[1] == len(sig)-2):     # check if sighash is missing&#xa;            sig.extend(b""\1"")		                   	# add SIGHASH_ALL for testing&#xa;    #assert (sig[-1] & 124 == 0) and (not not sig[-1]), ""Bad SIGHASH value""&#xa;&#xa;    if len(sig) < 9 or len(sig) > 73: return False&#xa;    if (sig[0] != 0x30): return False&#xa;    if (sig[1] != len(sig)-3): return False&#xa;    rlen = sig[3]&#xa;    if (5+rlen >= len(sig)): return False&#xa;    slen = sig[5+rlen]&#xa;    if (rlen + slen + 7 != len(sig)): return False&#xa;    if (sig[2] != 0x02): return False&#xa;    if (rlen == 0): return False&#xa;    if (sig[4] & 0x80): return False&#xa;    if (rlen > 1 and (sig[4] == 0x00) and not (sig[5] & 0x80)): return False&#xa;    if (sig[4+rlen] != 0x02): return False&#xa;    if (slen == 0): return False&#xa;    if (sig[rlen+6] & 0x80): return False&#xa;    if (slen > 1 and (sig[6+rlen] == 0x00) and not (sig[7+rlen] & 0x80)):&#xa;        return False&#xa;    return True&#xa;&#xa;def txhash(tx, hashcode=None):&#xa;    if isinstance(tx, str) and re.match('^[0-9a-fA-F]*$', tx):&#xa;        tx = changebase(tx, 16, 256)&#xa;    if hashcode is not None:&#xa;        return dbl_sha256(from_string_to_bytes(tx) + encode(int(hashcode), 256, 4)[::-1])&#xa;    else:&#xa;        return safe_hexlify(bin_dbl_sha256(tx)[::-1])&#xa;&#xa;&#xa;def bin_txhash(tx, hashcode=None):&#xa;    return binascii.unhexlify(txhash(tx, hashcode))&#xa;&#xa;&#xa;def ecdsa_tx_sign(tx, priv, hashcode=SIGHASH_ALL):&#xa;    rawsig = ecdsa_raw_sign(bin_txhash(tx, hashcode), priv)&#xa;    return der_encode_sig(*rawsig)+encode(hashcode, 16, 2)&#xa;&#xa;&#xa;def ecdsa_tx_verify(tx, sig, pub, hashcode=SIGHASH_ALL):&#xa;    return ecdsa_raw_verify(bin_txhash(tx, hashcode), der_decode_sig(sig), pub)&#xa;&#xa;&#xa;def ecdsa_tx_recover(tx, sig, hashcode=SIGHASH_ALL):&#xa;    z = bin_txhash(tx, hashcode)&#xa;    rlen, r, slen, s = der_decode_sig(sig)&#xa;    left = ecdsa_raw_recover(z, (0, r, s))&#xa;    right = ecdsa_raw_recover(z, (1, r, s))&#xa;    return (encode_pubkey(left, 'hex'), encode_pubkey(right, 'hex'))&#xa;&#xa;# Scripts&#xa;&#xa;&#xa;def mk_pubkey_script(addr):&#xa;    # Keep the auxiliary functions around for altcoins' sake&#xa;    return '76a914' + b58check_to_hex(addr) + '88ac'&#xa;&#xa;&#xa;def mk_scripthash_script(addr):&#xa;    return 'a914' + b58check_to_hex(addr) + '87'&#xa;&#xa;# Address representation to output script&#xa;&#xa;&#xa;def address_to_script(addr):&#xa;    if addr[0] == '3' or addr[0] == '2':&#xa;        return mk_scripthash_script(addr)&#xa;    else:&#xa;        return mk_pubkey_script(addr)&#xa;&#xa;# Output script to address representation&#xa;&#xa;&#xa;def script_to_address(script, vbyte=0):&#xa;    if re.match('^[0-9a-fA-F]*$', script):&#xa;        script = binascii.unhexlify(script)&#xa;    if script[:3] == b'\x76\xa9\x14' and script[-2:] == b'\x88\xac' and len(script) == 25:&#xa;        return bin_to_b58check(script[3:-2], vbyte)  # pubkey hash addresses&#xa;    else:&#xa;        if vbyte in [111, 196]:&#xa;            # Testnet&#xa;            scripthash_byte = 196&#xa;        elif vbyte == 0:&#xa;            # Mainnet&#xa;            scripthash_byte = 5&#xa;        else:&#xa;            scripthash_byte = vbyte&#xa;        # BIP0016 scripthash addresses&#xa;        return bin_to_b58check(script[2:-1], scripthash_byte)&#xa;&#xa;&#xa;def p2sh_scriptaddr(script, magicbyte=5):&#xa;    if re.match('^[0-9a-fA-F]*$', script):&#xa;        script = binascii.unhexlify(script)&#xa;    return hex_to_b58check(hash160(script), magicbyte)&#xa;scriptaddr = p2sh_scriptaddr&#xa;&#xa;&#xa;def deserialize_script(script):&#xa;    if isinstance(script, str) and re.match('^[0-9a-fA-F]*$', script):&#xa;       return json_changebase(deserialize_script(binascii.unhexlify(script)),&#xa;                              lambda x: safe_hexlify(x))&#xa;    out, pos = [], 0&#xa;    while pos < len(script):&#xa;        code = from_byte_to_int(script[pos])&#xa;        if code == 0:&#xa;            out.append(None)&#xa;            pos += 1&#xa;        elif code <= 75:&#xa;            out.append(script[pos+1:pos+1+code])&#xa;            pos += 1 + code&#xa;        elif code <= 78:&#xa;            szsz = pow(2, code - 76)&#xa;            sz = decode(script[pos+szsz: pos:-1], 256)&#xa;            out.append(script[pos + 1 + szsz:pos + 1 + szsz + sz])&#xa;            pos += 1 + szsz + sz&#xa;        elif code <= 96:&#xa;            out.append(code - 80)&#xa;            pos += 1&#xa;        else:&#xa;            out.append(code)&#xa;            pos += 1&#xa;    return out&#xa;&#xa;&#xa;def serialize_script_unit(unit):&#xa;    if isinstance(unit, int):&#xa;        if unit < 16:&#xa;            return from_int_to_byte(unit + 80)&#xa;        else:&#xa;            return from_int_to_byte(unit)&#xa;    elif unit is None:&#xa;        return b'\x00'&#xa;    else:&#xa;        if len(unit) <= 75:&#xa;            return from_int_to_byte(len(unit))+unit&#xa;        elif len(unit) < 256:&#xa;            return from_int_to_byte(76)+from_int_to_byte(len(unit))+unit&#xa;        elif len(unit) < 65536:&#xa;            return from_int_to_byte(77)+encode(len(unit), 256, 2)[::-1]+unit&#xa;        else:&#xa;            return from_int_to_byte(78)+encode(len(unit), 256, 4)[::-1]+unit&#xa;&#xa;&#xa;if is_python2:&#xa;    def serialize_script(script):&#xa;        if json_is_base(script, 16):&#xa;            return binascii.hexlify(serialize_script(json_changebase(script,&#xa;                                    lambda x: binascii.unhexlify(x))))&#xa;        return ''.join(map(serialize_script_unit, script))&#xa;else:&#xa;    def serialize_script(script):&#xa;        if json_is_base(script, 16):&#xa;            return safe_hexlify(serialize_script(json_changebase(script,&#xa;                                    lambda x: binascii.unhexlify(x))))&#xa;&#xa;        result = bytes()&#xa;        for b in map(serialize_script_unit, script):&#xa;            result += b if isinstance(b, bytes) else bytes(b, 'utf-8')&#xa;        return result&#xa;&#xa;&#xa;def mk_OPCS_multisig_script(form):&#xa;    script = []&#xa;    OP_CODESEPARATOR = from_int_to_byte(0xAB)&#xa;    OP_CHECKSIG = from_int_to_byte(0xAC)&#xa;    OP_CHECKSIGVERIFY = from_int_to_byte(0xAD)&#xa;    OP_CHECKMULTISIG = from_int_to_byte(0xAE)&#xa;    OP_CHECKMULTISIGVERIFY = from_int_to_byte(0xAF)&#xa;    """"""&#xa;    uses OP_CODESEPARATOR to build the multisig redeem script&#xa;    form:&#xa;&#xa;    {&#xa;    'keys': ['hex_pubkey_1',&#xa;             'hex_pubkey_2',&#xa;             'hex_pubkey_3'&#xa;    'schema': [&#xa;            {&#xa;                'reqs': 1,&#xa;                'keys': [0],&#xa;            },&#xa;            {&#xa;                'reqs': 1,&#xa;                'keys': [1, 2],&#xa;            }&#xa;        ]&#xa;    }&#xa;&#xa;    """"""&#xa;    for i, s in enumerate(form['schema']):&#xa;        subscript = []&#xa;        for k in s['keys']:&#xa;            subscript.append(from_int_to_byte(len(form['keys'][k]) // 2))&#xa;            subscript.append(safe_from_hex(form['keys'][k]))&#xa;        if len(s['keys']) == 1:&#xa;            subscript.append(OP_CHECKSIG if i == len(s) -1 else OP_CHECKSIGVERIFY)&#xa;        elif len(s['keys']) > 1:&#xa;            subscript.insert(from_int_to_byte(s['reqs']), 0)&#xa;            subscript.insert(from_int_to_byte(len(s['keys'])), len(s['keys']) + 1)&#xa;            subscript.append(OP_CHECKMULTISIG if i == len(s) -1 else OP_CHECKMULTISIGVERIFY)&#xa;        else:&#xa;            raise ValueError('no keys condition')&#xa;        if i != len(s) - 1:&#xa;            subscript.append(OP_CODESEPARATOR)&#xa;        script = script + subscript&#xa;    res = binascii.hexlify(b''.join(script)).decode('ascii')&#xa;    return res&#xa;&#xa;&#xa;def mk_multisig_script(*args):  # [pubs],k or pub1,pub2...pub[n],k&#xa;    if isinstance(args[0], list):&#xa;        pubs, k = args[0], int(args[1])&#xa;    elif len(args) == 1 and isinstance(args[0], dict):&#xa;        return mk_OPCS_multisig_script(args[0])&#xa;    else:&#xa;        pubs = list(filter(lambda x: len(str(x)) >= 32, args))&#xa;        k = int(args[len(pubs)])&#xa;    return serialize_script([k]+pubs+[len(pubs)]+[0xae])&#xa;&#xa;# Signing and verifying&#xa;&#xa;&#xa;def verify_tx_input(tx, i, script, sig, pub):&#xa;    if re.match('^[0-9a-fA-F]*$', tx):&#xa;        tx = binascii.unhexlify(tx)&#xa;    if re.match('^[0-9a-fA-F]*$', script):&#xa;        script = binascii.unhexlify(script)&#xa;    if not re.match('^[0-9a-fA-F]*$', sig):&#xa;        sig = safe_hexlify(sig)&#xa;    hashcode = decode(sig[-2:], 16)&#xa;    modtx = signature_form(tx, int(i), script, hashcode)&#xa;    return ecdsa_tx_verify(modtx, sig, pub, hashcode)&#xa;&#xa;&#xa;def sign(tx, i, priv, hashcode=SIGHASH_ALL):&#xa;    i = int(i)&#xa;    txobj = tx if isinstance(tx, dict) else deserialize(tx)&#xa;    if not isinstance(tx, dict) and ((not is_python2 and isinstance(re, bytes)) or not re.match('^[0-9a-fA-F]*$', tx)):&#xa;        return binascii.unhexlify(sign(safe_hexlify(tx), i, priv))&#xa;    if len(priv) <= 33:&#xa;        priv = safe_hexlify(priv)&#xa;    pub = privkey_to_pubkey(priv)&#xa;    address = pubkey_to_address(pub)&#xa;    signing_tx = signature_form(tx, i, mk_pubkey_script(address), hashcode)&#xa;    sig = ecdsa_tx_sign(signing_tx, priv, hashcode)&#xa;    txobj[""ins""][i]['script'] = serialize_script([sig, pub])&#xa;    return serialize(txobj)&#xa;&#xa;def p2pk_sign(tx, i, priv, hashcode=SIGHASH_ALL):&#xa;    i = int(i)&#xa;    txobj = tx if isinstance(tx, dict) else deserialize(tx)&#xa;    if not isinstance(tx, dict) and ((not is_python2 and isinstance(re, bytes)) or not re.match('^[0-9a-fA-F]*$', tx)):&#xa;        return binascii.unhexlify(sign(safe_hexlify(tx), i, priv))&#xa;    if len(priv) <= 33:&#xa;        priv = safe_hexlify(priv)&#xa;    pub = privkey_to_pubkey(priv)&#xa;    signing_tx = signature_form(tx, i, '21' + pub + 'ac', hashcode)&#xa;    sig = ecdsa_tx_sign(signing_tx, priv, hashcode)&#xa;    txobj[""ins""][i]['script'] = serialize_script([sig])&#xa;    return serialize(txobj)&#xa;&#xa;&#xa;def signall(tx, priv):&#xa;    # if priv is a dictionary, assume format is&#xa;    # { 'txinhash:txinidx' : privkey }&#xa;    if isinstance(priv, dict):&#xa;        for e, i in enumerate(deserialize(tx)[""ins""]):&#xa;            k = priv[""%s:%d"" % (i[""outpoint""][""hash""], i[""outpoint""][""index""])]&#xa;            tx = sign(tx, e, k)&#xa;    else:&#xa;        for i in range(len(deserialize(tx)[""ins""])):&#xa;            tx = sign(tx, i, priv)&#xa;    return tx&#xa;&#xa;&#xa;def multisign(tx, i, script, pk, hashcode=SIGHASH_ALL):&#xa;    if re.match('^[0-9a-fA-F]*$', tx):&#xa;        tx = binascii.unhexlify(tx)&#xa;    if re.match('^[0-9a-fA-F]*$', script):&#xa;        script = binascii.unhexlify(script)&#xa;    modtx = signature_form(tx, i, script, hashcode)&#xa;    return ecdsa_tx_sign(modtx, pk, hashcode)&#xa;&#xa;&#xa;def is_inp(arg):&#xa;    return len(arg) > 64 or ""output"" in arg or ""outpoint"" in arg&#xa;&#xa;&#xa;def mktx(*args, **kwargs):&#xa;    # [in0, in1...],[out0, out1...] or in0, in1 ... out0 out1 ...&#xa;    ser = kwargs.get('serialize', True)&#xa;    ins, outs = [], []&#xa;    for arg in args:&#xa;        if isinstance(arg, list):&#xa;            for a in arg: (ins if is_inp(a) else outs).append(a)&#xa;        else:&#xa;            (ins if is_inp(arg) else outs).append(arg)&#xa;&#xa;    txobj = {""locktime"": kwargs.get('locktime', 0), ""version"": 1, ""ins"": [], ""outs"": []}&#xa;    for i in ins:&#xa;&#xa;        "" begin segwit ""&#xa;        seg_input = isinstance(i, dict) and i.get('segregated')&#xa;        sequence = isinstance(i, dict) and i.get('sequence', None)&#xa;        "" end segwit ""&#xa;&#xa;        if isinstance(i, dict) and ""outpoint"" in i:&#xa;            txobj[""ins""].append(i)&#xa;        else:&#xa;            if isinstance(i, dict) and ""output"" in i:&#xa;                i = i[""output""]&#xa;            txobj[""ins""].append({&#xa;                ""outpoint"": {""hash"": i[:64], ""index"": int(i[65:])},&#xa;                ""script"": """",&#xa;                ""sequence"": 4294967295 if not sequence and sequence != 0 else sequence&#xa;            })&#xa;        if seg_input:&#xa;            txobj[""ins""][-1].update({'txinwitness': []})&#xa;&#xa;    for o in outs:&#xa;        if isinstance(o, string_or_bytes_types):&#xa;            addr = o[:o.find(':')]&#xa;            val = int(o[o.find(':')+1:])&#xa;            o = {}&#xa;            if re.match('^[0-9a-fA-F]*$', addr):&#xa;                o[""script""] = addr&#xa;            else:&#xa;                o[""address""] = addr&#xa;            o[""value""] = val&#xa;        outobj = {}&#xa;        if ""address"" in o:&#xa;            outobj[""script""] = address_to_script(o[""address""])&#xa;        elif ""script"" in o:&#xa;            outobj[""script""] = o[""script""]&#xa;        else:&#xa;            raise Exception(""Could not find 'address' or 'script' in output."")&#xa;        outobj[""value""] = o[""value""]&#xa;        txobj[""outs""].append(outobj)&#xa;    return txobj if not ser else serialize(txobj)&#xa;&#xa;&#xa;def select(unspent, value):&#xa;    value = int(value)&#xa;    high = [u for u in unspent if u[""value""] >= value]&#xa;    high.sort(key=lambda u: u[""value""])&#xa;    low = [u for u in unspent if u[""value""] < value]&#xa;    low.sort(key=lambda u: -u[""value""])&#xa;    if len(high):&#xa;        return [high[0]]&#xa;    i, tv = 0, 0&#xa;    while tv < value and i < len(low):&#xa;        tv += low[i][""value""]&#xa;        i += 1&#xa;    if tv < value:&#xa;        raise Exception(""Not enough funds"")&#xa;    return low[:i]&#xa;&#xa;# Only takes inputs of the form { ""output"": blah, ""value"": foo }&#xa;&#xa;&#xa;def mksend(*args, **kwargs):&#xa;    argz, change, fee = args[:-2], args[-2], int(args[-1])&#xa;    ins, outs = [], []&#xa;    for arg in argz:&#xa;        if isinstance(arg, list):&#xa;            for a in arg:&#xa;                (ins if is_inp(a) else outs).append(a)&#xa;        else:&#xa;            (ins if is_inp(arg) else outs).append(arg)&#xa;&#xa;    isum = sum([i[""value""] for i in ins])&#xa;    osum, outputs2 = 0, []&#xa;    for o in outs:&#xa;        if isinstance(o, string_types):&#xa;            o2 = {&#xa;                ""address"": o[:o.find(':')],&#xa;                ""value"": int(o[o.find(':')+1:])&#xa;            }&#xa;        else:&#xa;            o2 = o&#xa;        outputs2.append(o2)&#xa;        osum += o2[""value""]&#xa;&#xa;    if isum < osum+fee:&#xa;        raise Exception(""Not enough money"")&#xa;    elif isum > osum+fee+5430:&#xa;        outputs2 += [{""address"": change, ""value"": isum-osum-fee}]&#xa;&#xa;    return mktx(ins, outputs2, **kwargs)&#xa;&#xa;def mk_opreturn(msg, rawtx=None, json=0):&#xa;    def op_push(data):&#xa;        import struct&#xa;        if len(data) < 0x4c:&#xa;            return from_int_to_byte(len(data)) + data&#xa;        elif len(data) < 0xff:&#xa;            return from_int_to_byte(76) + struct.pack('<B', len(data)) + from_string_to_bytes(data)&#xa;        elif len(data) < 0xffff:&#xa;            return from_int_to_byte(77) + struct.pack('<H', len(data)) + from_string_to_bytes(data)&#xa;        elif len(data) < 0xffffffff:&#xa;            return from_int_to_byte(78) + struct.pack('<I', len(data)) + from_string_to_bytes(data)&#xa;        else:&#xa;            raise Exception(""Input data error. Rawtx must be hex chars"" \&#xa;                            + ""0xffffffff > len(data) > 0 ??"")&#xa;&#xa;    orhex = safe_hexlify(b'\x6a' + op_push(msg))&#xa;    orjson = {'script' : orhex, 'value' : 0}&#xa;    if rawtx is not None:&#xa;        try:&#xa;            txo = deserialize(rawtx)&#xa;            if not 'outs' in txo.keys(): raise Exception(""OP_Return cannot be the sole output!"")&#xa;            txo['outs'].append(orjson)&#xa;            newrawtx = serialize(txo)&#xa;            return newrawtx&#xa;        except:&#xa;            raise Exception(""Raw Tx Error!"")&#xa;    return orhex if not json else orjson"
2758159|"# source: http://stackoverflow.com/questions/2758159/how-to-embed-a-python-interpreter-in-a-pyqt-widget&#xa;&#xa;import sys, os, re&#xa;import traceback, platform&#xa;from PyQt4 import QtCore&#xa;from PyQt4 import QtGui&#xa;from electrum_nvc import util&#xa;&#xa;&#xa;if platform.system() == 'Windows':&#xa;    MONOSPACE_FONT = 'Lucida Console'&#xa;elif platform.system() == 'Darwin':&#xa;    MONOSPACE_FONT = 'Monaco'&#xa;else:&#xa;    MONOSPACE_FONT = 'monospace'&#xa;&#xa;&#xa;class Console(QtGui.QPlainTextEdit):&#xa;    def __init__(self, prompt='>> ', startup_message='', parent=None):&#xa;        QtGui.QPlainTextEdit.__init__(self, parent)&#xa;&#xa;        self.prompt = prompt&#xa;        self.history = []&#xa;        self.namespace = {}&#xa;        self.construct = []&#xa;&#xa;        self.setGeometry(50, 75, 600, 400)&#xa;        self.setWordWrapMode(QtGui.QTextOption.WrapAnywhere)&#xa;        self.setUndoRedoEnabled(False)&#xa;        self.document().setDefaultFont(QtGui.QFont(MONOSPACE_FONT, 10, QtGui.QFont.Normal))&#xa;        self.showMessage(startup_message)&#xa;&#xa;        self.updateNamespace({'run':self.run_script})&#xa;        self.set_json(False)&#xa;&#xa;    def set_json(self, b):&#xa;        self.is_json = b&#xa;    &#xa;    def run_script(self, filename):&#xa;        with open(filename) as f:&#xa;            script = f.read()&#xa;&#xa;        # eval is generally considered bad practice. use it wisely!&#xa;        result = eval(script, self.namespace, self.namespace)&#xa;&#xa;&#xa;&#xa;    def updateNamespace(self, namespace):&#xa;        self.namespace.update(namespace)&#xa;&#xa;    def showMessage(self, message):&#xa;        self.appendPlainText(message)&#xa;        self.newPrompt()&#xa;&#xa;    def clear(self):&#xa;        self.setPlainText('')&#xa;        self.newPrompt()&#xa;&#xa;    def newPrompt(self):&#xa;        if self.construct:&#xa;            prompt = '.' * len(self.prompt)&#xa;        else:&#xa;            prompt = self.prompt&#xa;&#xa;        self.completions_pos = self.textCursor().position()&#xa;        self.completions_visible = False&#xa;&#xa;        self.appendPlainText(prompt)&#xa;        self.moveCursor(QtGui.QTextCursor.End)&#xa;&#xa;    def getCommand(self):&#xa;        doc = self.document()&#xa;        curr_line = unicode(doc.findBlockByLineNumber(doc.lineCount() - 1).text())&#xa;        curr_line = curr_line.rstrip()&#xa;        curr_line = curr_line[len(self.prompt):]&#xa;        return curr_line&#xa;&#xa;    def setCommand(self, command):&#xa;        if self.getCommand() == command:&#xa;            return&#xa;&#xa;        doc = self.document()&#xa;        curr_line = unicode(doc.findBlockByLineNumber(doc.lineCount() - 1).text())&#xa;        self.moveCursor(QtGui.QTextCursor.End)&#xa;        for i in range(len(curr_line) - len(self.prompt)):&#xa;            self.moveCursor(QtGui.QTextCursor.Left, QtGui.QTextCursor.KeepAnchor)&#xa;&#xa;        self.textCursor().removeSelectedText()&#xa;        self.textCursor().insertText(command)&#xa;        self.moveCursor(QtGui.QTextCursor.End)&#xa;&#xa;&#xa;    def show_completions(self, completions):&#xa;        if self.completions_visible:&#xa;            self.hide_completions()&#xa;&#xa;        c = self.textCursor()&#xa;        c.setPosition(self.completions_pos)&#xa;&#xa;        completions = map(lambda x: x.split('.')[-1], completions)&#xa;        t = '\n' + ' '.join(completions)&#xa;        if len(t) > 500:&#xa;            t = t[:500] + '...'&#xa;        c.insertText(t)&#xa;        self.completions_end = c.position()&#xa;&#xa;        self.moveCursor(QtGui.QTextCursor.End)&#xa;        self.completions_visible = True&#xa;        &#xa;&#xa;    def hide_completions(self):&#xa;        if not self.completions_visible:&#xa;            return&#xa;        c = self.textCursor()&#xa;        c.setPosition(self.completions_pos)&#xa;        l = self.completions_end - self.completions_pos&#xa;        for x in range(l): c.deleteChar()&#xa;&#xa;        self.moveCursor(QtGui.QTextCursor.End)&#xa;        self.completions_visible = False&#xa;&#xa;&#xa;    def getConstruct(self, command):&#xa;        if self.construct:&#xa;            prev_command = self.construct[-1]&#xa;            self.construct.append(command)&#xa;            if not prev_command and not command:&#xa;                ret_val = '\n'.join(self.construct)&#xa;                self.construct = []&#xa;                return ret_val&#xa;            else:&#xa;                return ''&#xa;        else:&#xa;            if command and command[-1] == (':'):&#xa;                self.construct.append(command)&#xa;                return ''&#xa;            else:&#xa;                return command&#xa;&#xa;    def getHistory(self):&#xa;        return self.history&#xa;&#xa;    def setHisory(self, history):&#xa;        self.history = history&#xa;&#xa;    def addToHistory(self, command):&#xa;        if command.find(""importprivkey"") > -1:&#xa;            return&#xa;        &#xa;        if command and (not self.history or self.history[-1] != command):&#xa;            self.history.append(command)&#xa;        self.history_index = len(self.history)&#xa;&#xa;    def getPrevHistoryEntry(self):&#xa;        if self.history:&#xa;            self.history_index = max(0, self.history_index - 1)&#xa;            return self.history[self.history_index]&#xa;        return ''&#xa;&#xa;    def getNextHistoryEntry(self):&#xa;        if self.history:&#xa;            hist_len = len(self.history)&#xa;            self.history_index = min(hist_len, self.history_index + 1)&#xa;            if self.history_index < hist_len:&#xa;                return self.history[self.history_index]&#xa;        return ''&#xa;&#xa;    def getCursorPosition(self):&#xa;        c = self.textCursor()&#xa;        return c.position() - c.block().position() - len(self.prompt)&#xa;&#xa;    def setCursorPosition(self, position):&#xa;        self.moveCursor(QtGui.QTextCursor.StartOfLine)&#xa;        for i in range(len(self.prompt) + position):&#xa;            self.moveCursor(QtGui.QTextCursor.Right)&#xa;&#xa;    def register_command(self, c, func):&#xa;        methods = { c: func}&#xa;        self.updateNamespace(methods)&#xa;        &#xa;&#xa;    def runCommand(self):&#xa;        command = self.getCommand()&#xa;        self.addToHistory(command)&#xa;&#xa;        command = self.getConstruct(command)&#xa;&#xa;        if command:&#xa;            tmp_stdout = sys.stdout&#xa;&#xa;            class stdoutProxy():&#xa;                def __init__(self, write_func):&#xa;                    self.write_func = write_func&#xa;                    self.skip = False&#xa;&#xa;                def flush(self):&#xa;                    pass&#xa;&#xa;                def write(self, text):&#xa;                    if not self.skip:&#xa;                        stripped_text = text.rstrip('\n')&#xa;                        self.write_func(stripped_text)&#xa;                        QtCore.QCoreApplication.processEvents()&#xa;                    self.skip = not self.skip&#xa;&#xa;            if type(self.namespace.get(command)) == type(lambda:None):&#xa;                self.appendPlainText(""'%s' is a function. Type '%s()' to use it in the Python console.""%(command, command))&#xa;                self.newPrompt()&#xa;                return&#xa;&#xa;            sys.stdout = stdoutProxy(self.appendPlainText)&#xa;            try:&#xa;                try:&#xa;                    # eval is generally considered bad practice. use it wisely!&#xa;                    result = eval(command, self.namespace, self.namespace)&#xa;                    if result != None:&#xa;                        if self.is_json:&#xa;                            util.print_json(result)&#xa;                        else:&#xa;                            self.appendPlainText(repr(result))&#xa;                except SyntaxError:&#xa;                    # exec is generally considered bad practice. use it wisely!&#xa;                    exec command in self.namespace&#xa;            except SystemExit:&#xa;                self.close()&#xa;            except Exception:&#xa;                traceback_lines = traceback.format_exc().split('\n')&#xa;                # Remove traceback mentioning this file, and a linebreak&#xa;                for i in (3,2,1,-1):&#xa;                    traceback_lines.pop(i)&#xa;                self.appendPlainText('\n'.join(traceback_lines))&#xa;            sys.stdout = tmp_stdout&#xa;        self.newPrompt()&#xa;        self.set_json(False)&#xa;                    &#xa;&#xa;    def keyPressEvent(self, event):&#xa;        if event.key() == QtCore.Qt.Key_Tab:&#xa;            self.completions()&#xa;            return&#xa;&#xa;        self.hide_completions()&#xa;&#xa;        if event.key() in (QtCore.Qt.Key_Enter, QtCore.Qt.Key_Return):&#xa;            self.runCommand()&#xa;            return&#xa;        if event.key() == QtCore.Qt.Key_Home:&#xa;            self.setCursorPosition(0)&#xa;            return&#xa;        if event.key() == QtCore.Qt.Key_PageUp:&#xa;            return&#xa;        elif event.key() in (QtCore.Qt.Key_Left, QtCore.Qt.Key_Backspace):&#xa;            if self.getCursorPosition() == 0:&#xa;                return&#xa;        elif event.key() == QtCore.Qt.Key_Up:&#xa;            self.setCommand(self.getPrevHistoryEntry())&#xa;            return&#xa;        elif event.key() == QtCore.Qt.Key_Down:&#xa;            self.setCommand(self.getNextHistoryEntry())&#xa;            return&#xa;        elif event.key() == QtCore.Qt.Key_L and event.modifiers() == QtCore.Qt.ControlModifier:&#xa;            self.clear()&#xa;&#xa;        super(Console, self).keyPressEvent(event)&#xa;&#xa;&#xa;&#xa;    def completions(self):&#xa;        cmd = self.getCommand()&#xa;        lastword = re.split(' |\(|\)',cmd)[-1]&#xa;        beginning = cmd[0:-len(lastword)]&#xa;&#xa;        path = lastword.split('.')&#xa;        ns = self.namespace.keys()&#xa;&#xa;        if len(path) == 1:&#xa;            ns = ns&#xa;            prefix = ''&#xa;        else:&#xa;            obj = self.namespace.get(path[0])&#xa;            prefix = path[0] + '.'&#xa;            ns = dir(obj)&#xa;            &#xa;&#xa;        completions = []&#xa;        for x in ns:&#xa;            if x[0] == '_':continue&#xa;            xx = prefix + x&#xa;            if xx.startswith(lastword):&#xa;                completions.append(xx)&#xa;        completions.sort()&#xa;                &#xa;        if not completions:&#xa;            self.hide_completions()&#xa;        elif len(completions) == 1:&#xa;            self.hide_completions()&#xa;            self.setCommand(beginning + completions[0])&#xa;        else:&#xa;            # find common prefix&#xa;            p = os.path.commonprefix(completions)&#xa;            if len(p)>len(lastword):&#xa;                self.hide_completions()&#xa;                self.setCommand(beginning + p)&#xa;            else:&#xa;                self.show_completions(completions)&#xa;&#xa;&#xa;welcome_message = '''&#xa;   ---------------------------------------------------------------&#xa;     Welcome to a primitive Python interpreter.&#xa;   ---------------------------------------------------------------&#xa;'''&#xa;&#xa;if __name__ == '__main__':&#xa;    app = QtGui.QApplication(sys.argv)&#xa;    console = Console(startup_message=welcome_message)&#xa;    console.updateNamespace({'myVar1' : app, 'myVar2' : 1234})&#xa;    console.show();&#xa;    sys.exit(app.exec_())&#xa;"
956867|"'''Compatibility&#xa;==================&#xa;&#xa;Modules to ease compatibility between different versions of python and&#xa;types.&#xa;'''&#xa;&#xa;import sys&#xa;&#xa;__all__ = ('decode_dict', 'PY2', 'unicode_type', 'bytes_type')&#xa;&#xa;PY2 = sys.version_info[0] == 2&#xa;'''Whether the python version is 2.x (True), or 3.x (False).&#xa;'''&#xa;&#xa;&#xa;def unicode_type(val):&#xa;    ''' Converts ``val`` to ``unicode`` type.&#xa;&#xa;    For example in Python 2::&#xa;&#xa;        >>> unicode_type(55)&#xa;        u'55'&#xa;        >>> unicode_type(b'hey')&#xa;        u'hey'&#xa;        >>> unicode_type(u'hey')&#xa;        u'hey'&#xa;&#xa;    and in Python 3::&#xa;&#xa;        >>> unicode_type(55)&#xa;        '55'&#xa;        >>> unicode_type(b'hey')&#xa;        'hey'&#xa;        >>> unicode_type(u'hey')&#xa;        'hey'&#xa;    '''&#xa;    if isinstance(val, bytes):&#xa;        return val.decode('utf8')&#xa;    if PY2:&#xa;        if isinstance(val, unicode):&#xa;            return val&#xa;    elif isinstance(val, str):&#xa;        return val&#xa;    return (unicode if PY2 else str)(val)&#xa;&#xa;&#xa;def bytes_type(val):&#xa;    ''' Converts ``val`` to ``bytes`` type.&#xa;&#xa;    For example in Python 2::&#xa;&#xa;        >>> bytes_type(55)&#xa;        '55'&#xa;        >>> bytes_type(b'hey')&#xa;        'hey'&#xa;        >>> bytes_type(u'hey')&#xa;        'hey'&#xa;&#xa;    and in Python 3::&#xa;&#xa;        >>> bytes_type(55)&#xa;        b'55'&#xa;        >>> bytes_type(b'hey')&#xa;        b'hey'&#xa;        >>> bytes_type(u'hey')&#xa;        b'hey'&#xa;    '''&#xa;    if isinstance(val, bytes):&#xa;        return val&#xa;    if PY2:&#xa;        if isinstance(val, unicode):&#xa;            return val.encode('utf8')&#xa;    elif isinstance(val, str):&#xa;        return val.encode('utf8')&#xa;&#xa;    if PY2:&#xa;        return str(val)&#xa;    return str(val).encode('utf8')&#xa;&#xa;&#xa;def _decode_list(data):&#xa;    '''See :func:`decode_dict`.&#xa;    '''&#xa;    rv = []&#xa;    for item in data:&#xa;        if isinstance(item, unicode):&#xa;            item = item.encode('utf-8')&#xa;        elif isinstance(item, list):&#xa;            item = _decode_list(item)&#xa;        elif isinstance(item, dict):&#xa;            item = decode_dict(item)&#xa;        rv.append(item)&#xa;    return rv&#xa;&#xa;&#xa;def decode_dict(data):&#xa;    '''Method which takes a dict `data` and recursively converts its&#xa;    keys/values that are unicode objects to bytes objects.&#xa;&#xa;    This is typically used with json. See&#xa;    https://stackoverflow.com/questions/956867.&#xa;&#xa;    E.g.::&#xa;&#xa;        >>> import json&#xa;        >>> d_dump = json.dumps({'a': 55, 'b': '33', 4: {1: 'a'},&#xa;        ... 8: ['a', 'b']})&#xa;        >>> d_dump&#xa;        '{""a"": 55, ""8"": [""a"", ""b""], ""b"": ""33"", ""4"": {""1"": ""a""}}'&#xa;        >>> json.loads(d_dump)&#xa;        {u'a': 55, u'8': [u'a', u'b'], u'b': u'33', u'4': {u'1': u'a'}}&#xa;        >>> json.loads(d_dump, object_hook=decode_dict)&#xa;        {'a': 55, '8': ['a', 'b'], 'b': '33', '4': {'1': 'a'}}&#xa;&#xa;    .. warning::&#xa;        Function is only python2 compatible. It is not typically required&#xa;        in py3.&#xa;    '''&#xa;    rv = {}&#xa;    for key, value in data.items():&#xa;        if isinstance(key, unicode):&#xa;            key = key.encode('utf-8')&#xa;        if isinstance(value, unicode):&#xa;            value = value.encode('utf-8')&#xa;        elif isinstance(value, list):&#xa;            value = _decode_list(value)&#xa;        elif isinstance(value, dict):&#xa;            value = decode_dict(value)&#xa;        rv[key] = value&#xa;    return rv&#xa;"
3300464|"# Author: Nic Wolfe <nic@wolfeden.ca>&#xa;# URL: http://code.google.com/p/sickbeard/&#xa;#&#xa;# This file is part of Sick Beard.&#xa;#&#xa;# Sick Beard is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Sick Beard is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Sick Beard.  If not, see <http://www.gnu.org/licenses/>.&#xa;&#xa;from __future__ import with_statement&#xa;&#xa;import os.path&#xa;import re&#xa;import sqlite3&#xa;import time&#xa;import threading&#xa;&#xa;import sickbeard&#xa;&#xa;from sickbeard import encodingKludge as ek&#xa;from sickbeard import logger&#xa;from sickbeard.exceptions import ex&#xa;&#xa;db_lock = threading.Lock()&#xa;&#xa;def dbFilename(filename=""sickbeard.db"", suffix=None):&#xa;    """"""&#xa;    @param filename: The sqlite database filename to use. If not specified,&#xa;                     will be made to be sickbeard.db&#xa;    @param suffix: The suffix to append to the filename. A '.' will be added&#xa;                   automatically, i.e. suffix='v0' will make dbfile.db.v0&#xa;    @return: the correct location of the database file.&#xa;    """"""&#xa;    if suffix:&#xa;        filename = ""%s.%s"" % (filename, suffix)&#xa;    return ek.ek(os.path.join, sickbeard.DATA_DIR, filename)&#xa;&#xa;class DBConnection:&#xa;    def __init__(self, filename=""sickbeard.db"", suffix=None, row_type=None):&#xa;&#xa;        self.filename = filename&#xa;        self.connection = sqlite3.connect(dbFilename(filename), 20)&#xa;        if row_type == ""dict"":&#xa;            self.connection.row_factory = self._dict_factory&#xa;        else:&#xa;            self.connection.row_factory = sqlite3.Row&#xa;&#xa;    def checkDBVersion(self):&#xa;        try:&#xa;            result = self.select(""SELECT db_version FROM db_version"")&#xa;        except sqlite3.OperationalError, e:&#xa;            if ""no such table: db_version"" in e.message:&#xa;                return 0&#xa;&#xa;        if result:&#xa;            return int(result[0][""db_version""])&#xa;        else:&#xa;            return 0&#xa;&#xa;    def mass_action(self, querylist, logTransaction=False):&#xa;    &#xa;        with db_lock:&#xa;    &#xa;            if querylist == None:&#xa;                return&#xa;    &#xa;            sqlResult = []&#xa;            attempt = 0&#xa;    &#xa;            while attempt < 5:&#xa;                try:&#xa;                    for qu in querylist:&#xa;                        if len(qu) == 1:&#xa;                            if logTransaction:&#xa;                                logger.log(qu[0], logger.DEBUG)&#xa;                            sqlResult.append(self.connection.execute(qu[0]))&#xa;                        elif len(qu) > 1:&#xa;                            if logTransaction:&#xa;                                logger.log(qu[0] + "" with args "" + str(qu[1]), logger.DEBUG)&#xa;                            sqlResult.append(self.connection.execute(qu[0], qu[1]))&#xa;                    self.connection.commit()&#xa;                    logger.log(u""Transaction with "" + str(len(querylist)) + u"" query's executed"", logger.DEBUG)&#xa;                    return sqlResult&#xa;                except sqlite3.OperationalError, e:&#xa;                    sqlResult = []&#xa;                    if self.connection:&#xa;                        self.connection.rollback()&#xa;                    if ""unable to open database file"" in e.message or ""database is locked"" in e.message:&#xa;                        logger.log(u""DB error: "" + ex(e), logger.WARNING)&#xa;                        attempt += 1&#xa;                        time.sleep(1)&#xa;                    else:&#xa;                        if ""no such table: db_version"" in e.message:&#xa;                            logger.log(u""DB error: "" + ex(e), logger.WARNING)&#xa;                        else:&#xa;                            logger.log(u""DB error: "" + ex(e), logger.ERROR)&#xa;                        raise&#xa;                except sqlite3.DatabaseError, e:&#xa;                    sqlResult = []&#xa;                    if self.connection:&#xa;                        self.connection.rollback()&#xa;                    logger.log(u""Fatal error executing query: "" + ex(e), logger.ERROR)&#xa;                    raise&#xa;    &#xa;            return sqlResult&#xa;        &#xa;    def action(self, query, args=None):&#xa;&#xa;        with db_lock:&#xa;&#xa;            if query == None:&#xa;                return&#xa;    &#xa;            sqlResult = None&#xa;            attempt = 0&#xa;    &#xa;            while attempt < 5:&#xa;                try:&#xa;                    if args == None:&#xa;                        logger.log(self.filename+"": ""+query, logger.DB)&#xa;                        sqlResult = self.connection.execute(query)&#xa;                    else:&#xa;                        logger.log(self.filename+"": ""+query+"" with args ""+str(args), logger.DB)&#xa;                        sqlResult = self.connection.execute(query, args)&#xa;                    self.connection.commit()&#xa;                    # get out of the connection attempt loop since we were successful&#xa;                    break&#xa;                except sqlite3.OperationalError, e:&#xa;                    if ""unable to open database file"" in e.message or ""database is locked"" in e.message:&#xa;                        logger.log(u""DB error: ""+ex(e), logger.WARNING)&#xa;                        attempt += 1&#xa;                        time.sleep(1)&#xa;                    else:&#xa;                        if ""no such table: db_version"" in e.message:&#xa;                            logger.log(u""DB error: "" + ex(e), logger.WARNING)&#xa;                        else:&#xa;                            logger.log(u""DB error: "" + ex(e), logger.ERROR)&#xa;                        raise&#xa;                except sqlite3.DatabaseError, e:&#xa;                    logger.log(u""Fatal error executing query: "" + ex(e), logger.ERROR)&#xa;                    raise&#xa;    &#xa;            return sqlResult&#xa;&#xa;&#xa;    def select(self, query, args=None):&#xa;&#xa;        sqlResults = self.action(query, args).fetchall()&#xa;&#xa;        if sqlResults == None:&#xa;            return []&#xa;&#xa;        return sqlResults&#xa;&#xa;    def upsert(self, tableName, valueDict, keyDict):&#xa;&#xa;        changesBefore = self.connection.total_changes&#xa;&#xa;        genParams = lambda myDict : [x + "" = ?"" for x in myDict.keys()]&#xa;&#xa;        query = ""UPDATE ""+tableName+"" SET "" + "", "".join(genParams(valueDict)) + "" WHERE "" + "" AND "".join(genParams(keyDict))&#xa;&#xa;        self.action(query, valueDict.values() + keyDict.values())&#xa;&#xa;        if self.connection.total_changes == changesBefore:&#xa;            query = ""INSERT INTO ""+tableName+"" ("" + "", "".join(valueDict.keys() + keyDict.keys()) + "")"" + \&#xa;                     "" VALUES ("" + "", "".join([""?""] * len(valueDict.keys() + keyDict.keys())) + "")""&#xa;            self.action(query, valueDict.values() + keyDict.values())&#xa;&#xa;    def tableInfo(self, tableName):&#xa;        # FIXME ? binding is not supported here, but I cannot find a way to escape a string manually&#xa;        cursor = self.connection.execute(""PRAGMA table_info(%s)"" % tableName)&#xa;        columns = {}&#xa;        for column in cursor:&#xa;            columns[column['name']] = { 'type': column['type'] }&#xa;        return columns&#xa;    &#xa;    # http://stackoverflow.com/questions/3300464/how-can-i-get-dict-from-sqlite-query&#xa;    def _dict_factory(self, cursor, row):&#xa;        d = {}&#xa;        for idx, col in enumerate(cursor.description):&#xa;            d[col[0]] = row[idx]&#xa;        return d&#xa;    &#xa;def sanityCheckDatabase(connection, sanity_check):&#xa;    sanity_check(connection).check()&#xa;&#xa;class DBSanityCheck(object):&#xa;    def __init__(self, connection):&#xa;        self.connection = connection&#xa;&#xa;    def check(self):&#xa;        pass&#xa;&#xa;# ===============&#xa;# = Upgrade API =&#xa;# ===============&#xa;&#xa;def upgradeDatabase(connection, schema):&#xa;    logger.log(u""Checking database structure..."", logger.MESSAGE)&#xa;    _processUpgrade(connection, schema)&#xa;&#xa;def prettyName(str):&#xa;    return ' '.join([x.group() for x in re.finditer(""([A-Z])([a-z0-9]+)"", str)])&#xa;&#xa;def _processUpgrade(connection, upgradeClass):&#xa;    instance = upgradeClass(connection)&#xa;    logger.log(u""Checking "" + prettyName(upgradeClass.__name__) + "" database upgrade"", logger.DEBUG)&#xa;    if not instance.test():&#xa;        logger.log(u""Database upgrade required: "" + prettyName(upgradeClass.__name__), logger.MESSAGE)&#xa;        try:&#xa;            instance.execute()&#xa;        except sqlite3.DatabaseError, e:&#xa;            print ""Error in "" + str(upgradeClass.__name__) + "": "" + ex(e)&#xa;            raise&#xa;        logger.log(upgradeClass.__name__ + "" upgrade completed"", logger.DEBUG)&#xa;    else:&#xa;        logger.log(upgradeClass.__name__ + "" upgrade not required"", logger.DEBUG)&#xa;&#xa;    for upgradeSubClass in upgradeClass.__subclasses__():&#xa;        _processUpgrade(connection, upgradeSubClass)&#xa;&#xa;# Base migration class. All future DB changes should be subclassed from this class&#xa;class SchemaUpgrade (object):&#xa;    def __init__(self, connection):&#xa;        self.connection = connection&#xa;&#xa;    def hasTable(self, tableName):&#xa;        return len(self.connection.action(""SELECT 1 FROM sqlite_master WHERE name = ?;"", (tableName, )).fetchall()) > 0&#xa;&#xa;    def hasColumn(self, tableName, column):&#xa;        return column in self.connection.tableInfo(tableName)&#xa;&#xa;    def addColumn(self, table, column, type=""NUMERIC"", default=0):&#xa;        self.connection.action(""ALTER TABLE %s ADD %s %s"" % (table, column, type))&#xa;        self.connection.action(""UPDATE %s SET %s = ?"" % (table, column), (default,))&#xa;&#xa;    def checkDBVersion(self):&#xa;        return self.connection.checkDBVersion()&#xa;&#xa;    def incDBVersion(self):&#xa;        curVersion = self.checkDBVersion()&#xa;        self.connection.action(""UPDATE db_version SET db_version = ?"", [curVersion+1])&#xa;        return curVersion+1&#xa;"
279237|"#!/usr/bin/env python&#xa;&#xa;# Test whether a retained PUBLISH to a topic with QoS 0 is sent with&#xa;# retain=false to an already subscribed client.&#xa;&#xa;import inspect, os, sys&#xa;# From http://stackoverflow.com/questions/279237/python-import-a-module-from-a-folder&#xa;cmd_subfolder = os.path.realpath(os.path.abspath(os.path.join(os.path.split(inspect.getfile( inspect.currentframe() ))[0],"".."")))&#xa;if cmd_subfolder not in sys.path:&#xa;    sys.path.insert(0, cmd_subfolder)&#xa;&#xa;import mosq_test&#xa;&#xa;rc = 1&#xa;keepalive = 60&#xa;mid = 16&#xa;connect_packet = mosq_test.gen_connect(""retain-qos0-fresh-test"", keepalive=keepalive)&#xa;connack_packet = mosq_test.gen_connack(rc=0)&#xa;&#xa;publish_packet = mosq_test.gen_publish(""retain/qos0/test"", qos=0, payload=""retained message"", retain=True)&#xa;publish_fresh_packet = mosq_test.gen_publish(""retain/qos0/test"", qos=0, payload=""retained message"")&#xa;subscribe_packet = mosq_test.gen_subscribe(mid, ""retain/qos0/test"", 0)&#xa;suback_packet = mosq_test.gen_suback(mid, 0)&#xa;&#xa;cmd = ['../../src/mosquitto', '-p', '1888']&#xa;broker = mosq_test.start_broker(filename=os.path.basename(__file__), cmd=cmd)&#xa;&#xa;try:&#xa;    sock = mosq_test.do_client_connect(connect_packet, connack_packet)&#xa;    sock.send(subscribe_packet)&#xa;&#xa;    if mosq_test.expect_packet(sock, ""suback"", suback_packet):&#xa;        sock.send(publish_packet)&#xa;&#xa;        if mosq_test.expect_packet(sock, ""publish"", publish_fresh_packet):&#xa;            rc = 0&#xa;    sock.close()&#xa;finally:&#xa;    broker.terminate()&#xa;    broker.wait()&#xa;    if rc:&#xa;        (stdo, stde) = broker.communicate()&#xa;        print(stde)&#xa;&#xa;exit(rc)&#xa;&#xa;"
2572172|"# ==================================================================================================&#xa;# Copyright 2013 Twitter, Inc.&#xa;# --------------------------------------------------------------------------------------------------&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this work except in compliance with the License.&#xa;# You may obtain a copy of the License in the LICENSE file, or at:&#xa;#&#xa;#  http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;# ==================================================================================================&#xa;&#xa;import atexit&#xa;from collections import defaultdict&#xa;import contextlib&#xa;import errno&#xa;import os&#xa;import shutil&#xa;import sys&#xa;import stat&#xa;import tempfile&#xa;import threading&#xa;import zipfile&#xa;&#xa;&#xa;# See http://stackoverflow.com/questions/2572172/referencing-other-modules-in-atexit&#xa;class MktempTeardownRegistry(object):&#xa;  def __init__(self):&#xa;    self._registry = defaultdict(set)&#xa;    self._getpid = os.getpid&#xa;    self._lock = threading.RLock()&#xa;    self._exists = os.path.exists&#xa;    self._rmtree = shutil.rmtree&#xa;    atexit.register(self.teardown)&#xa;&#xa;  def __del__(self):&#xa;    self.teardown()&#xa;&#xa;  def register(self, path):&#xa;    with self._lock:&#xa;      self._registry[self._getpid()].add(path)&#xa;    return path&#xa;&#xa;  def teardown(self):&#xa;    for td in self._registry.pop(self._getpid(), []):&#xa;      if self._exists(td):&#xa;        self._rmtree(td)&#xa;&#xa;&#xa;_MKDTEMP_SINGLETON = MktempTeardownRegistry()&#xa;&#xa;&#xa;@contextlib.contextmanager&#xa;def open_zip(path, *args, **kwargs):&#xa;  """"""&#xa;    A with-context for zip files.  Passes through positional and kwargs to zipfile.ZipFile.&#xa;  """"""&#xa;  with contextlib.closing(zipfile.ZipFile(path, *args, **kwargs)) as zip:&#xa;    yield zip&#xa;&#xa;&#xa;def safe_mkdtemp(**kw):&#xa;  """"""&#xa;    Given the parameters to standard tempfile.mkdtemp, create a temporary directory&#xa;    that is cleaned up on process exit.&#xa;  """"""&#xa;  # proper lock sanitation on fork [issue 6721] would be desirable here.&#xa;  return _MKDTEMP_SINGLETON.register(tempfile.mkdtemp(**kw))&#xa;&#xa;&#xa;def register_rmtree(directory):&#xa;  """"""&#xa;    Register an existing directory to be cleaned up at process exit.&#xa;  """"""&#xa;  return _MKDTEMP_SINGLETON.register(directory)&#xa;&#xa;&#xa;def safe_mkdir(directory, clean=False):&#xa;  """"""&#xa;    Ensure a directory is present.  If it's not there, create it.  If it is,&#xa;    no-op. If clean is True, ensure the directory is empty.&#xa;  """"""&#xa;  if clean:&#xa;    safe_rmtree(directory)&#xa;  try:&#xa;    os.makedirs(directory)&#xa;  except OSError as e:&#xa;    if e.errno != errno.EEXIST:&#xa;      raise&#xa;&#xa;&#xa;def safe_open(filename, *args, **kwargs):&#xa;  """"""&#xa;    Open a file safely (ensuring that the directory components leading up to it&#xa;    have been created first.)&#xa;  """"""&#xa;  safe_mkdir(os.path.dirname(filename))&#xa;  return open(filename, *args, **kwargs)&#xa;&#xa;&#xa;def safe_delete(filename):&#xa;  """"""&#xa;    Delete a file safely. If it's not present, no-op.&#xa;  """"""&#xa;  try:&#xa;    os.unlink(filename)&#xa;  except OSError as e:&#xa;    if e.errno != errno.ENOENT:&#xa;      raise&#xa;&#xa;&#xa;def safe_rmtree(directory):&#xa;  """"""&#xa;    Delete a directory if it's present. If it's not present, no-op.&#xa;  """"""&#xa;  if os.path.exists(directory):&#xa;    shutil.rmtree(directory, True)&#xa;&#xa;&#xa;def chmod_plus_x(path):&#xa;  """"""&#xa;    Equivalent of unix `chmod a+x path`&#xa;  """"""&#xa;  path_mode = os.stat(path).st_mode&#xa;  path_mode &= int('777', 8)&#xa;  if path_mode & stat.S_IRUSR:&#xa;    path_mode |= stat.S_IXUSR&#xa;  if path_mode & stat.S_IRGRP:&#xa;    path_mode |= stat.S_IXGRP&#xa;  if path_mode & stat.S_IROTH:&#xa;    path_mode |= stat.S_IXOTH&#xa;  os.chmod(path, path_mode)&#xa;&#xa;&#xa;def chmod_plus_w(path):&#xa;  """"""&#xa;    Equivalent of unix `chmod +w path`&#xa;  """"""&#xa;  path_mode = os.stat(path).st_mode&#xa;  path_mode &= int('777', 8)&#xa;  path_mode |= stat.S_IWRITE&#xa;  os.chmod(path, path_mode)&#xa;&#xa;&#xa;def touch(file, times=None):&#xa;  """"""&#xa;    Equivalent of unix `touch path`.&#xa;&#xa;    :file The file to touch.&#xa;    :times Either a tuple of (atime, mtime) or else a single time to use for both.  If not&#xa;           specified both atime and mtime are updated to the current time.&#xa;  """"""&#xa;  if times:&#xa;    if len(times) > 2:&#xa;      raise ValueError('times must either be a tuple of (atime, mtime) or else a single time value '&#xa;                       'to use for both.')&#xa;&#xa;    if len(times) == 1:&#xa;      times = (times, times)&#xa;&#xa;  with safe_open(file, 'a'):&#xa;    os.utime(file, times)&#xa;&#xa;&#xa;class Chroot(object):&#xa;  """"""&#xa;    A chroot of files overlayed from one directory to another directory.&#xa;&#xa;    Files may be tagged when added in order to keep track of multiple overlays in&#xa;    the chroot.&#xa;  """"""&#xa;  class ChrootException(Exception): pass&#xa;&#xa;  class ChrootTaggingException(Exception):&#xa;    def __init__(self, filename, orig_tag, new_tag):&#xa;      Exception.__init__(self,&#xa;        ""Trying to add %s to fileset(%s) but already in fileset(%s)!"" % (&#xa;          filename, new_tag, orig_tag))&#xa;&#xa;  def __init__(self, chroot_base, name=None):&#xa;    """"""&#xa;      chroot_base = directory for the creation of the target chroot.&#xa;      name = if specified, create the chroot in a temporary directory underneath&#xa;        chroot_base with 'name' as the prefix, otherwise create the chroot directly&#xa;        into chroot_base&#xa;    """"""&#xa;    self.root = None&#xa;    try:&#xa;      safe_mkdir(chroot_base)&#xa;    except:&#xa;      raise Chroot.ChrootException('Unable to create chroot in %s' % chroot_base)&#xa;    if name is not None:&#xa;      self.chroot = tempfile.mkdtemp(dir=chroot_base, prefix='%s.' % name)&#xa;    else:&#xa;      self.chroot = chroot_base&#xa;    self.filesets = {}&#xa;&#xa;  def set_relative_root(self, root):&#xa;    """"""&#xa;      Make all source paths relative to this root path.&#xa;    """"""&#xa;    self.root = root&#xa;&#xa;  def clone(self, into=None):&#xa;    into = into or tempfile.mkdtemp()&#xa;    new_chroot = Chroot(into)&#xa;    new_chroot.root = self.root&#xa;    for label, fileset in self.filesets.items():&#xa;      for fn in fileset:&#xa;        new_chroot.link(os.path.join(self.chroot, self.root or '', fn),&#xa;                        fn, label=label)&#xa;    return new_chroot&#xa;&#xa;  def path(self):&#xa;    """"""The path of the chroot.""""""&#xa;    return self.chroot&#xa;&#xa;  def _check_tag(self, fn, label):&#xa;    for fs_label, fs in self.filesets.items():&#xa;      if fn in fs and fs_label != label:&#xa;        raise Chroot.ChrootTaggingException(fn, fs_label, label)&#xa;&#xa;  def _tag(self, fn, label):&#xa;    self._check_tag(fn, label)&#xa;    if label not in self.filesets:&#xa;      self.filesets[label] = set()&#xa;    self.filesets[label].add(fn)&#xa;&#xa;  def _mkdir_for(self, path):&#xa;    dirname = os.path.dirname(os.path.join(self.chroot, path))&#xa;    safe_mkdir(dirname)&#xa;&#xa;  def _rootjoin(self, path):&#xa;    return os.path.join(self.root or '', path)&#xa;&#xa;  def copy(self, src, dst, label=None):&#xa;    """"""&#xa;      Copy file from {root}/source to {chroot}/dest with optional label.&#xa;&#xa;      May raise anything shutil.copyfile can raise, e.g.&#xa;        IOError(Errno 21 'EISDIR')&#xa;&#xa;      May raise ChrootTaggingException if dst is already in a fileset&#xa;      but with a different label.&#xa;    """"""&#xa;    self._tag(dst, label)&#xa;    self._mkdir_for(dst)&#xa;    shutil.copyfile(self._rootjoin(src), os.path.join(self.chroot, dst))&#xa;&#xa;  def link(self, src, dst, label=None):&#xa;    """"""&#xa;      Hard link file from {root}/source to {chroot}/dest with optional label.&#xa;&#xa;      May raise anything os.link can raise, e.g.&#xa;        IOError(Errno 21 'EISDIR')&#xa;&#xa;      May raise ChrootTaggingException if dst is already in a fileset&#xa;      but with a different label.&#xa;    """"""&#xa;    self._tag(dst, label)&#xa;    self._mkdir_for(dst)&#xa;    abs_src = self._rootjoin(src)&#xa;    abs_dst = os.path.join(self.chroot, dst)&#xa;    if hasattr(os, 'link'):&#xa;      try:&#xa;        os.link(abs_src, abs_dst)&#xa;      except OSError as e:&#xa;        if e.errno == errno.EEXIST:&#xa;          # File already exists, skip&#xa;          pass&#xa;        elif e.errno == errno.EXDEV:&#xa;          # Hard link across devices, fall back on copying&#xa;          shutil.copyfile(abs_src, abs_dst)&#xa;        else:&#xa;          raise&#xa;    else:&#xa;      # Python for Windows doesn't support hardlinks; fall back on copying&#xa;      if os.path.normcase(os.path.abspath(abs_src)) != os.path.normcase(os.path.abspath(abs_dst)):&#xa;        shutil.copyfile(abs_src, abs_dst)&#xa;&#xa;  def write(self, data, dst, label=None, mode='wb'):&#xa;    """"""&#xa;      Write data to {chroot}/dest with optional label.&#xa;&#xa;      Has similar exceptional cases as Chroot.copy&#xa;    """"""&#xa;&#xa;    self._tag(dst, label)&#xa;    self._mkdir_for(dst)&#xa;    with open(os.path.join(self.chroot, dst), mode) as wp:&#xa;      wp.write(data)&#xa;&#xa;  def touch(self, dst, label=None):&#xa;    """"""&#xa;      Perform 'touch' on {chroot}/dest with optional label.&#xa;&#xa;      Has similar exceptional cases as Chroot.copy&#xa;    """"""&#xa;    self.write('', dst, label, mode='a')&#xa;&#xa;  def get(self, label):&#xa;    """"""Get all files labeled with 'label'""""""&#xa;    return self.filesets.get(label, set())&#xa;&#xa;  def files(self):&#xa;    """"""Get all files in the chroot.""""""&#xa;    all_files = set()&#xa;    for label in self.filesets:&#xa;      all_files.update(self.filesets[label])&#xa;    return all_files&#xa;&#xa;  def labels(self):&#xa;    return self.filesets.keys()&#xa;&#xa;  def __str__(self):&#xa;    return 'Chroot(%s {fs:%s})' % (self.chroot,&#xa;      ' '.join('%s' % foo for foo in self.filesets.keys()))&#xa;&#xa;  def delete(self):&#xa;    shutil.rmtree(self.chroot)&#xa;&#xa;  def zip(self, filename, mode='wb'):&#xa;    with contextlib.closing(zipfile.ZipFile(filename, mode)) as zf:&#xa;      for f in sorted(self.files()):&#xa;        zf.write(os.path.join(self.chroot, f), arcname=f, compress_type=zipfile.ZIP_DEFLATED)&#xa;"
898669|"#!/usr/bin/env python&#xa;&#xa;# Copyright 2015 The Kubernetes Authors.&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;&#xa;from __future__ import print_function&#xa;&#xa;import json&#xa;import mmap&#xa;import os&#xa;import re&#xa;import sys&#xa;import argparse&#xa;&#xa;parser = argparse.ArgumentParser()&#xa;parser.add_argument(""filenames"", help=""list of files to check, all files if unspecified"", nargs='*')&#xa;parser.add_argument(""-e"", ""--skip-exceptions"", help=""ignore hack/verify-flags/exceptions.txt and print all output"", action=""store_true"")&#xa;args = parser.parse_args()&#xa;&#xa;# Cargo culted from http://stackoverflow.com/questions/898669/how-can-i-detect-if-a-file-is-binary-non-text-in-python&#xa;def is_binary(pathname):&#xa;    """"""Return true if the given filename is binary.&#xa;    @raise EnvironmentError: if the file does not exist or cannot be accessed.&#xa;    @attention: found @ http://bytes.com/topic/python/answers/21222-determine-file-type-binary-text on 6/08/2010&#xa;    @author: Trent Mick <TrentM@ActiveState.com>&#xa;    @author: Jorge Orpinel <jorge@orpinel.com>""""""&#xa;    try:&#xa;        with open(pathname, 'r') as f:&#xa;            CHUNKSIZE = 1024&#xa;            while 1:&#xa;                chunk = f.read(CHUNKSIZE)&#xa;                if '\0' in chunk: # found null byte&#xa;                    return True&#xa;                if len(chunk) < CHUNKSIZE:&#xa;                    break # done&#xa;    except:&#xa;        return True&#xa;&#xa;    return False&#xa;&#xa;def get_all_files(rootdir):&#xa;    all_files = []&#xa;    for root, dirs, files in os.walk(rootdir):&#xa;        # don't visit certain dirs&#xa;        if 'vendor' in dirs:&#xa;            dirs.remove('vendor')&#xa;        if 'staging' in dirs:&#xa;            dirs.remove('staging')&#xa;        if '_output' in dirs:&#xa;            dirs.remove('_output')&#xa;        if '_gopath' in dirs:&#xa;            dirs.remove('_gopath')&#xa;        if 'third_party' in dirs:&#xa;            dirs.remove('third_party')&#xa;        if '.git' in dirs:&#xa;            dirs.remove('.git')&#xa;        if '.make' in dirs:&#xa;            dirs.remove('.make')&#xa;        if 'BUILD' in files:&#xa;           files.remove('BUILD')&#xa;        if 'exceptions.txt' in files:&#xa;            files.remove('exceptions.txt')&#xa;        if 'known-flags.txt' in files:&#xa;            files.remove('known-flags.txt')&#xa;&#xa;        for name in files:&#xa;            pathname = os.path.join(root, name)&#xa;            if is_binary(pathname):&#xa;                continue&#xa;            all_files.append(pathname)&#xa;    return all_files&#xa;&#xa;def normalize_files(rootdir, files):&#xa;    newfiles = []&#xa;    a = ['Godeps', '_gopath', 'third_party', '.git', 'exceptions.txt', 'known-flags.txt']&#xa;    for f in files:&#xa;        if any(x in f for x in a):&#xa;            continue&#xa;        if f.endswith("".svg""):&#xa;            continue&#xa;        if f.endswith("".gliffy""):&#xa;            continue&#xa;        if f.endswith("".md""):&#xa;            continue&#xa;        if f.endswith("".yaml""):&#xa;            continue&#xa;        newfiles.append(f)&#xa;    for i, f in enumerate(newfiles):&#xa;        if not os.path.isabs(f):&#xa;            newfiles[i] = os.path.join(rootdir, f)&#xa;    return newfiles&#xa;&#xa;def line_has_bad_flag(line, flagre):&#xa;    results  = flagre.findall(line)&#xa;    for result in results:&#xa;        if not ""_"" in result:&#xa;            return False&#xa;        # this should exclude many cases where jinja2 templates use kube flags&#xa;        # as variables, except it uses _ for the variable name&#xa;        if ""{% set"" + result + ""= \"""" in line:&#xa;            return False&#xa;        if ""pillar["" + result + ""]"" in line:&#xa;            return False&#xa;        if ""grains"" + result in line:&#xa;            return False&#xa;         # something common in juju variables...&#xa;        if ""template_data["" + result + ""]"" in line:&#xa;            return False&#xa;        return True&#xa;    return False&#xa;&#xa;def check_known_flags(rootdir):&#xa;    pathname = os.path.join(rootdir, ""hack/verify-flags/known-flags.txt"")&#xa;    f = open(pathname, 'r')&#xa;    flags = set(f.read().splitlines())&#xa;    f.close()&#xa;&#xa;    illegal_known_flags = set()&#xa;    for flag in flags:&#xa;        if len(flag) > 0:&#xa;            if not ""-"" in flag:&#xa;                illegal_known_flags.add(flag)&#xa;&#xa;    if len(illegal_known_flags) != 0:&#xa;        print(""All flags in hack/verify-flags/known-flags.txt should contain character -, found these flags without -"")&#xa;        l = list(illegal_known_flags)&#xa;        l.sort()&#xa;        print(""%s"" % ""\n"".join(l))&#xa;        sys.exit(1)&#xa;&#xa;&#xa;# The list of files might not be the whole repo. If someone only changed a&#xa;# couple of files we don't want to run all of the golang files looking for&#xa;# flags. Instead load the list of flags from hack/verify-flags/known-flags.txt&#xa;# If running the golang files finds a new flag not in that file, return an&#xa;# error and tell the user to add the flag to the flag list.&#xa;def get_flags(rootdir, files):&#xa;    # preload the 'known' flags&#xa;    pathname = os.path.join(rootdir, ""hack/verify-flags/known-flags.txt"")&#xa;    f = open(pathname, 'r')&#xa;    flags = set(f.read().splitlines())&#xa;    f.close()&#xa;&#xa;    # preload the 'known' flags which don't follow the - standard&#xa;    pathname = os.path.join(rootdir, ""hack/verify-flags/excluded-flags.txt"")&#xa;    f = open(pathname, 'r')&#xa;    excluded_flags = set(f.read().splitlines())&#xa;    f.close()&#xa;&#xa;    regexs = [ re.compile('Var[P]?\([^,]*, ""([^""]*)""'),&#xa;               re.compile('.String[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Int[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Bool[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.Duration[P]?\(""([^""]*)"",[^,]+,[^)]+\)'),&#xa;               re.compile('.StringSlice[P]?\(""([^""]*)"",[^,]+,[^)]+\)') ]&#xa;&#xa;    new_flags = set()&#xa;    new_excluded_flags = set()&#xa;    # walk all the files looking for any flags being declared&#xa;    for pathname in files:&#xa;        if not pathname.endswith("".go""):&#xa;            continue&#xa;        f = open(pathname, 'r')&#xa;        data = f.read()&#xa;        f.close()&#xa;        matches = []&#xa;        for regex in regexs:&#xa;            matches = matches + regex.findall(data)&#xa;        for flag in matches:&#xa;            if any(x in flag for x in excluded_flags):&#xa;                continue&#xa;            if ""_"" in flag:&#xa;                new_excluded_flags.add(flag)&#xa;            if not ""-"" in flag:&#xa;                continue&#xa;            if flag not in flags:&#xa;                new_flags.add(flag)&#xa;    if len(new_excluded_flags) != 0:&#xa;        print(""Found a flag declared with an _ but which is not explicitly listed as a valid flag name in hack/verify-flags/excluded-flags.txt"")&#xa;        print(""Are you certain this flag should not have been declared with an - instead?"")&#xa;        l = list(new_excluded_flags)&#xa;        l.sort()&#xa;        print(""%s"" % ""\n"".join(l))&#xa;        sys.exit(1)&#xa;    if len(new_flags) != 0:&#xa;        print(""Found flags with character - in golang files not in the list of known flags. Please add these to hack/verify-flags/known-flags.txt"")&#xa;        l = list(new_flags)&#xa;        l.sort()&#xa;        print(""%s"" % ""\n"".join(l))&#xa;        sys.exit(1)&#xa;    return list(flags)&#xa;&#xa;def flags_to_re(flags):&#xa;    """"""turn the list of all flags we found into a regex find both - and _ versions""""""&#xa;    dashRE = re.compile('[-_]')&#xa;    flagREs = []&#xa;    for flag in flags:&#xa;        # turn all flag names into regexs which will find both types&#xa;        newre = dashRE.sub('[-_]', flag)&#xa;        # only match if there is not a leading or trailing alphanumeric character&#xa;        flagREs.append(""[^\w${]"" + newre + ""[^\w]"")&#xa;    # turn that list of regex strings into a single large RE&#xa;    flagRE = ""|"".join(flagREs)&#xa;    flagRE = re.compile(flagRE)&#xa;    return flagRE&#xa;&#xa;def load_exceptions(rootdir):&#xa;    exceptions = set()&#xa;    if args.skip_exceptions:&#xa;        return exceptions&#xa;    exception_filename = os.path.join(rootdir, ""hack/verify-flags/exceptions.txt"")&#xa;    exception_file = open(exception_filename, 'r')&#xa;    for exception in exception_file.read().splitlines():&#xa;        out = exception.split("":"", 1)&#xa;        if len(out) != 2:&#xa;            print(""Invalid line in exceptions file: %s"" % exception)&#xa;            continue&#xa;        filename = out[0]&#xa;        line = out[1]&#xa;        exceptions.add((filename, line))&#xa;    return exceptions&#xa;&#xa;def main():&#xa;    rootdir = os.path.dirname(__file__) + ""/../""&#xa;    rootdir = os.path.abspath(rootdir)&#xa;&#xa;    exceptions = load_exceptions(rootdir)&#xa;&#xa;    if len(args.filenames) > 0:&#xa;        files = args.filenames&#xa;    else:&#xa;        files = get_all_files(rootdir)&#xa;    files = normalize_files(rootdir, files)&#xa;&#xa;    check_known_flags(rootdir)&#xa;&#xa;    flags = get_flags(rootdir, files)&#xa;    flagRE = flags_to_re(flags)&#xa;&#xa;    bad_lines = []&#xa;    # walk all the file looking for any flag that was declared and now has an _&#xa;    for pathname in files:&#xa;        relname = os.path.relpath(pathname, rootdir)&#xa;        f = open(pathname, 'r')&#xa;        for line in f.read().splitlines():&#xa;            if line_has_bad_flag(line, flagRE):&#xa;                if (relname, line) not in exceptions:&#xa;                    bad_lines.append((relname, line))&#xa;        f.close()&#xa;&#xa;    if len(bad_lines) != 0:&#xa;        if not args.skip_exceptions:&#xa;            print(""Found illegal 'flag' usage. If these are false negatives you should run `hack/verify-flags-underscore.py -e > hack/verify-flags/exceptions.txt` to update the list."")&#xa;        bad_lines.sort()&#xa;        for (relname, line) in bad_lines:&#xa;            print(""%s:%s"" % (relname, line))&#xa;        return 1&#xa;&#xa;if __name__ == ""__main__"":&#xa;  sys.exit(main())&#xa;"
35817|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CPPFLAGS) $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CPPFLAGS) $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?= $(CPPFLAGS_host) $(CFLAGS_host)&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?= $(CPPFLAGS_host) $(CXXFLAGS_host)&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = rm -rf ""$@"" && cp %(copy_archive_args)s ""$<"" ""$@""&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'netbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;&#xa;    # XXX(TooTallNate): disabling this code since we don't want this behavior...&#xa;    #if (self.type == 'shared_library' and&#xa;    #    (self.flavor != 'mac' or self.toolset != 'target')):&#xa;    #  # Install all shared libs into a common directory (per toolset) for&#xa;    #  # convenient access with LD_LIBRARY_PATH.&#xa;    #  return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  copy_archive_arguments = '-af'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;      'copy_archive_args': copy_archive_arguments,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'openbsd':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host', 'CC'), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host', 'AR'), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host', 'CXX'), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host', 'LINK'), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
23347387|"# -*- coding: utf-8 -*-&#xa;&#xa;import importlib&#xa;import json&#xa;import os&#xa;from collections import OrderedDict&#xa;&#xa;import django&#xa;from werkzeug.contrib.fixers import ProxyFix&#xa;&#xa;import framework&#xa;import website.models&#xa;from framework.addons.utils import render_addon_capabilities&#xa;from framework.flask import app, add_handlers&#xa;from framework.logging import logger&#xa;from framework.mongo import handlers as mongo_handlers&#xa;from framework.mongo import set_up_storage&#xa;from framework.postcommit_tasks import handlers as postcommit_handlers&#xa;from framework.sentry import sentry&#xa;from framework.celery_tasks import handlers as celery_task_handlers&#xa;from framework.transactions import handlers as transaction_handlers&#xa;from modularodm import storage&#xa;from website.addons.base import init_addon&#xa;from website.project.licenses import ensure_licenses&#xa;from website.project.model import ensure_schemas&#xa;from website.routes import make_url_map&#xa;from website import maintenance&#xa;&#xa;# This import is necessary to set up the archiver signal listeners&#xa;from website.archiver import listeners  # noqa&#xa;from website.mails import listeners  # noqa&#xa;from website.notifications import listeners  # noqa&#xa;from api.caching import listeners  # noqa&#xa;&#xa;&#xa;def init_addons(settings, routes=True):&#xa;    """"""Initialize each addon in settings.ADDONS_REQUESTED.&#xa;&#xa;    :param module settings: The settings module.&#xa;    :param bool routes: Add each addon's routing rules to the URL map.&#xa;    """"""&#xa;    settings.ADDONS_AVAILABLE = getattr(settings, 'ADDONS_AVAILABLE', [])&#xa;    settings.ADDONS_AVAILABLE_DICT = getattr(settings, 'ADDONS_AVAILABLE_DICT', OrderedDict())&#xa;    for addon_name in settings.ADDONS_REQUESTED:&#xa;        addon = init_addon(app, addon_name, routes=routes)&#xa;        if addon:&#xa;            if addon not in settings.ADDONS_AVAILABLE:&#xa;                settings.ADDONS_AVAILABLE.append(addon)&#xa;            settings.ADDONS_AVAILABLE_DICT[addon.short_name] = addon&#xa;    settings.ADDON_CAPABILITIES = render_addon_capabilities(settings.ADDONS_AVAILABLE)&#xa;&#xa;&#xa;def attach_handlers(app, settings):&#xa;    """"""Add callback handlers to ``app`` in the correct order.""""""&#xa;    # Add callback handlers to application&#xa;    add_handlers(app, mongo_handlers.handlers)&#xa;    add_handlers(app, celery_task_handlers.handlers)&#xa;    add_handlers(app, transaction_handlers.handlers)&#xa;    add_handlers(app, postcommit_handlers.handlers)&#xa;&#xa;    # Attach handler for checking view-only link keys.&#xa;    # NOTE: This must be attached AFTER the TokuMX to avoid calling&#xa;    # a commitTransaction (in toku's after_request handler) when no transaction&#xa;    # has been created&#xa;    add_handlers(app, {'before_request': framework.sessions.prepare_private_key})&#xa;    # framework.session's before_request handler must go after&#xa;    # prepare_private_key, else view-only links won't work&#xa;    add_handlers(app, {'before_request': framework.sessions.before_request,&#xa;                       'after_request': framework.sessions.after_request})&#xa;&#xa;    return app&#xa;&#xa;&#xa;def do_set_backends(settings):&#xa;    logger.debug('Setting storage backends')&#xa;    maintenance.ensure_maintenance_collection()&#xa;    set_up_storage(&#xa;        website.models.MODELS,&#xa;        storage.MongoStorage,&#xa;        addons=settings.ADDONS_AVAILABLE,&#xa;    )&#xa;&#xa;&#xa;def init_app(settings_module='website.settings', set_backends=True, routes=True,&#xa;             attach_request_handlers=True):&#xa;    """"""Initializes the OSF. A sort of pseudo-app factory that allows you to&#xa;    bind settings, set up routing, and set storage backends, but only acts on&#xa;    a single app instance (rather than creating multiple instances).&#xa;&#xa;    :param settings_module: A string, the settings module to use.&#xa;    :param set_backends: Whether to set the database storage backends.&#xa;    :param routes: Whether to set the url map.&#xa;&#xa;    """"""&#xa;    # The settings module&#xa;    settings = importlib.import_module(settings_module)&#xa;&#xa;    init_addons(settings, routes)&#xa;    with open(os.path.join(settings.STATIC_FOLDER, 'built', 'nodeCategories.json'), 'wb') as fp:&#xa;        json.dump(settings.NODE_CATEGORY_MAP, fp)&#xa;&#xa;    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'api.base.settings')&#xa;    django.setup()&#xa;&#xa;    app.debug = settings.DEBUG_MODE&#xa;&#xa;    # default config for flask app, however, this does not affect setting cookie using set_cookie()&#xa;    app.config['SESSION_COOKIE_SECURE'] = settings.SESSION_COOKIE_SECURE&#xa;    app.config['SESSION_COOKIE_HTTPONLY'] = settings.SESSION_COOKIE_HTTPONLY&#xa;&#xa;    if set_backends:&#xa;        do_set_backends(settings)&#xa;    if routes:&#xa;        try:&#xa;            make_url_map(app)&#xa;        except AssertionError:  # Route map has already been created&#xa;            pass&#xa;&#xa;    if attach_request_handlers:&#xa;        attach_handlers(app, settings)&#xa;&#xa;    if app.debug:&#xa;        logger.info(""Sentry disabled; Flask's debug mode enabled"")&#xa;    else:&#xa;        sentry.init_app(app)&#xa;        logger.info(""Sentry enabled; Flask's debug mode disabled"")&#xa;&#xa;    if set_backends:&#xa;        ensure_schemas()&#xa;        ensure_licenses()&#xa;    apply_middlewares(app, settings)&#xa;&#xa;    return app&#xa;&#xa;&#xa;def apply_middlewares(flask_app, settings):&#xa;    # Use ProxyFix to respect X-Forwarded-Proto header&#xa;    # https://stackoverflow.com/questions/23347387/x-forwarded-proto-and-flask&#xa;    if settings.LOAD_BALANCER:&#xa;        flask_app.wsgi_app = ProxyFix(flask_app.wsgi_app)&#xa;&#xa;    return flask_app&#xa;"
136168|"# ===============================================================================&#xa;# Copyright 2011 Jake Ross&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;# http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;# ===============================================================================&#xa;&#xa;# =============enthought library imports=======================&#xa;# =============standard library imports ========================&#xa;from __future__ import absolute_import&#xa;import logging&#xa;import os&#xa;import shutil&#xa;from logging.handlers import RotatingFileHandler&#xa;&#xa;from pychron.core.helpers.filetools import list_directory, unique_path2&#xa;from pychron.paths import paths&#xa;&#xa;NAME_WIDTH = 40&#xa;gFORMAT = '%(name)-{}s: %(asctime)s %(levelname)-9s (%(threadName)-10s) %(message)s'.format(NAME_WIDTH)&#xa;gLEVEL = logging.DEBUG&#xa;&#xa;&#xa;def simple_logger(name):&#xa;    logger = logging.getLogger(name)&#xa;    logger.setLevel(logging.DEBUG)&#xa;    h = logging.StreamHandler()&#xa;    h.setFormatter(logging.Formatter(gFORMAT))&#xa;    logger.addHandler(h)&#xa;    return logger&#xa;&#xa;&#xa;def get_log_text(n):&#xa;    root = logging.getLogger()&#xa;    for h in root.handlers:&#xa;        if isinstance(h, RotatingFileHandler):&#xa;            with open(h.baseFilename, 'rb') as rfile:&#xa;                return tail(rfile, n)&#xa;&#xa;&#xa;def tail(f, lines=20):&#xa;    """"""&#xa;    http://stackoverflow.com/questions/136168/get-last-n-lines-of-a-file-with-python-similar-to-tail&#xa;    """"""&#xa;    total_lines_wanted = lines&#xa;&#xa;    BLOCK_SIZE = 1024&#xa;    f.seek(0, 2)&#xa;    block_end_byte = f.tell()&#xa;    lines_to_go = total_lines_wanted&#xa;    block_number = -1&#xa;    blocks = []  # blocks of size BLOCK_SIZE, in reverse order starting&#xa;    # from the end of the file&#xa;    while lines_to_go > 0 and block_end_byte > 0:&#xa;        if block_end_byte - BLOCK_SIZE > 0:&#xa;            # read the last block we haven't yet read&#xa;            f.seek(block_number * BLOCK_SIZE, 2)&#xa;            blocks.append(f.read(BLOCK_SIZE))&#xa;        else:&#xa;            # file too small, start from begining&#xa;            f.seek(0, 0)&#xa;            # only read what was not read&#xa;            blocks.append(f.read(block_end_byte))&#xa;        lines_found = blocks[-1].count(b'\n')&#xa;        lines_to_go -= lines_found&#xa;        block_end_byte -= BLOCK_SIZE&#xa;        block_number -= 1&#xa;    all_read_text = b''.join(reversed(blocks))&#xa;    return b'\n'.join(all_read_text.splitlines()[-total_lines_wanted:]).decode('utf-8')&#xa;&#xa;&#xa;# def anomaly_setup(name):&#xa;#     ld = logging.Logger.manager.loggerDict&#xa;#     print 'anomaly setup ld={}'.format(ld)&#xa;#     if name not in ld:&#xa;#         bdir = paths.log_dir&#xa;#         name = add_extension(name, '.anomaly')&#xa;#         apath, _cnt = unique_path2(bdir, name, delimiter='-', extension='.log')&#xa;#         logger = logging.getLogger('anomalizer')&#xa;#         h = logging.FileHandler(apath)&#xa;#         logger.addHandler(h)&#xa;&#xa;&#xa;def logging_setup(name, use_archiver=True, root=None, use_file=True, **kw):&#xa;    """"""&#xa;    """"""&#xa;    # set up deprecation warnings&#xa;    # import warnings&#xa;    #     warnings.simplefilter('default')&#xa;    bdir = paths.log_dir if root is None else root&#xa;&#xa;    # make sure we have a log directory&#xa;    # if not os.path.isdir(bdir):&#xa;    #     os.mkdir(bdir)&#xa;&#xa;    if use_archiver:&#xa;        # archive logs older than 1 month&#xa;        # lazy load Archive because of circular dependency&#xa;        from pychron.core.helpers.archiver import Archiver&#xa;&#xa;        a = Archiver(archive_days=14,&#xa;                     archive_months=1,&#xa;                     root=bdir)&#xa;        a.clean()&#xa;&#xa;    if use_file:&#xa;        # create a new logging file&#xa;        logname = '{}.current.log'.format(name)&#xa;        logpath = os.path.join(bdir, logname)&#xa;&#xa;        if os.path.isfile(logpath):&#xa;            backup_logpath, _cnt = unique_path2(bdir, name, delimiter='-', extension='.log', width=5)&#xa;&#xa;            shutil.copyfile(logpath, backup_logpath)&#xa;            os.remove(logpath)&#xa;&#xa;            ps = list_directory(bdir, filtername=logname, remove_extension=False)&#xa;            for pi in ps:&#xa;                _h, t = os.path.splitext(pi)&#xa;                v = os.path.join(bdir, pi)&#xa;                shutil.copyfile(v, '{}{}'.format(backup_logpath, t))&#xa;                os.remove(v)&#xa;&#xa;    root = logging.getLogger()&#xa;    root.setLevel(gLEVEL)&#xa;    shandler = logging.StreamHandler()&#xa;&#xa;    handlers = [shandler]&#xa;    if use_file:&#xa;        rhandler = RotatingFileHandler(&#xa;            logpath, maxBytes=1e7, backupCount=50)&#xa;        handlers.append(rhandler)&#xa;&#xa;    fmt = logging.Formatter(gFORMAT)&#xa;    for hi in handlers:&#xa;        hi.setLevel(gLEVEL)&#xa;        hi.setFormatter(fmt)&#xa;        root.addHandler(hi)&#xa;&#xa;&#xa;def add_root_handler(path, level=None, strformat=None, **kw):&#xa;    if level is None:&#xa;        level = gLEVEL&#xa;    if format is None:&#xa;        strformat = gFORMAT&#xa;&#xa;    root = logging.getLogger()&#xa;    handler = logging.FileHandler(path, **kw)&#xa;    handler.setLevel(level)&#xa;    handler.setFormatter(logging.Formatter(strformat))&#xa;    root.addHandler(handler)&#xa;&#xa;    return handler&#xa;&#xa;&#xa;def remove_root_handler(handler):&#xa;    root = logging.getLogger()&#xa;    root.removeHandler(handler)&#xa;&#xa;&#xa;&#xa;def new_logger(name):&#xa;    name = '{:<{}}'.format(name, NAME_WIDTH)&#xa;    l = logging.getLogger(name)&#xa;    l.setLevel(gLEVEL)&#xa;&#xa;    return l&#xa;&#xa;&#xa;def wrap(items, width=40, indent=90, delimiter=','):&#xa;    """"""&#xa;        wrap a list&#xa;    """"""&#xa;    if isinstance(items, str):&#xa;        items = items.split(delimiter)&#xa;&#xa;    gcols = iter(items)&#xa;    t = 0&#xa;    rs = []&#xa;    r = []&#xa;&#xa;    while 1:&#xa;        try:&#xa;            c = next(gcols)&#xa;            t += 1 + len(c)&#xa;            if t < width:&#xa;                r.append(c)&#xa;            else:&#xa;                rs.append(','.join(r))&#xa;                r = [c]&#xa;                t = len(c)&#xa;&#xa;        except StopIteration:&#xa;            rs.append(','.join(r))&#xa;            break&#xa;&#xa;    return ',\n{}'.format(' ' * indent).join(rs)&#xa;&#xa;    # ============================== EOF ===================================&#xa;"
7102050|"#Alex Holcombe alex.holcombe@sydney.edu.au&#xa;#See the github repository for more information: https://github.com/alexholcombe/twoWords&#xa;from __future__ import print_function&#xa;from psychopy import monitors, visual, event, data, logging, core, sound, gui&#xa;import psychopy.info&#xa;import numpy as np&#xa;from math import atan, log, ceil&#xa;import copy&#xa;import time, sys, os#, pylab&#xa;try:&#xa;    from noiseStaircaseHelpers import printStaircase, toStaircase, outOfStaircase, createNoise, plotDataAndPsychometricCurve&#xa;except ImportError:&#xa;    print('Could not import from noiseStaircaseHelpers.py (you need that file to be in the same directory)')&#xa;try:&#xa;    import stringResponse&#xa;except ImportError:&#xa;    print('Could not import stringResponse.py (you need that file to be in the same directory)')&#xa;&#xa;wordEccentricity=3 #means degrees of angle away from fixation&#xa;tasks=['T1']; task = tasks[0]&#xa;#THINGS THAT COULD PREVENT SUCCESS ON A STRANGE MACHINE&#xa;#same screen or external screen? Set scrn=0 if one screen. scrn=1 means display stimulus on second screen.&#xa;#widthPix, heightPix&#xa;quitFinder = False #if checkRefreshEtc, quitFinder becomes True&#xa;autopilot=False&#xa;demo=False #False&#xa;exportImages= False #quits after one trial&#xa;subject='Hubert' #user is prompted to enter true subject name&#xa;if autopilot: subject='auto'&#xa;if os.path.isdir('.'+os.sep+'data'):&#xa;    dataDir='data'&#xa;else:&#xa;    print('""data"" directory does not exist, so saving data in present working directory')&#xa;    dataDir='.'&#xa;timeAndDateStr = time.strftime(""%d%b%Y_%H-%M"", time.localtime())&#xa;&#xa;showRefreshMisses=True #flicker fixation at refresh rate, to visualize if frames missed&#xa;feedback=True&#xa;autoLogging=False&#xa;refreshRate = 60.;  #100&#xa;if demo:&#xa;    refreshRate = 60.;  #100&#xa;&#xa;staircaseTrials = 25&#xa;prefaceStaircaseTrialsN = 20 #22&#xa;prefaceStaircaseNoise = np.array([5,20,20,20, 50,50,50,5,80,80,80,5,95,95,95]) #will be recycled / not all used, as needed&#xa;descendingPsycho = True #psychometric function- more noise means worse performance&#xa;threshCriterion = 0.58&#xa;&#xa;numWordsInStream = 24&#xa;wordsUnparsed=""the, and, for, you, say, but, his, not, she, can, who, get, her, all, one, out, see, him, now, how, its, our, two, way"" #24 most common words&#xa;lettersUnparsed = ""a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z"".upper()&#xa;wordList = wordsUnparsed.split("","") #split into list&#xa;for i in range(len(wordList)):&#xa;    wordList[i] = wordList[i].replace("" "", """") #delete spaces&#xa;&#xa;bgColor = [-.7,-.7,-.7] # [-1,-1,-1]&#xa;cueColor = [1.,1.,1.]&#xa;letterColor = [1.,1.,1.]&#xa;cueRadius = 7 #6 deg in Goodbourn & Holcombe&#xa;widthPix= 1280 #monitor width in pixels of Agosta&#xa;heightPix= 800 #800 #monitor height in pixels&#xa;monitorwidth = 38.7 #monitor width in cm&#xa;scrn=1 #0 to use main screen, 1 to use external screen connected to computer&#xa;fullscr=True #True to use fullscreen, False to not. Timing probably won't be quite right if fullscreen = False&#xa;allowGUI = False&#xa;if demo: monitorwidth = 23#18.0&#xa;if exportImages:&#xa;    widthPix = 600; heightPix = 600&#xa;    monitorwidth = 13.0&#xa;    fullscr=False; scrn=0&#xa;    framesSaved=0&#xa;if demo:    &#xa;    scrn=0; fullscr=False&#xa;    widthPix = 800; heightPix = 600&#xa;    monitorname='testMonitor'&#xa;    allowGUI = True&#xa;viewdist = 57. #cm&#xa;pixelperdegree = widthPix/ (atan(monitorwidth/viewdist) /np.pi*180)&#xa;print('pixelperdegree=',pixelperdegree)&#xa;    &#xa;# create a dialog from dictionary &#xa;infoFirst = { 'Do staircase (only)': False, 'Check refresh etc':True, 'Fullscreen (timing errors if not)': False, 'Screen refresh rate':refreshRate }&#xa;OK = gui.DlgFromDict(dictionary=infoFirst, &#xa;    title='Dual-RSVP experiment OR staircase to find thresh noise level for performance criterion', &#xa;    order=['Do staircase (only)', 'Check refresh etc', 'Fullscreen (timing errors if not)'], &#xa;    tip={'Check refresh etc': 'To confirm refresh rate and that can keep up, at least when drawing a grating'},&#xa;    #fixed=['Check refresh etc'])#this attribute can't be changed by the user&#xa;    )&#xa;if not OK.OK:&#xa;    print('User cancelled from dialog box'); core.quit()&#xa;doStaircase = infoFirst['Do staircase (only)']&#xa;checkRefreshEtc = infoFirst['Check refresh etc']&#xa;fullscr = infoFirst['Fullscreen (timing errors if not)']&#xa;refreshRate = infoFirst['Screen refresh rate']&#xa;if checkRefreshEtc:&#xa;    quitFinder = True &#xa;if quitFinder:&#xa;    import os&#xa;    applescript=""\'tell application \""Finder\"" to quit\'""&#xa;    shellCmd = 'osascript -e '+applescript&#xa;    os.system(shellCmd)&#xa;&#xa;#letter size 2.5 deg                      #I've changed the SOA and letterDurMs&#xa;SOAms = 180 #Battelli, Agosta, Goodbourn, Holcombe mostly using 133&#xa;#Minimum SOAms should be 84  because any shorter, I can't always notice the second ring when lag1.   71 in Martini E2 and E1b (actually he used 66.6 but that's because he had a crazy refresh rate of 90 Hz)&#xa;letterDurMs = 400 #23.6  in Martini E2 and E1b (actually he used 22.2 but that's because he had a crazy refresh rate of 90 Hz)&#xa;&#xa;ISIms = SOAms - letterDurMs&#xa;letterDurFrames = int( np.floor(letterDurMs / (1000./refreshRate)) )&#xa;cueDurFrames = letterDurFrames&#xa;ISIframes = int( np.floor(ISIms / (1000./refreshRate)) )&#xa;#have set ISIframes and letterDurFrames to integer that corresponds as close as possible to originally intended ms&#xa;rateInfo = 'total SOA=' + str(round(  (ISIframes + letterDurFrames)*1000./refreshRate, 2)) + ' or ' + str(ISIframes + letterDurFrames) + ' frames, comprising\n'&#xa;rateInfo+=  'ISIframes ='+str(ISIframes)+' or '+str(ISIframes*(1000./refreshRate))+' ms and letterDurFrames ='+str(letterDurFrames)+' or '+str(round( letterDurFrames*(1000./refreshRate), 2))+'ms'&#xa;logging.info(rateInfo); print(rateInfo)&#xa;&#xa;trialDurFrames = int( numWordsInStream*(ISIframes+letterDurFrames) ) #trial duration in frames&#xa;&#xa;monitorname = 'testmonitor'&#xa;waitBlank = False&#xa;mon = monitors.Monitor(monitorname,width=monitorwidth, distance=viewdist)#relying on  monitorwidth cm (39 for Mitsubishi to do deg calculations) and gamma info in calibratn&#xa;mon.setSizePix( (widthPix,heightPix) )&#xa;units='deg' #'cm'&#xa;def openMyStimWindow(): #make it a function because have to do it several times, want to be sure is identical each time&#xa;    myWin = visual.Window(monitor=mon,size=(widthPix,heightPix),allowGUI=allowGUI,units=units,color=bgColor,colorSpace='rgb',fullscr=fullscr,screen=scrn,waitBlanking=waitBlank) #Holcombe lab monitor&#xa;    return myWin&#xa;myWin = openMyStimWindow()&#xa;refreshMsg2 = ''&#xa;if not checkRefreshEtc:&#xa;    refreshMsg1 = 'REFRESH RATE WAS NOT CHECKED'&#xa;    refreshRateWrong = False&#xa;else: #checkRefreshEtc&#xa;    runInfo = psychopy.info.RunTimeInfo(&#xa;            # if you specify author and version here, it overrides the automatic detection of __author__ and __version__ in your script&#xa;            #author='<your name goes here, plus whatever you like, e.g., your lab or contact info>',&#xa;            #version=""<your experiment version info>"",&#xa;            win=myWin,    ## a psychopy.visual.Window() instance; None = default temp window used; False = no win, no win.flips()&#xa;            refreshTest='grating', ## None, True, or 'grating' (eye-candy to avoid a blank screen)&#xa;            verbose=True, ## True means report on everything &#xa;            userProcsDetailed=True  ## if verbose and userProcsDetailed, return (command, process-ID) of the user's processes&#xa;            )&#xa;    #print(runInfo)&#xa;    logging.info(runInfo)&#xa;    print('Finished runInfo- which assesses the refresh and processes of this computer') &#xa;    #check screen refresh is what assuming it is ##############################################&#xa;    Hzs=list()&#xa;    myWin.flip(); myWin.flip();myWin.flip();myWin.flip();&#xa;    myWin.setRecordFrameIntervals(True) #otherwise myWin.fps won't work&#xa;    print('About to measure frame flips') &#xa;    for i in range(50):&#xa;        myWin.flip()&#xa;        Hzs.append( myWin.fps() )  #varies wildly on successive runs!&#xa;    myWin.setRecordFrameIntervals(False)&#xa;    # end testing of screen refresh########################################################&#xa;    Hzs = np.array( Hzs );     Hz= np.median(Hzs)&#xa;    msPerFrame= 1000./Hz&#xa;    refreshMsg1= 'Frames per second ~='+ str( np.round(Hz,1) )&#xa;    refreshRateTolerancePct = 3&#xa;    pctOff = abs( (np.median(Hzs)-refreshRate) / refreshRate)&#xa;    refreshRateWrong =  pctOff > (refreshRateTolerancePct/100.)&#xa;    if refreshRateWrong:&#xa;        refreshMsg1 += ' BUT'&#xa;        refreshMsg1 += ' program assumes ' + str(refreshRate)&#xa;        refreshMsg2 =  'which is off by more than' + str(round(refreshRateTolerancePct,0)) + '%!!'&#xa;    else:&#xa;        refreshMsg1 += ', which is close enough to desired val of ' + str( round(refreshRate,1) )&#xa;    myWinRes = myWin.size&#xa;    myWin.allowGUI =True&#xa;myWin.close() #have to close window to show dialog box&#xa;&#xa;defaultNoiseLevel = 0.0 #to use if no staircase, can be set by user&#xa;trialsPerCondition = 5 #default value&#xa;dlgLabelsOrdered = list()&#xa;if doStaircase:&#xa;    myDlg = gui.Dlg(title=""Staircase to find appropriate noisePercent"", pos=(200,400))&#xa;else: &#xa;    myDlg = gui.Dlg(title=""RSVP experiment"", pos=(200,400))&#xa;if not autopilot:&#xa;    myDlg.addField('Subject name (default=""Hubert""):', 'Hubert', tip='or subject code')&#xa;    dlgLabelsOrdered.append('subject')&#xa;if doStaircase:&#xa;    easyTrialsCondText = 'Num preassigned noise trials to preface staircase with (default=' + str(prefaceStaircaseTrialsN) + '):'&#xa;    myDlg.addField(easyTrialsCondText, tip=str(prefaceStaircaseTrialsN))&#xa;    dlgLabelsOrdered.append('easyTrials')&#xa;    myDlg.addField('Staircase trials (default=' + str(staircaseTrials) + '):', tip=""Staircase will run until this number is reached or it thinks it has precise estimate of threshold"")&#xa;    dlgLabelsOrdered.append('staircaseTrials')&#xa;    pctCompletedBreak = 101&#xa;else:&#xa;    myDlg.addField('\tPercent noise dots=',  defaultNoiseLevel, tip=str(defaultNoiseLevel))&#xa;    dlgLabelsOrdered.append('defaultNoiseLevel')&#xa;    myDlg.addField('Trials per condition (default=' + str(trialsPerCondition) + '):', trialsPerCondition, tip=str(trialsPerCondition))&#xa;    dlgLabelsOrdered.append('trialsPerCondition')&#xa;    pctCompletedBreak = 50&#xa;    &#xa;myDlg.addText(refreshMsg1, color='Black')&#xa;if refreshRateWrong:&#xa;    myDlg.addText(refreshMsg2, color='Red')&#xa;if refreshRateWrong:&#xa;    logging.error(refreshMsg1+refreshMsg2)&#xa;else: logging.info(refreshMsg1+refreshMsg2)&#xa;&#xa;if checkRefreshEtc and (not demo) and (myWinRes != [widthPix,heightPix]).any():&#xa;    msgWrongResolution = 'Screen apparently NOT the desired resolution of '+ str(widthPix)+'x'+str(heightPix)+ ' pixels!!'&#xa;    myDlg.addText(msgWrongResolution, color='Red')&#xa;    logging.error(msgWrongResolution)&#xa;    print(msgWrongResolution)&#xa;myDlg.addText('Note: to abort press ESC at a trials response screen', color=[-1.,1.,-1.]) # color='DimGrey') color names stopped working along the way, for unknown reason&#xa;myDlg.show()&#xa;&#xa;if myDlg.OK: #unpack information from dialogue box&#xa;   thisInfo = myDlg.data #this will be a list of data returned from each field added in order&#xa;   if not autopilot:&#xa;       name=thisInfo[dlgLabelsOrdered.index('subject')]&#xa;       if len(name) > 0: #if entered something&#xa;         subject = name #change subject default name to what user entered&#xa;   if doStaircase:&#xa;       if len(thisInfo[dlgLabelsOrdered.index('staircaseTrials')]) >0:&#xa;           staircaseTrials = int( thisInfo[ dlgLabelsOrdered.index('staircaseTrials') ] ) #convert string to integer&#xa;           print('staircaseTrials entered by user=',staircaseTrials)&#xa;           logging.info('staircaseTrials entered by user=',staircaseTrials)&#xa;       if len(thisInfo[dlgLabelsOrdered.index('easyTrials')]) >0:&#xa;           prefaceStaircaseTrialsN = int( thisInfo[ dlgLabelsOrdered.index('easyTrials') ] ) #convert string to integer&#xa;           print('prefaceStaircaseTrialsN entered by user=',thisInfo[dlgLabelsOrdered.index('easyTrials')])&#xa;           logging.info('prefaceStaircaseTrialsN entered by user=',prefaceStaircaseTrialsN)&#xa;   else: #not doing staircase&#xa;       trialsPerCondition = int( thisInfo[ dlgLabelsOrdered.index('trialsPerCondition') ] ) #convert string to integer&#xa;       print('trialsPerCondition=',trialsPerCondition)&#xa;       logging.info('trialsPerCondition =',trialsPerCondition)&#xa;       defaultNoiseLevel = int (thisInfo[ dlgLabelsOrdered.index('defaultNoiseLevel') ])&#xa;else: &#xa;   print('User cancelled from dialog box.')&#xa;   logging.flush()&#xa;   core.quit()&#xa;if not demo: &#xa;    allowGUI = False&#xa;&#xa;myWin = openMyStimWindow()&#xa;#set up output data file, log file,  copy of program code, and logging&#xa;infix = ''&#xa;if doStaircase:&#xa;    infix = 'staircase_'&#xa;fileName = os.path.join(dataDir, subject + '_' + infix+ timeAndDateStr)&#xa;if not demo and not exportImages:&#xa;    dataFile = open(fileName+'.txt', 'w')&#xa;    saveCodeCmd = 'cp \'' + sys.argv[0] + '\' '+ fileName + '.py'&#xa;    os.system(saveCodeCmd)  #save a copy of the code as it was when that subject was run&#xa;    logFname = fileName+'.log'&#xa;    ppLogF = logging.LogFile(logFname, &#xa;        filemode='w',#if you set this to 'a' it will append instead of overwriting&#xa;        level=logging.INFO)#errors, data and warnings will be sent to this logfile&#xa;if demo or exportImages: &#xa;  dataFile = sys.stdout; logF = sys.stdout&#xa;  logging.console.setLevel(logging.ERROR)  #only show this level  messages and higher&#xa;logging.console.setLevel(logging.ERROR) #DEBUG means set  console to receive nearly all messges, INFO next level, EXP, DATA, WARNING and ERROR &#xa;&#xa;if fullscr and not demo and not exportImages:&#xa;    runInfo = psychopy.info.RunTimeInfo(&#xa;        # if you specify author and version here, it overrides the automatic detection of __author__ and __version__ in your script&#xa;        #author='<your name goes here, plus whatever you like, e.g., your lab or contact info>',&#xa;        #version=""<your experiment version info>"",&#xa;        win=myWin,    ## a psychopy.visual.Window() instance; None = default temp window used; False = no win, no win.flips()&#xa;        refreshTest='grating', ## None, True, or 'grating' (eye-candy to avoid a blank screen)&#xa;        verbose=False, ## True means report on everything &#xa;        userProcsDetailed=True,  ## if verbose and userProcsDetailed, return (command, process-ID) of the user's processes&#xa;        #randomSeed='set:42', ## a way to record, and optionally set, a random seed of type str for making reproducible random sequences&#xa;            ## None -> default &#xa;            ## 'time' will use experimentRuntime.epoch as the value for the seed, different value each time the script is run&#xa;            ##'set:time' --> seed value is set to experimentRuntime.epoch, and initialized: random.seed(info['randomSeed'])&#xa;            ##'set:42' --> set & initialize to str('42'), and will give the same sequence of random.random() for all runs of the script&#xa;        )&#xa;    logging.info(runInfo)&#xa;logging.flush()&#xa;&#xa;wordListThisTrial = list() #to put in KiuYan stuff&#xa;&#xa;textStimuliStream1 = list()&#xa;textStimuliStream2 = list() #used for second, simultaneous RSVP stream&#xa;def calcAndPredrawStimuli(thisTrial,wordList): #called before each trial&#xa;    global wordListThisTrial, textStimuliStream1, textStimuliStream2&#xa;    textStimuliStream1 = list()&#xa;    textStimuliStream2 = list()&#xa;    if len(wordList) < numWordsInStream:&#xa;        print('Error! Your word list must have at least ',numWordsInStream,'strings')&#xa;    idxsIntoWordList = np.arange( len(wordList) ) #create a list of indexes of the entire word liste.g 0, 1, 2, 3, 4, 5 up to 23&#xa;    print('wordList=',wordList)&#xa;    #if the condition is the orthographically similar one&#xa;    orthographicallySimilarLists = [['bed','now','ban','tap','ton','dab','paw','map','win'],['top','bib','ten','box','wet','man','urn','pit','but']]&#xa;    orthographicallyDissimilarLists = [['off','egg','her','for','elk','joy','zoo','fax','yes'],['sky','fox','fig','elf','ski','zig','cog','woo','his']]&#xa;    print('thisTrial[orthographicalsim]= ',thisTrial['orthographicalsim'])&#xa;    whichList = np.random.randint(0, 1)&#xa;    if thisTrial['orthographicalsim'] == True:&#xa;        listForThisCondition = copy.deepcopy(orthographicallySimilarLists[whichList])&#xa;    elif  thisTrial['orthographicalsim'] == False:&#xa;        listForThisCondition = copy.deepcopy(orthographicallyDissimilarLists[whichList])&#xa;        print('Using dissimilar list')&#xa;    listForThisConditionLeft = copy.deepcopy(listForThisCondition)&#xa;    listForThisConditionRight =  copy.deepcopy(listForThisCondition)&#xa;    np.random.shuffle(listForThisConditionLeft)&#xa;    np.random.shuffle(listForThisConditionRight)&#xa;    print('listForThisConditionLeft=',listForThisConditionLeft)&#xa;    print('length of wordList before kiuyan modification=',len(wordList))&#xa;    wordListThisTrial = copy.deepcopy(wordList) #to put in KiuYan stuff&#xa;&#xa;    for word in listForThisConditionLeft:&#xa;        print('adding left intron to position ',len(wordListThisTrial), ' in wordListThisTrial')&#xa;        #print('inside loop:',word)&#xa;        wordListThisTrial.append(word) #add new words to end of wordList&#xa;    for word in listForThisConditionRight: #after the left ones, add to the wordList even more by putting the right ones&#xa;        print('adding right intron to position ',len(wordListThisTrial), ' in wordListThisTrial')&#xa;        #print('inside loop:',word)&#xa;        wordListThisTrial.append(word) #add new words to end of wordList    &#xa;    numWordsToLoadUp = numWordsInStream + 2*len(listForThisCondition)&#xa;    print('numWordsToLoadUp=',numWordsToLoadUp, ' length of wordListThisTrial=',len(wordListThisTrial))&#xa;    for i in range(0,numWordsToLoadUp): #draw the words that will be used on this trial, the first 26 of the shuffled list&#xa;       word = wordListThisTrial[ i ]  #     #[ idxsIntoWordList[i] ] the below is the way that Psychopy draws texton screen&#xa;       #bucket of words on left stream&#xa;       textStimulusStream1 = visual.TextStim(myWin,text=word,height=ltrHeight,colorSpace='rgb',color=letterColor,alignHoriz='center',alignVert='center',units='deg',autoLog=autoLogging)&#xa;       #bucket of words on right stream&#xa;       textStimulusStream2 = visual.TextStim(myWin,text=word,height=ltrHeight,colorSpace='rgb',color=letterColor,alignHoriz='center',alignVert='center',units='deg',autoLog=autoLogging)&#xa;       textStimulusStream1.setPos([-wordEccentricity,0]) #left this is bucket of words&#xa;       textStimuliStream1.append(textStimulusStream1) #add to list of text stimuli that comprise  stream 1  &#xa;       textStimulusStream2.setPos([wordEccentricity,0]) #right this is bucket of words&#xa;       textStimuliStream2.append(textStimulusStream2)  #add to list of text stimuli&#xa;    print('Kiuyan modified wordListThisTrial =',wordListThisTrial)&#xa;    idxsStream1 = idxsIntoWordList #first RSVP stream&#xa;    np.random.shuffle(idxsStream1)&#xa;    idxsStream2 = copy.deepcopy(idxsIntoWordList)&#xa;    np.random.shuffle(idxsStream2)&#xa;    toSubstituteOnLeft = range(9)&#xa;    np.random.shuffle(toSubstituteOnLeft)&#xa;    toSubstituteOnRight = range(9)&#xa;    np.random.shuffle(toSubstituteOnRight)&#xa;    for i in range(9):&#xa;        idxOfWordToSubstituteLeft =  numWordsInStream+toSubstituteOnLeft[i]&#xa;        #print('Using # ',idxOfWordToSubstituteLeft,' as intron for left')&#xa;        idxsStream1[i+8] = idxOfWordToSubstituteLeft&#xa;        idxOfWordToSubstituteRight =  numWordsInStream + 9 + toSubstituteOnRight[i]&#xa;        print('Using # ',idxOfWordToSubstituteRight,' as intron for right')&#xa;        idxsStream2[i+8] = idxOfWordToSubstituteRight&#xa;    print('idxsStream1=',idxsStream1, ' idxsStream2=',idxsStream2)&#xa;    return idxsStream1, idxsStream2&#xa;    &#xa;#create click sound for keyboard&#xa;try:&#xa;    click=sound.Sound('406__tictacshutup__click-1-d.wav')&#xa;except: #in case file missing, create inferiro click manually&#xa;    logging.warn('Could not load the desired click sound file, instead using manually created inferior click')&#xa;    click=sound.Sound('D',octave=4, sampleRate=22050, secs=0.015, bits=8)&#xa;&#xa;if showRefreshMisses:&#xa;    fixSizePix = 32 #2.6  #make fixation bigger so flicker more conspicuous&#xa;else: fixSizePix = 32&#xa;fixColor = [1,1,1]&#xa;if exportImages: fixColor= [0,0,0]&#xa;fixatnNoiseTexture = np.round( np.random.rand(fixSizePix/4,fixSizePix/4) ,0 )   *2.0-1 #Can counterphase flicker  noise texture to create salient flicker if you break fixation&#xa;&#xa;fixation= visual.PatchStim(myWin, tex=fixatnNoiseTexture, size=(fixSizePix,fixSizePix), units='pix', mask='circle', interpolate=False, autoLog=False)&#xa;fixationBlank= visual.PatchStim(myWin, tex= -1*fixatnNoiseTexture, size=(fixSizePix,fixSizePix), units='pix', mask='circle', interpolate=False, autoLog=False) #reverse contrast&#xa;fixationPoint= visual.PatchStim(myWin,tex='none',colorSpace='rgb',color=(1,1,1),size=4,units='pix',autoLog=autoLogging)&#xa;&#xa;respPromptStim = visual.TextStim(myWin,pos=(0, -.9),colorSpace='rgb',color=(1,1,1),alignHoriz='center', alignVert='center',height=.1,units='norm',autoLog=autoLogging)&#xa;acceptTextStim = visual.TextStim(myWin,pos=(0, -.8),colorSpace='rgb',color=(1,1,1),alignHoriz='center', alignVert='center',height=.1,units='norm',autoLog=autoLogging)&#xa;acceptTextStim.setText('Hit ENTER to accept. Backspace to edit')&#xa;respStim = visual.TextStim(myWin,pos=(0,0),colorSpace='rgb',color=(1,1,0),alignHoriz='center', alignVert='center',height=3,units='deg',autoLog=autoLogging)&#xa;clickSound, badKeySound = stringResponse.setupSoundsForResponse()&#xa;requireAcceptance = False&#xa;nextText = visual.TextStim(myWin,pos=(0, .1),colorSpace='rgb',color = (1,1,1),alignHoriz='center', alignVert='center',height=.1,units='norm',autoLog=autoLogging)&#xa;NextRemindCountText = visual.TextStim(myWin,pos=(0,.2),colorSpace='rgb',color= (1,1,1),alignHoriz='center', alignVert='center',height=.1,units='norm',autoLog=autoLogging)&#xa;screenshot= False; screenshotDone = False&#xa;stimList = []&#xa;#SETTING THE CONDITIONS&#xa;cuePositions =  np.array([10, 11, 12, 13, 14]) #changed this experiment from 6-10 to 10-14 so that its centred in the stream&#xa;for cuePos in cuePositions:&#xa;   for rightResponseFirst in [False,True]:&#xa;      for bothWordsFlipped in [False]:&#xa;        for orthographicalsim in [True,False]:&#xa;          stimList.append( {'cuePos':cuePos, 'rightResponseFirst':rightResponseFirst, 'flipHoriz':bothWordsFlipped, 'flipVert':bothWordsFlipped,&#xa;                                    'leftStreamFlip':bothWordsFlipped,  'orthographicalsim':orthographicalsim, 'rightStreamFlip':bothWordsFlipped} ) # add 'orthographicalsim':orthographical similarity&#xa;&#xa;trials = data.TrialHandler(stimList,trialsPerCondition,method='random') #'random' #constant stimuli method&#xa;trialsForPossibleStaircase = data.TrialHandler(stimList,trialsPerCondition) #independent randomization, just to create random trials for staircase phase&#xa;numRightWrongEachCuepos = np.zeros([ len(cuePositions), 1 ]); #summary results to print out at end&#xa;&#xa;logging.info( 'numtrials=' + str(trials.nTotal) + ' and each trialDurFrames='+str(trialDurFrames)+' or '+str(trialDurFrames*(1000./refreshRate))+ \&#xa;               ' ms' + '  task=' + task)&#xa;&#xa;def numberToLetter(number): #0 = A, 25 = Z&#xa;    #if it's not really a letter, return @&#xa;    if number < 0 or number > 25:&#xa;        return ('@')&#xa;    else: #it's probably a letter&#xa;        try:&#xa;            return chr( ord('A')+number )&#xa;        except:&#xa;            return('@')&#xa;&#xa;def letterToNumber(letter): #A = 0, Z = 25&#xa;    #if it's not really a letter, return -999&#xa;    #HOW CAN I GENERICALLY TEST FOR LENGTH. EVEN IN CASE OF A NUMBER THAT' SNOT PART OF AN ARRAY?&#xa;    try:&#xa;        #if len(letter) > 1:&#xa;        #    return (-999)&#xa;        if letter < 'A' or letter > 'Z':&#xa;            return (-999)&#xa;        else: #it's a letter&#xa;            return ord(letter)-ord('A')&#xa;    except:&#xa;        return (-999)&#xa;&#xa;def wordToIdx(word,wordList):&#xa;    #if it's not in the list of stimuli, return -999&#xa;    try:&#xa;        #http://stackoverflow.com/questions/7102050/how-can-i-get-a-python-generator-to-return-none-rather-than-stopiteration&#xa;        firstMatchIdx = next((i for i, val in enumerate(wordList) if val.upper()==word), None) #return i (index) unless no matches, in which case return None&#xa;        #print('Looked for ',word,' in ',wordList,'\nfirstMatchIdx =',firstMatchIdx)&#xa;        return firstMatchIdx&#xa;    except:&#xa;        print('Unexpected error in wordToIdx with word=',word)&#xa;        return (None)&#xa;        &#xa;#print header for data file&#xa;print('experimentPhase\ttrialnum\tsubject\ttask\t',file=dataFile,end='')&#xa;print('noisePercent\tleftStreamFlip\trightStreamFlip\tflipHoriz\tflipVert\torthographicalsim\t',end='',file=dataFile)#added tabs for the two flips&#xa;if task=='T1':&#xa;    numRespsWanted = 2&#xa;dataFile.write('rightResponseFirst\t')&#xa;for i in range(numRespsWanted):&#xa;   dataFile.write('cuePos'+str(i)+'\t')   #have to use write to avoid ' ' between successive text, at least until Python 3&#xa;   dataFile.write('answer'+str(i)+'\t')&#xa;   dataFile.write('response'+str(i)+'\t')&#xa;   dataFile.write('correct'+str(i)+'\t')&#xa;   dataFile.write('responsePosRelative'+str(i)+'\t')&#xa;print('timingBlips',file=dataFile)&#xa;#end of header&#xa;&#xa;def  oneFrameOfStim( n,cue,seq1,seq2,cueDurFrames,letterDurFrames,ISIframes,thisTrial,textStimuliStream1,textStimuliStream2,&#xa;                                       noise,proportnNoise,allFieldCoords,numNoiseDots ): &#xa;#defining a function to draw each frame of stim.&#xa;#seq1 is an array of indices corresponding to the appropriate pre-drawn stimulus, contained in textStimuli&#xa;  SOAframes = letterDurFrames+ISIframes&#xa;  cueFrames = thisTrial['cuePos']*SOAframes  #cuesPos is global variable&#xa;  stimN = int( np.floor(n/SOAframes) )&#xa;  frameOfThisLetter = n % SOAframes #every SOAframes, new letter&#xa;  showLetter = frameOfThisLetter < letterDurFrames #if true, it's not time for the blank ISI.  it's still time to draw the letter&#xa;  #print 'n=',n,' SOAframes=',SOAframes, ' letterDurFrames=', letterDurFrames, ' (n % SOAframes) =', (n % SOAframes)  #DEBUGOFF&#xa;  thisStimIdx = seq1[stimN] #which letter, from A to Z (1 to 26), should be shown?&#xa;  if seq2 is not None:&#xa;    thisStim2Idx = seq2[stimN]&#xa;  #so that any timing problems occur just as often for every frame, always draw the letter and the cue, but simply draw it in the bgColor when it's not meant to be on&#xa;  cue.setLineColor( bgColor )&#xa;  if type(cueFrames) not in [tuple,list,np.ndarray]: #scalar. But need collection to do loop based on it&#xa;    cueFrames = list([cueFrames])&#xa;  for cueFrame in cueFrames: #check whether it's time for any cue&#xa;      if n>=cueFrame and n<cueFrame+cueDurFrames:&#xa;         cue.setLineColor( cueColor )&#xa;&#xa;  if showLetter:&#xa;    textStimuliStream1[thisStimIdx].setColor( letterColor )&#xa;    #print('thisStimIdx=',thisStimIdx,' thisStim2Idx=',thisStim2Idx)&#xa;    textStimuliStream2[thisStim2Idx].setColor( letterColor )&#xa;  else: &#xa;    textStimuliStream1[thisStimIdx].setColor( bgColor )&#xa;    textStimuliStream2[thisStim2Idx].setColor( bgColor )&#xa;  textStimuliStream1[thisStimIdx].flipVert = True #added this and the next line in to try to invert&#xa;  textStimuliStream2[thisStim2Idx].flipVert = True&#xa;  textStimuliStream1[thisStimIdx].flipHoriz = thisTrial['leftStreamFlip']&#xa;  textStimuliStream2[thisStim2Idx].flipHoriz = thisTrial['rightStreamFlip']&#xa;  textStimuliStream1[thisStimIdx].draw()&#xa;  textStimuliStream2[thisStim2Idx].draw()&#xa;  cue.draw()&#xa;  refreshNoise = False #Not recommended because takes longer than a frame, even to shuffle apparently. Or may be setXYs step&#xa;  if proportnNoise>0 and refreshNoise: &#xa;    if frameOfThisLetter ==0: &#xa;        np.random.shuffle(8, 9, 10, 11, 12, 13, 14, 15, 16) &#xa;        dotCoords = allFieldCoords[0:numNoiseDots]&#xa;        noise.setXYs(dotCoords)&#xa;  if proportnNoise>0:&#xa;    noise.draw()&#xa;  return True &#xa;# #######End of function definition that displays the stimuli!!!! #####################################&#xa;#############################################################################################################################&#xa;&#xa;cue = visual.Circle(myWin, &#xa;                 radius=cueRadius,#Martini used circles with diameter of 12 deg&#xa;                 lineColorSpace = 'rgb',&#xa;                 lineColor=bgColor,&#xa;                 lineWidth=4.0, #in pixels. Was thinner (2 pixels) in letter AB experiments&#xa;                 units = 'deg',&#xa;                 fillColorSpace = 'rgb',&#xa;                 fillColor=None, #beware, with convex shapes fill colors don't work&#xa;                 pos= [0,0], #the anchor (rotation and vertices are position with respect to this)&#xa;                 interpolate=True,&#xa;                 autoLog=False)#this stim changes too much for autologging to be useful&#xa;&#xa;ltrHeight = 2.5 #Martini letters were 2.5deg high&#xa;#All noise dot coordinates ultimately in pixels, so can specify each dot is one pixel &#xa;noiseFieldWidthDeg=ltrHeight *1.0&#xa;noiseFieldWidthPix = int( round( noiseFieldWidthDeg*pixelperdegree ) )&#xa;&#xa;def timingCheckAndLog(ts,trialN):&#xa;    #check for timing problems and log them&#xa;    #ts is a list of the times of the clock after each frame&#xa;    interframeIntervs = np.diff(ts)*1000&#xa;    #print '   interframe intervs were ',around(interframeIntervs,1) #DEBUGOFF&#xa;    frameTimeTolerance=.3 #proportion longer than refreshRate that will not count as a miss&#xa;    longFrameLimit = np.round(1000/refreshRate*(1.0+frameTimeTolerance),2)&#xa;    idxsInterframeLong = np.where( interframeIntervs > longFrameLimit ) [0] #frames that exceeded 150% of expected duration&#xa;    numCasesInterframeLong = len( idxsInterframeLong )&#xa;    if numCasesInterframeLong >0 and (not demo):&#xa;       longFramesStr =  'ERROR,'+str(numCasesInterframeLong)+' frames were longer than '+str(longFrameLimit)+' ms'&#xa;       if demo: &#xa;         longFramesStr += 'not printing them all because in demo mode'&#xa;       else:&#xa;           longFramesStr += ' apparently screen refreshes skipped, interframe durs were:'+\&#xa;                    str( np.around(  interframeIntervs[idxsInterframeLong] ,1  ) )+ ' and was these frames: '+ str(idxsInterframeLong)&#xa;       if longFramesStr != None:&#xa;                logging.error( 'trialnum='+str(trialN)+' '+longFramesStr )&#xa;                if not demo:&#xa;                    flankingAlso=list()&#xa;                    for idx in idxsInterframeLong: #also print timing of one before and one after long frame&#xa;                        if idx-1>=0:&#xa;                            flankingAlso.append(idx-1)&#xa;                        else: flankingAlso.append(np.NaN)&#xa;                        flankingAlso.append(idx)&#xa;                        if idx+1<len(interframeIntervs):  flankingAlso.append(idx+1)&#xa;                        else: flankingAlso.append(np.NaN)&#xa;                    flankingAlso = np.array(flankingAlso)&#xa;                    flankingAlso = flankingAlso[np.negative(np.isnan(flankingAlso))]  #remove nan values&#xa;                    flankingAlso = flankingAlso.astype(np.integer) #cast as integers, so can use as subscripts&#xa;                    logging.info( 'flankers also='+str( np.around( interframeIntervs[flankingAlso], 1) )  ) #because this is not an essential error message, as previous one already indicates error&#xa;                      #As INFO, at least it won't fill up the console when console set to WARNING or higher&#xa;    return numCasesInterframeLong&#xa;    #end timing check&#xa;    &#xa;trialClock = core.Clock()&#xa;numTrialsCorrect = 0; &#xa;numTrialsApproxCorrect = 0; #this is not recorded in the datafile as a variable. #added Trials into the string to define hopefully!&#xa;numTrialsEachCorrect= np.zeros( numRespsWanted )&#xa;numTrialsEachApproxCorrect= np.zeros( numRespsWanted )&#xa;&#xa;def do_RSVP_stim(thisTrial, seq1, seq2, proportnNoise,trialN):&#xa;    #relies on global variables:&#xa;    #   textStimuli, logging, bgColor&#xa;    #  thisTrial should have 'cuePos'&#xa;    global framesSaved #because change this variable. Can only change a global variable if you declare it&#xa;    cuesPos = [] #will contain the positions in the stream of all the cues (targets)&#xa;&#xa;    cuesPos.append(thisTrial['cuePos'])&#xa;    cuesPos = np.array(cuesPos)&#xa;    noise = None; allFieldCoords=None; numNoiseDots=0&#xa;    if proportnNoise > 0: #generating noise is time-consuming, so only do it once per trial. Then shuffle noise coordinates for each letter&#xa;        (noise,allFieldCoords,numNoiseDots) = createNoise(proportnNoise,myWin,noiseFieldWidthPix, bgColor)&#xa;&#xa;    preDrawStimToGreasePipeline = list() #I don't know why this works, but without drawing it I have consistent timing blip first time that draw ringInnerR for phantom contours&#xa;    cue.setLineColor(bgColor)&#xa;    preDrawStimToGreasePipeline.extend([cue])&#xa;    for stim in preDrawStimToGreasePipeline:&#xa;        stim.draw()&#xa;    myWin.flip(); myWin.flip()&#xa;    #end preparation of stimuli&#xa;    &#xa;    core.wait(.1);&#xa;    trialClock.reset()&#xa;    fixatnPeriodMin = 0.3&#xa;    fixatnPeriodFrames = int(   (np.random.rand(1)/2.+fixatnPeriodMin)   *refreshRate)  #random interval between 800ms and 1.3s&#xa;    ts = list(); #to store time of each drawing, to check whether skipped frames&#xa;    for i in range(fixatnPeriodFrames+20):  #prestim fixation interval&#xa;        #if i%4>=2 or demo or exportImages: #flicker fixation on and off at framerate to see when skip frame&#xa;        #      fixation.draw()&#xa;        #else: fixationBlank.draw()&#xa;        fixationPoint.draw()&#xa;        myWin.flip()  #end fixation interval&#xa;    #myWin.setRecordFrameIntervals(True);  #can't get it to stop detecting superlong frames&#xa;    t0 = trialClock.getTime()&#xa;&#xa;    for n in range(trialDurFrames): #this is the loop for this trial's stimulus!&#xa;        worked = oneFrameOfStim( n,cue,seq1,seq2,cueDurFrames,letterDurFrames,ISIframes,thisTrial,textStimuliStream1,textStimuliStream2,&#xa;                                                     noise,proportnNoise,allFieldCoords,numNoiseDots ) #draw letter and possibly cue and noise on top&#xa;        fixationPoint.draw()&#xa;        if exportImages:&#xa;            myWin.getMovieFrame(buffer='back') #for later saving&#xa;            framesSaved +=1&#xa;        myWin.flip()&#xa;        t=trialClock.getTime()-t0;  ts.append(t);&#xa;    #end of big stimulus loop&#xa;    myWin.setRecordFrameIntervals(False);&#xa;&#xa;    if task=='T1':&#xa;        respPromptStim.setText('What was circled?',log=False)   &#xa;    else: respPromptStim.setText('Error: unexpected task',log=False)&#xa;    postCueNumBlobsAway=-999 #doesn't apply to non-tracking and click tracking task&#xa;    correctAnswerIdxsStream1 = np.array( seq1[cuesPos] )&#xa;    correctAnswerIdxsStream2 = np.array( seq2[cuesPos] )&#xa;    #print('correctAnswerIdxsStream1=',correctAnswerIdxsStream1, 'wordList[correctAnswerIdxsStream1[0]]=',wordList[correctAnswerIdxsStream1[0]])&#xa;    return cuesPos,correctAnswerIdxsStream1,correctAnswerIdxsStream2,ts&#xa;    &#xa;def handleAndScoreResponse(passThisTrial,response,responseAutopilot,task,stimSequence,cuePos,correctAnswerIdx):&#xa;    #Handle response, calculate whether correct, ########################################&#xa;    #responses are actual characters&#xa;    #correctAnswer is index into stimSequence&#xa;    #autopilot is global variable&#xa;    if autopilot or passThisTrial:&#xa;        response = responseAutopilot&#xa;    #print('handleAndScoreResponse correctAnswerIdxs=',correctAnswerIdxs,'\nstimSequence=',stimSequence, '\nwords=',wordList)&#xa;    correct = 0&#xa;    approxCorrect = 0&#xa;    posOfResponse = -999&#xa;    responsePosRelative = -999&#xa;    idx = correctAnswerIdx&#xa;    correctAnswer = wordListThisTrial[idx].upper()&#xa;    responseString= ''.join(['%s' % char for char in response])&#xa;    responseString= responseString.upper()&#xa;    #print('correctAnswer=',correctAnswer ,' responseString=',responseString)&#xa;    if correctAnswer == responseString:&#xa;        correct = 1&#xa;    #print('correct=',correct)&#xa;    wordsPresented = list() #complicated for Kiuyan, not wordListThisTrial because of intron generation algorithm weird&#xa;    for i in stimSequence:&#xa;        wordsPresented.append( wordListThisTrial[i] )&#xa;    responseWordIdxInWordsPresented = wordToIdx(responseString,wordsPresented) #This could be wrong for Kiuyan because wordListThisTrial contains multiple&#xa;    responseWordIdx = stimSequence[ responseWordIdxInWordsPresented ] #Convert back to stimSequence&#xa;    #copies of introns. Seems we need a clean wordListThisTrial that contais only those presented&#xa;    print('responseString=',responseString, '  wordListThisTrial=',wordListThisTrial,' wordsPresented=',wordsPresented, ' responseWordIdx=',responseWordIdx)&#xa;    if responseWordIdx is None: #response is not in the wordList&#xa;        posOfResponse = -999&#xa;        logging.warn('Response was not present in the stimulus stream')&#xa;    else:&#xa;        posOfResponse= np.where( responseWordIdx==stimSequence )&#xa;        posOfResponse= posOfResponse[0] #list with two entries, want first which will be array of places where the response was found in the sequence&#xa;        if len(posOfResponse) > 1:&#xa;            logging.error('Expected response to have occurred in only one position in stream')&#xa;        print('posOfResponse=',posOfResponse, 'responseWordIdx=',responseWordIdx,' stimSequence=',stimSequence)&#xa;        posOfResponse = posOfResponse[0] #first element of list (should be only one element long &#xa;        responsePosRelative = posOfResponse - cuePos&#xa;        approxCorrect = abs(responsePosRelative)<= 3 #Vul efficacy measure of getting it right to within plus/minus&#xa;    #print('wordToIdx(',responseString,',',wordList,')=',responseWordIdx,' stimSequence=',stimSequence,'\nposOfResponse = ',posOfResponse) #debugON&#xa;    #print response stuff to dataFile&#xa;    #header was answerPos0, answer0, response0, correct0, responsePosRelative0&#xa;    print(cuePos,'\t', end='', file=dataFile)&#xa;    print(correctAnswer, '\t', end='', file=dataFile) #answer0&#xa;    print(responseString, '\t', end='', file=dataFile) #response0&#xa;    print(correct, '\t', end='',file=dataFile)   #correct0&#xa;    print(responsePosRelative, '\t', end='',file=dataFile) #responsePosRelative0&#xa;&#xa;    return correct,approxCorrect,responsePosRelative&#xa;    #end handleAndScoreResponses&#xa;&#xa;def play_high_tone_correct_low_incorrect(correct, passThisTrial=False):&#xa;    highA = sound.Sound('G',octave=5, sampleRate=6000, secs=.3, bits=8)&#xa;    low = sound.Sound('F',octave=3, sampleRate=6000, secs=.3, bits=8)&#xa;    highA.setVolume(0.9)&#xa;    low.setVolume(1.0)&#xa;    if correct:&#xa;        highA.play()&#xa;    elif passThisTrial:&#xa;        high= sound.Sound('G',octave=4, sampleRate=2000, secs=.08, bits=8)&#xa;        for i in range(2): &#xa;            high.play();  low.play(); &#xa;    else: #incorrect&#xa;        low.play()&#xa;&#xa;expStop=False&#xa;nDoneMain = -1 #change to zero once start main part of experiment&#xa;if doStaircase:&#xa;    #create the staircase handler&#xa;    useQuest = True&#xa;    if  useQuest:&#xa;        staircase = data.QuestHandler(startVal = 95, &#xa;                              startValSd = 80,&#xa;                              stopInterval= 1, #sd of posterior has to be this small or smaller for staircase to stop, unless nTrials reached&#xa;                              nTrials = staircaseTrials,&#xa;                              #extraInfo = thisInfo,&#xa;                              pThreshold = threshCriterion, #0.25,    &#xa;                              gamma = 1./26,&#xa;                              delta=0.02, #lapse rate, I suppose for Weibull function fit&#xa;                              method = 'quantile', #uses the median of the posterior as the final answer&#xa;                              stepType = 'log',  #will home in on the 80% threshold. But stepType = 'log' doesn't usually work&#xa;                              minVal=1, maxVal = 100&#xa;                              )&#xa;        print('created QUEST staircase')&#xa;    else:&#xa;        stepSizesLinear = [.2,.2,.1,.1,.05,.05]&#xa;        stepSizesLog = [log(1.4,10),log(1.4,10),log(1.3,10),log(1.3,10),log(1.2,10)]&#xa;        staircase = data.StairHandler(startVal = 0.1,&#xa;                                  stepType = 'log', #if log, what do I want to multiply it by&#xa;                                  stepSizes = stepSizesLog,    #step size to use after each reversal&#xa;                                  minVal=0, maxVal=1,&#xa;                                  nUp=1, nDown=3,  #will home in on the 80% threshold&#xa;                                  nReversals = 2, #The staircase terminates when nTrials have been exceeded, or when both nReversals and nTrials have been exceeded&#xa;                                  nTrials=1)&#xa;        print('created conventional staircase')&#xa;        &#xa;    if prefaceStaircaseTrialsN > len(prefaceStaircaseNoise): #repeat array to accommodate desired number of easyStarterTrials&#xa;        prefaceStaircaseNoise = np.tile( prefaceStaircaseNoise, ceil( prefaceStaircaseTrialsN/len(prefaceStaircaseNoise) ) )&#xa;    prefaceStaircaseNoise = prefaceStaircaseNoise[0:prefaceStaircaseTrialsN]&#xa;    &#xa;    phasesMsg = ('Doing '+str(prefaceStaircaseTrialsN)+'trials with noisePercent= '+str(prefaceStaircaseNoise)+' then doing a max '+str(staircaseTrials)+'-trial staircase')&#xa;    print(phasesMsg); logging.info(phasesMsg)&#xa;&#xa;    #staircaseStarterNoise PHASE OF EXPERIMENT&#xa;    corrEachTrial = list() #only needed for easyStaircaseStarterNoise&#xa;    staircaseTrialN = -1; mainStaircaseGoing = False&#xa;    while (not staircase.finished) and expStop==False: #staircase.thisTrialN < staircase.nTrials&#xa;        if staircaseTrialN+1 < len(prefaceStaircaseNoise): #still doing easyStaircaseStarterNoise&#xa;            staircaseTrialN += 1&#xa;            noisePercent = prefaceStaircaseNoise[staircaseTrialN]&#xa;        else:&#xa;            if staircaseTrialN+1 == len(prefaceStaircaseNoise): #add these non-staircase trials so QUEST knows about them&#xa;                mainStaircaseGoing = True&#xa;                print('Importing ',corrEachTrial,' and intensities ',prefaceStaircaseNoise)&#xa;                staircase.importData(100-prefaceStaircaseNoise, np.array(corrEachTrial))&#xa;                printStaircase(staircase, descendingPsycho, briefTrialUpdate=False, printInternalVal=True, alsoLog=False)&#xa;            try: #advance the staircase&#xa;                printStaircase(staircase, descendingPsycho, briefTrialUpdate=True, printInternalVal=True, alsoLog=False)&#xa;                noisePercent = 100. - staircase.next()  #will step through the staircase, based on whether told it (addResponse) got it right or wrong&#xa;                staircaseTrialN += 1&#xa;            except StopIteration: #Need this here, even though test for finished above. I can't understand why finished test doesn't accomplish this.&#xa;                print('stopping because staircase.next() returned a StopIteration, which it does when it is finished')&#xa;                break #break out of the trials loop&#xa;        #print('staircaseTrialN=',staircaseTrialN)&#xa;        idxsStream1, idxsStream2 = calcAndPredrawStimuli(wordList)&#xa;        for i in range(8,16): #make sure each position points to one of the orthographically dis/similar one&#xa;            print(i)#&#xa;        idxsStream1&#xa;        cuesPos,correctAnswerIdxsStream1,correctAnswerIdxsStream2, ts  = \&#xa;                                        do_RSVP_stim(cuePos, idxsStream1, idxsStream2, noisePercent/100.,staircaseTrialN)&#xa;        numCasesInterframeLong = timingCheckAndLog(ts,staircaseTrialN)&#xa;        expStop,passThisTrial,responses,responsesAutopilot = \&#xa;                stringResponse.collectStringResponse(numRespsWanted,respPromptStim,respStim,acceptTextStim,myWin,clickSound,badKeySound,&#xa;                                                                               requireAcceptance,autopilot,responseDebug=True)&#xa;&#xa;        if not expStop:&#xa;            if mainStaircaseGoing:&#xa;                print('staircase\t', end='', file=dataFile)&#xa;            else: &#xa;                print('staircase_preface\t', end='', file=dataFile)&#xa;             #header start      'trialnum\tsubject\ttask\t'&#xa;            print(staircaseTrialN,'\t', end='', file=dataFile) #first thing printed on each line of dataFile&#xa;            print(subject,'\t',task,'\t', round(noisePercent,2),'\t', end='', file=dataFile)&#xa;            correct,approxCorrect,responsePosRelative= handleAndScoreResponse(&#xa;                                                passThisTrial,responses,responseAutopilot,task,sequenceLeft,cuesPos[0],correctAnswerIdx )&#xa;&#xa;            print(numCasesInterframeLong, file=dataFile) #timingBlips, last thing recorded on each line of dataFile&#xa;            core.wait(.06)&#xa;            if feedback: &#xa;                play_high_tone_correct_low_incorrect(correct, passThisTrial=FALSE)&#xa;            print('staircaseTrialN=', staircaseTrialN,' noisePercent=',round(noisePercent,3),' T1approxCorrect=',T1approxCorrect) #debugON&#xa;            corrEachTrial.append(T1approxCorrect)&#xa;            if mainStaircaseGoing: &#xa;                staircase.addResponse(T1approxCorrect, intensity = 100-noisePercent) #Add a 1 or 0 to signify a correct/detected or incorrect/missed trial&#xa;                #print('Have added an intensity of','{:.3f}'.format(100-noisePercent), 'T1approxCorrect =', T1approxCorrect, ' to staircase') #debugON&#xa;    #ENDING STAIRCASE PHASE&#xa;&#xa;    if staircaseTrialN+1 < len(prefaceStaircaseNoise) and (staircaseTrialN>=0): #exp stopped before got through staircase preface trials, so haven't imported yet&#xa;        print('Importing ',corrEachTrial,' and intensities ',prefaceStaircaseNoise[0:staircaseTrialN+1])&#xa;        staircase.importData(100-prefaceStaircaseNoise[0:staircaseTrialN], np.array(corrEachTrial)) &#xa;    print('framesSaved after staircase=',framesSaved) #debugON&#xa;&#xa;    timeAndDateStr = time.strftime(""%H:%M on %d %b %Y"", time.localtime())&#xa;    msg = ('prefaceStaircase phase' if expStop else '')&#xa;    msg += ('ABORTED' if expStop else 'Finished') + ' staircase part of experiment at ' + timeAndDateStr&#xa;    logging.info(msg); print(msg)&#xa;    printStaircase(staircase, descendingPsycho, briefTrialUpdate=True, printInternalVal=True, alsoLog=False)&#xa;    #print('staircase.quantile=',round(staircase.quantile(),2),' sd=',round(staircase.sd(),2))&#xa;    threshNoise = round(staircase.quantile(),3)&#xa;    if descendingPsycho:&#xa;        threshNoise = 100- threshNoise&#xa;    threshNoise = max( 0, threshNoise ) #e.g. ff get all trials wrong, posterior peaks at a very negative number&#xa;    msg= 'Staircase estimate of threshold = ' + str(threshNoise) + ' with sd=' + str(round(staircase.sd(),2))&#xa;    logging.info(msg); print(msg)&#xa;    myWin.close()&#xa;    #Fit and plot data&#xa;    fit = None&#xa;    try:&#xa;        intensityForCurveFitting = staircase.intensities&#xa;        if descendingPsycho: &#xa;            intensityForCurveFitting = 100-staircase.intensities #because fitWeibull assumes curve is ascending&#xa;        fit = data.FitWeibull(intensityForCurveFitting, staircase.data, expectedMin=1/26., sems = 1.0/len(staircase.intensities))&#xa;    except:&#xa;        print(""Fit failed."")&#xa;    plotDataAndPsychometricCurve(staircase,fit,descendingPsycho,threshCriterion)&#xa;    #save figure to file&#xa;    pylab.savefig(fileName+'.pdf')&#xa;    print('The plot has been saved, as '+fileName+'.pdf')&#xa;    pylab.show() #must call this to actually show plot&#xa;else: #not staircase&#xa;    noisePercent = defaultNoiseLevel&#xa;    phasesMsg = 'Experiment will have '+str(trials.nTotal)+' trials. Letters will be drawn with superposed noise of ' + ""{:.2%}"".format(defaultNoiseLevel)&#xa;    print(phasesMsg); logging.info(phasesMsg)&#xa;    nDoneMain =0&#xa;    while nDoneMain < trials.nTotal and expStop==False: #MAIN EXPERIMENT LOOP&#xa;        if nDoneMain==0:&#xa;            msg='Starting main (non-staircase) part of experiment'&#xa;            logging.info(msg); print(msg)&#xa;        thisTrial = trials.next() #get a proper (non-staircase) trial&#xa;        sequenceStream1, sequenceStream2 = calcAndPredrawStimuli(thisTrial,wordList)&#xa;        cuesPos,correctAnswerIdxsStream1,correctAnswerIdxsStream2, ts  = \&#xa;                                                                    do_RSVP_stim(thisTrial, sequenceStream1, sequenceStream2, noisePercent/100.,nDoneMain)&#xa;        numCasesInterframeLong = timingCheckAndLog(ts,nDoneMain)&#xa;        #call for each response&#xa;        expStop = list(); passThisTrial = list(); responses=list(); responsesAutopilot=list()&#xa;        numCharsInResponse = len(wordList[0])&#xa;        dL = [None]*numRespsWanted #dummy list for null values&#xa;        expStop = copy.deepcopy(dL); responses = copy.deepcopy(dL); responsesAutopilot = copy.deepcopy(dL); passThisTrial=copy.deepcopy(dL)&#xa;        responseOrder = range(numRespsWanted)&#xa;        if thisTrial['rightResponseFirst']: #change order of indices depending on rightResponseFirst. response0, answer0 etc refer to which one had to be reported first&#xa;                responseOrder.reverse()&#xa;        for i in responseOrder:&#xa;            x = 3* wordEccentricity*(i*2-1) #put it 3 times farther out than stimulus, so participant is sure which is left and which right&#xa;            expStop[i],passThisTrial[i],responses[i],responsesAutopilot[i] = stringResponse.collectStringResponse(&#xa;                                      numCharsInResponse,x,respPromptStim,respStim,acceptTextStim,fixationPoint,myWin,clickSound,badKeySound,&#xa;                                                                                   requireAcceptance,autopilot,responseDebug=True)                                                                               &#xa;        expStop = np.array(expStop).any(); passThisTrial = np.array(passThisTrial).any()&#xa;        if not expStop:&#xa;            print('main\t', end='', file=dataFile) #first thing printed on each line of dataFile to indicate main part of experiment, not staircase&#xa;            print(nDoneMain,'\t', end='', file=dataFile)&#xa;            print(subject,'\t',task,'\t', round(noisePercent,3),'\t', end='', file=dataFile)&#xa;            print(thisTrial['leftStreamFlip'],'\t', end='', file=dataFile)&#xa;            print(thisTrial['rightStreamFlip'],'\t', end='', file=dataFile)&#xa;            print(thisTrial['flipHoriz'],'\t', end='', file=dataFile)&#xa;            print(thisTrial['flipVert'],'\t', end='', file=dataFile)&#xa;            print(thisTrial['rightResponseFirst'],'\t', end='', file=dataFile)&#xa;            print(thisTrial['orthographicalsim'],'\t', end='', file=dataFile)&#xa;            i = 0&#xa;            eachCorrect = np.ones(numRespsWanted)*-999; eachApproxCorrect = np.ones(numRespsWanted)*-999&#xa;            for i in range(numRespsWanted): #scored and printed to dataFile in left first, right second order even if collected in different order&#xa;                if i==0:&#xa;                    sequenceStream = sequenceStream1; correctAnswerIdxs = correctAnswerIdxsStream1; &#xa;                else: sequenceStream = sequenceStream2; correctAnswerIdxs = correctAnswerIdxsStream2; &#xa;                correct,approxCorrect,responsePosRelative = (&#xa;                        handleAndScoreResponse(passThisTrial,responses[i],responsesAutopilot[i],task,sequenceStream,thisTrial['cuePos'],correctAnswerIdxs ) )&#xa;                eachCorrect[i] = correct&#xa;                eachApproxCorrect[i] = approxCorrect&#xa;            print(numCasesInterframeLong, file=dataFile) #timingBlips, last thing recorded on each line of dataFile&#xa;            print('correct=',correct,' approxCorrect=',approxCorrect,' eachCorrect=',eachCorrect, ' responsePosRelative=', responsePosRelative)&#xa;            numTrialsCorrect += eachCorrect.all() #so count -1 as 0&#xa;            numTrialsEachCorrect += eachCorrect #list numRespsWanted long&#xa;            numTrialsApproxCorrect += eachApproxCorrect.all()&#xa;            numTrialsEachApproxCorrect += eachApproxCorrect #list numRespsWanted long&#xa;                &#xa;            if exportImages:  #catches one frame of response&#xa;                 myWin.getMovieFrame() #I cant explain why another getMovieFrame, and core.wait is needed&#xa;                 framesSaved +=1; core.wait(.1)&#xa;                 myWin.saveMovieFrames('images_sounds_movies/frames.png') #mov not currently supported &#xa;                 expStop=True&#xa;            core.wait(.1)&#xa;            if feedback: play_high_tone_correct_low_incorrect(correct, passThisTrial=False)&#xa;            nDoneMain+=1&#xa;            &#xa;            dataFile.flush(); logging.flush()&#xa;            print('nDoneMain=', nDoneMain,' trials.nTotal=',trials.nTotal) #' trials.thisN=',trials.thisN&#xa;            if (trials.nTotal > 6 and nDoneMain > 2 and nDoneMain %&#xa;                 ( trials.nTotal*pctCompletedBreak/100. ) ==1):  #dont modulus 0 because then will do it for last trial&#xa;                    nextText.setText('Press ""SPACE"" to continue!')&#xa;                    nextText.draw()&#xa;                    progressMsg = 'Completed ' + str(nDoneMain) + ' of ' + str(trials.nTotal) + ' trials'&#xa;                    NextRemindCountText.setText(progressMsg)&#xa;                    NextRemindCountText.draw()&#xa;                    myWin.flip() # myWin.flip(clearBuffer=True) &#xa;                    waiting=True&#xa;                    while waiting:&#xa;                       if autopilot: break&#xa;                       elif expStop == True:break&#xa;                       for key in event.getKeys():      #check if pressed abort-type key&#xa;                             if key in ['space','ESCAPE']: &#xa;                                waiting=False&#xa;                             if key in ['ESCAPE']:&#xa;                                expStop = True&#xa;                    myWin.clearBuffer()&#xa;            core.wait(.2); time.sleep(.2)&#xa;        #end main trials loop&#xa;timeAndDateStr = time.strftime(""%H:%M on %d %b %Y"", time.localtime())&#xa;msg = 'Finishing at '+timeAndDateStr&#xa;print(msg); logging.info(msg)&#xa;if expStop:&#xa;    msg = 'user aborted experiment on keypress with trials done=' + str(nDoneMain) + ' of ' + str(trials.nTotal+1)&#xa;    print(msg); logging.error(msg)&#xa;&#xa;if not doStaircase and (nDoneMain >0):&#xa;    msg = 'Of ' + str(nDoneMain)+' trials, on '+str(numTrialsCorrect*1.0/nDoneMain*100.)+'% of all trials all targets reported exactly correct'&#xa;    print(msg); logging.info(msg)&#xa;    msg= 'All targets approximately correct in '+ str( round(numTrialsApproxCorrect*1.0/nDoneMain*100,1)) + '% of trials'&#xa;    print(msg); logging.info(msg)&#xa;    for i in range(numRespsWanted):&#xa;        msg = 'stream'+str(i)+': '+str( round(numTrialsEachCorrect[i]*1.0/nDoneMain*100.,2) ) + '% correct'&#xa;        print(msg); logging.info(msg)&#xa;        msg = 'stream' + str(i) + ': '+ str( round(numTrialsEachApproxCorrect[i]*1.0/nDoneMain*100,2) ) +'% approximately correct'&#xa;        print(msg); logging.info(msg)&#xa;&#xa;logging.flush(); dataFile.close()&#xa;myWin.close() #have to close window if want to show a plot"
10060069|"""""""&#xa;Safe version of tarfile.extractall which does not extract any files that would&#xa;be, or symlink to a file that is, outside of the directory extracted in.&#xa;&#xa;Adapted from:&#xa;http://stackoverflow.com/questions/10060069/safely-extract-zip-or-tar-using-python&#xa;""""""&#xa;from os.path import abspath, realpath, dirname, join as joinpath&#xa;from django.core.exceptions import SuspiciousOperation&#xa;import logging&#xa;&#xa;log = logging.getLogger(__name__)&#xa;&#xa;&#xa;def resolved(rpath):&#xa;    """"""&#xa;    Returns the canonical absolute path of `rpath`.&#xa;    """"""&#xa;    return realpath(abspath(rpath))&#xa;&#xa;&#xa;def _is_bad_path(path, base):&#xa;    """"""&#xa;    Is (the canonical absolute path of) `path` outside `base`?&#xa;    """"""&#xa;    return not resolved(joinpath(base, path)).startswith(base)&#xa;&#xa;&#xa;def _is_bad_link(info, base):&#xa;    """"""&#xa;    Does the file sym- ord hard-link to files outside `base`?&#xa;    """"""&#xa;    # Links are interpreted relative to the directory containing the link&#xa;    tip = resolved(joinpath(base, dirname(info.name)))&#xa;    return _is_bad_path(info.linkname, base=tip)&#xa;&#xa;&#xa;def safemembers(members):&#xa;    """"""&#xa;    Check that all elements of a tar file are safe.&#xa;    """"""&#xa;&#xa;    base = resolved(""."")&#xa;&#xa;    for finfo in members:&#xa;        if _is_bad_path(finfo.name, base):&#xa;            log.debug(""File %r is blocked (illegal path)"", finfo.name)&#xa;            raise SuspiciousOperation(""Illegal path"")&#xa;        elif finfo.issym() and _is_bad_link(finfo, base):&#xa;            log.debug(""File %r is blocked: Hard link to %r"", finfo.name, finfo.linkname)&#xa;            raise SuspiciousOperation(""Hard link"")&#xa;        elif finfo.islnk() and _is_bad_link(finfo, base):&#xa;            log.debug(""File %r is blocked: Symlink to %r"", finfo.name,&#xa;                      finfo.linkname)&#xa;            raise SuspiciousOperation(""Symlink"")&#xa;        elif finfo.isdev():&#xa;            log.debug(""File %r is blocked: FIFO, device or character file"",&#xa;                      finfo.name)&#xa;            raise SuspiciousOperation(""Dev file"")&#xa;&#xa;    return members&#xa;&#xa;&#xa;def safetar_extractall(tarf, *args, **kwargs):&#xa;    """"""&#xa;    Safe version of `tarf.extractall()`.&#xa;    """"""&#xa;    return tarf.extractall(members=safemembers(tarf), *args, **kwargs)&#xa;"
1633833|"from __future__ import print_function, division&#xa;&#xa;from collections import defaultdict&#xa;from itertools import combinations, permutations, product, product as cartes&#xa;import random&#xa;from operator import gt&#xa;&#xa;from sympy.core.decorators import deprecated&#xa;from sympy.core import Basic&#xa;&#xa;# this is the logical location of these functions&#xa;from sympy.core.compatibility import (&#xa;    as_int, combinations_with_replacement, default_sort_key, is_sequence,&#xa;    iterable, ordered, range&#xa;)&#xa;&#xa;from sympy.utilities.enumerative import (&#xa;    multiset_partitions_taocp, list_visitor, MultisetPartitionTraverser)&#xa;&#xa;&#xa;def flatten(iterable, levels=None, cls=None):&#xa;    """"""&#xa;    Recursively denest iterable containers.&#xa;&#xa;    >>> from sympy.utilities.iterables import flatten&#xa;&#xa;    >>> flatten([1, 2, 3])&#xa;    [1, 2, 3]&#xa;    >>> flatten([1, 2, [3]])&#xa;    [1, 2, 3]&#xa;    >>> flatten([1, [2, 3], [4, 5]])&#xa;    [1, 2, 3, 4, 5]&#xa;    >>> flatten([1.0, 2, (1, None)])&#xa;    [1.0, 2, 1, None]&#xa;&#xa;    If you want to denest only a specified number of levels of&#xa;    nested containers, then set ``levels`` flag to the desired&#xa;    number of levels::&#xa;&#xa;    >>> ls = [[(-2, -1), (1, 2)], [(0, 0)]]&#xa;&#xa;    >>> flatten(ls, levels=1)&#xa;    [(-2, -1), (1, 2), (0, 0)]&#xa;&#xa;    If cls argument is specified, it will only flatten instances of that&#xa;    class, for example:&#xa;&#xa;    >>> from sympy.core import Basic&#xa;    >>> class MyOp(Basic):&#xa;    ...     pass&#xa;    ...&#xa;    >>> flatten([MyOp(1, MyOp(2, 3))], cls=MyOp)&#xa;    [1, 2, 3]&#xa;&#xa;    adapted from http://kogs-www.informatik.uni-hamburg.de/~meine/python_tricks&#xa;    """"""&#xa;    if levels is not None:&#xa;        if not levels:&#xa;            return iterable&#xa;        elif levels > 0:&#xa;            levels -= 1&#xa;        else:&#xa;            raise ValueError(&#xa;                ""expected non-negative number of levels, got %s"" % levels)&#xa;&#xa;    if cls is None:&#xa;        reducible = lambda x: is_sequence(x, set)&#xa;    else:&#xa;        reducible = lambda x: isinstance(x, cls)&#xa;&#xa;    result = []&#xa;&#xa;    for el in iterable:&#xa;        if reducible(el):&#xa;            if hasattr(el, 'args'):&#xa;                el = el.args&#xa;            result.extend(flatten(el, levels=levels, cls=cls))&#xa;        else:&#xa;            result.append(el)&#xa;&#xa;    return result&#xa;&#xa;&#xa;def unflatten(iter, n=2):&#xa;    """"""Group ``iter`` into tuples of length ``n``. Raise an error if&#xa;    the length of ``iter`` is not a multiple of ``n``.&#xa;    """"""&#xa;    if n < 1 or len(iter) % n:&#xa;        raise ValueError('iter length is not a multiple of %i' % n)&#xa;    return list(zip(*(iter[i::n] for i in range(n))))&#xa;&#xa;&#xa;def reshape(seq, how):&#xa;    """"""Reshape the sequence according to the template in ``how``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities import reshape&#xa;    >>> seq = list(range(1, 9))&#xa;&#xa;    >>> reshape(seq, [4]) # lists of 4&#xa;    [[1, 2, 3, 4], [5, 6, 7, 8]]&#xa;&#xa;    >>> reshape(seq, (4,)) # tuples of 4&#xa;    [(1, 2, 3, 4), (5, 6, 7, 8)]&#xa;&#xa;    >>> reshape(seq, (2, 2)) # tuples of 4&#xa;    [(1, 2, 3, 4), (5, 6, 7, 8)]&#xa;&#xa;    >>> reshape(seq, (2, [2])) # (i, i, [i, i])&#xa;    [(1, 2, [3, 4]), (5, 6, [7, 8])]&#xa;&#xa;    >>> reshape(seq, ((2,), [2])) # etc....&#xa;    [((1, 2), [3, 4]), ((5, 6), [7, 8])]&#xa;&#xa;    >>> reshape(seq, (1, [2], 1))&#xa;    [(1, [2, 3], 4), (5, [6, 7], 8)]&#xa;&#xa;    >>> reshape(tuple(seq), ([[1], 1, (2,)],))&#xa;    (([[1], 2, (3, 4)],), ([[5], 6, (7, 8)],))&#xa;&#xa;    >>> reshape(tuple(seq), ([1], 1, (2,)))&#xa;    (([1], 2, (3, 4)), ([5], 6, (7, 8)))&#xa;&#xa;    >>> reshape(list(range(12)), [2, [3], set([2]), (1, (3,), 1)])&#xa;    [[0, 1, [2, 3, 4], set([5, 6]), (7, (8, 9, 10), 11)]]&#xa;&#xa;    """"""&#xa;    m = sum(flatten(how))&#xa;    n, rem = divmod(len(seq), m)&#xa;    if m < 0 or rem:&#xa;        raise ValueError('template must sum to positive number '&#xa;        'that divides the length of the sequence')&#xa;    i = 0&#xa;    container = type(how)&#xa;    rv = [None]*n&#xa;    for k in range(len(rv)):&#xa;        rv[k] = []&#xa;        for hi in how:&#xa;            if type(hi) is int:&#xa;                rv[k].extend(seq[i: i + hi])&#xa;                i += hi&#xa;            else:&#xa;                n = sum(flatten(hi))&#xa;                hi_type = type(hi)&#xa;                rv[k].append(hi_type(reshape(seq[i: i + n], hi)[0]))&#xa;                i += n&#xa;        rv[k] = container(rv[k])&#xa;    return type(seq)(rv)&#xa;&#xa;&#xa;def group(seq, multiple=True):&#xa;    """"""&#xa;    Splits a sequence into a list of lists of equal, adjacent elements.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import group&#xa;&#xa;    >>> group([1, 1, 1, 2, 2, 3])&#xa;    [[1, 1, 1], [2, 2], [3]]&#xa;    >>> group([1, 1, 1, 2, 2, 3], multiple=False)&#xa;    [(1, 3), (2, 2), (3, 1)]&#xa;    >>> group([1, 1, 3, 2, 2, 1], multiple=False)&#xa;    [(1, 2), (3, 1), (2, 2), (1, 1)]&#xa;&#xa;    See Also&#xa;    ========&#xa;    multiset&#xa;    """"""&#xa;    if not seq:&#xa;        return []&#xa;&#xa;    current, groups = [seq[0]], []&#xa;&#xa;    for elem in seq[1:]:&#xa;        if elem == current[-1]:&#xa;            current.append(elem)&#xa;        else:&#xa;            groups.append(current)&#xa;            current = [elem]&#xa;&#xa;    groups.append(current)&#xa;&#xa;    if multiple:&#xa;        return groups&#xa;&#xa;    for i, current in enumerate(groups):&#xa;        groups[i] = (current[0], len(current))&#xa;&#xa;    return groups&#xa;&#xa;&#xa;def multiset(seq):&#xa;    """"""Return the hashable sequence in multiset form with values being the&#xa;    multiplicity of the item in the sequence.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import multiset&#xa;    >>> multiset('mississippi')&#xa;    {'i': 4, 'm': 1, 'p': 2, 's': 4}&#xa;&#xa;    See Also&#xa;    ========&#xa;    group&#xa;    """"""&#xa;    rv = defaultdict(int)&#xa;    for s in seq:&#xa;        rv[s] += 1&#xa;    return dict(rv)&#xa;&#xa;&#xa;def postorder_traversal(node, keys=None):&#xa;    """"""&#xa;    Do a postorder traversal of a tree.&#xa;&#xa;    This generator recursively yields nodes that it has visited in a postorder&#xa;    fashion. That is, it descends through the tree depth-first to yield all of&#xa;    a node's children's postorder traversal before yielding the node itself.&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    node : sympy expression&#xa;        The expression to traverse.&#xa;    keys : (default None) sort key(s)&#xa;        The key(s) used to sort args of Basic objects. When None, args of Basic&#xa;        objects are processed in arbitrary order. If key is defined, it will&#xa;        be passed along to ordered() as the only key(s) to use to sort the&#xa;        arguments; if ``key`` is simply True then the default keys of&#xa;        ``ordered`` will be used (node count and default_sort_key).&#xa;&#xa;    Yields&#xa;    ======&#xa;    subtree : sympy expression&#xa;        All of the subtrees in the tree.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import postorder_traversal&#xa;    >>> from sympy.abc import w, x, y, z&#xa;&#xa;    The nodes are returned in the order that they are encountered unless key&#xa;    is given; simply passing key=True will guarantee that the traversal is&#xa;    unique.&#xa;&#xa;    >>> list(postorder_traversal(w + (x + y)*z)) # doctest: +SKIP&#xa;    [z, y, x, x + y, z*(x + y), w, w + z*(x + y)]&#xa;    >>> list(postorder_traversal(w + (x + y)*z, keys=True))&#xa;    [w, z, x, y, x + y, z*(x + y), w + z*(x + y)]&#xa;&#xa;&#xa;    """"""&#xa;    if isinstance(node, Basic):&#xa;        args = node.args&#xa;        if keys:&#xa;            if keys != True:&#xa;                args = ordered(args, keys, default=False)&#xa;            else:&#xa;                args = ordered(args)&#xa;        for arg in args:&#xa;            for subtree in postorder_traversal(arg, keys):&#xa;                yield subtree&#xa;    elif iterable(node):&#xa;        for item in node:&#xa;            for subtree in postorder_traversal(item, keys):&#xa;                yield subtree&#xa;    yield node&#xa;&#xa;&#xa;def interactive_traversal(expr):&#xa;    """"""Traverse a tree asking a user which branch to choose. """"""&#xa;    from sympy.printing import pprint&#xa;&#xa;    RED, BRED = '\033[0;31m', '\033[1;31m'&#xa;    GREEN, BGREEN = '\033[0;32m', '\033[1;32m'&#xa;    YELLOW, BYELLOW = '\033[0;33m', '\033[1;33m'&#xa;    BLUE, BBLUE = '\033[0;34m', '\033[1;34m'&#xa;    MAGENTA, BMAGENTA = '\033[0;35m', '\033[1;35m'&#xa;    CYAN, BCYAN = '\033[0;36m', '\033[1;36m'&#xa;    END = '\033[0m'&#xa;&#xa;    def cprint(*args):&#xa;        print("""".join(map(str, args)) + END)&#xa;&#xa;    def _interactive_traversal(expr, stage):&#xa;        if stage > 0:&#xa;            print()&#xa;&#xa;        cprint(""Current expression (stage "", BYELLOW, stage, END, ""):"")&#xa;        print(BCYAN)&#xa;        pprint(expr)&#xa;        print(END)&#xa;&#xa;        if isinstance(expr, Basic):&#xa;            if expr.is_Add:&#xa;                args = expr.as_ordered_terms()&#xa;            elif expr.is_Mul:&#xa;                args = expr.as_ordered_factors()&#xa;            else:&#xa;                args = expr.args&#xa;        elif hasattr(expr, ""__iter__""):&#xa;            args = list(expr)&#xa;        else:&#xa;            return expr&#xa;&#xa;        n_args = len(args)&#xa;&#xa;        if not n_args:&#xa;            return expr&#xa;&#xa;        for i, arg in enumerate(args):&#xa;            cprint(GREEN, ""["", BGREEN, i, GREEN, ""] "", BLUE, type(arg), END)&#xa;            pprint(arg)&#xa;            print&#xa;&#xa;        if n_args == 1:&#xa;            choices = '0'&#xa;        else:&#xa;            choices = '0-%d' % (n_args - 1)&#xa;&#xa;        try:&#xa;            choice = raw_input(""Your choice [%s,f,l,r,d,?]: "" % choices)&#xa;        except EOFError:&#xa;            result = expr&#xa;            print()&#xa;        else:&#xa;            if choice == '?':&#xa;                cprint(RED, ""%s - select subexpression with the given index"" %&#xa;                       choices)&#xa;                cprint(RED, ""f - select the first subexpression"")&#xa;                cprint(RED, ""l - select the last subexpression"")&#xa;                cprint(RED, ""r - select a random subexpression"")&#xa;                cprint(RED, ""d - done\n"")&#xa;&#xa;                result = _interactive_traversal(expr, stage)&#xa;            elif choice in ['d', '']:&#xa;                result = expr&#xa;            elif choice == 'f':&#xa;                result = _interactive_traversal(args[0], stage + 1)&#xa;            elif choice == 'l':&#xa;                result = _interactive_traversal(args[-1], stage + 1)&#xa;            elif choice == 'r':&#xa;                result = _interactive_traversal(random.choice(args), stage + 1)&#xa;            else:&#xa;                try:&#xa;                    choice = int(choice)&#xa;                except ValueError:&#xa;                    cprint(BRED,&#xa;                           ""Choice must be a number in %s range\n"" % choices)&#xa;                    result = _interactive_traversal(expr, stage)&#xa;                else:&#xa;                    if choice < 0 or choice >= n_args:&#xa;                        cprint(BRED, ""Choice must be in %s range\n"" % choices)&#xa;                        result = _interactive_traversal(expr, stage)&#xa;                    else:&#xa;                        result = _interactive_traversal(args[choice], stage + 1)&#xa;&#xa;        return result&#xa;&#xa;    return _interactive_traversal(expr, 0)&#xa;&#xa;&#xa;def ibin(n, bits=0, str=False):&#xa;    """"""Return a list of length ``bits`` corresponding to the binary value&#xa;    of ``n`` with small bits to the right (last). If bits is omitted, the&#xa;    length will be the number required to represent ``n``. If the bits are&#xa;    desired in reversed order, use the [::-1] slice of the returned list.&#xa;&#xa;    If a sequence of all bits-length lists starting from [0, 0,..., 0]&#xa;    through [1, 1, ..., 1] are desired, pass a non-integer for bits, e.g.&#xa;    'all'.&#xa;&#xa;    If the bit *string* is desired pass ``str=True``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import ibin&#xa;    >>> ibin(2)&#xa;    [1, 0]&#xa;    >>> ibin(2, 4)&#xa;    [0, 0, 1, 0]&#xa;    >>> ibin(2, 4)[::-1]&#xa;    [0, 1, 0, 0]&#xa;&#xa;    If all lists corresponding to 0 to 2**n - 1, pass a non-integer&#xa;    for bits:&#xa;&#xa;    >>> bits = 2&#xa;    >>> for i in ibin(2, 'all'):&#xa;    ...     print(i)&#xa;    (0, 0)&#xa;    (0, 1)&#xa;    (1, 0)&#xa;    (1, 1)&#xa;&#xa;    If a bit string is desired of a given length, use str=True:&#xa;&#xa;    >>> n = 123&#xa;    >>> bits = 10&#xa;    >>> ibin(n, bits, str=True)&#xa;    '0001111011'&#xa;    >>> ibin(n, bits, str=True)[::-1]  # small bits left&#xa;    '1101111000'&#xa;    >>> list(ibin(3, 'all', str=True))&#xa;    ['000', '001', '010', '011', '100', '101', '110', '111']&#xa;&#xa;    """"""&#xa;    if not str:&#xa;        try:&#xa;            bits = as_int(bits)&#xa;            return [1 if i == ""1"" else 0 for i in bin(n)[2:].rjust(bits, ""0"")]&#xa;        except ValueError:&#xa;            return variations(list(range(2)), n, repetition=True)&#xa;    else:&#xa;        try:&#xa;            bits = as_int(bits)&#xa;            return bin(n)[2:].rjust(bits, ""0"")&#xa;        except ValueError:&#xa;            return (bin(i)[2:].rjust(n, ""0"") for i in range(2**n))&#xa;&#xa;&#xa;def variations(seq, n, repetition=False):&#xa;    """"""Returns a generator of the n-sized variations of ``seq`` (size N).&#xa;    ``repetition`` controls whether items in ``seq`` can appear more than once;&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    variations(seq, n) will return N! / (N - n)! permutations without&#xa;    repetition of seq's elements:&#xa;&#xa;        >>> from sympy.utilities.iterables import variations&#xa;        >>> list(variations([1, 2], 2))&#xa;        [(1, 2), (2, 1)]&#xa;&#xa;    variations(seq, n, True) will return the N**n permutations obtained&#xa;    by allowing repetition of elements:&#xa;&#xa;        >>> list(variations([1, 2], 2, repetition=True))&#xa;        [(1, 1), (1, 2), (2, 1), (2, 2)]&#xa;&#xa;    If you ask for more items than are in the set you get the empty set unless&#xa;    you allow repetitions:&#xa;&#xa;        >>> list(variations([0, 1], 3, repetition=False))&#xa;        []&#xa;        >>> list(variations([0, 1], 3, repetition=True))[:4]&#xa;        [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1)]&#xa;&#xa;    See Also&#xa;    ========&#xa;&#xa;    sympy.core.compatibility.permutations&#xa;    sympy.core.compatibility.product&#xa;    """"""&#xa;    if not repetition:&#xa;        seq = tuple(seq)&#xa;        if len(seq) < n:&#xa;            return&#xa;        for i in permutations(seq, n):&#xa;            yield i&#xa;    else:&#xa;        if n == 0:&#xa;            yield ()&#xa;        else:&#xa;            for i in product(seq, repeat=n):&#xa;                yield i&#xa;&#xa;&#xa;def subsets(seq, k=None, repetition=False):&#xa;    """"""Generates all k-subsets (combinations) from an n-element set, seq.&#xa;&#xa;       A k-subset of an n-element set is any subset of length exactly k. The&#xa;       number of k-subsets of an n-element set is given by binomial(n, k),&#xa;       whereas there are 2**n subsets all together. If k is None then all&#xa;       2**n subsets will be returned from shortest to longest.&#xa;&#xa;       Examples&#xa;       ========&#xa;&#xa;       >>> from sympy.utilities.iterables import subsets&#xa;&#xa;       subsets(seq, k) will return the n!/k!/(n - k)! k-subsets (combinations)&#xa;       without repetition, i.e. once an item has been removed, it can no&#xa;       longer be ""taken"":&#xa;&#xa;           >>> list(subsets([1, 2], 2))&#xa;           [(1, 2)]&#xa;           >>> list(subsets([1, 2]))&#xa;           [(), (1,), (2,), (1, 2)]&#xa;           >>> list(subsets([1, 2, 3], 2))&#xa;           [(1, 2), (1, 3), (2, 3)]&#xa;&#xa;&#xa;       subsets(seq, k, repetition=True) will return the (n - 1 + k)!/k!/(n - 1)!&#xa;       combinations *with* repetition:&#xa;&#xa;           >>> list(subsets([1, 2], 2, repetition=True))&#xa;           [(1, 1), (1, 2), (2, 2)]&#xa;&#xa;       If you ask for more items than are in the set you get the empty set unless&#xa;       you allow repetitions:&#xa;&#xa;           >>> list(subsets([0, 1], 3, repetition=False))&#xa;           []&#xa;           >>> list(subsets([0, 1], 3, repetition=True))&#xa;           [(0, 0, 0), (0, 0, 1), (0, 1, 1), (1, 1, 1)]&#xa;&#xa;       """"""&#xa;    if k is None:&#xa;        for k in range(len(seq) + 1):&#xa;            for i in subsets(seq, k, repetition):&#xa;                yield i&#xa;    else:&#xa;        if not repetition:&#xa;            for i in combinations(seq, k):&#xa;                yield i&#xa;        else:&#xa;            for i in combinations_with_replacement(seq, k):&#xa;                yield i&#xa;&#xa;&#xa;def filter_symbols(iterator, exclude):&#xa;    """"""&#xa;    Only yield elements from `iterator` that do not occur in `exclude`.&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    iterator : iterable&#xa;    iterator to take elements from&#xa;&#xa;    exclude : iterable&#xa;    elements to exclude&#xa;&#xa;    Returns&#xa;    =======&#xa;&#xa;    iterator : iterator&#xa;    filtered iterator&#xa;    """"""&#xa;    exclude = set(exclude)&#xa;    for s in iterator:&#xa;        if s not in exclude:&#xa;            yield s&#xa;&#xa;def numbered_symbols(prefix='x', cls=None, start=0, exclude=[], *args, **assumptions):&#xa;    """"""&#xa;    Generate an infinite stream of Symbols consisting of a prefix and&#xa;    increasing subscripts provided that they do not occur in `exclude`.&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    prefix : str, optional&#xa;        The prefix to use. By default, this function will generate symbols of&#xa;        the form ""x0"", ""x1"", etc.&#xa;&#xa;    cls : class, optional&#xa;        The class to use. By default, it uses Symbol, but you can also use Wild or Dummy.&#xa;&#xa;    start : int, optional&#xa;        The start number.  By default, it is 0.&#xa;&#xa;    Returns&#xa;    =======&#xa;&#xa;    sym : Symbol&#xa;        The subscripted symbols.&#xa;    """"""&#xa;    exclude = set(exclude or [])&#xa;    if cls is None:&#xa;        # We can't just make the default cls=Symbol because it isn't&#xa;        # imported yet.&#xa;        from sympy import Symbol&#xa;        cls = Symbol&#xa;&#xa;    while True:&#xa;        name = '%s%s' % (prefix, start)&#xa;        s = cls(name, *args, **assumptions)&#xa;        if s not in exclude:&#xa;            yield s&#xa;        start += 1&#xa;&#xa;&#xa;def capture(func):&#xa;    """"""Return the printed output of func().&#xa;&#xa;    `func` should be a function without arguments that produces output with&#xa;    print statements.&#xa;&#xa;    >>> from sympy.utilities.iterables import capture&#xa;    >>> from sympy import pprint&#xa;    >>> from sympy.abc import x&#xa;    >>> def foo():&#xa;    ...     print('hello world!')&#xa;    ...&#xa;    >>> 'hello' in capture(foo) # foo, not foo()&#xa;    True&#xa;    >>> capture(lambda: pprint(2/x))&#xa;    '2\\n-\\nx\\n'&#xa;&#xa;    """"""&#xa;    from sympy.core.compatibility import StringIO&#xa;    import sys&#xa;&#xa;    stdout = sys.stdout&#xa;    sys.stdout = file = StringIO()&#xa;    try:&#xa;        func()&#xa;    finally:&#xa;        sys.stdout = stdout&#xa;    return file.getvalue()&#xa;&#xa;&#xa;def sift(seq, keyfunc):&#xa;    """"""&#xa;    Sift the sequence, ``seq`` into a dictionary according to keyfunc.&#xa;&#xa;    OUTPUT: each element in expr is stored in a list keyed to the value&#xa;    of keyfunc for the element.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities import sift&#xa;    >>> from sympy.abc import x, y&#xa;    >>> from sympy import sqrt, exp&#xa;&#xa;    >>> sift(range(5), lambda x: x % 2)&#xa;    {0: [0, 2, 4], 1: [1, 3]}&#xa;&#xa;    sift() returns a defaultdict() object, so any key that has no matches will&#xa;    give [].&#xa;&#xa;    >>> sift([x], lambda x: x.is_commutative)&#xa;    {True: [x]}&#xa;    >>> _[False]&#xa;    []&#xa;&#xa;    Sometimes you won't know how many keys you will get:&#xa;&#xa;    >>> sift([sqrt(x), exp(x), (y**x)**2],&#xa;    ...      lambda x: x.as_base_exp()[0])&#xa;    {E: [exp(x)], x: [sqrt(x)], y: [y**(2*x)]}&#xa;&#xa;    If you need to sort the sifted items it might be better to use&#xa;    ``ordered`` which can economically apply multiple sort keys&#xa;    to a squence while sorting.&#xa;&#xa;    See Also&#xa;    ========&#xa;    ordered&#xa;    """"""&#xa;    m = defaultdict(list)&#xa;    for i in seq:&#xa;        m[keyfunc(i)].append(i)&#xa;    return m&#xa;&#xa;&#xa;def take(iter, n):&#xa;    """"""Return ``n`` items from ``iter`` iterator. """"""&#xa;    return [ value for _, value in zip(range(n), iter) ]&#xa;&#xa;&#xa;def dict_merge(*dicts):&#xa;    """"""Merge dictionaries into a single dictionary. """"""&#xa;    merged = {}&#xa;&#xa;    for dict in dicts:&#xa;        merged.update(dict)&#xa;&#xa;    return merged&#xa;&#xa;&#xa;def common_prefix(*seqs):&#xa;    """"""Return the subsequence that is a common start of sequences in ``seqs``.&#xa;&#xa;    >>> from sympy.utilities.iterables import common_prefix&#xa;    >>> common_prefix(list(range(3)))&#xa;    [0, 1, 2]&#xa;    >>> common_prefix(list(range(3)), list(range(4)))&#xa;    [0, 1, 2]&#xa;    >>> common_prefix([1, 2, 3], [1, 2, 5])&#xa;    [1, 2]&#xa;    >>> common_prefix([1, 2, 3], [1, 3, 5])&#xa;    [1]&#xa;    """"""&#xa;    if any(not s for s in seqs):&#xa;        return []&#xa;    elif len(seqs) == 1:&#xa;        return seqs[0]&#xa;    i = 0&#xa;    for i in range(min(len(s) for s in seqs)):&#xa;        if not all(seqs[j][i] == seqs[0][i] for j in range(len(seqs))):&#xa;            break&#xa;    else:&#xa;        i += 1&#xa;    return seqs[0][:i]&#xa;&#xa;&#xa;def common_suffix(*seqs):&#xa;    """"""Return the subsequence that is a common ending of sequences in ``seqs``.&#xa;&#xa;    >>> from sympy.utilities.iterables import common_suffix&#xa;    >>> common_suffix(list(range(3)))&#xa;    [0, 1, 2]&#xa;    >>> common_suffix(list(range(3)), list(range(4)))&#xa;    []&#xa;    >>> common_suffix([1, 2, 3], [9, 2, 3])&#xa;    [2, 3]&#xa;    >>> common_suffix([1, 2, 3], [9, 7, 3])&#xa;    [3]&#xa;    """"""&#xa;&#xa;    if any(not s for s in seqs):&#xa;        return []&#xa;    elif len(seqs) == 1:&#xa;        return seqs[0]&#xa;    i = 0&#xa;    for i in range(-1, -min(len(s) for s in seqs) - 1, -1):&#xa;        if not all(seqs[j][i] == seqs[0][i] for j in range(len(seqs))):&#xa;            break&#xa;    else:&#xa;        i -= 1&#xa;    if i == -1:&#xa;        return []&#xa;    else:&#xa;        return seqs[0][i + 1:]&#xa;&#xa;&#xa;def prefixes(seq):&#xa;    """"""&#xa;    Generate all prefixes of a sequence.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import prefixes&#xa;&#xa;    >>> list(prefixes([1,2,3,4]))&#xa;    [[1], [1, 2], [1, 2, 3], [1, 2, 3, 4]]&#xa;&#xa;    """"""&#xa;    n = len(seq)&#xa;&#xa;    for i in range(n):&#xa;        yield seq[:i + 1]&#xa;&#xa;&#xa;def postfixes(seq):&#xa;    """"""&#xa;    Generate all postfixes of a sequence.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import postfixes&#xa;&#xa;    >>> list(postfixes([1,2,3,4]))&#xa;    [[4], [3, 4], [2, 3, 4], [1, 2, 3, 4]]&#xa;&#xa;    """"""&#xa;    n = len(seq)&#xa;&#xa;    for i in range(n):&#xa;        yield seq[n - i - 1:]&#xa;&#xa;&#xa;def topological_sort(graph, key=None):&#xa;    r""""""&#xa;    Topological sort of graph's vertices.&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    ``graph`` : ``tuple[list, list[tuple[T, T]]``&#xa;        A tuple consisting of a list of vertices and a list of edges of&#xa;        a graph to be sorted topologically.&#xa;&#xa;    ``key`` : ``callable[T]`` (optional)&#xa;        Ordering key for vertices on the same level. By default the natural&#xa;        (e.g. lexicographic) ordering is used (in this case the base type&#xa;        must implement ordering relations).&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    Consider a graph::&#xa;&#xa;        +---+     +---+     +---+&#xa;        | 7 |\    | 5 |     | 3 |&#xa;        +---+ \   +---+     +---+&#xa;          |   _\___/ ____   _/ |&#xa;          |  /  \___/    \ /   |&#xa;          V  V           V V   |&#xa;         +----+         +---+  |&#xa;         | 11 |         | 8 |  |&#xa;         +----+         +---+  |&#xa;          | | \____   ___/ _   |&#xa;          | \      \ /    / \  |&#xa;          V  \     V V   /  V  V&#xa;        +---+ \   +---+ |  +----+&#xa;        | 2 |  |  | 9 | |  | 10 |&#xa;        +---+  |  +---+ |  +----+&#xa;               \________/&#xa;&#xa;    where vertices are integers. This graph can be encoded using&#xa;    elementary Python's data structures as follows::&#xa;&#xa;        >>> V = [2, 3, 5, 7, 8, 9, 10, 11]&#xa;        >>> E = [(7, 11), (7, 8), (5, 11), (3, 8), (3, 10),&#xa;        ...      (11, 2), (11, 9), (11, 10), (8, 9)]&#xa;&#xa;    To compute a topological sort for graph ``(V, E)`` issue::&#xa;&#xa;        >>> from sympy.utilities.iterables import topological_sort&#xa;&#xa;        >>> topological_sort((V, E))&#xa;        [3, 5, 7, 8, 11, 2, 9, 10]&#xa;&#xa;    If specific tie breaking approach is needed, use ``key`` parameter::&#xa;&#xa;        >>> topological_sort((V, E), key=lambda v: -v)&#xa;        [7, 5, 11, 3, 10, 8, 9, 2]&#xa;&#xa;    Only acyclic graphs can be sorted. If the input graph has a cycle,&#xa;    then :py:exc:`ValueError` will be raised::&#xa;&#xa;        >>> topological_sort((V, E + [(10, 7)]))&#xa;        Traceback (most recent call last):&#xa;        ...&#xa;        ValueError: cycle detected&#xa;&#xa;    .. seealso:: http://en.wikipedia.org/wiki/Topological_sorting&#xa;&#xa;    """"""&#xa;    V, E = graph&#xa;&#xa;    L = []&#xa;    S = set(V)&#xa;    E = list(E)&#xa;&#xa;    for v, u in E:&#xa;        S.discard(u)&#xa;&#xa;    if key is None:&#xa;        key = lambda value: value&#xa;&#xa;    S = sorted(S, key=key, reverse=True)&#xa;&#xa;    while S:&#xa;        node = S.pop()&#xa;        L.append(node)&#xa;&#xa;        for u, v in list(E):&#xa;            if u == node:&#xa;                E.remove((u, v))&#xa;&#xa;                for _u, _v in E:&#xa;                    if v == _v:&#xa;                        break&#xa;                else:&#xa;                    kv = key(v)&#xa;&#xa;                    for i, s in enumerate(S):&#xa;                        ks = key(s)&#xa;&#xa;                        if kv > ks:&#xa;                            S.insert(i, v)&#xa;                            break&#xa;                    else:&#xa;                        S.append(v)&#xa;&#xa;    if E:&#xa;        raise ValueError(""cycle detected"")&#xa;    else:&#xa;        return L&#xa;&#xa;&#xa;def rotate_left(x, y):&#xa;    """"""&#xa;    Left rotates a list x by the number of steps specified&#xa;    in y.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import rotate_left&#xa;    >>> a = [0, 1, 2]&#xa;    >>> rotate_left(a, 1)&#xa;    [1, 2, 0]&#xa;    """"""&#xa;    if len(x) == 0:&#xa;        return []&#xa;    y = y % len(x)&#xa;    return x[y:] + x[:y]&#xa;&#xa;&#xa;def rotate_right(x, y):&#xa;    """"""&#xa;    Right rotates a list x by the number of steps specified&#xa;    in y.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import rotate_right&#xa;    >>> a = [0, 1, 2]&#xa;    >>> rotate_right(a, 1)&#xa;    [2, 0, 1]&#xa;    """"""&#xa;    if len(x) == 0:&#xa;        return []&#xa;    y = len(x) - y % len(x)&#xa;    return x[y:] + x[:y]&#xa;&#xa;&#xa;def multiset_combinations(m, n, g=None):&#xa;    """"""&#xa;    Return the unique combinations of size ``n`` from multiset ``m``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import multiset_combinations&#xa;    >>> from itertools import combinations&#xa;    >>> [''.join(i) for i in  multiset_combinations('baby', 3)]&#xa;    ['abb', 'aby', 'bby']&#xa;&#xa;    >>> def count(f, s): return len(list(f(s, 3)))&#xa;&#xa;    The number of combinations depends on the number of letters; the&#xa;    number of unique combinations depends on how the letters are&#xa;    repeated.&#xa;&#xa;    >>> s1 = 'abracadabra'&#xa;    >>> s2 = 'banana tree'&#xa;    >>> count(combinations, s1), count(multiset_combinations, s1)&#xa;    (165, 23)&#xa;    >>> count(combinations, s2), count(multiset_combinations, s2)&#xa;    (165, 54)&#xa;&#xa;    """"""&#xa;    if g is None:&#xa;        if type(m) is dict:&#xa;            if n > sum(m.values()):&#xa;                return&#xa;            g = [[k, m[k]] for k in ordered(m)]&#xa;        else:&#xa;            m = list(m)&#xa;            if n > len(m):&#xa;                return&#xa;            try:&#xa;                m = multiset(m)&#xa;                g = [(k, m[k]) for k in ordered(m)]&#xa;            except TypeError:&#xa;                m = list(ordered(m))&#xa;                g = [list(i) for i in group(m, multiple=False)]&#xa;        del m&#xa;    if sum(v for k, v in g) < n or not n:&#xa;        yield []&#xa;    else:&#xa;        for i, (k, v) in enumerate(g):&#xa;            if v >= n:&#xa;                yield [k]*n&#xa;                v = n - 1&#xa;            for v in range(min(n, v), 0, -1):&#xa;                for j in multiset_combinations(None, n - v, g[i + 1:]):&#xa;                    rv = [k]*v + j&#xa;                    if len(rv) == n:&#xa;                        yield rv&#xa;&#xa;&#xa;def multiset_permutations(m, size=None, g=None):&#xa;    """"""&#xa;    Return the unique permutations of multiset ``m``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import multiset_permutations&#xa;    >>> from sympy import factorial&#xa;    >>> [''.join(i) for i in multiset_permutations('aab')]&#xa;    ['aab', 'aba', 'baa']&#xa;    >>> factorial(len('banana'))&#xa;    720&#xa;    >>> len(list(multiset_permutations('banana')))&#xa;    60&#xa;    """"""&#xa;    if g is None:&#xa;        if type(m) is dict:&#xa;            g = [[k, m[k]] for k in ordered(m)]&#xa;        else:&#xa;            m = list(ordered(m))&#xa;            g = [list(i) for i in group(m, multiple=False)]&#xa;        del m&#xa;    do = [gi for gi in g if gi[1] > 0]&#xa;    SUM = sum([gi[1] for gi in do])&#xa;    if not do or size is not None and (size > SUM or size < 1):&#xa;        if size < 1:&#xa;            yield []&#xa;        return&#xa;    elif size == 1:&#xa;        for k, v in do:&#xa;            yield [k]&#xa;    elif len(do) == 1:&#xa;        k, v = do[0]&#xa;        v = v if size is None else (size if size <= v else 0)&#xa;        yield [k for i in range(v)]&#xa;    elif all(v == 1 for k, v in do):&#xa;        for p in permutations([k for k, v in do], size):&#xa;            yield list(p)&#xa;    else:&#xa;        size = size if size is not None else SUM&#xa;        for i, (k, v) in enumerate(do):&#xa;            do[i][1] -= 1&#xa;            for j in multiset_permutations(None, size - 1, do):&#xa;                if j:&#xa;                    yield [k] + j&#xa;            do[i][1] += 1&#xa;&#xa;&#xa;def _partition(seq, vector, m=None):&#xa;    """"""&#xa;    Return the partion of seq as specified by the partition vector.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import _partition&#xa;    >>> _partition('abcde', [1, 0, 1, 2, 0])&#xa;    [['b', 'e'], ['a', 'c'], ['d']]&#xa;&#xa;    Specifying the number of bins in the partition is optional:&#xa;&#xa;    >>> _partition('abcde', [1, 0, 1, 2, 0], 3)&#xa;    [['b', 'e'], ['a', 'c'], ['d']]&#xa;&#xa;    The output of _set_partitions can be passed as follows:&#xa;&#xa;    >>> output = (3, [1, 0, 1, 2, 0])&#xa;    >>> _partition('abcde', *output)&#xa;    [['b', 'e'], ['a', 'c'], ['d']]&#xa;&#xa;    See Also&#xa;    ========&#xa;    combinatorics.partitions.Partition.from_rgs()&#xa;&#xa;    """"""&#xa;    if m is None:&#xa;        m = max(vector) + 1&#xa;    elif type(vector) is int:  # entered as m, vector&#xa;        vector, m = m, vector&#xa;    p = [[] for i in range(m)]&#xa;    for i, v in enumerate(vector):&#xa;        p[v].append(seq[i])&#xa;    return p&#xa;&#xa;&#xa;def _set_partitions(n):&#xa;    """"""Cycle through all partions of n elements, yielding the&#xa;    current number of partitions, ``m``, and a mutable list, ``q``&#xa;    such that element[i] is in part q[i] of the partition.&#xa;&#xa;    NOTE: ``q`` is modified in place and generally should not be changed&#xa;    between function calls.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import _set_partitions, _partition&#xa;    >>> for m, q in _set_partitions(3):&#xa;    ...     print('%s %s %s' % (m, q, _partition('abc', q, m)))&#xa;    1 [0, 0, 0] [['a', 'b', 'c']]&#xa;    2 [0, 0, 1] [['a', 'b'], ['c']]&#xa;    2 [0, 1, 0] [['a', 'c'], ['b']]&#xa;    2 [0, 1, 1] [['a'], ['b', 'c']]&#xa;    3 [0, 1, 2] [['a'], ['b'], ['c']]&#xa;&#xa;    Notes&#xa;    =====&#xa;&#xa;    This algorithm is similar to, and solves the same problem as,&#xa;    Algorithm 7.2.1.5H, from volume 4A of Knuth's The Art of Computer&#xa;    Programming.  Knuth uses the term ""restricted growth string"" where&#xa;    this code refers to a ""partition vector"". In each case, the meaning is&#xa;    the same: the value in the ith element of the vector specifies to&#xa;    which part the ith set element is to be assigned.&#xa;&#xa;    At the lowest level, this code implements an n-digit big-endian&#xa;    counter (stored in the array q) which is incremented (with carries) to&#xa;    get the next partition in the sequence.  A special twist is that a&#xa;    digit is constrained to be at most one greater than the maximum of all&#xa;    the digits to the left of it.  The array p maintains this maximum, so&#xa;    that the code can efficiently decide when a digit can be incremented&#xa;    in place or whether it needs to be reset to 0 and trigger a carry to&#xa;    the next digit.  The enumeration starts with all the digits 0 (which&#xa;    corresponds to all the set elements being assigned to the same 0th&#xa;    part), and ends with 0123...n, which corresponds to each set element&#xa;    being assigned to a different, singleton, part.&#xa;&#xa;    This routine was rewritten to use 0-based lists while trying to&#xa;    preserve the beauty and efficiency of the original algorithm.&#xa;&#xa;    Reference&#xa;    =========&#xa;&#xa;    Nijenhuis, Albert and Wilf, Herbert. (1978) Combinatorial Algorithms,&#xa;    2nd Ed, p 91, algorithm ""nexequ"". Available online from&#xa;    http://www.math.upenn.edu/~wilf/website/CombAlgDownld.html (viewed&#xa;    November 17, 2012).&#xa;&#xa;    """"""&#xa;    p = [0]*n&#xa;    q = [0]*n&#xa;    nc = 1&#xa;    yield nc, q&#xa;    while nc != n:&#xa;        m = n&#xa;        while 1:&#xa;            m -= 1&#xa;            i = q[m]&#xa;            if p[i] != 1:&#xa;                break&#xa;            q[m] = 0&#xa;        i += 1&#xa;        q[m] = i&#xa;        m += 1&#xa;        nc += m - n&#xa;        p[0] += n - m&#xa;        if i == nc:&#xa;            p[nc] = 0&#xa;            nc += 1&#xa;        p[i - 1] -= 1&#xa;        p[i] += 1&#xa;        yield nc, q&#xa;&#xa;&#xa;def multiset_partitions(multiset, m=None):&#xa;    """"""&#xa;    Return unique partitions of the given multiset (in list form).&#xa;    If ``m`` is None, all multisets will be returned, otherwise only&#xa;    partitions with ``m`` parts will be returned.&#xa;&#xa;    If ``multiset`` is an integer, a range [0, 1, ..., multiset - 1]&#xa;    will be supplied.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import multiset_partitions&#xa;    >>> list(multiset_partitions([1, 2, 3, 4], 2))&#xa;    [[[1, 2, 3], [4]], [[1, 2, 4], [3]], [[1, 2], [3, 4]],&#xa;    [[1, 3, 4], [2]], [[1, 3], [2, 4]], [[1, 4], [2, 3]],&#xa;    [[1], [2, 3, 4]]]&#xa;    >>> list(multiset_partitions([1, 2, 3, 4], 1))&#xa;    [[[1, 2, 3, 4]]]&#xa;&#xa;    Only unique partitions are returned and these will be returned in a&#xa;    canonical order regardless of the order of the input:&#xa;&#xa;    >>> a = [1, 2, 2, 1]&#xa;    >>> ans = list(multiset_partitions(a, 2))&#xa;    >>> a.sort()&#xa;    >>> list(multiset_partitions(a, 2)) == ans&#xa;    True&#xa;    >>> a = range(3, 1, -1)&#xa;    >>> (list(multiset_partitions(a)) ==&#xa;    ...  list(multiset_partitions(sorted(a))))&#xa;    True&#xa;&#xa;    If m is omitted then all partitions will be returned:&#xa;&#xa;    >>> list(multiset_partitions([1, 1, 2]))&#xa;    [[[1, 1, 2]], [[1, 1], [2]], [[1, 2], [1]], [[1], [1], [2]]]&#xa;    >>> list(multiset_partitions([1]*3))&#xa;    [[[1, 1, 1]], [[1], [1, 1]], [[1], [1], [1]]]&#xa;&#xa;    Counting&#xa;    ========&#xa;&#xa;    The number of partitions of a set is given by the bell number:&#xa;&#xa;    >>> from sympy import bell&#xa;    >>> len(list(multiset_partitions(5))) == bell(5) == 52&#xa;    True&#xa;&#xa;    The number of partitions of length k from a set of size n is given by the&#xa;    Stirling Number of the 2nd kind:&#xa;&#xa;    >>> def S2(n, k):&#xa;    ...     from sympy import Dummy, binomial, factorial, Sum&#xa;    ...     if k > n:&#xa;    ...         return 0&#xa;    ...     j = Dummy()&#xa;    ...     arg = (-1)**(k-j)*j**n*binomial(k,j)&#xa;    ...     return 1/factorial(k)*Sum(arg,(j,0,k)).doit()&#xa;    ...&#xa;    >>> S2(5, 2) == len(list(multiset_partitions(5, 2))) == 15&#xa;    True&#xa;&#xa;    These comments on counting apply to *sets*, not multisets.&#xa;&#xa;    Notes&#xa;    =====&#xa;&#xa;    When all the elements are the same in the multiset, the order&#xa;    of the returned partitions is determined by the ``partitions``&#xa;    routine. If one is counting partitions then it is better to use&#xa;    the ``nT`` function.&#xa;&#xa;    See Also&#xa;    ========&#xa;    partitions&#xa;    sympy.combinatorics.partitions.Partition&#xa;    sympy.combinatorics.partitions.IntegerPartition&#xa;    sympy.functions.combinatorial.numbers.nT&#xa;    """"""&#xa;&#xa;    # This function looks at the supplied input and dispatches to&#xa;    # several special-case routines as they apply.&#xa;    if type(multiset) is int:&#xa;        n = multiset&#xa;        if m and m > n:&#xa;            return&#xa;        multiset = list(range(n))&#xa;        if m == 1:&#xa;            yield [multiset[:]]&#xa;            return&#xa;&#xa;        # If m is not None, it can sometimes be faster to use&#xa;        # MultisetPartitionTraverser.enum_range() even for inputs&#xa;        # which are sets.  Since the _set_partitions code is quite&#xa;        # fast, this is only advantageous when the overall set&#xa;        # partitions outnumber those with the desired number of parts&#xa;        # by a large factor.  (At least 60.)  Such a switch is not&#xa;        # currently implemented.&#xa;        for nc, q in _set_partitions(n):&#xa;            if m is None or nc == m:&#xa;                rv = [[] for i in range(nc)]&#xa;                for i in range(n):&#xa;                    rv[q[i]].append(multiset[i])&#xa;                yield rv&#xa;        return&#xa;&#xa;    if len(multiset) == 1 and type(multiset) is str:&#xa;        multiset = [multiset]&#xa;&#xa;    if not has_variety(multiset):&#xa;        # Only one component, repeated n times.  The resulting&#xa;        # partitions correspond to partitions of integer n.&#xa;        n = len(multiset)&#xa;        if m and m > n:&#xa;            return&#xa;        if m == 1:&#xa;            yield [multiset[:]]&#xa;            return&#xa;        x = multiset[:1]&#xa;        for size, p in partitions(n, m, size=True):&#xa;            if m is None or size == m:&#xa;                rv = []&#xa;                for k in sorted(p):&#xa;                    rv.extend([x*k]*p[k])&#xa;                yield rv&#xa;    else:&#xa;        multiset = list(ordered(multiset))&#xa;        n = len(multiset)&#xa;        if m and m > n:&#xa;            return&#xa;        if m == 1:&#xa;            yield [multiset[:]]&#xa;            return&#xa;&#xa;        # Split the information of the multiset into two lists -&#xa;        # one of the elements themselves, and one (of the same length)&#xa;        # giving the number of repeats for the corresponding element.&#xa;        elements, multiplicities = zip(*group(multiset, False))&#xa;&#xa;        if len(elements) < len(multiset):&#xa;            # General case - multiset with more than one distinct element&#xa;            # and at least one element repeated more than once.&#xa;            if m:&#xa;                mpt = MultisetPartitionTraverser()&#xa;                for state in mpt.enum_range(multiplicities, m-1, m):&#xa;                    yield list_visitor(state, elements)&#xa;            else:&#xa;                for state in multiset_partitions_taocp(multiplicities):&#xa;                    yield list_visitor(state, elements)&#xa;        else:&#xa;            # Set partitions case - no repeated elements. Pretty much&#xa;            # same as int argument case above, with same possible, but&#xa;            # currently unimplemented optimization for some cases when&#xa;            # m is not None&#xa;            for nc, q in _set_partitions(n):&#xa;                if m is None or nc == m:&#xa;                    rv = [[] for i in range(nc)]&#xa;                    for i in range(n):&#xa;                        rv[q[i]].append(i)&#xa;                    yield [[multiset[j] for j in i] for i in rv]&#xa;&#xa;&#xa;def partitions(n, m=None, k=None, size=False):&#xa;    """"""Generate all partitions of integer n (>= 0).&#xa;&#xa;    Parameters&#xa;    ==========&#xa;&#xa;    ``m`` : integer (default gives partitions of all sizes)&#xa;        limits number of parts in parition (mnemonic: m, maximum parts)&#xa;    ``k`` : integer (default gives partitions number from 1 through n)&#xa;        limits the numbers that are kept in the partition (mnemonic: k, keys)&#xa;    ``size`` : bool (default False, only partition is returned)&#xa;        when ``True`` then (M, P) is returned where M is the sum of the&#xa;        multiplicities and P is the generated partition.&#xa;&#xa;    Each partition is represented as a dictionary, mapping an integer&#xa;    to the number of copies of that integer in the partition.  For example,&#xa;    the first partition of 4 returned is {4: 1}, ""4: one of them"".&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import partitions&#xa;&#xa;    The numbers appearing in the partition (the key of the returned dict)&#xa;    are limited with k:&#xa;&#xa;    >>> for p in partitions(6, k=2):  # doctest: +SKIP&#xa;    ...     print(p)&#xa;    {2: 3}&#xa;    {1: 2, 2: 2}&#xa;    {1: 4, 2: 1}&#xa;    {1: 6}&#xa;&#xa;    The maximum number of parts in the partion (the sum of the values in&#xa;    the returned dict) are limited with m:&#xa;&#xa;    >>> for p in partitions(6, m=2):  # doctest: +SKIP&#xa;    ...     print(p)&#xa;    ...&#xa;    {6: 1}&#xa;    {1: 1, 5: 1}&#xa;    {2: 1, 4: 1}&#xa;    {3: 2}&#xa;&#xa;    Note that the _same_ dictionary object is returned each time.&#xa;    This is for speed:  generating each partition goes quickly,&#xa;    taking constant time, independent of n.&#xa;&#xa;    >>> [p for p in partitions(6, k=2)]&#xa;    [{1: 6}, {1: 6}, {1: 6}, {1: 6}]&#xa;&#xa;    If you want to build a list of the returned dictionaries then&#xa;    make a copy of them:&#xa;&#xa;    >>> [p.copy() for p in partitions(6, k=2)]  # doctest: +SKIP&#xa;    [{2: 3}, {1: 2, 2: 2}, {1: 4, 2: 1}, {1: 6}]&#xa;    >>> [(M, p.copy()) for M, p in partitions(6, k=2, size=True)]  # doctest: +SKIP&#xa;    [(3, {2: 3}), (4, {1: 2, 2: 2}), (5, {1: 4, 2: 1}), (6, {1: 6})]&#xa;&#xa;    Reference:&#xa;        modified from Tim Peter's version to allow for k and m values:&#xa;        code.activestate.com/recipes/218332-generator-for-integer-partitions/&#xa;&#xa;    See Also&#xa;    ========&#xa;    sympy.combinatorics.partitions.Partition&#xa;    sympy.combinatorics.partitions.IntegerPartition&#xa;&#xa;    """"""&#xa;    if n < 0:&#xa;        raise ValueError(""n must be >= 0"")&#xa;    if m == 0:&#xa;        raise ValueError(""m must be > 0"")&#xa;    m = min(m or n, n)&#xa;    if m < 1:&#xa;        raise ValueError(""maximum numbers in partition, m, must be > 0"")&#xa;    k = min(k or n, n)&#xa;    if k < 1:&#xa;        raise ValueError(""maximum value in partition, k, must be > 0"")&#xa;&#xa;    if m*k < n:&#xa;        return&#xa;&#xa;    n, m, k = as_int(n), as_int(m), as_int(k)&#xa;    q, r = divmod(n, k)&#xa;    ms = {k: q}&#xa;    keys = [k]  # ms.keys(), from largest to smallest&#xa;    if r:&#xa;        ms[r] = 1&#xa;        keys.append(r)&#xa;    room = m - q - bool(r)&#xa;    if size:&#xa;        yield sum(ms.values()), ms&#xa;    else:&#xa;        yield ms&#xa;&#xa;    while keys != [1]:&#xa;        # Reuse any 1's.&#xa;        if keys[-1] == 1:&#xa;            del keys[-1]&#xa;            reuse = ms.pop(1)&#xa;            room += reuse&#xa;        else:&#xa;            reuse = 0&#xa;&#xa;        while 1:&#xa;            # Let i be the smallest key larger than 1.  Reuse one&#xa;            # instance of i.&#xa;            i = keys[-1]&#xa;            newcount = ms[i] = ms[i] - 1&#xa;            reuse += i&#xa;            if newcount == 0:&#xa;                del keys[-1], ms[i]&#xa;            room += 1&#xa;&#xa;            # Break the remainder into pieces of size i-1.&#xa;            i -= 1&#xa;            q, r = divmod(reuse, i)&#xa;            need = q + bool(r)&#xa;            if need > room:&#xa;                if not keys:&#xa;                    return&#xa;                continue&#xa;&#xa;            ms[i] = q&#xa;            keys.append(i)&#xa;            if r:&#xa;                ms[r] = 1&#xa;                keys.append(r)&#xa;            break&#xa;        room -= need&#xa;        if size:&#xa;            yield sum(ms.values()), ms&#xa;        else:&#xa;            yield ms&#xa;&#xa;&#xa;def binary_partitions(n):&#xa;    """"""&#xa;    Generates the binary partition of n.&#xa;&#xa;    A binary partition consists only of numbers that are&#xa;    powers of two. Each step reduces a 2**(k+1) to 2**k and&#xa;    2**k. Thus 16 is converted to 8 and 8.&#xa;&#xa;    Reference: TAOCP 4, section 7.2.1.5, problem 64&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import binary_partitions&#xa;    >>> for i in binary_partitions(5):&#xa;    ...     print(i)&#xa;    ...&#xa;    [4, 1]&#xa;    [2, 2, 1]&#xa;    [2, 1, 1, 1]&#xa;    [1, 1, 1, 1, 1]&#xa;    """"""&#xa;    from math import ceil, log&#xa;    pow = int(2**(ceil(log(n, 2))))&#xa;    sum = 0&#xa;    partition = []&#xa;    while pow:&#xa;        if sum + pow <= n:&#xa;            partition.append(pow)&#xa;            sum += pow&#xa;        pow >>= 1&#xa;&#xa;    last_num = len(partition) - 1 - (n & 1)&#xa;    while last_num >= 0:&#xa;        yield partition&#xa;        if partition[last_num] == 2:&#xa;            partition[last_num] = 1&#xa;            partition.append(1)&#xa;            last_num -= 1&#xa;            continue&#xa;        partition.append(1)&#xa;        partition[last_num] >>= 1&#xa;        x = partition[last_num + 1] = partition[last_num]&#xa;        last_num += 1&#xa;        while x > 1:&#xa;            if x <= len(partition) - last_num - 1:&#xa;                del partition[-x + 1:]&#xa;                last_num += 1&#xa;                partition[last_num] = x&#xa;            else:&#xa;                x >>= 1&#xa;    yield [1]*n&#xa;&#xa;&#xa;def has_dups(seq):&#xa;    """"""Return True if there are any duplicate elements in ``seq``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import has_dups&#xa;    >>> from sympy import Dict, Set&#xa;&#xa;    >>> has_dups((1, 2, 1))&#xa;    True&#xa;    >>> has_dups(range(3))&#xa;    False&#xa;    >>> all(has_dups(c) is False for c in (set(), Set(), dict(), Dict()))&#xa;    True&#xa;    """"""&#xa;    from sympy.core.containers import Dict&#xa;    from sympy.sets.sets import Set&#xa;    if isinstance(seq, (dict, set, Dict, Set)):&#xa;        return False&#xa;    uniq = set()&#xa;    return any(True for s in seq if s in uniq or uniq.add(s))&#xa;&#xa;&#xa;def has_variety(seq):&#xa;    """"""Return True if there are any different elements in ``seq``.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import has_variety&#xa;&#xa;    >>> has_variety((1, 2, 1))&#xa;    True&#xa;    >>> has_variety((1, 1, 1))&#xa;    False&#xa;    """"""&#xa;    for i, s in enumerate(seq):&#xa;        if i == 0:&#xa;            sentinel = s&#xa;        else:&#xa;            if s != sentinel:&#xa;                return True&#xa;    return False&#xa;&#xa;&#xa;def uniq(seq, result=None):&#xa;    """"""&#xa;    Yield unique elements from ``seq`` as an iterator. The second&#xa;    parameter ``result``  is used internally; it is not necessary to pass&#xa;    anything for this.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import uniq&#xa;    >>> dat = [1, 4, 1, 5, 4, 2, 1, 2]&#xa;    >>> type(uniq(dat)) in (list, tuple)&#xa;    False&#xa;&#xa;    >>> list(uniq(dat))&#xa;    [1, 4, 5, 2]&#xa;    >>> list(uniq(x for x in dat))&#xa;    [1, 4, 5, 2]&#xa;    >>> list(uniq([[1], [2, 1], [1]]))&#xa;    [[1], [2, 1]]&#xa;    """"""&#xa;    try:&#xa;        seen = set()&#xa;        result = result or []&#xa;        for i, s in enumerate(seq):&#xa;            if not (s in seen or seen.add(s)):&#xa;                yield s&#xa;    except TypeError:&#xa;        if s not in result:&#xa;            yield s&#xa;            result.append(s)&#xa;        if hasattr(seq, '__getitem__'):&#xa;            for s in uniq(seq[i + 1:], result):&#xa;                yield s&#xa;        else:&#xa;            for s in uniq(seq, result):&#xa;                yield s&#xa;&#xa;&#xa;def generate_bell(n):&#xa;    """"""Return permutations of [0, 1, ..., n - 1] such that each permutation&#xa;    differs from the last by the exchange of a single pair of neighbors.&#xa;    The ``n!`` permutations are returned as an iterator. In order to obtain&#xa;    the next permutation from a random starting permutation, use the&#xa;    ``next_trotterjohnson`` method of the Permutation class (which generates&#xa;    the same sequence in a different manner).&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from itertools import permutations&#xa;    >>> from sympy.utilities.iterables import generate_bell&#xa;    >>> from sympy import zeros, Matrix&#xa;&#xa;    This is the sort of permutation used in the ringing of physical bells,&#xa;    and does not produce permutations in lexicographical order. Rather, the&#xa;    permutations differ from each other by exactly one inversion, and the&#xa;    position at which the swapping occurs varies periodically in a simple&#xa;    fashion. Consider the first few permutations of 4 elements generated&#xa;    by ``permutations`` and ``generate_bell``:&#xa;&#xa;    >>> list(permutations(range(4)))[:5]&#xa;    [(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 1, 3), (0, 2, 3, 1), (0, 3, 1, 2)]&#xa;    >>> list(generate_bell(4))[:5]&#xa;    [(0, 1, 2, 3), (0, 1, 3, 2), (0, 3, 1, 2), (3, 0, 1, 2), (3, 0, 2, 1)]&#xa;&#xa;    Notice how the 2nd and 3rd lexicographical permutations have 3 elements&#xa;    out of place whereas each ""bell"" permutation always has only two&#xa;    elements out of place relative to the previous permutation (and so the&#xa;    signature (+/-1) of a permutation is opposite of the signature of the&#xa;    previous permutation).&#xa;&#xa;    How the position of inversion varies across the elements can be seen&#xa;    by tracing out where the largest number appears in the permutations:&#xa;&#xa;    >>> m = zeros(4, 24)&#xa;    >>> for i, p in enumerate(generate_bell(4)):&#xa;    ...     m[:, i] = Matrix([j - 3 for j in list(p)])  # make largest zero&#xa;    >>> m.print_nonzero('X')&#xa;    [XXX  XXXXXX  XXXXXX  XXX]&#xa;    [XX XX XXXX XX XXXX XX XX]&#xa;    [X XXXX XX XXXX XX XXXX X]&#xa;    [ XXXXXX  XXXXXX  XXXXXX ]&#xa;&#xa;    See Also&#xa;    ========&#xa;    sympy.combinatorics.Permutation.next_trotterjohnson&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    * http://en.wikipedia.org/wiki/Method_ringing&#xa;    * http://stackoverflow.com/questions/4856615/recursive-permutation/4857018&#xa;    * http://programminggeeks.com/bell-algorithm-for-permutation/&#xa;    * http://en.wikipedia.org/wiki/Steinhaus%E2%80%93Johnson%E2%80%93Trotter_algorithm&#xa;    * Generating involutions, derangements, and relatives by ECO&#xa;      Vincent Vajnovszki, DMTCS vol 1 issue 12, 2010&#xa;&#xa;    """"""&#xa;    n = as_int(n)&#xa;    if n < 1:&#xa;        raise ValueError('n must be a positive integer')&#xa;    if n == 1:&#xa;        yield (0,)&#xa;    elif n == 2:&#xa;        yield (0, 1)&#xa;        yield (1, 0)&#xa;    elif n == 3:&#xa;        for li in [(0, 1, 2), (0, 2, 1), (2, 0, 1), (2, 1, 0), (1, 2, 0), (1, 0, 2)]:&#xa;            yield li&#xa;    else:&#xa;        m = n - 1&#xa;        op = [0] + [-1]*m&#xa;        l = list(range(n))&#xa;        while True:&#xa;            yield tuple(l)&#xa;            # find biggest element with op&#xa;            big = None, -1  # idx, value&#xa;            for i in range(n):&#xa;                if op[i] and l[i] > big[1]:&#xa;                    big = i, l[i]&#xa;            i, _ = big&#xa;            if i is None:&#xa;                break  # there are no ops left&#xa;            # swap it with neighbor in the indicated direction&#xa;            j = i + op[i]&#xa;            l[i], l[j] = l[j], l[i]&#xa;            op[i], op[j] = op[j], op[i]&#xa;            # if it landed at the end or if the neighbor in the same&#xa;            # direction is bigger then turn off op&#xa;            if j == 0 or j == m or l[j + op[j]] > l[j]:&#xa;                op[j] = 0&#xa;            # any element bigger to the left gets +1 op&#xa;            for i in range(j):&#xa;                if l[i] > l[j]:&#xa;                    op[i] = 1&#xa;            # any element bigger to the right gets -1 op&#xa;            for i in range(j + 1, n):&#xa;                if l[i] > l[j]:&#xa;                    op[i] = -1&#xa;&#xa;&#xa;def generate_involutions(n):&#xa;    """"""&#xa;    Generates involutions.&#xa;&#xa;    An involution is a permutation that when multiplied&#xa;    by itself equals the identity permutation. In this&#xa;    implementation the involutions are generated using&#xa;    Fixed Points.&#xa;&#xa;    Alternatively, an involution can be considered as&#xa;    a permutation that does not contain any cycles with&#xa;    a length that is greater than two.&#xa;&#xa;    Reference:&#xa;    http://mathworld.wolfram.com/PermutationInvolution.html&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import generate_involutions&#xa;    >>> list(generate_involutions(3))&#xa;    [(0, 1, 2), (0, 2, 1), (1, 0, 2), (2, 1, 0)]&#xa;    >>> len(list(generate_involutions(4)))&#xa;    10&#xa;    """"""&#xa;    idx = list(range(n))&#xa;    for p in permutations(idx):&#xa;        for i in idx:&#xa;            if p[p[i]] != i:&#xa;                break&#xa;        else:&#xa;            yield p&#xa;&#xa;&#xa;def generate_derangements(perm):&#xa;    """"""&#xa;    Routine to generate unique derangements.&#xa;&#xa;    TODO: This will be rewritten to use the&#xa;    ECO operator approach once the permutations&#xa;    branch is in master.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import generate_derangements&#xa;    >>> list(generate_derangements([0, 1, 2]))&#xa;    [[1, 2, 0], [2, 0, 1]]&#xa;    >>> list(generate_derangements([0, 1, 2, 3]))&#xa;    [[1, 0, 3, 2], [1, 2, 3, 0], [1, 3, 0, 2], [2, 0, 3, 1], \&#xa;    [2, 3, 0, 1], [2, 3, 1, 0], [3, 0, 1, 2], [3, 2, 0, 1], \&#xa;    [3, 2, 1, 0]]&#xa;    >>> list(generate_derangements([0, 1, 1]))&#xa;    []&#xa;&#xa;    See Also&#xa;    ========&#xa;    sympy.functions.combinatorial.factorials.subfactorial&#xa;    """"""&#xa;    p = multiset_permutations(perm)&#xa;    indices = range(len(perm))&#xa;    p0 = next(p)&#xa;    for pi in p:&#xa;        if all(pi[i] != p0[i] for i in indices):&#xa;            yield pi&#xa;&#xa;&#xa;@deprecated(&#xa;    useinstead=""bracelets"", deprecated_since_version=""0.7.3"")&#xa;def unrestricted_necklace(n, k):&#xa;    """"""Wrapper to necklaces to return a free (unrestricted) necklace.""""""&#xa;    return necklaces(n, k, free=True)&#xa;&#xa;&#xa;def necklaces(n, k, free=False):&#xa;    """"""&#xa;    A routine to generate necklaces that may (free=True) or may not&#xa;    (free=False) be turned over to be viewed. The ""necklaces"" returned&#xa;    are comprised of ``n`` integers (beads) with ``k`` different&#xa;    values (colors). Only unique necklaces are returned.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import necklaces, bracelets&#xa;    >>> def show(s, i):&#xa;    ...     return ''.join(s[j] for j in i)&#xa;&#xa;    The ""unrestricted necklace"" is sometimes also referred to as a&#xa;    ""bracelet"" (an object that can be turned over, a sequence that can&#xa;    be reversed) and the term ""necklace"" is used to imply a sequence&#xa;    that cannot be reversed. So ACB == ABC for a bracelet (rotate and&#xa;    reverse) while the two are different for a necklace since rotation&#xa;    alone cannot make the two sequences the same.&#xa;&#xa;    (mnemonic: Bracelets can be viewed Backwards, but Not Necklaces.)&#xa;&#xa;    >>> B = [show('ABC', i) for i in bracelets(3, 3)]&#xa;    >>> N = [show('ABC', i) for i in necklaces(3, 3)]&#xa;    >>> set(N) - set(B)&#xa;    set(['ACB'])&#xa;&#xa;    >>> list(necklaces(4, 2))&#xa;    [(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 1, 1),&#xa;     (0, 1, 0, 1), (0, 1, 1, 1), (1, 1, 1, 1)]&#xa;&#xa;    >>> [show('.o', i) for i in bracelets(4, 2)]&#xa;    ['....', '...o', '..oo', '.o.o', '.ooo', 'oooo']&#xa;&#xa;    References&#xa;    ==========&#xa;&#xa;    http://mathworld.wolfram.com/Necklace.html&#xa;&#xa;    """"""&#xa;    return uniq(minlex(i, directed=not free) for i in&#xa;        variations(list(range(k)), n, repetition=True))&#xa;&#xa;&#xa;def bracelets(n, k):&#xa;    """"""Wrapper to necklaces to return a free (unrestricted) necklace.""""""&#xa;    return necklaces(n, k, free=True)&#xa;&#xa;&#xa;def generate_oriented_forest(n):&#xa;    """"""&#xa;    This algorithm generates oriented forests.&#xa;&#xa;    An oriented graph is a directed graph having no symmetric pair of directed&#xa;    edges. A forest is an acyclic graph, i.e., it has no cycles. A forest can&#xa;    also be described as a disjoint union of trees, which are graphs in which&#xa;    any two vertices are connected by exactly one simple path.&#xa;&#xa;    Reference:&#xa;    [1] T. Beyer and S.M. Hedetniemi: constant time generation of \&#xa;        rooted trees, SIAM J. Computing Vol. 9, No. 4, November 1980&#xa;    [2] http://stackoverflow.com/questions/1633833/oriented-forest-taocp-algorithm-in-python&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import generate_oriented_forest&#xa;    >>> list(generate_oriented_forest(4))&#xa;    [[0, 1, 2, 3], [0, 1, 2, 2], [0, 1, 2, 1], [0, 1, 2, 0], \&#xa;    [0, 1, 1, 1], [0, 1, 1, 0], [0, 1, 0, 1], [0, 1, 0, 0], [0, 0, 0, 0]]&#xa;    """"""&#xa;    P = list(range(-1, n))&#xa;    while True:&#xa;        yield P[1:]&#xa;        if P[n] > 0:&#xa;            P[n] = P[P[n]]&#xa;        else:&#xa;            for p in range(n - 1, 0, -1):&#xa;                if P[p] != 0:&#xa;                    target = P[p] - 1&#xa;                    for q in range(p - 1, 0, -1):&#xa;                        if P[q] == target:&#xa;                            break&#xa;                    offset = p - q&#xa;                    for i in range(p, n + 1):&#xa;                        P[i] = P[i - offset]&#xa;                    break&#xa;            else:&#xa;                break&#xa;&#xa;&#xa;def minlex(seq, directed=True, is_set=False, small=None):&#xa;    """"""&#xa;    Return a tuple where the smallest element appears first; if&#xa;    ``directed`` is True (default) then the order is preserved, otherwise&#xa;    the sequence will be reversed if that gives a smaller ordering.&#xa;&#xa;    If every element appears only once then is_set can be set to True&#xa;    for more efficient processing.&#xa;&#xa;    If the smallest element is known at the time of calling, it can be&#xa;    passed and the calculation of the smallest element will be omitted.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.combinatorics.polyhedron import minlex&#xa;    >>> minlex((1, 2, 0))&#xa;    (0, 1, 2)&#xa;    >>> minlex((1, 0, 2))&#xa;    (0, 2, 1)&#xa;    >>> minlex((1, 0, 2), directed=False)&#xa;    (0, 1, 2)&#xa;&#xa;    >>> minlex('11010011000', directed=True)&#xa;    '00011010011'&#xa;    >>> minlex('11010011000', directed=False)&#xa;    '00011001011'&#xa;&#xa;    """"""&#xa;    is_str = isinstance(seq, str)&#xa;    seq = list(seq)&#xa;    if small is None:&#xa;        small = min(seq, key=default_sort_key)&#xa;    if is_set:&#xa;        i = seq.index(small)&#xa;        if not directed:&#xa;            n = len(seq)&#xa;            p = (i + 1) % n&#xa;            m = (i - 1) % n&#xa;            if default_sort_key(seq[p]) > default_sort_key(seq[m]):&#xa;                seq = list(reversed(seq))&#xa;                i = n - i - 1&#xa;        if i:&#xa;            seq = rotate_left(seq, i)&#xa;        best = seq&#xa;    else:&#xa;        count = seq.count(small)&#xa;        if count == 1 and directed:&#xa;            best = rotate_left(seq, seq.index(small))&#xa;        else:&#xa;            # if not directed, and not a set, we can't just&#xa;            # pass this off to minlex with is_set True since&#xa;            # peeking at the neighbor may not be sufficient to&#xa;            # make the decision so we continue...&#xa;            best = seq&#xa;            for i in range(count):&#xa;                seq = rotate_left(seq, seq.index(small, count != 1))&#xa;                if seq < best:&#xa;                    best = seq&#xa;                # it's cheaper to rotate now rather than search&#xa;                # again for these in reversed order so we test&#xa;                # the reverse now&#xa;                if not directed:&#xa;                    seq = rotate_left(seq, 1)&#xa;                    seq = list(reversed(seq))&#xa;                    if seq < best:&#xa;                        best = seq&#xa;                    seq = list(reversed(seq))&#xa;                    seq = rotate_right(seq, 1)&#xa;    # common return&#xa;    if is_str:&#xa;        return ''.join(best)&#xa;    return tuple(best)&#xa;&#xa;&#xa;def runs(seq, op=gt):&#xa;    """"""Group the sequence into lists in which successive elements&#xa;    all compare the same with the comparison operator, ``op``:&#xa;    op(seq[i + 1], seq[i]) is True from all elements in a run.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import runs&#xa;    >>> from operator import ge&#xa;    >>> runs([0, 1, 2, 2, 1, 4, 3, 2, 2])&#xa;    [[0, 1, 2], [2], [1, 4], [3], [2], [2]]&#xa;    >>> runs([0, 1, 2, 2, 1, 4, 3, 2, 2], op=ge)&#xa;    [[0, 1, 2, 2], [1, 4], [3], [2, 2]]&#xa;    """"""&#xa;    cycles = []&#xa;    seq = iter(seq)&#xa;    try:&#xa;        run = [next(seq)]&#xa;    except StopIteration:&#xa;        return []&#xa;    while True:&#xa;        try:&#xa;            ei = next(seq)&#xa;        except StopIteration:&#xa;            break&#xa;        if op(ei, run[-1]):&#xa;            run.append(ei)&#xa;            continue&#xa;        else:&#xa;            cycles.append(run)&#xa;            run = [ei]&#xa;    if run:&#xa;        cycles.append(run)&#xa;    return cycles&#xa;&#xa;&#xa;def kbins(l, k, ordered=None):&#xa;    """"""&#xa;    Return sequence ``l`` partitioned into ``k`` bins.&#xa;&#xa;    Examples&#xa;    ========&#xa;&#xa;    >>> from sympy.utilities.iterables import kbins&#xa;&#xa;    The default is to give the items in the same order, but grouped&#xa;    into k partitions without any reordering:&#xa;&#xa;    >>> from __future__ import print_function&#xa;    >>> for p in kbins(list(range(5)), 2):&#xa;    ...     print(p)&#xa;    ...&#xa;    [[0], [1, 2, 3, 4]]&#xa;    [[0, 1], [2, 3, 4]]&#xa;    [[0, 1, 2], [3, 4]]&#xa;    [[0, 1, 2, 3], [4]]&#xa;&#xa;    The ``ordered`` flag which is either None (to give the simple partition&#xa;    of the the elements) or is a 2 digit integer indicating whether the order of&#xa;    the bins and the order of the items in the bins matters. Given::&#xa;&#xa;        A = [[0], [1, 2]]&#xa;        B = [[1, 2], [0]]&#xa;        C = [[2, 1], [0]]&#xa;        D = [[0], [2, 1]]&#xa;&#xa;    the following values for ``ordered`` have the shown meanings::&#xa;&#xa;        00 means A == B == C == D&#xa;        01 means A == B&#xa;        10 means A == D&#xa;        11 means A == A&#xa;&#xa;    >>> for ordered in [None, 0, 1, 10, 11]:&#xa;    ...     print('ordered = %s' % ordered)&#xa;    ...     for p in kbins(list(range(3)), 2, ordered=ordered):&#xa;    ...         print('     %s' % p)&#xa;    ...&#xa;    ordered = None&#xa;         [[0], [1, 2]]&#xa;         [[0, 1], [2]]&#xa;    ordered = 0&#xa;         [[0, 1], [2]]&#xa;         [[0, 2], [1]]&#xa;         [[0], [1, 2]]&#xa;    ordered = 1&#xa;         [[0], [1, 2]]&#xa;         [[0], [2, 1]]&#xa;         [[1], [0, 2]]&#xa;         [[1], [2, 0]]&#xa;         [[2], [0, 1]]&#xa;         [[2], [1, 0]]&#xa;    ordered = 10&#xa;         [[0, 1], [2]]&#xa;         [[2], [0, 1]]&#xa;         [[0, 2], [1]]&#xa;         [[1], [0, 2]]&#xa;         [[0], [1, 2]]&#xa;         [[1, 2], [0]]&#xa;    ordered = 11&#xa;         [[0], [1, 2]]&#xa;         [[0, 1], [2]]&#xa;         [[0], [2, 1]]&#xa;         [[0, 2], [1]]&#xa;         [[1], [0, 2]]&#xa;         [[1, 0], [2]]&#xa;         [[1], [2, 0]]&#xa;         [[1, 2], [0]]&#xa;         [[2], [0, 1]]&#xa;         [[2, 0], [1]]&#xa;         [[2], [1, 0]]&#xa;         [[2, 1], [0]]&#xa;&#xa;    See Also&#xa;    ========&#xa;    partitions, multiset_partitions&#xa;&#xa;    """"""&#xa;    def partition(lista, bins):&#xa;        #  EnricoGiampieri's partition generator from&#xa;        #  http://stackoverflow.com/questions/13131491/&#xa;        #  partition-n-items-into-k-bins-in-python-lazily&#xa;        if len(lista) == 1 or bins == 1:&#xa;            yield [lista]&#xa;        elif len(lista) > 1 and bins > 1:&#xa;            for i in range(1, len(lista)):&#xa;                for part in partition(lista[i:], bins - 1):&#xa;                    if len([lista[:i]] + part) == bins:&#xa;                        yield [lista[:i]] + part&#xa;&#xa;    if ordered is None:&#xa;        for p in partition(l, k):&#xa;            yield p&#xa;    elif ordered == 11:&#xa;        for pl in multiset_permutations(l):&#xa;            pl = list(pl)&#xa;            for p in partition(pl, k):&#xa;                yield p&#xa;    elif ordered == 00:&#xa;        for p in multiset_partitions(l, k):&#xa;            yield p&#xa;    elif ordered == 10:&#xa;        for p in multiset_partitions(l, k):&#xa;            for perm in permutations(p):&#xa;                yield list(perm)&#xa;    elif ordered == 1:&#xa;        for kgot, p in partitions(len(l), k, size=True):&#xa;            if kgot != k:&#xa;                continue&#xa;            for li in multiset_permutations(l):&#xa;                rv = []&#xa;                i = j = 0&#xa;                li = list(li)&#xa;                for size, multiplicity in sorted(p.items()):&#xa;                    for m in range(multiplicity):&#xa;                        j = i + size&#xa;                        rv.append(li[i: j])&#xa;                        i = j&#xa;                yield rv&#xa;    else:&#xa;        raise ValueError(&#xa;            'ordered must be one of 00, 01, 10 or 11, not %s' % ordered)&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;#&#xa;# Note: flock is used to seralize linking. Linking is a memory-intensive&#xa;# process so running parallel links can often lead to thrashing.  To disable&#xa;# the serialization, override LINK via an envrionment variable as follows:&#xa;#&#xa;#   export LINK=g++&#xa;#&#xa;# This will allow make to invoke N linker processes as specified in -jN.&#xa;LINK ?= %(flock)s $(builddir)/linker.lock $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = rm -rf ""$@"" && cp -af ""$<"" ""$@""&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter:&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs + ['FORCE_DO_CMD'], actions)&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                      part_of_all=True)&#xa;      bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    # TODO(evanm): just make order_only a list of deps instead of these hacks.&#xa;    if order_only:&#xa;      order_insert = '| '&#xa;      pick_output = ' '.join(outputs)&#xa;    else:&#xa;      order_insert = ''&#xa;      pick_output = outputs[0]&#xa;    if force:&#xa;      force_append = ' FORCE_DO_CMD'&#xa;    else:&#xa;      force_append = ''&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    self.WriteLn('%s: %s%s%s' % (pick_output, order_insert, ' '.join(inputs),&#xa;                                 force_append))&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    if not order_only and len(outputs) > 1:&#xa;      # If we have more than one output, a rule like&#xa;      #   foo bar: baz&#xa;      # that for *each* output we must run the action, potentially&#xa;      # in parallel.  That is not what we're trying to write -- what&#xa;      # we want is that we run the action once and it generates all&#xa;      # the files.&#xa;      # http://www.gnu.org/software/hello/manual/automake/Multiple-Outputs.html&#xa;      # discusses this problem and has this solution:&#xa;      # 1) Write the naive rule that would produce parallel runs of&#xa;      # the action.&#xa;      # 2) Make the outputs seralized on each other, so we won't start&#xa;      # a parallel run until the first run finishes, at which point&#xa;      # we'll have generated all the outputs and we're done.&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs[1:]), outputs[0]))&#xa;      # Add a dummy command to the ""extra outputs"" rule, otherwise make seems to&#xa;      # think these outputs haven't (couldn't have?) changed, and thus doesn't&#xa;      # flag them as changed (i.e. include in '$?') when evaluating dependent&#xa;      # rules, which in turn causes do_cmd() to skip running dependent commands.&#xa;      self.WriteLn('%s: ;' % (' '.join(outputs[1:])))&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;&#xa;    # XXX(TooTallNate): disabling this code since we don't want this behavior...&#xa;    #if (self.type == 'shared_library' and&#xa;    #    (self.flavor != 'mac' or self.toolset != 'target')):&#xa;    #  # Install all shared libs into a common directory (per toolset) for&#xa;    #  # convenient access with LD_LIBRARY_PATH.&#xa;    #  return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host',), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  wrappers['LINK'] = '%s $(builddir)/linker.lock' % flock_command&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
566746|"# Adapted from&#xa;# https://gist.github.com/jtriley/1108174&#xa;# pylint: disable=bare-except,unpacking-non-sequence&#xa;import os&#xa;import shlex&#xa;import struct&#xa;import platform&#xa;import subprocess&#xa;&#xa;&#xa;def get_terminal_size():&#xa;    """""" getTerminalSize()&#xa;     - get width and height of console&#xa;     - works on linux,os x,windows,cygwin(windows)&#xa;     originally retrieved from:&#xa;     http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python&#xa;    """"""&#xa;    current_os = platform.system()&#xa;    tuple_xy = None&#xa;    if current_os == 'Windows':&#xa;        tuple_xy = _get_terminal_size_windows()&#xa;        if tuple_xy is None:&#xa;            # needed for window's python in cygwin's xterm&#xa;            tuple_xy = _get_terminal_size_tput()&#xa;    if current_os in ['Linux', 'Darwin'] or current_os.startswith('CYGWIN'):&#xa;        tuple_xy = _get_terminal_size_linux()&#xa;    if tuple_xy is None or tuple_xy == (0, 0):&#xa;        tuple_xy = (80, 25)      # assume ""standard"" terminal&#xa;    return tuple_xy&#xa;&#xa;&#xa;def _get_terminal_size_windows():&#xa;    # pylint: disable=unused-variable,redefined-outer-name,too-many-locals&#xa;    try:&#xa;        from ctypes import windll, create_string_buffer&#xa;        # stdin handle is -10&#xa;        # stdout handle is -11&#xa;        # stderr handle is -12&#xa;        h = windll.kernel32.GetStdHandle(-12)&#xa;        csbi = create_string_buffer(22)&#xa;        res = windll.kernel32.GetConsoleScreenBufferInfo(h, csbi)&#xa;        if res:&#xa;            (bufx, bufy, curx, cury, wattr,&#xa;             left, top, right, bottom,&#xa;             maxx, maxy) = struct.unpack(""hhhhHhhhhhh"", csbi.raw)&#xa;            sizex = right - left + 1&#xa;            sizey = bottom - top + 1&#xa;            return sizex, sizey&#xa;    except:&#xa;        pass&#xa;&#xa;&#xa;def _get_terminal_size_tput():&#xa;    # get terminal width&#xa;    # src: http://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window&#xa;    try:&#xa;        cols = int(subprocess.check_call(shlex.split('tput cols')))&#xa;        rows = int(subprocess.check_call(shlex.split('tput lines')))&#xa;        return (cols, rows)&#xa;    except:&#xa;        pass&#xa;&#xa;&#xa;def _get_terminal_size_linux():&#xa;    def ioctl_GWINSZ(fd):&#xa;        try:&#xa;            import fcntl&#xa;            import termios&#xa;            cr = struct.unpack('hh',&#xa;                               fcntl.ioctl(fd, termios.TIOCGWINSZ, '1234'))&#xa;            return cr&#xa;        except:&#xa;            pass&#xa;    cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)&#xa;    if not cr:&#xa;        try:&#xa;            fd = os.open(os.ctermid(), os.O_RDONLY)&#xa;            cr = ioctl_GWINSZ(fd)&#xa;            os.close(fd)&#xa;        except:&#xa;            pass&#xa;    if not cr:&#xa;        try:&#xa;            cr = (os.environ['LINES'], os.environ['COLUMNS'])&#xa;        except:&#xa;            return None&#xa;    return int(cr[1]), int(cr[0])&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;    sizex, sizey = get_terminal_size()&#xa;    print 'width =', sizex, 'height =', sizey&#xa;"
111945|"# -*- coding: utf-8 -*-&#xa;#&#xa;# This file is part of Invenio.&#xa;# Copyright (C) 2012 CERN.&#xa;#&#xa;# Invenio is free software; you can redistribute it and/or&#xa;# modify it under the terms of the GNU General Public License as&#xa;# published by the Free Software Foundation; either version 2 of the&#xa;# License, or (at your option) any later version.&#xa;#&#xa;# Invenio is distributed in the hope that it will be useful, but&#xa;# WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;# General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Invenio; if not, write to the Free Software Foundation, Inc.,&#xa;# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.&#xa;&#xa;""""""&#xa;Invenio Tasklet.&#xa;&#xa;Notify a URL, and post data if wanted.&#xa;""""""&#xa;import urlparse&#xa;import urllib2&#xa;import time&#xa;&#xa;from invenio.config import \&#xa;     CFG_SITE_ADMIN_EMAIL, \&#xa;     CFG_SITE_NAME&#xa;from invenio.legacy.bibsched.bibtask import write_message, \&#xa;     task_sleep_now_if_required&#xa;from invenio.ext.email import send_email&#xa;&#xa;def bst_notify_url(url, data=None,&#xa;                   content_type='text/plain',&#xa;                   attempt_times=1,&#xa;                   attempt_sleeptime=10,&#xa;                   admin_emails=None):&#xa;    """"""&#xa;    Access given URL, and post given data if specified.&#xa;&#xa;    @param url: the URL to access&#xa;    @type url: string&#xa;    @param data: the data to be posted to the given URL&#xa;    @type data: string&#xa;    @param data: the content-type header to use to post data&#xa;    @type data: string&#xa;    @param attempt_times: number of tries&#xa;    @type attempt_times: int&#xa;    @param attempt_sleeptime: seconds in between tries&#xa;    @type attempt_sleeptime: int&#xa;    @param admin_emails: a comma-separated list of emails to notify in case of failure&#xa;    @type admin_emails: string or list (as accepted by mailutils.send_email)&#xa;&#xa;    If accessing fails, try to send it ATTEMPT_TIMES, and wait for&#xa;    ATTEMPT_SLEEPTIME seconds in between tries. When the maximum&#xa;    number of attempts is reached, send an email notification to the&#xa;    recipients specified in ADMIN_EMAILS.&#xa;    """"""&#xa;    attempt_times = int(attempt_times)&#xa;    attempt_sleeptime = int(attempt_sleeptime)&#xa;    remaining_attempts = attempt_times&#xa;&#xa;    success_p = False&#xa;    reason_failure = """"&#xa;&#xa;    write_message(""Going to notify URL: %(url)s"" %  {'url': url})&#xa;&#xa;    while not success_p and remaining_attempts > 0:&#xa;        ## <scheme>://<netloc>/<path>?<query>#<fragment>&#xa;        scheme, netloc, path, query, fragment = urlparse.urlsplit(url)&#xa;        ## See: http://stackoverflow.com/questions/111945/is-there-any-way-to-do-http-put-in-python&#xa;        if scheme == 'http':&#xa;            opener = urllib2.build_opener(urllib2.HTTPHandler)&#xa;        elif scheme == 'https':&#xa;            opener = urllib2.build_opener(urllib2.HTTPSHandler)&#xa;        else:&#xa;            raise ValueError(""Scheme not handled %s for url %s"" % (scheme, url))&#xa;        request = urllib2.Request(url, data=data)&#xa;        if data:&#xa;            request.add_header('Content-Type', content_type)&#xa;            request.get_method = lambda: 'POST'&#xa;        try:&#xa;            opener.open(request)&#xa;            success_p = True&#xa;        except urllib2.URLError as e:&#xa;            success_p = False&#xa;            reason_failure = repr(e)&#xa;        if not success_p:&#xa;            remaining_attempts -= 1&#xa;            if remaining_attempts > 0: # sleep only if we shall retry again&#xa;                task_sleep_now_if_required(can_stop_too=True)&#xa;                time.sleep(attempt_sleeptime)&#xa;&#xa;        # Report about success/failure&#xa;        if success_p:&#xa;            write_message(""URL successfully notified"")&#xa;        else:&#xa;            write_message(""Failed at notifying URL. Reason:\n%(reason_failure)s"" % \&#xa;                          {'reason_failure': reason_failure})&#xa;&#xa;    if not success_p and admin_emails:&#xa;        # We could not access the specified URL. Send an email to the&#xa;        # specified contacts.&#xa;        write_message(""Notifying by email %(admin_emails)s"" % \&#xa;                      {'admin_emails': str(admin_emails)})&#xa;        subject = ""%(CFG_SITE_NAME)s could not contact %(url)s"" % \&#xa;                  {'CFG_SITE_NAME': CFG_SITE_NAME,&#xa;                   'url': url}&#xa;        content = """"""\n%(CFG_SITE_NAME)s unsuccessfully tried to contact %(url)s.&#xa;&#xa;Number of attempts: %(attempt_times)i. No further attempts will be made.&#xa;&#xa;"""""" % \&#xa;                  {'CFG_SITE_NAME': CFG_SITE_NAME,&#xa;                   'url': url,&#xa;                   'attempt_times': attempt_times}&#xa;        if data:&#xa;            max_data_length = 10000&#xa;            content += ""The following data should have been posted:\n%(data)s%(extension)s"" % \&#xa;                      {'data': data[:max_data_length],&#xa;                       'extension': len(data) > max_data_length and ' [...]' or ''}&#xa;        # Send email. If sending fails, we will stop the queue&#xa;        return send_email(fromaddr=CFG_SITE_ADMIN_EMAIL,&#xa;                          toaddr=admin_emails,&#xa;                          subject=subject,&#xa;                          content=content)&#xa;&#xa;    # We do not really want to stop the queue now, even in case of&#xa;    # failure as an email would have been sent if necessary.&#xa;    return 1&#xa;"
7648200|"# Screenshot-related features of PyAutoGUI&#xa;&#xa;""""""&#xa;So, apparently Pillow support on Ubuntu 64-bit has several additional steps since it doesn't have JPEG/PNG support out of the box. Description here:&#xa;&#xa;https://stackoverflow.com/questions/7648200/pip-install-pil-e-tickets-1-no-jpeg-png-support&#xa;http://ubuntuforums.org/showthread.php?t=1751455&#xa;""""""&#xa;&#xa;import datetime&#xa;import os&#xa;import subprocess&#xa;import sys&#xa;from PIL import Image&#xa;from PIL import ImageOps&#xa;&#xa;RUNNING_PYTHON_2 = sys.version_info[0] == 2&#xa;&#xa;scrotExists = False&#xa;try:&#xa;    if sys.platform not in ('java', 'darwin', 'win32'):&#xa;        whichProc = subprocess.Popen(['which', 'scrot'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)&#xa;        scrotExists = whichProc.wait() == 0&#xa;except:&#xa;    # if there is no ""which"" program to find scrot, then assume there is no scrot.&#xa;    pass&#xa;&#xa;def locateAll(needleImage, haystackImage, grayscale=False, limit=None):&#xa;    needleFileObj = None&#xa;    haystackFileObj = None&#xa;    if isinstance(needleImage, str):&#xa;        # 'image' is a filename, load the Image object&#xa;        needleFileObj = open(needleImage, 'rb')&#xa;        needleImage = Image.open(needleFileObj)&#xa;    if isinstance(haystackImage, str):&#xa;        # 'image' is a filename, load the Image object&#xa;        haystackFileObj = open(haystackImage, 'rb')&#xa;        haystackImage = Image.open(haystackFileObj)&#xa;&#xa;&#xa;    if grayscale:&#xa;        needleImage = ImageOps.grayscale(needleImage)&#xa;        haystackImage = ImageOps.grayscale(haystackImage)&#xa;&#xa;    needleWidth, needleHeight = needleImage.size&#xa;    haystackWidth, haystackHeight = haystackImage.size&#xa;&#xa;    needleImageData = tuple(needleImage.getdata()) # TODO - rename to needleImageData??&#xa;    haystackImageData = tuple(haystackImage.getdata())&#xa;&#xa;    needleImageRows = [needleImageData[y * needleWidth:(y+1) * needleWidth] for y in range(needleHeight)] # LEFT OFF - check this&#xa;    needleImageFirstRow = needleImageRows[0]&#xa;&#xa;    assert len(needleImageFirstRow) == needleWidth&#xa;    assert [len(row) for row in needleImageRows] == [needleWidth] * needleHeight&#xa;&#xa;    numMatchesFound = 0&#xa;&#xa;    for y in range(haystackHeight):&#xa;        for matchx in _kmp(needleImageFirstRow, haystackImageData[y * haystackWidth:(y+1) * haystackWidth]):&#xa;            foundMatch = True&#xa;            for searchy in range(1, needleHeight):&#xa;                haystackStart = (searchy + y) * haystackWidth + matchx&#xa;                if needleImageData[searchy * needleWidth:(searchy+1) * needleWidth] != haystackImageData[haystackStart:haystackStart + needleWidth]:&#xa;                    foundMatch = False&#xa;                    break&#xa;            if foundMatch:&#xa;                # Match found, report the x, y, width, height of where the matching region is in haystack.&#xa;                numMatchesFound += 1&#xa;                yield (matchx, y, needleWidth, needleHeight)&#xa;                if limit is not None and numMatchesFound >= limit:&#xa;                    # Limit has been reached. Close file handles.&#xa;                    if needleFileObj is not None:&#xa;                        needleFileObj.close()&#xa;                    if haystackFileObj is not None:&#xa;                        haystackFileObj.close()&#xa;&#xa;&#xa;    # There was no limit or the limit wasn't reached, but close the file handles anyway.&#xa;    if needleFileObj is not None:&#xa;        needleFileObj.close()&#xa;    if haystackFileObj is not None:&#xa;        haystackFileObj.close()&#xa;&#xa;&#xa;def locate(needleImage, haystackImage, grayscale=False):&#xa;    # Note: The gymnastics in this function is because we want to make sure to exhaust the iterator so that the needle and haystack files are closed in locateAll.&#xa;    points = tuple(locateAll(needleImage, haystackImage, grayscale, 1))&#xa;    if len(points) > 0:&#xa;        return points[0]&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;def locateOnScreen(image, grayscale=False):&#xa;    screenshotIm = screenshot()&#xa;    retVal = locate(image, screenshotIm, grayscale)&#xa;    if 'fp' in dir(screenshotIm):&#xa;        screenshotIm.fp.close() # Screenshots on Windows won't have an fp since they came from ImageGrab, not a file.&#xa;    return retVal&#xa;&#xa;&#xa;def locateAllOnScreen(image, grayscale=False, limit=None):&#xa;    screenshotIm = screenshot()&#xa;    retVal = locateAll(image, screenshotIm, grayscale, limit)&#xa;    if 'fp' in dir(screenshotIm):&#xa;        screenshotIm.fp.close() # Screenshots on Windows won't have an fp since they came from ImageGrab, not a file.&#xa;    return retVal&#xa;&#xa;&#xa;def locateCenterOnScreen(image, grayscale=False):&#xa;    return center(locateOnScreen(image, grayscale))&#xa;&#xa;&#xa;def _screenshot_win32(imageFilename=None):&#xa;    im = ImageGrab.grab()&#xa;    if imageFilename is not None:&#xa;        im.save(imageFilename)&#xa;    return im&#xa;&#xa;&#xa;def _screenshot_osx(imageFilename=None):&#xa;    if imageFilename is None:&#xa;        tmpFilename = '.screenshot%s.png' % (datetime.datetime.now().strftime('%Y-%m%d_%H-%M-%S-%f'))&#xa;    else:&#xa;        tmpFilename = imageFilename&#xa;    subprocess.call(['screencapture', '-x', tmpFilename])&#xa;    im = Image.open(tmpFilename)&#xa;    if imageFilename is None:&#xa;        os.unlink(tmpFilename)&#xa;    return im&#xa;&#xa;&#xa;def _screenshot_linux(imageFilename=None):&#xa;    if not scrotExists:&#xa;        raise NotImplementedError('""scrot"" must be installed to use screenshot functions in Linux. Run: sudo apt-get install scrot')&#xa;    if imageFilename is None:&#xa;        tmpFilename = '.screenshot%s.png' % (datetime.datetime.now().strftime('%Y-%m%d_%H-%M-%S-%f'))&#xa;    else:&#xa;        tmpFilename = imageFilename&#xa;    if scrotExists:&#xa;        subprocess.call(['scrot', tmpFilename])&#xa;        im = Image.open(tmpFilename)&#xa;        if imageFilename is None:&#xa;            os.unlink(tmpFilename)&#xa;        return im&#xa;    else:&#xa;        raise Exception('The scrot program must be installed to take a screenshot with PyAutoGUI on Linux. Run: sudo apt-get install scrot')&#xa;&#xa;&#xa;&#xa;def _kmp(needle, haystack): # Knuth-Morris-Pratt search algorithm implementation (to be used by screen capture)&#xa;    # build table of shift amounts&#xa;    shifts = [1] * (len(needle) + 1)&#xa;    shift = 1&#xa;    for pos in range(len(needle)):&#xa;        while shift <= pos and needle[pos] != needle[pos-shift]:&#xa;            shift += shifts[pos-shift]&#xa;        shifts[pos+1] = shift&#xa;&#xa;    # do the actual search&#xa;    startPos = 0&#xa;    matchLen = 0&#xa;    for c in haystack:&#xa;        while matchLen == len(needle) or \&#xa;              matchLen >= 0 and needle[matchLen] != c:&#xa;            startPos += shifts[matchLen]&#xa;            matchLen -= shifts[matchLen]&#xa;        matchLen += 1&#xa;        if matchLen == len(needle):&#xa;            yield startPos&#xa;&#xa;&#xa;def center(coords):&#xa;    return (coords[0] + int(coords[2] / 2), coords[1] + int(coords[3] / 2))&#xa;&#xa;&#xa;def pixelMatchesColor(x, y, expectedRGBColor, tolerance=0):&#xa;    r, g, b = screenshot().getpixel((x, y))&#xa;    exR, exG, exB = expectedRGBColor&#xa;&#xa;    return (abs(r - exR) <= tolerance) and (abs(g - exG) <= tolerance) and (abs(b - exB) <= tolerance)&#xa;&#xa;&#xa;def pixel(x, y):&#xa;    return screenshot().getpixel((x, y))&#xa;&#xa;&#xa;# set the screenshot() function based on the platform running this module&#xa;if sys.platform.startswith('java'):&#xa;    raise NotImplementedError('Jython is not yet supported by PyAutoGUI.')&#xa;elif sys.platform == 'darwin':&#xa;    screenshot = _screenshot_osx&#xa;elif sys.platform == 'win32':&#xa;    screenshot = _screenshot_win32&#xa;    from PIL import ImageGrab&#xa;else:&#xa;    screenshot = _screenshot_linux&#xa;&#xa;&#xa;grab = screenshot # for compatibility with Pillow/PIL's ImageGrab module.&#xa;"
23301941|"from sympy.solvers.diophantine import (descent, diop_bf_DN, diop_DN,&#xa;    diop_solve, diophantine, divisible, equivalent, find_DN, ldescent, length,&#xa;    pairwise_prime, partition, power_representation,&#xa;    prime_as_sum_of_two_squares, square_factor, sum_of_four_squares,&#xa;    sum_of_three_squares, transformation_to_DN, transformation_to_normal)&#xa;&#xa;from sympy import (Add, factor_list, igcd, Integer, Matrix, Mul, S, simplify,&#xa;    Subs, Symbol, symbols)&#xa;&#xa;from sympy.core.function import _mexpand&#xa;from sympy.core.compatibility import range&#xa;from sympy.functions.elementary.trigonometric import sin&#xa;from sympy.utilities.pytest import slow, raises&#xa;from sympy.utilities import default_sort_key&#xa;&#xa;x, y, z, w, t, X, Y, Z = symbols(""x, y, z, w, t, X, Y, Z"", integer=True)&#xa;t_0, t_1, t_2, t_3, t_4, t_5, t_6 = symbols(""t_0, t_1, t_2, t_3, t_4, t_5, t_6"", integer=True)&#xa;&#xa;def test_input_format():&#xa;    raises(TypeError, lambda: diophantine(sin(x)))&#xa;&#xa;def test_univariate():&#xa;    assert diop_solve((x - 1)*(x - 2)**2) == set([(Integer(1),), (Integer(2),)])&#xa;    assert diop_solve((x - 1)*(x - 2)) == set([(Integer(1),), (Integer(2),)])&#xa;&#xa;&#xa;def test_linear():&#xa;    assert diop_solve(x) == (0,)&#xa;    assert diop_solve(1*x) == (0,)&#xa;    assert diop_solve(3*x) == (0,)&#xa;    assert diop_solve(x + 1) == (-1,)&#xa;    assert diop_solve(2*x + 1) == (None,)&#xa;    assert diop_solve(2*x + 4) == (-2,)&#xa;    assert diop_solve(y + x) == (t_0, -t_0)&#xa;    assert diop_solve(y + x + 0) == (t_0, -t_0)&#xa;    assert diop_solve(y + x - 0) == (t_0, -t_0)&#xa;    assert diop_solve(0*x - y - 5) == (-5,)&#xa;    assert diop_solve(3*y + 2*x - 5) == (3*t_0 - 5, -2*t_0 + 5)&#xa;    assert diop_solve(2*x - 3*y - 5) == (-3*t_0 - 5, -2*t_0 - 5)&#xa;    assert diop_solve(-2*x - 3*y - 5) == (-3*t_0 + 5, 2*t_0 - 5)&#xa;    assert diop_solve(7*x + 5*y) == (5*t_0, -7*t_0)&#xa;    assert diop_solve(2*x + 4*y) == (2*t_0, -t_0)&#xa;    assert diop_solve(4*x + 6*y - 4) == (3*t_0 - 2, -2*t_0 + 2)&#xa;    assert diop_solve(4*x + 6*y - 3) == (None, None)&#xa;    assert diop_solve(0*x + 3*y - 4*z + 5) == (-4*t_0 + 5, -3*t_0 + 5)&#xa;    assert diop_solve(4*x + 3*y - 4*z + 5) == (t_0, -4*t_1 + 5, t_0 - 3*t_1 + 5)&#xa;    assert diop_solve(4*x + 2*y + 8*z - 5) == (None, None, None)&#xa;    assert diop_solve(5*x + 7*y - 2*z - 6) == (t_0, -7*t_0 - 2*t_1 + 6, -22*t_0 - 7*t_1 + 18)&#xa;    assert diop_solve(3*x - 6*y + 12*z - 9) == (2*t_0 + 3, t_0 + 2*t_1, t_1)&#xa;    assert diop_solve(6*w + 9*x + 20*y - z) == (t_0, t_1, -t_1 - t_2, 6*t_0 - 11*t_1 - 20*t_2)&#xa;&#xa;&#xa;def test_quadratic_simple_hyperbolic_case():&#xa;    # Simple Hyperbolic case: A = C = 0 and B != 0&#xa;    assert diop_solve(3*x*y + 34*x - 12*y + 1) == \&#xa;        set([(-Integer(133), -Integer(11)), (Integer(5), -Integer(57))])&#xa;    assert diop_solve(6*x*y + 2*x + 3*y + 1) == set([])&#xa;    assert diop_solve(-13*x*y + 2*x - 4*y - 54) == set([(Integer(27), Integer(0))])&#xa;    assert diop_solve(-27*x*y - 30*x - 12*y - 54) == set([(-Integer(14), -Integer(1))])&#xa;    assert diop_solve(2*x*y + 5*x + 56*y + 7) == set([(-Integer(161), -Integer(3)),\&#xa;        (-Integer(47),-Integer(6)), (-Integer(35), -Integer(12)), (-Integer(29), -Integer(69)),\&#xa;        (-Integer(27), Integer(64)), (-Integer(21), Integer(7)),(-Integer(9), Integer(1)),\&#xa;        (Integer(105), -Integer(2))])&#xa;    assert diop_solve(6*x*y + 9*x + 2*y + 3) == set([])&#xa;    assert diop_solve(x*y + x + y + 1) == set([(-Integer(1), t), (t, -Integer(1))])&#xa;    assert diophantine(48*x*y)&#xa;&#xa;&#xa;def test_quadratic_elliptical_case():&#xa;    # Elliptical case: B**2 - 4AC < 0&#xa;    # Two test cases highlighted require lot of memory due to quadratic_congruence() method.&#xa;    # This above method should be replaced by Pernici's square_mod() method when his PR gets merged.&#xa;&#xa;    #assert diop_solve(42*x**2 + 8*x*y + 15*y**2 + 23*x + 17*y - 4915) == set([(-Integer(11), -Integer(1))])&#xa;    assert diop_solve(4*x**2 + 3*y**2 + 5*x - 11*y + 12) == set([])&#xa;    assert diop_solve(x**2 + y**2 + 2*x + 2*y + 2) == set([(-Integer(1), -Integer(1))])&#xa;    #assert diop_solve(15*x**2 - 9*x*y + 14*y**2 - 23*x - 14*y - 4950) == set([(-Integer(15), Integer(6))])&#xa;    assert diop_solve(10*x**2 + 12*x*y + 12*y**2 - 34) == \&#xa;        set([(Integer(1), -Integer(2)), (-Integer(1), -Integer(1)),(Integer(1), Integer(1)), (-Integer(1), Integer(2))])&#xa;&#xa;&#xa;def test_quadratic_parabolic_case():&#xa;    # Parabolic case: B**2 - 4AC = 0&#xa;    assert check_solutions(8*x**2 - 24*x*y + 18*y**2 + 5*x + 7*y + 16)&#xa;    assert check_solutions(8*x**2 - 24*x*y + 18*y**2 + 6*x + 12*y - 6)&#xa;    assert check_solutions(8*x**2 + 24*x*y + 18*y**2 + 4*x + 6*y - 7)&#xa;    assert check_solutions(x**2 + 2*x*y + y**2 + 2*x + 2*y + 1)&#xa;    assert check_solutions(x**2 - 2*x*y + y**2 + 2*x + 2*y + 1)&#xa;    assert check_solutions(y**2 - 41*x + 40)&#xa;&#xa;&#xa;def test_quadratic_perfect_square():&#xa;    # B**2 - 4*A*C > 0&#xa;    # B**2 - 4*A*C is a perfect square&#xa;    assert check_solutions(48*x*y)&#xa;    assert check_solutions(4*x**2 - 5*x*y + y**2 + 2)&#xa;    assert check_solutions(-2*x**2 - 3*x*y + 2*y**2 -2*x - 17*y + 25)&#xa;    assert check_solutions(12*x**2 + 13*x*y + 3*y**2 - 2*x + 3*y - 12)&#xa;    assert check_solutions(8*x**2 + 10*x*y + 2*y**2 - 32*x - 13*y - 23)&#xa;    assert check_solutions(4*x**2 - 4*x*y - 3*y- 8*x - 3)&#xa;    assert check_solutions(- 4*x*y - 4*y**2 - 3*y- 5*x - 10)&#xa;    assert check_solutions(x**2 - y**2 - 2*x - 2*y)&#xa;    assert check_solutions(x**2 - 9*y**2 - 2*x - 6*y)&#xa;    assert check_solutions(4*x**2 - 9*y**2 - 4*x - 12*y - 3)&#xa;&#xa;&#xa;def test_quadratic_non_perfect_square():&#xa;    # B**2 - 4*A*C is not a perfect square&#xa;    # Used check_solutions() since the solutions are complex expressions involving&#xa;    # square roots and exponents&#xa;    assert check_solutions(x**2 - 2*x - 5*y**2)&#xa;    assert check_solutions(3*x**2 - 2*y**2 - 2*x - 2*y)&#xa;    assert check_solutions(x**2 - x*y - y**2 - 3*y)&#xa;    assert check_solutions(x**2 - 9*y**2 - 2*x - 6*y)&#xa;&#xa;&#xa;def test_issue_9106():&#xa;    assert check_integrality(-48 - 2*x*(3*x - 1) + y*(3*y - 1))&#xa;&#xa;&#xa;@slow&#xa;def test_quadratic_non_perfect_slow():&#xa;    assert check_solutions(8*x**2 + 10*x*y - 2*y**2 - 32*x - 13*y - 23)&#xa;    # This leads to very large numbers.&#xa;    # assert check_solutions(5*x**2 - 13*x*y + y**2 - 4*x - 4*y - 15)&#xa;    assert check_solutions(-3*x**2 - 2*x*y + 7*y**2 - 5*x - 7)&#xa;    assert check_solutions(-4 - x + 4*x**2 - y - 3*x*y - 4*y**2)&#xa;    assert check_solutions(1 + 2*x + 2*x**2 + 2*y + x*y - 2*y**2)&#xa;&#xa;&#xa;def test_DN():&#xa;    # Most of the test cases were adapted from,&#xa;    # Solving the generalized Pell equation x**2 - D*y**2 = N, John P. Robertson, July 31, 2004.&#xa;    # http://www.jpr2718.org/pell.pdf&#xa;    # others are verified using Wolfram Alpha.&#xa;&#xa;    # Covers cases where D <= 0 or D > 0 and D is a square or N = 0&#xa;    # Solutions are straightforward in these cases.&#xa;    assert diop_DN(3, 0) == [(0, 0)]&#xa;    assert diop_DN(-17, -5) == []&#xa;    assert diop_DN(-19, 23) == [(2, 1)]&#xa;    assert diop_DN(-13, 17) == [(2, 1)]&#xa;    assert diop_DN(-15, 13) == []&#xa;    assert diop_DN(0, 5) == []&#xa;    assert diop_DN(0, 9) == [(3, t)]&#xa;    assert diop_DN(9, 0) == [(3*t, t)]&#xa;    assert diop_DN(16, 24) == []&#xa;    assert diop_DN(9, 180) == [(18, 4)]&#xa;    assert diop_DN(9, -180) == [(12, 6)]&#xa;    assert diop_DN(7, 0) == [(0, 0)]&#xa;&#xa;    # When equation is x**2 + y**2 = N&#xa;    # Solutions are interchangeable&#xa;    assert diop_DN(-1, 5) == [(2, 1)]&#xa;    assert diop_DN(-1, 169) == [(12, 5), (0, 13)]&#xa;&#xa;    # D > 0 and D is not a square&#xa;&#xa;    # N = 1&#xa;    assert diop_DN(13, 1) == [(649, 180)]&#xa;    assert diop_DN(980, 1) == [(51841, 1656)]&#xa;    assert diop_DN(981, 1) == [(158070671986249, 5046808151700)]&#xa;    assert diop_DN(986, 1) == [(49299, 1570)]&#xa;    assert diop_DN(991, 1) == [(379516400906811930638014896080, 12055735790331359447442538767)]&#xa;    assert diop_DN(17, 1) == [(33, 8)]&#xa;    assert diop_DN(19, 1) == [(170, 39)]&#xa;&#xa;    # N = -1&#xa;    assert diop_DN(13, -1) == [(18, 5)]&#xa;    assert diop_DN(991, -1) == []&#xa;    assert diop_DN(41, -1) == [(32, 5)]&#xa;    assert diop_DN(290, -1) == [(17, 1)]&#xa;    assert diop_DN(21257, -1) == [(13913102721304, 95427381109)]&#xa;    assert diop_DN(32, -1) == []&#xa;&#xa;    # |N| > 1&#xa;    # Some tests were created using calculator at&#xa;    # http://www.numbertheory.org/php/patz.html&#xa;&#xa;    assert diop_DN(13, -4) == [(3, 1), (393, 109), (36, 10)]&#xa;    # Source I referred returned (3, 1), (393, 109) and (-3, 1) as fundamental solutions&#xa;    # So (-3, 1) and (393, 109) should be in the same equivalent class&#xa;    assert equivalent(-3, 1, 393, 109, 13, -4) == True&#xa;&#xa;    assert diop_DN(13, 27) == [(220, 61), (40, 11), (768, 213), (12, 3)]&#xa;    assert set(diop_DN(157, 12)) == \&#xa;    set([(Integer(13), Integer(1)), (Integer(10663), Integer(851)), (Integer(579160), Integer(46222)), \&#xa;        (Integer(483790960),Integer(38610722)), (Integer(26277068347), Integer(2097138361)), (Integer(21950079635497), Integer(1751807067011))])&#xa;    assert diop_DN(13, 25) == [(3245, 900)]&#xa;    assert diop_DN(192, 18) == []&#xa;    assert diop_DN(23, 13) == [(-6, 1), (6, 1)]&#xa;    assert diop_DN(167, 2) == [(13, 1)]&#xa;    assert diop_DN(167, -2) == []&#xa;&#xa;    assert diop_DN(123, -2) == [(11, 1)]&#xa;    # One calculator returned [(11, 1), (-11, 1)] but both of these are in&#xa;    # the same equivalence class&#xa;    assert equivalent(11, 1, -11, 1, 123, -2)&#xa;&#xa;    assert diop_DN(123, -23) == [(-10, 1), (10, 1)]&#xa;&#xa;&#xa;def test_bf_pell():&#xa;    assert diop_bf_DN(13, -4) == [(3, 1), (-3, 1), (36, 10)]&#xa;    assert diop_bf_DN(13, 27) == [(12, 3), (-12, 3), (40, 11), (-40, 11)]&#xa;    assert diop_bf_DN(167, -2) == []&#xa;    assert diop_bf_DN(1729, 1) == [(44611924489705, 1072885712316)]&#xa;    assert diop_bf_DN(89, -8) == [(9, 1), (-9, 1)]&#xa;    assert diop_bf_DN(21257, -1) == [(13913102721304, 95427381109)]&#xa;    assert diop_bf_DN(340, -4) == [(756, 41)]&#xa;&#xa;&#xa;def test_length():&#xa;    assert length(-2, 4, 5) == 3&#xa;    assert length(-5, 4, 17) == 4&#xa;    assert length(0, 4, 13) == 6&#xa;    assert length(-31, 8, 613) == 67&#xa;    assert length(7, 13, 11) == 23&#xa;    assert length(-40, 5, 23) == 4&#xa;&#xa;&#xa;def is_pell_transformation_ok(eq):&#xa;    """"""&#xa;    Test whether X*Y, X, or Y terms are present in the equation&#xa;    after transforming the equation using the transformation returned&#xa;    by transformation_to_pell(). If they are not present we are good.&#xa;    Moreover, coefficient of X**2 should be a divisor of coefficient of&#xa;    Y**2 and the constant term.&#xa;    """"""&#xa;    A, B = transformation_to_DN(eq)&#xa;    u = (A*Matrix([X, Y]) + B)[0]&#xa;    v = (A*Matrix([X, Y]) + B)[1]&#xa;    simplified = _mexpand(Subs(eq, (x, y), (u, v)).doit())&#xa;&#xa;    coeff = dict([reversed(t.as_independent(*[X, Y])) for t in simplified.args])&#xa;&#xa;    for term in [X*Y, X, Y]:&#xa;        if term in coeff.keys():&#xa;            return False&#xa;&#xa;    for term in [X**2, Y**2, Integer(1)]:&#xa;        if term not in coeff.keys():&#xa;            coeff[term] = Integer(0)&#xa;&#xa;    if coeff[X**2] != 0:&#xa;        return isinstance(S(coeff[Y**2])/coeff[X**2], Integer) and isinstance(S(coeff[Integer(1)])/coeff[X**2], Integer)&#xa;&#xa;    return True&#xa;&#xa;&#xa;def test_transformation_to_pell():&#xa;    assert is_pell_transformation_ok(-13*x**2 - 7*x*y + y**2 + 2*x - 2*y - 14)&#xa;    assert is_pell_transformation_ok(-17*x**2 + 19*x*y - 7*y**2 - 5*x - 13*y - 23)&#xa;    assert is_pell_transformation_ok(x**2 - y**2 + 17)&#xa;    assert is_pell_transformation_ok(-x**2 + 7*y**2 - 23)&#xa;    assert is_pell_transformation_ok(25*x**2 - 45*x*y + 5*y**2 - 5*x - 10*y + 5)&#xa;    assert is_pell_transformation_ok(190*x**2 + 30*x*y + y**2 - 3*y - 170*x - 130)&#xa;    assert is_pell_transformation_ok(x**2 - 2*x*y -190*y**2 - 7*y - 23*x - 89)&#xa;    assert is_pell_transformation_ok(15*x**2 - 9*x*y + 14*y**2 - 23*x - 14*y - 4950)&#xa;&#xa;&#xa;def test_find_DN():&#xa;    assert find_DN(x**2 - 2*x - y**2) == (1, 1)&#xa;    assert find_DN(x**2 - 3*y**2 - 5) == (3, 5)&#xa;    assert find_DN(x**2 - 2*x*y - 4*y**2 - 7) == (5, 7)&#xa;    assert find_DN(4*x**2 - 8*x*y - y**2 - 9) == (20, 36)&#xa;    assert find_DN(7*x**2 - 2*x*y - y**2 - 12) == (8, 84)&#xa;    assert find_DN(-3*x**2 + 4*x*y -y**2) == (1, 0)&#xa;    assert find_DN(-13*x**2 - 7*x*y + y**2 + 2*x - 2*y -14) == (101, -7825480)&#xa;&#xa;&#xa;def test_ldescent():&#xa;    # Equations which have solutions&#xa;    u = ([(13, 23), (3, -11), (41, -113), (4, -7), (-7, 4), (91, -3), (1, 1), (1, -1),&#xa;        (4, 32), (17, 13), (123689, 1), (19, -570)])&#xa;    for a, b in u:&#xa;        w, x, y = ldescent(a, b)&#xa;        assert a*x**2 + b*y**2 == w**2&#xa;&#xa;&#xa;def test_diop_ternary_quadratic_normal():&#xa;    assert check_solutions(234*x**2 - 65601*y**2 - z**2)&#xa;    assert check_solutions(23*x**2 + 616*y**2 - z**2)&#xa;    assert check_solutions(5*x**2 + 4*y**2 - z**2)&#xa;    assert check_solutions(3*x**2 + 6*y**2 - 3*z**2)&#xa;    assert check_solutions(x**2 + 3*y**2 - z**2)&#xa;    assert check_solutions(4*x**2 + 5*y**2 - z**2)&#xa;    assert check_solutions(x**2 + y**2 - z**2)&#xa;    assert check_solutions(16*x**2 + y**2 - 25*z**2)&#xa;    assert check_solutions(6*x**2 - y**2 + 10*z**2)&#xa;    assert check_solutions(213*x**2 + 12*y**2 - 9*z**2)&#xa;    assert check_solutions(34*x**2 - 3*y**2 - 301*z**2)&#xa;    assert check_solutions(124*x**2 - 30*y**2 - 7729*z**2)&#xa;&#xa;&#xa;def is_normal_transformation_ok(eq):&#xa;    A = transformation_to_normal(eq)&#xa;    X, Y, Z = A*Matrix([x, y, z])&#xa;    simplified = _mexpand(Subs(eq, (x, y, z), (X, Y, Z)).doit())&#xa;&#xa;    coeff = dict([reversed(t.as_independent(*[X, Y, Z])) for t in simplified.args])&#xa;    for term in [X*Y, Y*Z, X*Z]:&#xa;        if term in coeff.keys():&#xa;            return False&#xa;&#xa;    return True&#xa;&#xa;&#xa;def test_transformation_to_normal():&#xa;    assert is_normal_transformation_ok(x**2 + 3*y**2 + z**2 - 13*x*y - 16*y*z + 12*x*z)&#xa;    assert is_normal_transformation_ok(x**2 + 3*y**2 - 100*z**2)&#xa;    assert is_normal_transformation_ok(x**2 + 23*y*z)&#xa;    assert is_normal_transformation_ok(3*y**2 - 100*z**2 - 12*x*y)&#xa;    assert is_normal_transformation_ok(x**2 + 23*x*y - 34*y*z + 12*x*z)&#xa;    assert is_normal_transformation_ok(z**2 + 34*x*y - 23*y*z + x*z)&#xa;    assert is_normal_transformation_ok(x**2 + y**2 + z**2 - x*y - y*z - x*z)&#xa;&#xa;&#xa;def test_diop_ternary_quadratic():&#xa;    # Commented out test cases should be uncommented after&#xa;    # the bug with factor_list() gets merged.&#xa;&#xa;    assert check_solutions(2*x**2 + z**2 + y**2 - 4*x*y)&#xa;    assert check_solutions(x**2 - y**2 - z**2 - x*y - y*z)&#xa;    assert check_solutions(3*x**2 - x*y - y*z - x*z)&#xa;    assert check_solutions(x**2 - y*z - x*z)&#xa;    #assert check_solutions(5*x**2 - 3*x*y - x*z)&#xa;    assert check_solutions(4*x**2 - 5*y**2 - x*z)&#xa;    assert check_solutions(3*x**2 + 2*y**2 - z**2 - 2*x*y + 5*y*z - 7*y*z)&#xa;    assert check_solutions(8*x**2 - 12*y*z)&#xa;    assert check_solutions(45*x**2 - 7*y**2 - 8*x*y - z**2)&#xa;    assert check_solutions(x**2 - 49*y**2 - z**2 + 13*z*y -8*x*y)&#xa;    assert check_solutions(90*x**2 + 3*y**2 + 5*x*y + 2*z*y + 5*x*z)&#xa;    assert check_solutions(x**2 + 3*y**2 + z**2 - x*y - 17*y*z)&#xa;    assert check_solutions(x**2 + 3*y**2 + z**2 - x*y - 16*y*z + 12*x*z)&#xa;    assert check_solutions(x**2 + 3*y**2 + z**2 - 13*x*y - 16*y*z + 12*x*z)&#xa;    assert check_solutions(x*y - 7*y*z + 13*x*z)&#xa;&#xa;&#xa;def test_pairwise_prime():&#xa;    assert pairwise_prime(6, 10, 15) == (5, 3, 2)&#xa;    assert pairwise_prime(2, 3, 5) == (2, 3, 5)&#xa;    assert pairwise_prime(1, 4, 7) == (1, 4, 7)&#xa;    assert pairwise_prime(4, 6, 5) == (1, 6, 5)&#xa;    assert pairwise_prime(6, 10, -15) == (5, 3, -2)&#xa;    assert pairwise_prime(-6, -10, -15) == (-5, -3, -2)&#xa;    assert pairwise_prime(4, -6, -5) == (1, -6, -5)&#xa;&#xa;&#xa;def test_square_factor():&#xa;    assert square_factor(1) == square_factor(-1) == 1&#xa;    assert square_factor(0) == 1&#xa;    assert square_factor(5) == square_factor(-5) == 1&#xa;    assert square_factor(4) == square_factor(-4) == 2&#xa;    assert square_factor(12) == square_factor(-12) == 2&#xa;    assert square_factor(6) == 1&#xa;    assert square_factor(18) == 3&#xa;    assert square_factor(52) == 2&#xa;    assert square_factor(49) == 7&#xa;    assert square_factor(392) == 14&#xa;&#xa;&#xa;def test_parametrize_ternary_quadratic():&#xa;    assert check_solutions(x**2 + y**2 - z**2)&#xa;    assert check_solutions(x**2 + 2*x*y + z**2)&#xa;    assert check_solutions(234*x**2 - 65601*y**2 - z**2)&#xa;    assert check_solutions(3*x**2 + 2*y**2 - z**2 - 2*x*y + 5*y*z - 7*y*z)&#xa;    assert check_solutions(x**2 - y**2 - z**2)&#xa;    assert check_solutions(x**2 - 49*y**2 - z**2 + 13*z*y - 8*x*y)&#xa;    assert check_solutions(8*x*y + z**2)&#xa;    assert check_solutions(124*x**2 - 30*y**2 - 7729*z**2)&#xa;    assert check_solutions(236*x**2 - 225*y**2 - 11*x*y - 13*y*z - 17*x*z)&#xa;    assert check_solutions(90*x**2 + 3*y**2 + 5*x*y + 2*z*y + 5*x*z)&#xa;    assert check_solutions(124*x**2 - 30*y**2 - 7729*z**2)&#xa;&#xa;&#xa;def test_no_square_ternary_quadratic():&#xa;    # Commented out test cases should be uncommented after&#xa;    # the bug with factor_list() gets merged.&#xa;&#xa;    assert check_solutions(2*x*y + y*z - 3*x*z)&#xa;    assert check_solutions(189*x*y - 345*y*z - 12*x*z)&#xa;    #assert check_solutions(23*x*y + 34*y*z)&#xa;    assert check_solutions(x*y + y*z + z*x)&#xa;    assert check_solutions(23*x*y + 23*y*z + 23*x*z)&#xa;&#xa;&#xa;def test_descent():&#xa;&#xa;    u = ([(13, 23), (3, -11), (41, -113), (91, -3), (1, 1), (1, -1), (17, 13), (123689, 1), (19, -570)])&#xa;    for a, b in u:&#xa;        w, x, y = descent(a, b)&#xa;        assert a*x**2 + b*y**2 == w**2&#xa;&#xa;&#xa;def test_diophantine():&#xa;    # Commented out test cases should be uncommented after&#xa;    # the bug with factor_list() gets merged.&#xa;&#xa;    assert check_solutions((x - y)*(y - z)*(z - x))&#xa;    assert check_solutions((x - y)*(x**2 + y**2 - z**2))&#xa;    assert check_solutions((x - 3*y + 7*z)*(x**2 + y**2 - z**2))&#xa;    assert check_solutions((x**2 - 3*y**2 - 1))&#xa;    #assert check_solutions(y**2 + 7*x*y)&#xa;    #assert check_solutions(x**2 - 3*x*y + y**2)&#xa;    #assert check_solutions(z*(x**2 - y**2 - 15))&#xa;    #assert check_solutions(x*(2*y - 2*z + 5))&#xa;    assert check_solutions((x**2 - 3*y**2 - 1)*(x**2 - y**2 - 15))&#xa;    assert check_solutions((x**2 - 3*y**2 - 1)*(y - 7*z))&#xa;    assert check_solutions((x**2 + y**2 - z**2)*(x - 7*y - 3*z + 4*w))&#xa;    # Following test case caused problems in parametric representation&#xa;    # But this can be solved by factroing out y.&#xa;    # No need to use methods for ternary quadratic equations.&#xa;    #assert check_solutions(y**2 - 7*x*y + 4*y*z)&#xa;    assert check_solutions(x**2 - 2*x + 1)&#xa;&#xa;&#xa;def test_general_pythagorean():&#xa;    from sympy.abc import a, b, c, d, e&#xa;&#xa;    assert check_solutions(a**2 + b**2 + c**2 - d**2)&#xa;    assert check_solutions(a**2 + 4*b**2 + 4*c**2 - d**2)&#xa;    assert check_solutions(9*a**2 + 4*b**2 + 4*c**2 - d**2)&#xa;    assert check_solutions(9*a**2 + 4*b**2 - 25*d**2 + 4*c**2 )&#xa;    assert check_solutions(9*a**2 - 16*d**2 + 4*b**2 + 4*c**2)&#xa;    assert check_solutions(-e**2 + 9*a**2 + 4*b**2 + 4*c**2 + 25*d**2)&#xa;    assert check_solutions(16*a**2 - b**2 + 9*c**2 + d**2 + 25*e**2)&#xa;&#xa;&#xa;def test_diop_general_sum_of_squares():&#xa;    from sympy.abc import a, b, c, d, e, f, g, h, i&#xa;&#xa;    assert check_solutions(a**2 + b**2 + c**2 - 5)&#xa;    assert check_solutions(a**2 + b**2 + c**2 - 57)&#xa;    assert check_solutions(a**2 + b**2 + c**2 - 349560695)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 - 304)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 - 23345)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 - 23345494)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 + e**2 - 1344545)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 + e**2 + f**2 - 6933949)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 + e**2 + f**2 + g**2 - 753934)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 + e**2 + f**2 + g**2 + h**2 - 5)&#xa;    assert check_solutions(a**2 + b**2 + c**2 + d**2 + e**2 + f**2 + g**2 + h**2 + i**2 - 693940)&#xa;&#xa;&#xa;def test_partition():&#xa;    tests = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]&#xa;&#xa;    for test in tests:&#xa;        f = partition(test)&#xa;        while True:&#xa;            try:&#xa;                l = next(f)&#xa;            except StopIteration:&#xa;                break&#xa;&#xa;    tests_k = [8, 10]&#xa;&#xa;    for test in tests_k:&#xa;        for k in range(8):&#xa;            f = partition(test, k)&#xa;&#xa;            while True:&#xa;                try:&#xa;                    l = next(f)&#xa;                    assert len(l) == k&#xa;                except StopIteration:&#xa;                    break&#xa;&#xa;&#xa;def test_prime_as_sum_of_two_squares():&#xa;    for i in [5, 13, 17, 29, 37, 41, 2341, 3557, 34841, 64601]:&#xa;        a, b = prime_as_sum_of_two_squares(i)&#xa;        assert a**2 + b**2 == i&#xa;&#xa;&#xa;def test_sum_of_three_squares():&#xa;    for i in [0, 1, 2, 34, 123, 34304595905, 34304595905394941, 343045959052344,&#xa;              800, 801, 802, 803, 804, 805, 806]:&#xa;        a, b, c = sum_of_three_squares(i)&#xa;        assert a**2 + b**2 + c**2 == i&#xa;&#xa;    assert sum_of_three_squares(7) == (None, None, None)&#xa;    assert sum_of_three_squares((4**5)*15) == (None, None, None)&#xa;&#xa;&#xa;def test_sum_of_four_squares():&#xa;    from random import randint&#xa;&#xa;    for i in range(10):&#xa;        n = randint(1, 100000000000000)&#xa;        a, b, c, d = sum_of_four_squares(n)&#xa;        assert a**2 + b**2 + c**2 + d**2 == n&#xa;&#xa;&#xa;def test_power_representation():&#xa;    tests = [(1729, 3, 2), (234, 2, 4), (2, 1, 2), (3, 1, 3), (5, 2, 2), (12352, 2, 4),&#xa;             (32760, 2, 3)]&#xa;&#xa;    for test in tests:&#xa;        n, p, k = test&#xa;        f = power_representation(n, p, k)&#xa;&#xa;        while True:&#xa;            try:&#xa;                l = next(f)&#xa;                assert len(l) == k&#xa;&#xa;                chk_sum = 0&#xa;                for l_i in l:&#xa;                    chk_sum = chk_sum + l_i**p&#xa;                assert chk_sum == n&#xa;&#xa;            except StopIteration:&#xa;                break&#xa;&#xa;&#xa;def test_assumptions():&#xa;    """"""&#xa;    Test whether diophantine respects the assumptions.&#xa;    """"""&#xa;    #Test case taken from the below so question regarding assumptions in diophantine module&#xa;    #http://stackoverflow.com/questions/23301941/how-can-i-declare-natural-symbols-with-sympy&#xa;    m, n = symbols('m n', integer=True, positive=True)&#xa;    diof = diophantine(n ** 2 + m * n - 500)&#xa;    assert diof == set([(5, 20), (40, 10), (95, 5), (121, 4), (248, 2), (499, 1)])&#xa;&#xa;    a, b = symbols('a b', integer=True, positive=False)&#xa;    diof = diophantine(a*b + 2*a + 3*b - 6)&#xa;    assert diof == set([(-15, -3), (-9, -4), (-7, -5), (-6, -6), (-5, -8), (-4, -14)])&#xa;&#xa;&#xa;def check_solutions(eq):&#xa;    """"""&#xa;    Determines whether solutions returned by diophantine() satisfy the original&#xa;    equation. Hope to generalize this so we can remove functions like check_ternay_quadratic,&#xa;    check_solutions_normal, check_solutions()&#xa;    """"""&#xa;    s = diophantine(eq)&#xa;&#xa;    terms = factor_list(eq)[1]&#xa;&#xa;    var = list(eq.free_symbols)&#xa;    var.sort(key=default_sort_key)&#xa;&#xa;    okay = True&#xa;&#xa;    while len(s) and okay:&#xa;        solution = s.pop()&#xa;&#xa;        okay = False&#xa;&#xa;        for term in terms:&#xa;            subeq = term[0]&#xa;&#xa;            if simplify(_mexpand(Subs(subeq, var, solution).doit())) == 0:&#xa;                okay = True&#xa;                break&#xa;&#xa;    return okay&#xa;&#xa;&#xa;def check_integrality(eq):&#xa;    """"""&#xa;    Check that the solutions returned by diophantine() are integers.&#xa;    This should be seldom needed except for general quadratic&#xa;    equations which are solved with rational transformations.&#xa;    """"""&#xa;    def _check_values(x):&#xa;        """""" Check a number of values. """"""&#xa;        for i in range(-4, 4):&#xa;            if not isinstance(simplify(x.subs(t, i)), Integer):&#xa;                return False&#xa;        return True&#xa;&#xa;    for soln in diophantine(eq, param=t):&#xa;        for x in soln:&#xa;            if not _check_values(x):&#xa;                return False&#xa;&#xa;    return True&#xa;"
267399|"# module pyparsing.py&#xa;#&#xa;# Copyright (c) 2003-2016  Paul T. McGuire&#xa;#&#xa;# Permission is hereby granted, free of charge, to any person obtaining&#xa;# a copy of this software and associated documentation files (the&#xa;# ""Software""), to deal in the Software without restriction, including&#xa;# without limitation the rights to use, copy, modify, merge, publish,&#xa;# distribute, sublicense, and/or sell copies of the Software, and to&#xa;# permit persons to whom the Software is furnished to do so, subject to&#xa;# the following conditions:&#xa;#&#xa;# The above copyright notice and this permission notice shall be&#xa;# included in all copies or substantial portions of the Software.&#xa;#&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,&#xa;# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF&#xa;# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.&#xa;# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY&#xa;# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,&#xa;# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE&#xa;# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;#&#xa;&#xa;__doc__ = \&#xa;""""""&#xa;pyparsing module - Classes and methods to define and execute parsing grammars&#xa;&#xa;The pyparsing module is an alternative approach to creating and executing simple grammars,&#xa;vs. the traditional lex/yacc approach, or the use of regular expressions.  With pyparsing, you&#xa;don't need to learn a new syntax for defining grammars or matching expressions - the parsing module&#xa;provides a library of classes that you use to construct the grammar directly in Python.&#xa;&#xa;Here is a program to parse ""Hello, World!"" (or any greeting of the form &#xa;C{""<salutation>, <addressee>!""}), built up using L{Word}, L{Literal}, and L{And} elements &#xa;(L{'+'<ParserElement.__add__>} operator gives L{And} expressions, strings are auto-converted to&#xa;L{Literal} expressions)::&#xa;&#xa;    from pyparsing import Word, alphas&#xa;&#xa;    # define grammar of a greeting&#xa;    greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;&#xa;    hello = ""Hello, World!""&#xa;    print (hello, ""->"", greet.parseString(hello))&#xa;&#xa;The program outputs the following::&#xa;&#xa;    Hello, World! -> ['Hello', ',', 'World', '!']&#xa;&#xa;The Python representation of the grammar is quite readable, owing to the self-explanatory&#xa;class names, and the use of '+', '|' and '^' operators.&#xa;&#xa;The L{ParseResults} object returned from L{ParserElement.parseString<ParserElement.parseString>} can be accessed as a nested list, a dictionary, or an&#xa;object with named attributes.&#xa;&#xa;The pyparsing module handles some of the problems that are typically vexing when writing text parsers:&#xa; - extra or missing whitespace (the above program will also handle ""Hello,World!"", ""Hello  ,  World  !"", etc.)&#xa; - quoted strings&#xa; - embedded comments&#xa;""""""&#xa;&#xa;__version__ = ""2.1.10""&#xa;__versionTime__ = ""07 Oct 2016 01:31 UTC""&#xa;__author__ = ""Paul McGuire <ptmcg@users.sourceforge.net>""&#xa;&#xa;import string&#xa;from weakref import ref as wkref&#xa;import copy&#xa;import sys&#xa;import warnings&#xa;import re&#xa;import sre_constants&#xa;import collections&#xa;import pprint&#xa;import traceback&#xa;import types&#xa;from datetime import datetime&#xa;&#xa;try:&#xa;    from _thread import RLock&#xa;except ImportError:&#xa;    from threading import RLock&#xa;&#xa;try:&#xa;    from collections import OrderedDict as _OrderedDict&#xa;except ImportError:&#xa;    try:&#xa;        from ordereddict import OrderedDict as _OrderedDict&#xa;    except ImportError:&#xa;        _OrderedDict = None&#xa;&#xa;#~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )&#xa;&#xa;__all__ = [&#xa;'And', 'CaselessKeyword', 'CaselessLiteral', 'CharsNotIn', 'Combine', 'Dict', 'Each', 'Empty',&#xa;'FollowedBy', 'Forward', 'GoToColumn', 'Group', 'Keyword', 'LineEnd', 'LineStart', 'Literal',&#xa;'MatchFirst', 'NoMatch', 'NotAny', 'OneOrMore', 'OnlyOnce', 'Optional', 'Or',&#xa;'ParseBaseException', 'ParseElementEnhance', 'ParseException', 'ParseExpression', 'ParseFatalException',&#xa;'ParseResults', 'ParseSyntaxException', 'ParserElement', 'QuotedString', 'RecursiveGrammarException',&#xa;'Regex', 'SkipTo', 'StringEnd', 'StringStart', 'Suppress', 'Token', 'TokenConverter', &#xa;'White', 'Word', 'WordEnd', 'WordStart', 'ZeroOrMore',&#xa;'alphanums', 'alphas', 'alphas8bit', 'anyCloseTag', 'anyOpenTag', 'cStyleComment', 'col',&#xa;'commaSeparatedList', 'commonHTMLEntity', 'countedArray', 'cppStyleComment', 'dblQuotedString',&#xa;'dblSlashComment', 'delimitedList', 'dictOf', 'downcaseTokens', 'empty', 'hexnums',&#xa;'htmlComment', 'javaStyleComment', 'line', 'lineEnd', 'lineStart', 'lineno',&#xa;'makeHTMLTags', 'makeXMLTags', 'matchOnlyAtCol', 'matchPreviousExpr', 'matchPreviousLiteral',&#xa;'nestedExpr', 'nullDebugAction', 'nums', 'oneOf', 'opAssoc', 'operatorPrecedence', 'printables',&#xa;'punc8bit', 'pythonStyleComment', 'quotedString', 'removeQuotes', 'replaceHTMLEntity', &#xa;'replaceWith', 'restOfLine', 'sglQuotedString', 'srange', 'stringEnd',&#xa;'stringStart', 'traceParseAction', 'unicodeString', 'upcaseTokens', 'withAttribute',&#xa;'indentedBlock', 'originalTextFor', 'ungroup', 'infixNotation','locatedExpr', 'withClass',&#xa;'CloseMatch', 'tokenMap', 'pyparsing_common',&#xa;]&#xa;&#xa;system_version = tuple(sys.version_info)[:3]&#xa;PY_3 = system_version[0] == 3&#xa;if PY_3:&#xa;    _MAX_INT = sys.maxsize&#xa;    basestring = str&#xa;    unichr = chr&#xa;    _ustr = str&#xa;&#xa;    # build list of single arg builtins, that can be used as parse actions&#xa;    singleArgBuiltins = [sum, len, sorted, reversed, list, tuple, set, any, all, min, max]&#xa;&#xa;else:&#xa;    _MAX_INT = sys.maxint&#xa;    range = xrange&#xa;&#xa;    def _ustr(obj):&#xa;        """"""Drop-in replacement for str(obj) that tries to be Unicode friendly. It first tries&#xa;           str(obj). If that fails with a UnicodeEncodeError, then it tries unicode(obj). It&#xa;           then < returns the unicode object | encodes it with the default encoding | ... >.&#xa;        """"""&#xa;        if isinstance(obj,unicode):&#xa;            return obj&#xa;&#xa;        try:&#xa;            # If this works, then _ustr(obj) has the same behaviour as str(obj), so&#xa;            # it won't break any existing code.&#xa;            return str(obj)&#xa;&#xa;        except UnicodeEncodeError:&#xa;            # Else encode it&#xa;            ret = unicode(obj).encode(sys.getdefaultencoding(), 'xmlcharrefreplace')&#xa;            xmlcharref = Regex('&#\d+;')&#xa;            xmlcharref.setParseAction(lambda t: '\\u' + hex(int(t[0][2:-1]))[2:])&#xa;            return xmlcharref.transformString(ret)&#xa;&#xa;    # build list of single arg builtins, tolerant of Python version, that can be used as parse actions&#xa;    singleArgBuiltins = []&#xa;    import __builtin__&#xa;    for fname in ""sum len sorted reversed list tuple set any all min max"".split():&#xa;        try:&#xa;            singleArgBuiltins.append(getattr(__builtin__,fname))&#xa;        except AttributeError:&#xa;            continue&#xa;            &#xa;_generatorType = type((y for y in range(1)))&#xa; &#xa;def _xml_escape(data):&#xa;    """"""Escape &, <, >, "", ', etc. in a string of data.""""""&#xa;&#xa;    # ampersand must be replaced first&#xa;    from_symbols = '&><""\''&#xa;    to_symbols = ('&'+s+';' for s in ""amp gt lt quot apos"".split())&#xa;    for from_,to_ in zip(from_symbols, to_symbols):&#xa;        data = data.replace(from_, to_)&#xa;    return data&#xa;&#xa;class _Constants(object):&#xa;    pass&#xa;&#xa;alphas     = string.ascii_uppercase + string.ascii_lowercase&#xa;nums       = ""0123456789""&#xa;hexnums    = nums + ""ABCDEFabcdef""&#xa;alphanums  = alphas + nums&#xa;_bslash    = chr(92)&#xa;printables = """".join(c for c in string.printable if c not in string.whitespace)&#xa;&#xa;class ParseBaseException(Exception):&#xa;    """"""base exception class for all parsing runtime exceptions""""""&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, pstr, loc=0, msg=None, elem=None ):&#xa;        self.loc = loc&#xa;        if msg is None:&#xa;            self.msg = pstr&#xa;            self.pstr = """"&#xa;        else:&#xa;            self.msg = msg&#xa;            self.pstr = pstr&#xa;        self.parserElement = elem&#xa;        self.args = (pstr, loc, msg)&#xa;&#xa;    @classmethod&#xa;    def _from_exception(cls, pe):&#xa;        """"""&#xa;        internal factory method to simplify creating one type of ParseException &#xa;        from another - avoids having __init__ signature conflicts among subclasses&#xa;        """"""&#xa;        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)&#xa;&#xa;    def __getattr__( self, aname ):&#xa;        """"""supported attributes by name are:&#xa;            - lineno - returns the line number of the exception text&#xa;            - col - returns the column number of the exception text&#xa;            - line - returns the line containing the exception text&#xa;        """"""&#xa;        if( aname == ""lineno"" ):&#xa;            return lineno( self.loc, self.pstr )&#xa;        elif( aname in (""col"", ""column"") ):&#xa;            return col( self.loc, self.pstr )&#xa;        elif( aname == ""line"" ):&#xa;            return line( self.loc, self.pstr )&#xa;        else:&#xa;            raise AttributeError(aname)&#xa;&#xa;    def __str__( self ):&#xa;        return ""%s (at char %d), (line:%d, col:%d)"" % \&#xa;                ( self.msg, self.loc, self.lineno, self.column )&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;    def markInputline( self, markerString = "">!<"" ):&#xa;        """"""Extracts the exception line from the input string, and marks&#xa;           the location of the exception with a special symbol.&#xa;        """"""&#xa;        line_str = self.line&#xa;        line_column = self.column - 1&#xa;        if markerString:&#xa;            line_str = """".join((line_str[:line_column],&#xa;                                markerString, line_str[line_column:]))&#xa;        return line_str.strip()&#xa;    def __dir__(self):&#xa;        return ""lineno col line"".split() + dir(type(self))&#xa;&#xa;class ParseException(ParseBaseException):&#xa;    """"""&#xa;    Exception thrown when parse expressions don't match class;&#xa;    supported attributes by name are:&#xa;     - lineno - returns the line number of the exception text&#xa;     - col - returns the column number of the exception text&#xa;     - line - returns the line containing the exception text&#xa;        &#xa;    Example::&#xa;        try:&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")&#xa;        except ParseException as pe:&#xa;            print(pe)&#xa;            print(""column: {}"".format(pe.col))&#xa;            &#xa;    prints::&#xa;       Expected integer (at char 0), (line:1, col:1)&#xa;        column: 1&#xa;    """"""&#xa;    pass&#xa;&#xa;class ParseFatalException(ParseBaseException):&#xa;    """"""user-throwable exception thrown when inconsistent parse content&#xa;       is found; stops all parsing immediately""""""&#xa;    pass&#xa;&#xa;class ParseSyntaxException(ParseFatalException):&#xa;    """"""just like L{ParseFatalException}, but thrown internally when an&#xa;       L{ErrorStop<And._ErrorStop>} ('-' operator) indicates that parsing is to stop &#xa;       immediately because an unbacktrackable syntax error has been found""""""&#xa;    pass&#xa;&#xa;#~ class ReparseException(ParseBaseException):&#xa;    #~ """"""Experimental class - parse actions can raise this exception to cause&#xa;       #~ pyparsing to reparse the input string:&#xa;        #~ - with a modified input string, and/or&#xa;        #~ - with a modified start location&#xa;       #~ Set the values of the ReparseException in the constructor, and raise the&#xa;       #~ exception in a parse action to cause pyparsing to use the new string/location.&#xa;       #~ Setting the values as None causes no change to be made.&#xa;       #~ """"""&#xa;    #~ def __init_( self, newstring, restartLoc ):&#xa;        #~ self.newParseText = newstring&#xa;        #~ self.reparseLoc = restartLoc&#xa;&#xa;class RecursiveGrammarException(Exception):&#xa;    """"""exception thrown by L{ParserElement.validate} if the grammar could be improperly recursive""""""&#xa;    def __init__( self, parseElementList ):&#xa;        self.parseElementTrace = parseElementList&#xa;&#xa;    def __str__( self ):&#xa;        return ""RecursiveGrammarException: %s"" % self.parseElementTrace&#xa;&#xa;class _ParseResultsWithOffset(object):&#xa;    def __init__(self,p1,p2):&#xa;        self.tup = (p1,p2)&#xa;    def __getitem__(self,i):&#xa;        return self.tup[i]&#xa;    def __repr__(self):&#xa;        return repr(self.tup[0])&#xa;    def setOffset(self,i):&#xa;        self.tup = (self.tup[0],i)&#xa;&#xa;class ParseResults(object):&#xa;    """"""&#xa;    Structured parse results, to provide multiple means of access to the parsed data:&#xa;       - as a list (C{len(results)})&#xa;       - by list index (C{results[0], results[1]}, etc.)&#xa;       - by attribute (C{results.<resultsName>} - see L{ParserElement.setResultsName})&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;        # equivalent form:&#xa;        # date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;&#xa;        # parseString returns a ParseResults object&#xa;        result = date_str.parseString(""1999/12/31"")&#xa;&#xa;        def test(s, fn=repr):&#xa;            print(""%s -> %s"" % (s, fn(eval(s))))&#xa;        test(""list(result)"")&#xa;        test(""result[0]"")&#xa;        test(""result['month']"")&#xa;        test(""result.day"")&#xa;        test(""'month' in result"")&#xa;        test(""'minutes' in result"")&#xa;        test(""result.dump()"", str)&#xa;    prints::&#xa;        list(result) -> ['1999', '/', '12', '/', '31']&#xa;        result[0] -> '1999'&#xa;        result['month'] -> '12'&#xa;        result.day -> '31'&#xa;        'month' in result -> True&#xa;        'minutes' in result -> False&#xa;        result.dump() -> ['1999', '/', '12', '/', '31']&#xa;        - day: 31&#xa;        - month: 12&#xa;        - year: 1999&#xa;    """"""&#xa;    def __new__(cls, toklist=None, name=None, asList=True, modal=True ):&#xa;        if isinstance(toklist, cls):&#xa;            return toklist&#xa;        retobj = object.__new__(cls)&#xa;        retobj.__doinit = True&#xa;        return retobj&#xa;&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance ):&#xa;        if self.__doinit:&#xa;            self.__doinit = False&#xa;            self.__name = None&#xa;            self.__parent = None&#xa;            self.__accumNames = {}&#xa;            self.__asList = asList&#xa;            self.__modal = modal&#xa;            if toklist is None:&#xa;                toklist = []&#xa;            if isinstance(toklist, list):&#xa;                self.__toklist = toklist[:]&#xa;            elif isinstance(toklist, _generatorType):&#xa;                self.__toklist = list(toklist)&#xa;            else:&#xa;                self.__toklist = [toklist]&#xa;            self.__tokdict = dict()&#xa;&#xa;        if name is not None and name:&#xa;            if not modal:&#xa;                self.__accumNames[name] = 0&#xa;            if isinstance(name,int):&#xa;                name = _ustr(name) # will always return a str, but use _ustr for consistency&#xa;            self.__name = name&#xa;            if not (isinstance(toklist, (type(None), basestring, list)) and toklist in (None,'',[])):&#xa;                if isinstance(toklist,basestring):&#xa;                    toklist = [ toklist ]&#xa;                if asList:&#xa;                    if isinstance(toklist,ParseResults):&#xa;                        self[name] = _ParseResultsWithOffset(toklist.copy(),0)&#xa;                    else:&#xa;                        self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]),0)&#xa;                    self[name].__name = name&#xa;                else:&#xa;                    try:&#xa;                        self[name] = toklist[0]&#xa;                    except (KeyError,TypeError,IndexError):&#xa;                        self[name] = toklist&#xa;&#xa;    def __getitem__( self, i ):&#xa;        if isinstance( i, (int,slice) ):&#xa;            return self.__toklist[i]&#xa;        else:&#xa;            if i not in self.__accumNames:&#xa;                return self.__tokdict[i][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[i] ])&#xa;&#xa;    def __setitem__( self, k, v, isinstance=isinstance ):&#xa;        if isinstance(v,_ParseResultsWithOffset):&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]&#xa;            sub = v[0]&#xa;        elif isinstance(k,(int,slice)):&#xa;            self.__toklist[k] = v&#xa;            sub = v&#xa;        else:&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [_ParseResultsWithOffset(v,0)]&#xa;            sub = v&#xa;        if isinstance(sub,ParseResults):&#xa;            sub.__parent = wkref(self)&#xa;&#xa;    def __delitem__( self, i ):&#xa;        if isinstance(i,(int,slice)):&#xa;            mylen = len( self.__toklist )&#xa;            del self.__toklist[i]&#xa;&#xa;            # convert int to slice&#xa;            if isinstance(i, int):&#xa;                if i < 0:&#xa;                    i += mylen&#xa;                i = slice(i, i+1)&#xa;            # get removed indices&#xa;            removed = list(range(*i.indices(mylen)))&#xa;            removed.reverse()&#xa;            # fixup indices in token dictionary&#xa;            for name,occurrences in self.__tokdict.items():&#xa;                for j in removed:&#xa;                    for k, (value, position) in enumerate(occurrences):&#xa;                        occurrences[k] = _ParseResultsWithOffset(value, position - (position > j))&#xa;        else:&#xa;            del self.__tokdict[i]&#xa;&#xa;    def __contains__( self, k ):&#xa;        return k in self.__tokdict&#xa;&#xa;    def __len__( self ): return len( self.__toklist )&#xa;    def __bool__(self): return ( not not self.__toklist )&#xa;    __nonzero__ = __bool__&#xa;    def __iter__( self ): return iter( self.__toklist )&#xa;    def __reversed__( self ): return iter( self.__toklist[::-1] )&#xa;    def _iterkeys( self ):&#xa;        if hasattr(self.__tokdict, ""iterkeys""):&#xa;            return self.__tokdict.iterkeys()&#xa;        else:&#xa;            return iter(self.__tokdict)&#xa;&#xa;    def _itervalues( self ):&#xa;        return (self[k] for k in self._iterkeys())&#xa;            &#xa;    def _iteritems( self ):&#xa;        return ((k, self[k]) for k in self._iterkeys())&#xa;&#xa;    if PY_3:&#xa;        keys = _iterkeys       &#xa;        """"""Returns an iterator of all named result keys (Python 3.x only).""""""&#xa;&#xa;        values = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 3.x only).""""""&#xa;&#xa;        items = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 3.x only).""""""&#xa;&#xa;    else:&#xa;        iterkeys = _iterkeys&#xa;        """"""Returns an iterator of all named result keys (Python 2.x only).""""""&#xa;&#xa;        itervalues = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 2.x only).""""""&#xa;&#xa;        iteritems = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 2.x only).""""""&#xa;&#xa;        def keys( self ):&#xa;            """"""Returns all named result keys (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iterkeys())&#xa;&#xa;        def values( self ):&#xa;            """"""Returns all named result values (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.itervalues())&#xa;                &#xa;        def items( self ):&#xa;            """"""Returns all named result key-values (as a list of tuples in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iteritems())&#xa;&#xa;    def haskeys( self ):&#xa;        """"""Since keys() returns an iterator, this method is helpful in bypassing&#xa;           code that looks for the existence of any defined results names.""""""&#xa;        return bool(self.__tokdict)&#xa;        &#xa;    def pop( self, *args, **kwargs):&#xa;        """"""&#xa;        Removes and returns item at specified index (default=C{last}).&#xa;        Supports both C{list} and C{dict} semantics for C{pop()}. If passed no&#xa;        argument or an integer argument, it will use C{list} semantics&#xa;        and pop tokens from the list of parsed tokens. If passed a &#xa;        non-integer argument (most likely a string), it will use C{dict}&#xa;        semantics and pop the corresponding value from any defined &#xa;        results names. A second default return value argument is &#xa;        supported, just as in C{dict.pop()}.&#xa;&#xa;        Example::&#xa;            def remove_first(tokens):&#xa;                tokens.pop(0)&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(""0 123 321"")) # -> ['123', '321']&#xa;&#xa;            label = Word(alphas)&#xa;            patt = label(""LABEL"") + OneOrMore(Word(nums))&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;&#xa;            # Use pop() in a parse action to remove named result (note that corresponding value is not&#xa;            # removed from list form of results)&#xa;            def remove_LABEL(tokens):&#xa;                tokens.pop(""LABEL"")&#xa;                return tokens&#xa;            patt.addParseAction(remove_LABEL)&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;        prints::&#xa;            ['AAB', '123', '321']&#xa;            - LABEL: AAB&#xa;&#xa;            ['AAB', '123', '321']&#xa;        """"""&#xa;        if not args:&#xa;            args = [-1]&#xa;        for k,v in kwargs.items():&#xa;            if k == 'default':&#xa;                args = (args[0], v)&#xa;            else:&#xa;                raise TypeError(""pop() got an unexpected keyword argument '%s'"" % k)&#xa;        if (isinstance(args[0], int) or &#xa;                        len(args) == 1 or &#xa;                        args[0] in self):&#xa;            index = args[0]&#xa;            ret = self[index]&#xa;            del self[index]&#xa;            return ret&#xa;        else:&#xa;            defaultvalue = args[1]&#xa;            return defaultvalue&#xa;&#xa;    def get(self, key, defaultValue=None):&#xa;        """"""&#xa;        Returns named result matching the given key, or if there is no&#xa;        such name, then returns the given C{defaultValue} or C{None} if no&#xa;        C{defaultValue} is specified.&#xa;&#xa;        Similar to C{dict.get()}.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            result = date_str.parseString(""1999/12/31"")&#xa;            print(result.get(""year"")) # -> '1999'&#xa;            print(result.get(""hour"", ""not specified"")) # -> 'not specified'&#xa;            print(result.get(""hour"")) # -> None&#xa;        """"""&#xa;        if key in self:&#xa;            return self[key]&#xa;        else:&#xa;            return defaultValue&#xa;&#xa;    def insert( self, index, insStr ):&#xa;        """"""&#xa;        Inserts new element at location index in the list of parsed tokens.&#xa;        &#xa;        Similar to C{list.insert()}.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;&#xa;            # use a parse action to insert the parse location in the front of the parsed results&#xa;            def insert_locn(locn, tokens):&#xa;                tokens.insert(0, locn)&#xa;            print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString(""0 123 321"")) # -> [0, '0', '123', '321']&#xa;        """"""&#xa;        self.__toklist.insert(index, insStr)&#xa;        # fixup indices in token dictionary&#xa;        for name,occurrences in self.__tokdict.items():&#xa;            for k, (value, position) in enumerate(occurrences):&#xa;                occurrences[k] = _ParseResultsWithOffset(value, position + (position > index))&#xa;&#xa;    def append( self, item ):&#xa;        """"""&#xa;        Add single element to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            &#xa;            # use a parse action to compute the sum of the parsed integers, and add it to the end&#xa;            def append_sum(tokens):&#xa;                tokens.append(sum(map(int, tokens)))&#xa;            print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString(""0 123 321"")) # -> ['0', '123', '321', 444]&#xa;        """"""&#xa;        self.__toklist.append(item)&#xa;&#xa;    def extend( self, itemseq ):&#xa;        """"""&#xa;        Add sequence of elements to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            &#xa;            # use a parse action to append the reverse of the matched strings, to make a palindrome&#xa;            def make_palindrome(tokens):&#xa;                tokens.extend(reversed([t[::-1] for t in tokens]))&#xa;                return ''.join(tokens)&#xa;            print(patt.addParseAction(make_palindrome).parseString(""lskdj sdlkjf lksd"")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'&#xa;        """"""&#xa;        if isinstance(itemseq, ParseResults):&#xa;            self += itemseq&#xa;        else:&#xa;            self.__toklist.extend(itemseq)&#xa;&#xa;    def clear( self ):&#xa;        """"""&#xa;        Clear all elements and results names.&#xa;        """"""&#xa;        del self.__toklist[:]&#xa;        self.__tokdict.clear()&#xa;&#xa;    def __getattr__( self, name ):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            return """"&#xa;            &#xa;        if name in self.__tokdict:&#xa;            if name not in self.__accumNames:&#xa;                return self.__tokdict[name][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[name] ])&#xa;        else:&#xa;            return """"&#xa;&#xa;    def __add__( self, other ):&#xa;        ret = self.copy()&#xa;        ret += other&#xa;        return ret&#xa;&#xa;    def __iadd__( self, other ):&#xa;        if other.__tokdict:&#xa;            offset = len(self.__toklist)&#xa;            addoffset = lambda a: offset if a<0 else a+offset&#xa;            otheritems = other.__tokdict.items()&#xa;            otherdictitems = [(k, _ParseResultsWithOffset(v[0],addoffset(v[1])) )&#xa;                                for (k,vlist) in otheritems for v in vlist]&#xa;            for k,v in otherdictitems:&#xa;                self[k] = v&#xa;                if isinstance(v[0],ParseResults):&#xa;                    v[0].__parent = wkref(self)&#xa;            &#xa;        self.__toklist += other.__toklist&#xa;        self.__accumNames.update( other.__accumNames )&#xa;        return self&#xa;&#xa;    def __radd__(self, other):&#xa;        if isinstance(other,int) and other == 0:&#xa;            # useful for merging many ParseResults using sum() builtin&#xa;            return self.copy()&#xa;        else:&#xa;            # this may raise a TypeError - so be it&#xa;            return other + self&#xa;        &#xa;    def __repr__( self ):&#xa;        return ""(%s, %s)"" % ( repr( self.__toklist ), repr( self.__tokdict ) )&#xa;&#xa;    def __str__( self ):&#xa;        return '[' + ', '.join(_ustr(i) if isinstance(i, ParseResults) else repr(i) for i in self.__toklist) + ']'&#xa;&#xa;    def _asStringList( self, sep='' ):&#xa;        out = []&#xa;        for item in self.__toklist:&#xa;            if out and sep:&#xa;                out.append(sep)&#xa;            if isinstance( item, ParseResults ):&#xa;                out += item._asStringList()&#xa;            else:&#xa;                out.append( _ustr(item) )&#xa;        return out&#xa;&#xa;    def asList( self ):&#xa;        """"""&#xa;        Returns the parse results as a nested list of matching tokens, all converted to strings.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            result = patt.parseString(""sldkj lsdkj sldkj"")&#xa;            # even though the result prints in string-like form, it is actually a pyparsing ParseResults&#xa;            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']&#xa;            &#xa;            # Use asList() to create an actual list&#xa;            result_list = result.asList()&#xa;            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']&#xa;        """"""&#xa;        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]&#xa;&#xa;    def asDict( self ):&#xa;        """"""&#xa;        Returns the named parse results as a nested dictionary.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})&#xa;            &#xa;            result_dict = result.asDict()&#xa;            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}&#xa;&#xa;            # even though a ParseResults supports dict-like access, sometime you just need to have a dict&#xa;            import json&#xa;            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable&#xa;            print(json.dumps(result.asDict())) # -> {""month"": ""31"", ""day"": ""1999"", ""year"": ""12""}&#xa;        """"""&#xa;        if PY_3:&#xa;            item_fn = self.items&#xa;        else:&#xa;            item_fn = self.iteritems&#xa;            &#xa;        def toItem(obj):&#xa;            if isinstance(obj, ParseResults):&#xa;                if obj.haskeys():&#xa;                    return obj.asDict()&#xa;                else:&#xa;                    return [toItem(v) for v in obj]&#xa;            else:&#xa;                return obj&#xa;                &#xa;        return dict((k,toItem(v)) for k,v in item_fn())&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Returns a new copy of a C{ParseResults} object.&#xa;        """"""&#xa;        ret = ParseResults( self.__toklist )&#xa;        ret.__tokdict = self.__tokdict.copy()&#xa;        ret.__parent = self.__parent&#xa;        ret.__accumNames.update( self.__accumNames )&#xa;        ret.__name = self.__name&#xa;        return ret&#xa;&#xa;    def asXML( self, doctag=None, namedItemsOnly=False, indent="""", formatted=True ):&#xa;        """"""&#xa;        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.&#xa;        """"""&#xa;        nl = ""\n""&#xa;        out = []&#xa;        namedItems = dict((v[1],k) for (k,vlist) in self.__tokdict.items()&#xa;                                                            for v in vlist)&#xa;        nextLevelIndent = indent + ""  ""&#xa;&#xa;        # collapse out indents if formatting is not desired&#xa;        if not formatted:&#xa;            indent = """"&#xa;            nextLevelIndent = """"&#xa;            nl = """"&#xa;&#xa;        selfTag = None&#xa;        if doctag is not None:&#xa;            selfTag = doctag&#xa;        else:&#xa;            if self.__name:&#xa;                selfTag = self.__name&#xa;&#xa;        if not selfTag:&#xa;            if namedItemsOnly:&#xa;                return """"&#xa;            else:&#xa;                selfTag = ""ITEM""&#xa;&#xa;        out += [ nl, indent, ""<"", selfTag, "">"" ]&#xa;&#xa;        for i,res in enumerate(self.__toklist):&#xa;            if isinstance(res,ParseResults):&#xa;                if i in namedItems:&#xa;                    out += [ res.asXML(namedItems[i],&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;                else:&#xa;                    out += [ res.asXML(None,&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;            else:&#xa;                # individual token, see if there is a name for it&#xa;                resTag = None&#xa;                if i in namedItems:&#xa;                    resTag = namedItems[i]&#xa;                if not resTag:&#xa;                    if namedItemsOnly:&#xa;                        continue&#xa;                    else:&#xa;                        resTag = ""ITEM""&#xa;                xmlBodyText = _xml_escape(_ustr(res))&#xa;                out += [ nl, nextLevelIndent, ""<"", resTag, "">"",&#xa;                                                xmlBodyText,&#xa;                                                ""</"", resTag, "">"" ]&#xa;&#xa;        out += [ nl, indent, ""</"", selfTag, "">"" ]&#xa;        return """".join(out)&#xa;&#xa;    def __lookup(self,sub):&#xa;        for k,vlist in self.__tokdict.items():&#xa;            for v,loc in vlist:&#xa;                if sub is v:&#xa;                    return k&#xa;        return None&#xa;&#xa;    def getName(self):&#xa;        """"""&#xa;        Returns the results name for this token expression. Useful when several &#xa;        different expressions might match at a particular location.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            ssn_expr = Regex(r""\d\d\d-\d\d-\d\d\d\d"")&#xa;            house_number_expr = Suppress('#') + Word(nums, alphanums)&#xa;            user_data = (Group(house_number_expr)(""house_number"") &#xa;                        | Group(ssn_expr)(""ssn"")&#xa;                        | Group(integer)(""age""))&#xa;            user_info = OneOrMore(user_data)&#xa;            &#xa;            result = user_info.parseString(""22 111-22-3333 #221B"")&#xa;            for item in result:&#xa;                print(item.getName(), ':', item[0])&#xa;        prints::&#xa;            age : 22&#xa;            ssn : 111-22-3333&#xa;            house_number : 221B&#xa;        """"""&#xa;        if self.__name:&#xa;            return self.__name&#xa;        elif self.__parent:&#xa;            par = self.__parent()&#xa;            if par:&#xa;                return par.__lookup(self)&#xa;            else:&#xa;                return None&#xa;        elif (len(self) == 1 and&#xa;               len(self.__tokdict) == 1 and&#xa;               next(iter(self.__tokdict.values()))[0][1] in (0,-1)):&#xa;            return next(iter(self.__tokdict.keys()))&#xa;        else:&#xa;            return None&#xa;&#xa;    def dump(self, indent='', depth=0, full=True):&#xa;        """"""&#xa;        Diagnostic method for listing out the contents of a C{ParseResults}.&#xa;        Accepts an optional C{indent} argument so that this string can be embedded&#xa;        in a nested display of other data.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(result.dump())&#xa;        prints::&#xa;            ['12', '/', '31', '/', '1999']&#xa;            - day: 1999&#xa;            - month: 31&#xa;            - year: 12&#xa;        """"""&#xa;        out = []&#xa;        NL = '\n'&#xa;        out.append( indent+_ustr(self.asList()) )&#xa;        if full:&#xa;            if self.haskeys():&#xa;                items = sorted((str(k), v) for k,v in self.items())&#xa;                for k,v in items:&#xa;                    if out:&#xa;                        out.append(NL)&#xa;                    out.append( ""%s%s- %s: "" % (indent,('  '*depth), k) )&#xa;                    if isinstance(v,ParseResults):&#xa;                        if v:&#xa;                            out.append( v.dump(indent,depth+1) )&#xa;                        else:&#xa;                            out.append(_ustr(v))&#xa;                    else:&#xa;                        out.append(repr(v))&#xa;            elif any(isinstance(vv,ParseResults) for vv in self):&#xa;                v = self&#xa;                for i,vv in enumerate(v):&#xa;                    if isinstance(vv,ParseResults):&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),vv.dump(indent,depth+1) ))&#xa;                    else:&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),_ustr(vv)))&#xa;            &#xa;        return """".join(out)&#xa;&#xa;    def pprint(self, *args, **kwargs):&#xa;        """"""&#xa;        Pretty-printer for parsed results as a list, using the C{pprint} module.&#xa;        Accepts additional positional or keyword args as defined for the &#xa;        C{pprint.pprint} method. (U{http://docs.python.org/3/library/pprint.html#pprint.pprint})&#xa;&#xa;        Example::&#xa;            ident = Word(alphas, alphanums)&#xa;            num = Word(nums)&#xa;            func = Forward()&#xa;            term = ident | num | Group('(' + func + ')')&#xa;            func <<= ident + Group(Optional(delimitedList(term)))&#xa;            result = func.parseString(""fna a,b,(fnb c,d,200),100"")&#xa;            result.pprint(width=40)&#xa;        prints::&#xa;            ['fna',&#xa;             ['a',&#xa;              'b',&#xa;              ['(', 'fnb', ['c', 'd', '200'], ')'],&#xa;              '100']]&#xa;        """"""&#xa;        pprint.pprint(self.asList(), *args, **kwargs)&#xa;&#xa;    # add support for pickle protocol&#xa;    def __getstate__(self):&#xa;        return ( self.__toklist,&#xa;                 ( self.__tokdict.copy(),&#xa;                   self.__parent is not None and self.__parent() or None,&#xa;                   self.__accumNames,&#xa;                   self.__name ) )&#xa;&#xa;    def __setstate__(self,state):&#xa;        self.__toklist = state[0]&#xa;        (self.__tokdict,&#xa;         par,&#xa;         inAccumNames,&#xa;         self.__name) = state[1]&#xa;        self.__accumNames = {}&#xa;        self.__accumNames.update(inAccumNames)&#xa;        if par is not None:&#xa;            self.__parent = wkref(par)&#xa;        else:&#xa;            self.__parent = None&#xa;&#xa;    def __getnewargs__(self):&#xa;        return self.__toklist, self.__name, self.__asList, self.__modal&#xa;&#xa;    def __dir__(self):&#xa;        return (dir(type(self)) + list(self.keys()))&#xa;&#xa;collections.MutableMapping.register(ParseResults)&#xa;&#xa;def col (loc,strg):&#xa;    """"""Returns current column within a string, counting newlines as line separators.&#xa;   The first column is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    s = strg&#xa;    return 1 if 0<loc<len(s) and s[loc-1] == '\n' else loc - s.rfind(""\n"", 0, loc)&#xa;&#xa;def lineno(loc,strg):&#xa;    """"""Returns current line number within a string, counting newlines as line separators.&#xa;   The first line is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    return strg.count(""\n"",0,loc) + 1&#xa;&#xa;def line( loc, strg ):&#xa;    """"""Returns the line of text containing loc within a string, counting newlines as line separators.&#xa;       """"""&#xa;    lastCR = strg.rfind(""\n"", 0, loc)&#xa;    nextCR = strg.find(""\n"", loc)&#xa;    if nextCR >= 0:&#xa;        return strg[lastCR+1:nextCR]&#xa;    else:&#xa;        return strg[lastCR+1:]&#xa;&#xa;def _defaultStartDebugAction( instring, loc, expr ):&#xa;    print ((""Match "" + _ustr(expr) + "" at loc "" + _ustr(loc) + ""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) )))&#xa;&#xa;def _defaultSuccessDebugAction( instring, startloc, endloc, expr, toks ):&#xa;    print (""Matched "" + _ustr(expr) + "" -> "" + str(toks.asList()))&#xa;&#xa;def _defaultExceptionDebugAction( instring, loc, expr, exc ):&#xa;    print (""Exception raised:"" + _ustr(exc))&#xa;&#xa;def nullDebugAction(*args):&#xa;    """"""'Do-nothing' debug action, to suppress debugging output during parsing.""""""&#xa;    pass&#xa;&#xa;# Only works on Python 3.x - nonlocal is toxic to Python 2 installs&#xa;#~ 'decorator to trim function calls to match the arity of the target'&#xa;#~ def _trim_arity(func, maxargs=3):&#xa;    #~ if func in singleArgBuiltins:&#xa;        #~ return lambda s,l,t: func(t)&#xa;    #~ limit = 0&#xa;    #~ foundArity = False&#xa;    #~ def wrapper(*args):&#xa;        #~ nonlocal limit,foundArity&#xa;        #~ while 1:&#xa;            #~ try:&#xa;                #~ ret = func(*args[limit:])&#xa;                #~ foundArity = True&#xa;                #~ return ret&#xa;            #~ except TypeError:&#xa;                #~ if limit == maxargs or foundArity:&#xa;                    #~ raise&#xa;                #~ limit += 1&#xa;                #~ continue&#xa;    #~ return wrapper&#xa;&#xa;# this version is Python 2.x-3.x cross-compatible&#xa;'decorator to trim function calls to match the arity of the target'&#xa;def _trim_arity(func, maxargs=2):&#xa;    if func in singleArgBuiltins:&#xa;        return lambda s,l,t: func(t)&#xa;    limit = [0]&#xa;    foundArity = [False]&#xa;    &#xa;    # traceback return data structure changed in Py3.5 - normalize back to plain tuples&#xa;    if system_version[:2] >= (3,5):&#xa;        def extract_stack(limit=0):&#xa;            # special handling for Python 3.5.0 - extra deep call stack by 1&#xa;            offset = -3 if system_version == (3,5,0) else -2&#xa;            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;        def extract_tb(tb, limit=0):&#xa;            frames = traceback.extract_tb(tb, limit=limit)&#xa;            frame_summary = frames[-1]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;    else:&#xa;        extract_stack = traceback.extract_stack&#xa;        extract_tb = traceback.extract_tb&#xa;    &#xa;    # synthesize what would be returned by traceback.extract_stack at the call to &#xa;    # user's parse action 'func', so that we don't incur call penalty at parse time&#xa;    &#xa;    LINE_DIFF = 6&#xa;    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND &#xa;    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!&#xa;    this_line = extract_stack(limit=2)[-1]&#xa;    pa_call_line_synth = (this_line[0], this_line[1]+LINE_DIFF)&#xa;&#xa;    def wrapper(*args):&#xa;        while 1:&#xa;            try:&#xa;                ret = func(*args[limit[0]:])&#xa;                foundArity[0] = True&#xa;                return ret&#xa;            except TypeError:&#xa;                # re-raise TypeErrors if they did not come from our arity testing&#xa;                if foundArity[0]:&#xa;                    raise&#xa;                else:&#xa;                    try:&#xa;                        tb = sys.exc_info()[-1]&#xa;                        if not extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth:&#xa;                            raise&#xa;                    finally:&#xa;                        del tb&#xa;&#xa;                if limit[0] <= maxargs:&#xa;                    limit[0] += 1&#xa;                    continue&#xa;                raise&#xa;&#xa;    # copy func name to wrapper for sensible debug output&#xa;    func_name = ""<parse action>""&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    wrapper.__name__ = func_name&#xa;&#xa;    return wrapper&#xa;&#xa;class ParserElement(object):&#xa;    """"""Abstract base level parser element class.""""""&#xa;    DEFAULT_WHITE_CHARS = "" \n\t\r""&#xa;    verbose_stacktrace = False&#xa;&#xa;    @staticmethod&#xa;    def setDefaultWhitespaceChars( chars ):&#xa;        r""""""&#xa;        Overrides the default whitespace chars&#xa;&#xa;        Example::&#xa;            # default whitespace chars are space, <TAB> and newline&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def', 'ghi', 'jkl']&#xa;            &#xa;            # change to just treat newline as significant&#xa;            ParserElement.setDefaultWhitespaceChars("" \t"")&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def']&#xa;        """"""&#xa;        ParserElement.DEFAULT_WHITE_CHARS = chars&#xa;&#xa;    @staticmethod&#xa;    def inlineLiteralsUsing(cls):&#xa;        """"""&#xa;        Set class to be used for inclusion of string literals into a parser.&#xa;        &#xa;        Example::&#xa;            # default literal class used is Literal&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;&#xa;            # change to Suppress&#xa;            ParserElement.inlineLiteralsUsing(Suppress)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '12', '31']&#xa;        """"""&#xa;        ParserElement._literalStringClass = cls&#xa;&#xa;    def __init__( self, savelist=False ):&#xa;        self.parseAction = list()&#xa;        self.failAction = None&#xa;        #~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall&#xa;        self.strRepr = None&#xa;        self.resultsName = None&#xa;        self.saveAsList = savelist&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        self.copyDefaultWhiteChars = True&#xa;        self.mayReturnEmpty = False # used when checking for left-recursion&#xa;        self.keepTabs = False&#xa;        self.ignoreExprs = list()&#xa;        self.debug = False&#xa;        self.streamlined = False&#xa;        self.mayIndexError = True # used to optimize exception handling for subclasses that don't advance parse index&#xa;        self.errmsg = """"&#xa;        self.modalResults = True # used to mark results names as modal (report only last) or cumulative (list all)&#xa;        self.debugActions = ( None, None, None ) #custom debug actions&#xa;        self.re = None&#xa;        self.callPreparse = True # used to avoid redundant calls to preParse&#xa;        self.callDuringTry = False&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Make a copy of this C{ParserElement}.  Useful for defining different parse actions&#xa;        for the same parsing pattern, using copies of the original parse element.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress(""K"")&#xa;            integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;            &#xa;            print(OneOrMore(integerK | integerM | integer).parseString(""5K 100 640K 256M""))&#xa;        prints::&#xa;            [5120, 100, 655360, 268435456]&#xa;        Equivalent form of C{expr.copy()} is just C{expr()}::&#xa;            integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;        """"""&#xa;        cpy = copy.copy( self )&#xa;        cpy.parseAction = self.parseAction[:]&#xa;        cpy.ignoreExprs = self.ignoreExprs[:]&#xa;        if self.copyDefaultWhiteChars:&#xa;            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        return cpy&#xa;&#xa;    def setName( self, name ):&#xa;        """"""&#xa;        Define name for this expression, makes debugging and exception messages clearer.&#xa;        &#xa;        Example::&#xa;            Word(nums).parseString(""ABC"")  # -> Exception: Expected W:(0123...) (at char 0), (line:1, col:1)&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")  # -> Exception: Expected integer (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        self.name = name&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        if hasattr(self,""exception""):&#xa;            self.exception.msg = self.errmsg&#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        """"""&#xa;        Define name for referencing matching tokens as a nested attribute&#xa;        of the returned parse results.&#xa;        NOTE: this returns a *copy* of the original C{ParserElement} object;&#xa;        this is so that the client can define a basic element, such as an&#xa;        integer, and reference it in multiple places with different names.&#xa;&#xa;        You can also set results names using the abbreviated syntax,&#xa;        C{expr(""name"")} in place of C{expr.setResultsName(""name"")} - &#xa;        see L{I{__call__}<__call__>}.&#xa;&#xa;        Example::&#xa;            date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;&#xa;            # equivalent form:&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;        """"""&#xa;        newself = self.copy()&#xa;        if name.endswith(""*""):&#xa;            name = name[:-1]&#xa;            listAllMatches=True&#xa;        newself.resultsName = name&#xa;        newself.modalResults = not listAllMatches&#xa;        return newself&#xa;&#xa;    def setBreak(self,breakFlag = True):&#xa;        """"""Method to invoke the Python pdb debugger when this element is&#xa;           about to be parsed. Set C{breakFlag} to True to enable, False to&#xa;           disable.&#xa;        """"""&#xa;        if breakFlag:&#xa;            _parseMethod = self._parse&#xa;            def breaker(instring, loc, doActions=True, callPreParse=True):&#xa;                import pdb&#xa;                pdb.set_trace()&#xa;                return _parseMethod( instring, loc, doActions, callPreParse )&#xa;            breaker._originalParseMethod = _parseMethod&#xa;            self._parse = breaker&#xa;        else:&#xa;            if hasattr(self._parse,""_originalParseMethod""):&#xa;                self._parse = self._parse._originalParseMethod&#xa;        return self&#xa;&#xa;    def setParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Define action to perform when successfully matching parse element definition.&#xa;        Parse action fn is a callable method with 0-3 arguments, called as C{fn(s,loc,toks)},&#xa;        C{fn(loc,toks)}, C{fn(toks)}, or just C{fn()}, where:&#xa;         - s   = the original string being parsed (see note below)&#xa;         - loc = the location of the matching substring&#xa;         - toks = a list of the matched tokens, packaged as a C{L{ParseResults}} object&#xa;        If the functions in fns modify the tokens, they can return them as the return&#xa;        value from fn, and the modified list of tokens will replace the original.&#xa;        Otherwise, fn does not need to return any value.&#xa;&#xa;        Optional keyword arguments:&#xa;         - callDuringTry = (default=C{False}) indicate if parse action should be run during lookaheads and alternate testing&#xa;&#xa;        Note: the default parsing behavior is to expand tabs in the input string&#xa;        before starting the parsing process.  See L{I{parseString}<parseString>} for more information&#xa;        on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;        consistent view of the parsed string, the parse location, and line and column&#xa;        positions within the parsed string.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;            # use parse action to convert to ints at parse time&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            # note that integer fields are now ints, not strings&#xa;            date_str.parseString(""1999/12/31"")  # -> [1999, '/', 12, '/', 31]&#xa;        """"""&#xa;        self.parseAction = list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Add parse action to expression's list of parse actions. See L{I{setParseAction}<setParseAction>}.&#xa;        &#xa;        See examples in L{I{copy}<copy>}.&#xa;        """"""&#xa;        self.parseAction += list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addCondition(self, *fns, **kwargs):&#xa;        """"""Add a boolean predicate function to expression's list of parse actions. See &#xa;        L{I{setParseAction}<setParseAction>} for function call signatures. Unlike C{setParseAction}, &#xa;        functions passed to C{addCondition} need to return boolean success/fail of the condition.&#xa;&#xa;        Optional keyword arguments:&#xa;         - message = define a custom message to be used in the raised exception&#xa;         - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException&#xa;         &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            year_int = integer.copy()&#xa;            year_int.addCondition(lambda toks: toks[0] >= 2000, message=""Only support years 2000 and later"")&#xa;            date_str = year_int + '/' + integer + '/' + integer&#xa;&#xa;            result = date_str.parseString(""1999/12/31"")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        msg = kwargs.get(""message"", ""failed user-defined condition"")&#xa;        exc_type = ParseFatalException if kwargs.get(""fatal"", False) else ParseException&#xa;        for fn in fns:&#xa;            def pa(s,l,t):&#xa;                if not bool(_trim_arity(fn)(s,l,t)):&#xa;                    raise exc_type(s,l,msg)&#xa;            self.parseAction.append(pa)&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def setFailAction( self, fn ):&#xa;        """"""Define action to perform if parsing fails at this expression.&#xa;           Fail acton fn is a callable function that takes the arguments&#xa;           C{fn(s,loc,expr,err)} where:&#xa;            - s = string being parsed&#xa;            - loc = location where expression match was attempted and failed&#xa;            - expr = the parse expression that failed&#xa;            - err = the exception thrown&#xa;           The function returns no value.  It may throw C{L{ParseFatalException}}&#xa;           if it is desired to stop parsing immediately.""""""&#xa;        self.failAction = fn&#xa;        return self&#xa;&#xa;    def _skipIgnorables( self, instring, loc ):&#xa;        exprsFound = True&#xa;        while exprsFound:&#xa;            exprsFound = False&#xa;            for e in self.ignoreExprs:&#xa;                try:&#xa;                    while 1:&#xa;                        loc,dummy = e._parse( instring, loc )&#xa;                        exprsFound = True&#xa;                except ParseException:&#xa;                    pass&#xa;        return loc&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if self.ignoreExprs:&#xa;            loc = self._skipIgnorables( instring, loc )&#xa;&#xa;        if self.skipWhitespace:&#xa;            wt = self.whiteChars&#xa;            instrlen = len(instring)&#xa;            while loc < instrlen and instring[loc] in wt:&#xa;                loc += 1&#xa;&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        return loc, []&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return tokenlist&#xa;&#xa;    #~ @profile&#xa;    def _parseNoCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        debugging = ( self.debug ) #and doActions )&#xa;&#xa;        if debugging or self.failAction:&#xa;            #~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))&#xa;            if (self.debugActions[0] ):&#xa;                self.debugActions[0]( instring, loc, self )&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            try:&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            except ParseBaseException as err:&#xa;                #~ print (""Exception raised:"", err)&#xa;                if self.debugActions[2]:&#xa;                    self.debugActions[2]( instring, tokensStart, self, err )&#xa;                if self.failAction:&#xa;                    self.failAction( instring, tokensStart, self, err )&#xa;                raise&#xa;        else:&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            if self.mayIndexError or loc >= len(instring):&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            else:&#xa;                loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;&#xa;        tokens = self.postParse( instring, loc, tokens )&#xa;&#xa;        retTokens = ParseResults( tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults )&#xa;        if self.parseAction and (doActions or self.callDuringTry):&#xa;            if debugging:&#xa;                try:&#xa;                    for fn in self.parseAction:&#xa;                        tokens = fn( instring, tokensStart, retTokens )&#xa;                        if tokens is not None:&#xa;                            retTokens = ParseResults( tokens,&#xa;                                                      self.resultsName,&#xa;                                                      asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                      modal=self.modalResults )&#xa;                except ParseBaseException as err:&#xa;                    #~ print ""Exception raised in user parse action:"", err&#xa;                    if (self.debugActions[2] ):&#xa;                        self.debugActions[2]( instring, tokensStart, self, err )&#xa;                    raise&#xa;            else:&#xa;                for fn in self.parseAction:&#xa;                    tokens = fn( instring, tokensStart, retTokens )&#xa;                    if tokens is not None:&#xa;                        retTokens = ParseResults( tokens,&#xa;                                                  self.resultsName,&#xa;                                                  asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                  modal=self.modalResults )&#xa;&#xa;        if debugging:&#xa;            #~ print (""Matched"",self,""->"",retTokens.asList())&#xa;            if (self.debugActions[1] ):&#xa;                self.debugActions[1]( instring, tokensStart, loc, self, retTokens )&#xa;&#xa;        return loc, retTokens&#xa;&#xa;    def tryParse( self, instring, loc ):&#xa;        try:&#xa;            return self._parse( instring, loc, doActions=False )[0]&#xa;        except ParseFatalException:&#xa;            raise ParseException( instring, loc, self.errmsg, self)&#xa;    &#xa;    def canParseNext(self, instring, loc):&#xa;        try:&#xa;            self.tryParse(instring, loc)&#xa;        except (ParseException, IndexError):&#xa;            return False&#xa;        else:&#xa;            return True&#xa;&#xa;    class _UnboundedCache(object):&#xa;        def __init__(self):&#xa;            cache = {}&#xa;            self.not_in_cache = not_in_cache = object()&#xa;&#xa;            def get(self, key):&#xa;                return cache.get(key, not_in_cache)&#xa;&#xa;            def set(self, key, value):&#xa;                cache[key] = value&#xa;&#xa;            def clear(self):&#xa;                cache.clear()&#xa;&#xa;            self.get = types.MethodType(get, self)&#xa;            self.set = types.MethodType(set, self)&#xa;            self.clear = types.MethodType(clear, self)&#xa;&#xa;    if _OrderedDict is not None:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = _OrderedDict()&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.popitem(False)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    else:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = {}&#xa;                key_fifo = collections.deque([], size)&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.pop(key_fifo.popleft(), None)&#xa;                    key_fifo.append(key)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;                    key_fifo.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    # argument cache for optimizing repeated calls when backtracking through recursive expressions&#xa;    packrat_cache = {} # this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail&#xa;    packrat_cache_lock = RLock()&#xa;    packrat_cache_stats = [0, 0]&#xa;&#xa;    # this method gets repeatedly called during backtracking with the same arguments -&#xa;    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression&#xa;    def _parseCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        HIT, MISS = 0, 1&#xa;        lookup = (self, instring, loc, callPreParse, doActions)&#xa;        with ParserElement.packrat_cache_lock:&#xa;            cache = ParserElement.packrat_cache&#xa;            value = cache.get(lookup)&#xa;            if value is cache.not_in_cache:&#xa;                ParserElement.packrat_cache_stats[MISS] += 1&#xa;                try:&#xa;                    value = self._parseNoCache(instring, loc, doActions, callPreParse)&#xa;                except ParseBaseException as pe:&#xa;                    # cache a copy of the exception, without the traceback&#xa;                    cache.set(lookup, pe.__class__(*pe.args))&#xa;                    raise&#xa;                else:&#xa;                    cache.set(lookup, (value[0], value[1].copy()))&#xa;                    return value&#xa;            else:&#xa;                ParserElement.packrat_cache_stats[HIT] += 1&#xa;                if isinstance(value, Exception):&#xa;                    raise value&#xa;                return (value[0], value[1].copy())&#xa;&#xa;    _parse = _parseNoCache&#xa;&#xa;    @staticmethod&#xa;    def resetCache():&#xa;        ParserElement.packrat_cache.clear()&#xa;        ParserElement.packrat_cache_stats[:] = [0] * len(ParserElement.packrat_cache_stats)&#xa;&#xa;    _packratEnabled = False&#xa;    @staticmethod&#xa;    def enablePackrat(cache_size_limit=128):&#xa;        """"""Enables ""packrat"" parsing, which adds memoizing to the parsing logic.&#xa;           Repeated parse attempts at the same string location (which happens&#xa;           often in many complex grammars) can immediately return a cached value,&#xa;           instead of re-executing parsing/validating code.  Memoizing is done of&#xa;           both valid results and parsing exceptions.&#xa;           &#xa;           Parameters:&#xa;            - cache_size_limit - (default=C{128}) - if an integer value is provided&#xa;              will limit the size of the packrat cache; if None is passed, then&#xa;              the cache size will be unbounded; if 0 is passed, the cache will&#xa;              be effectively disabled.&#xa;            &#xa;           This speedup may break existing programs that use parse actions that&#xa;           have side-effects.  For this reason, packrat parsing is disabled when&#xa;           you first import pyparsing.  To activate the packrat feature, your&#xa;           program must call the class method C{ParserElement.enablePackrat()}.  If&#xa;           your program uses C{psyco} to ""compile as you go"", you must call&#xa;           C{enablePackrat} before calling C{psyco.full()}.  If you do not do this,&#xa;           Python will crash.  For best results, call C{enablePackrat()} immediately&#xa;           after importing pyparsing.&#xa;           &#xa;           Example::&#xa;               import pyparsing&#xa;               pyparsing.ParserElement.enablePackrat()&#xa;        """"""&#xa;        if not ParserElement._packratEnabled:&#xa;            ParserElement._packratEnabled = True&#xa;            if cache_size_limit is None:&#xa;                ParserElement.packrat_cache = ParserElement._UnboundedCache()&#xa;            else:&#xa;                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)&#xa;            ParserElement._parse = ParserElement._parseCache&#xa;&#xa;    def parseString( self, instring, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression with the given string.&#xa;        This is the main interface to the client code, once the complete&#xa;        expression has been built.&#xa;&#xa;        If you want the grammar to require that the entire input string be&#xa;        successfully parsed, then set C{parseAll} to True (equivalent to ending&#xa;        the grammar with C{L{StringEnd()}}).&#xa;&#xa;        Note: C{parseString} implicitly calls C{expandtabs()} on the input string,&#xa;        in order to report proper column numbers in parse actions.&#xa;        If the input string contains tabs and&#xa;        the grammar uses parse actions that use the C{loc} argument to index into the&#xa;        string being parsed, you can ensure you have a consistent view of the input&#xa;        string by:&#xa;         - calling C{parseWithTabs} on your grammar before calling C{parseString}&#xa;           (see L{I{parseWithTabs}<parseWithTabs>})&#xa;         - define your parse action using the full C{(s,loc,toks)} signature, and&#xa;           reference the input string using the parse action's C{s} argument&#xa;         - explictly expand the tabs in your input string before calling&#xa;           C{parseString}&#xa;        &#xa;        Example::&#xa;            Word('a').parseString('aaaaabaaa')  # -> ['aaaaa']&#xa;            Word('a').parseString('aaaaabaaa', parseAll=True)  # -> Exception: Expected end of text&#xa;        """"""&#xa;        ParserElement.resetCache()&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;            #~ self.saveAsList = True&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;        if not self.keepTabs:&#xa;            instring = instring.expandtabs()&#xa;        try:&#xa;            loc, tokens = self._parse( instring, 0 )&#xa;            if parseAll:&#xa;                loc = self.preParse( instring, loc )&#xa;                se = Empty() + StringEnd()&#xa;                se._parse( instring, loc )&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;        else:&#xa;            return tokens&#xa;&#xa;    def scanString( self, instring, maxMatches=_MAX_INT, overlap=False ):&#xa;        """"""&#xa;        Scan the input string for expression matches.  Each match will return the&#xa;        matching tokens, start location, and end location.  May be called with optional&#xa;        C{maxMatches} argument, to clip scanning after 'n' matches are found.  If&#xa;        C{overlap} is specified, then overlapping matches will be reported.&#xa;&#xa;        Note that the start and end locations are reported relative to the string&#xa;        being parsed.  See L{I{parseString}<parseString>} for more information on parsing&#xa;        strings with embedded tabs.&#xa;&#xa;        Example::&#xa;            source = ""sldjf123lsdjjkf345sldkjf879lkjsfd987""&#xa;            print(source)&#xa;            for tokens,start,end in Word(alphas).scanString(source):&#xa;                print(' '*start + '^'*(end-start))&#xa;                print(' '*start + tokens[0])&#xa;        &#xa;        prints::&#xa;        &#xa;            sldjf123lsdjjkf345sldkjf879lkjsfd987&#xa;            ^^^^^&#xa;            sldjf&#xa;                    ^^^^^^^&#xa;                    lsdjjkf&#xa;                              ^^^^^^&#xa;                              sldkjf&#xa;                                       ^^^^^^&#xa;                                       lkjsfd&#xa;        """"""&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;&#xa;        if not self.keepTabs:&#xa;            instring = _ustr(instring).expandtabs()&#xa;        instrlen = len(instring)&#xa;        loc = 0&#xa;        preparseFn = self.preParse&#xa;        parseFn = self._parse&#xa;        ParserElement.resetCache()&#xa;        matches = 0&#xa;        try:&#xa;            while loc <= instrlen and matches < maxMatches:&#xa;                try:&#xa;                    preloc = preparseFn( instring, loc )&#xa;                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )&#xa;                except ParseException:&#xa;                    loc = preloc+1&#xa;                else:&#xa;                    if nextLoc > loc:&#xa;                        matches += 1&#xa;                        yield tokens, preloc, nextLoc&#xa;                        if overlap:&#xa;                            nextloc = preparseFn( instring, loc )&#xa;                            if nextloc > loc:&#xa;                                loc = nextLoc&#xa;                            else:&#xa;                                loc += 1&#xa;                        else:&#xa;                            loc = nextLoc&#xa;                    else:&#xa;                        loc = preloc+1&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def transformString( self, instring ):&#xa;        """"""&#xa;        Extension to C{L{scanString}}, to modify matching text with modified tokens that may&#xa;        be returned from a parse action.  To use C{transformString}, define a grammar and&#xa;        attach a parse action to it that modifies the returned token list.&#xa;        Invoking C{transformString()} on a target string will then scan for matches,&#xa;        and replace the matched text patterns according to the logic in the parse&#xa;        action.  C{transformString()} returns the resulting transformed string.&#xa;        &#xa;        Example::&#xa;            wd = Word(alphas)&#xa;            wd.setParseAction(lambda toks: toks[0].title())&#xa;            &#xa;            print(wd.transformString(""now is the winter of our discontent made glorious summer by this sun of york.""))&#xa;        Prints::&#xa;            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.&#xa;        """"""&#xa;        out = []&#xa;        lastE = 0&#xa;        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to&#xa;        # keep string locs straight between transformString and scanString&#xa;        self.keepTabs = True&#xa;        try:&#xa;            for t,s,e in self.scanString( instring ):&#xa;                out.append( instring[lastE:s] )&#xa;                if t:&#xa;                    if isinstance(t,ParseResults):&#xa;                        out += t.asList()&#xa;                    elif isinstance(t,list):&#xa;                        out += t&#xa;                    else:&#xa;                        out.append(t)&#xa;                lastE = e&#xa;            out.append(instring[lastE:])&#xa;            out = [o for o in out if o]&#xa;            return """".join(map(_ustr,_flatten(out)))&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def searchString( self, instring, maxMatches=_MAX_INT ):&#xa;        """"""&#xa;        Another extension to C{L{scanString}}, simplifying the access to the tokens found&#xa;        to match the given parse expression.  May be called with optional&#xa;        C{maxMatches} argument, to clip searching after 'n' matches are found.&#xa;        &#xa;        Example::&#xa;            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters&#xa;            cap_word = Word(alphas.upper(), alphas.lower())&#xa;            &#xa;            print(cap_word.searchString(""More than Iron, more than Lead, more than Gold I need Electricity""))&#xa;        prints::&#xa;            ['More', 'Iron', 'Lead', 'Gold', 'I']&#xa;        """"""&#xa;        try:&#xa;            return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):&#xa;        """"""&#xa;        Generator method to split a string using the given expression as a separator.&#xa;        May be called with optional C{maxsplit} argument, to limit the number of splits;&#xa;        and the optional C{includeSeparators} argument (default=C{False}), if the separating&#xa;        matching text should be included in the split results.&#xa;        &#xa;        Example::        &#xa;            punc = oneOf(list("".,;:/-!?""))&#xa;            print(list(punc.split(""This, this?, this sentence, is badly punctuated!"")))&#xa;        prints::&#xa;            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']&#xa;        """"""&#xa;        splits = 0&#xa;        last = 0&#xa;        for t,s,e in self.scanString(instring, maxMatches=maxsplit):&#xa;            yield instring[last:s]&#xa;            if includeSeparators:&#xa;                yield t[0]&#xa;            last = e&#xa;        yield instring[last:]&#xa;&#xa;    def __add__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator - returns C{L{And}}. Adding strings to a ParserElement&#xa;        converts them to L{Literal}s by default.&#xa;        &#xa;        Example::&#xa;            greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;            hello = ""Hello, World!""&#xa;            print (hello, ""->"", greet.parseString(hello))&#xa;        Prints::&#xa;            Hello, World! -> ['Hello', ',', 'World', '!']&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, other ] )&#xa;&#xa;    def __radd__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other + self&#xa;&#xa;    def __sub__(self, other):&#xa;        """"""&#xa;        Implementation of - operator, returns C{L{And}} with error stop&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, And._ErrorStop(), other ] )&#xa;&#xa;    def __rsub__(self, other ):&#xa;        """"""&#xa;        Implementation of - operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other - self&#xa;&#xa;    def __mul__(self,other):&#xa;        """"""&#xa;        Implementation of * operator, allows use of C{expr * 3} in place of&#xa;        C{expr + expr + expr}.  Expressions may also me multiplied by a 2-integer&#xa;        tuple, similar to C{{min,max}} multipliers in regular expressions.  Tuples&#xa;        may also include C{None} as in:&#xa;         - C{expr*(n,None)} or C{expr*(n,)} is equivalent&#xa;              to C{expr*n + L{ZeroOrMore}(expr)}&#xa;              (read as ""at least n instances of C{expr}"")&#xa;         - C{expr*(None,n)} is equivalent to C{expr*(0,n)}&#xa;              (read as ""0 to n instances of C{expr}"")&#xa;         - C{expr*(None,None)} is equivalent to C{L{ZeroOrMore}(expr)}&#xa;         - C{expr*(1,None)} is equivalent to C{L{OneOrMore}(expr)}&#xa;&#xa;        Note that C{expr*(None,n)} does not raise an exception if&#xa;        more than n exprs exist in the input stream; that is,&#xa;        C{expr*(None,n)} does not enforce a maximum number of expr&#xa;        occurrences.  If this behavior is desired, then write&#xa;        C{expr*(None,n) + ~expr}&#xa;        """"""&#xa;        if isinstance(other,int):&#xa;            minElements, optElements = other,0&#xa;        elif isinstance(other,tuple):&#xa;            other = (other + (None, None))[:2]&#xa;            if other[0] is None:&#xa;                other = (0, other[1])&#xa;            if isinstance(other[0],int) and other[1] is None:&#xa;                if other[0] == 0:&#xa;                    return ZeroOrMore(self)&#xa;                if other[0] == 1:&#xa;                    return OneOrMore(self)&#xa;                else:&#xa;                    return self*other[0] + ZeroOrMore(self)&#xa;            elif isinstance(other[0],int) and isinstance(other[1],int):&#xa;                minElements, optElements = other&#xa;                optElements -= minElements&#xa;            else:&#xa;                raise TypeError(""cannot multiply 'ParserElement' and ('%s','%s') objects"", type(other[0]),type(other[1]))&#xa;        else:&#xa;            raise TypeError(""cannot multiply 'ParserElement' and '%s' objects"", type(other))&#xa;&#xa;        if minElements < 0:&#xa;            raise ValueError(""cannot multiply ParserElement by negative value"")&#xa;        if optElements < 0:&#xa;            raise ValueError(""second tuple value must be greater or equal to first tuple value"")&#xa;        if minElements == optElements == 0:&#xa;            raise ValueError(""cannot multiply ParserElement by 0 or (0,0)"")&#xa;&#xa;        if (optElements):&#xa;            def makeOptionalList(n):&#xa;                if n>1:&#xa;                    return Optional(self + makeOptionalList(n-1))&#xa;                else:&#xa;                    return Optional(self)&#xa;            if minElements:&#xa;                if minElements == 1:&#xa;                    ret = self + makeOptionalList(optElements)&#xa;                else:&#xa;                    ret = And([self]*minElements) + makeOptionalList(optElements)&#xa;            else:&#xa;                ret = makeOptionalList(optElements)&#xa;        else:&#xa;            if minElements == 1:&#xa;                ret = self&#xa;            else:&#xa;                ret = And([self]*minElements)&#xa;        return ret&#xa;&#xa;    def __rmul__(self, other):&#xa;        return self.__mul__(other)&#xa;&#xa;    def __or__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator - returns C{L{MatchFirst}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return MatchFirst( [ self, other ] )&#xa;&#xa;    def __ror__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other | self&#xa;&#xa;    def __xor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator - returns C{L{Or}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Or( [ self, other ] )&#xa;&#xa;    def __rxor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other ^ self&#xa;&#xa;    def __and__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator - returns C{L{Each}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Each( [ self, other ] )&#xa;&#xa;    def __rand__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other & self&#xa;&#xa;    def __invert__( self ):&#xa;        """"""&#xa;        Implementation of ~ operator - returns C{L{NotAny}}&#xa;        """"""&#xa;        return NotAny( self )&#xa;&#xa;    def __call__(self, name=None):&#xa;        """"""&#xa;        Shortcut for C{L{setResultsName}}, with C{listAllMatches=False}.&#xa;        &#xa;        If C{name} is given with a trailing C{'*'} character, then C{listAllMatches} will be&#xa;        passed as C{True}.&#xa;           &#xa;        If C{name} is omitted, same as calling C{L{copy}}.&#xa;&#xa;        Example::&#xa;            # these are equivalent&#xa;            userdata = Word(alphas).setResultsName(""name"") + Word(nums+""-"").setResultsName(""socsecno"")&#xa;            userdata = Word(alphas)(""name"") + Word(nums+""-"")(""socsecno"")             &#xa;        """"""&#xa;        if name is not None:&#xa;            return self.setResultsName(name)&#xa;        else:&#xa;            return self.copy()&#xa;&#xa;    def suppress( self ):&#xa;        """"""&#xa;        Suppresses the output of this C{ParserElement}; useful to keep punctuation from&#xa;        cluttering up returned output.&#xa;        """"""&#xa;        return Suppress( self )&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""&#xa;        Disables the skipping of whitespace before matching the characters in the&#xa;        C{ParserElement}'s defined pattern.  This is normally only used internally by&#xa;        the pyparsing module, but may be needed in some whitespace-sensitive grammars.&#xa;        """"""&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def setWhitespaceChars( self, chars ):&#xa;        """"""&#xa;        Overrides the default whitespace chars&#xa;        """"""&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = chars&#xa;        self.copyDefaultWhiteChars = False&#xa;        return self&#xa;&#xa;    def parseWithTabs( self ):&#xa;        """"""&#xa;        Overrides default behavior to expand C{<TAB>}s to spaces before parsing the input string.&#xa;        Must be called before C{parseString} when the input grammar contains elements that&#xa;        match C{<TAB>} characters.&#xa;        """"""&#xa;        self.keepTabs = True&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        """"""&#xa;        Define expression to be ignored (e.g., comments) while doing pattern&#xa;        matching; may be called repeatedly, to define multiple comment or other&#xa;        ignorable patterns.&#xa;        &#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']&#xa;            &#xa;            patt.ignore(cStyleComment)&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']&#xa;        """"""&#xa;        if isinstance(other, basestring):&#xa;            other = Suppress(other)&#xa;&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                self.ignoreExprs.append(other)&#xa;        else:&#xa;            self.ignoreExprs.append( Suppress( other.copy() ) )&#xa;        return self&#xa;&#xa;    def setDebugActions( self, startAction, successAction, exceptionAction ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        """"""&#xa;        self.debugActions = (startAction or _defaultStartDebugAction,&#xa;                             successAction or _defaultSuccessDebugAction,&#xa;                             exceptionAction or _defaultExceptionDebugAction)&#xa;        self.debug = True&#xa;        return self&#xa;&#xa;    def setDebug( self, flag=True ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        Set C{flag} to True to enable, False to disable.&#xa;&#xa;        Example::&#xa;            wd = Word(alphas).setName(""alphaword"")&#xa;            integer = Word(nums).setName(""numword"")&#xa;            term = wd | integer&#xa;            &#xa;            # turn on debugging for wd&#xa;            wd.setDebug()&#xa;&#xa;            OneOrMore(term).parseString(""abc 123 xyz 890"")&#xa;        &#xa;        prints::&#xa;            Match alphaword at loc 0(1,1)&#xa;            Matched alphaword -> ['abc']&#xa;            Match alphaword at loc 3(1,4)&#xa;            Exception raised:Expected alphaword (at char 4), (line:1, col:5)&#xa;            Match alphaword at loc 7(1,8)&#xa;            Matched alphaword -> ['xyz']&#xa;            Match alphaword at loc 11(1,12)&#xa;            Exception raised:Expected alphaword (at char 12), (line:1, col:13)&#xa;            Match alphaword at loc 15(1,16)&#xa;            Exception raised:Expected alphaword (at char 15), (line:1, col:16)&#xa;&#xa;        The output shown is that produced by the default debug actions - custom debug actions can be&#xa;        specified using L{setDebugActions}. Prior to attempting&#xa;        to match the C{wd} expression, the debugging message C{""Match <exprname> at loc <n>(<line>,<col>)""}&#xa;        is shown. Then if the parse succeeds, a C{""Matched""} message is shown, or an C{""Exception raised""}&#xa;        message is shown. Also note the use of L{setName} to assign a human-readable name to the expression,&#xa;        which makes debugging and exception messages easier to understand - for instance, the default&#xa;        name created for the C{Word} expression without calling C{setName} is C{""W:(ABCD...)""}.&#xa;        """"""&#xa;        if flag:&#xa;            self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )&#xa;        else:&#xa;            self.debug = False&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        return self.name&#xa;&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;&#xa;    def streamline( self ):&#xa;        self.streamlined = True&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        pass&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        """"""&#xa;        Check defined expressions for valid structure, check for infinite recursive definitions.&#xa;        """"""&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def parseFile( self, file_or_filename, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression on the given file or filename.&#xa;        If a filename is specified (instead of a file object),&#xa;        the entire file is opened, read, and closed before parsing.&#xa;        """"""&#xa;        try:&#xa;            file_contents = file_or_filename.read()&#xa;        except AttributeError:&#xa;            with open(file_or_filename, ""r"") as f:&#xa;                file_contents = f.read()&#xa;        try:&#xa;            return self.parseString(file_contents, parseAll)&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def __eq__(self,other):&#xa;        if isinstance(other, ParserElement):&#xa;            return self is other or vars(self) == vars(other)&#xa;        elif isinstance(other, basestring):&#xa;            return self.matches(other)&#xa;        else:&#xa;            return super(ParserElement,self)==other&#xa;&#xa;    def __ne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def __hash__(self):&#xa;        return hash(id(self))&#xa;&#xa;    def __req__(self,other):&#xa;        return self == other&#xa;&#xa;    def __rne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def matches(self, testString, parseAll=True):&#xa;        """"""&#xa;        Method for quick testing of a parser against a test string. Good for simple &#xa;        inline microtests of sub expressions while building up larger parser.&#xa;           &#xa;        Parameters:&#xa;         - testString - to test against this expression for a match&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests&#xa;            &#xa;        Example::&#xa;            expr = Word(nums)&#xa;            assert expr.matches(""100"")&#xa;        """"""&#xa;        try:&#xa;            self.parseString(_ustr(testString), parseAll=parseAll)&#xa;            return True&#xa;        except ParseBaseException:&#xa;            return False&#xa;                &#xa;    def runTests(self, tests, parseAll=True, comment='#', fullDump=True, printResults=True, failureTests=False):&#xa;        """"""&#xa;        Execute the parse expression on a series of test strings, showing each&#xa;        test, the parsed results or where the parse failed. Quick and easy way to&#xa;        run a parse expression against a list of sample strings.&#xa;           &#xa;        Parameters:&#xa;         - tests - a list of separate test strings, or a multiline string of test strings&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests           &#xa;         - comment - (default=C{'#'}) - expression for indicating embedded comments in the test &#xa;              string; pass None to disable comment filtering&#xa;         - fullDump - (default=C{True}) - dump results as list followed by results names in nested outline;&#xa;              if False, only dump nested list&#xa;         - printResults - (default=C{True}) prints test output to stdout&#xa;         - failureTests - (default=C{False}) indicates if these tests are expected to fail parsing&#xa;&#xa;        Returns: a (success, results) tuple, where success indicates that all tests succeeded&#xa;        (or failed if C{failureTests} is True), and the results contain a list of lines of each &#xa;        test's output&#xa;        &#xa;        Example::&#xa;            number_expr = pyparsing_common.number.copy()&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # unsigned integer&#xa;                100&#xa;                # negative integer&#xa;                -100&#xa;                # float with scientific notation&#xa;                6.02e23&#xa;                # integer with scientific notation&#xa;                1e-12&#xa;                ''')&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # stray character&#xa;                100Z&#xa;                # missing leading digit before '.'&#xa;                -.100&#xa;                # too many '.'&#xa;                3.14.159&#xa;                ''', failureTests=True)&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;        prints::&#xa;            # unsigned integer&#xa;            100&#xa;            [100]&#xa;&#xa;            # negative integer&#xa;            -100&#xa;            [-100]&#xa;&#xa;            # float with scientific notation&#xa;            6.02e23&#xa;            [6.02e+23]&#xa;&#xa;            # integer with scientific notation&#xa;            1e-12&#xa;            [1e-12]&#xa;&#xa;            Success&#xa;            &#xa;            # stray character&#xa;            100Z&#xa;               ^&#xa;            FAIL: Expected end of text (at char 3), (line:1, col:4)&#xa;&#xa;            # missing leading digit before '.'&#xa;            -.100&#xa;            ^&#xa;            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)&#xa;&#xa;            # too many '.'&#xa;            3.14.159&#xa;                ^&#xa;            FAIL: Expected end of text (at char 4), (line:1, col:5)&#xa;&#xa;            Success&#xa;&#xa;        Each test string must be on a single line. If you want to test a string that spans multiple&#xa;        lines, create a test like this::&#xa;&#xa;            expr.runTest(r""this is a test\\n of strings that spans \\n 3 lines"")&#xa;        &#xa;        (Note that this is a raw string literal, you must include the leading 'r'.)&#xa;        """"""&#xa;        if isinstance(tests, basestring):&#xa;            tests = list(map(str.strip, tests.rstrip().splitlines()))&#xa;        if isinstance(comment, basestring):&#xa;            comment = Literal(comment)&#xa;        allResults = []&#xa;        comments = []&#xa;        success = True&#xa;        for t in tests:&#xa;            if comment is not None and comment.matches(t, False) or comments and not t:&#xa;                comments.append(t)&#xa;                continue&#xa;            if not t:&#xa;                continue&#xa;            out = ['\n'.join(comments), t]&#xa;            comments = []&#xa;            try:&#xa;                t = t.replace(r'\n','\n')&#xa;                result = self.parseString(t, parseAll=parseAll)&#xa;                out.append(result.dump(full=fullDump))&#xa;                success = success and not failureTests&#xa;            except ParseBaseException as pe:&#xa;                fatal = ""(FATAL)"" if isinstance(pe, ParseFatalException) else """"&#xa;                if '\n' in t:&#xa;                    out.append(line(pe.loc, t))&#xa;                    out.append(' '*(col(pe.loc,t)-1) + '^' + fatal)&#xa;                else:&#xa;                    out.append(' '*pe.loc + '^' + fatal)&#xa;                out.append(""FAIL: "" + str(pe))&#xa;                success = success and failureTests&#xa;                result = pe&#xa;            except Exception as exc:&#xa;                out.append(""FAIL-EXCEPTION: "" + str(exc))&#xa;                success = success and failureTests&#xa;                result = exc&#xa;&#xa;            if printResults:&#xa;                if fullDump:&#xa;                    out.append('')&#xa;                print('\n'.join(out))&#xa;&#xa;            allResults.append((t, result))&#xa;        &#xa;        return success, allResults&#xa;&#xa;        &#xa;class Token(ParserElement):&#xa;    """"""&#xa;    Abstract C{ParserElement} subclass, for defining atomic matching patterns.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Token,self).__init__( savelist=False )&#xa;&#xa;&#xa;class Empty(Token):&#xa;    """"""&#xa;    An empty token, will always match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Empty,self).__init__()&#xa;        self.name = ""Empty""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;&#xa;class NoMatch(Token):&#xa;    """"""&#xa;    A token that will never match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(NoMatch,self).__init__()&#xa;        self.name = ""NoMatch""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.errmsg = ""Unmatchable token""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Literal(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string.&#xa;    &#xa;    Example::&#xa;        Literal('blah').parseString('blah')  # -> ['blah']&#xa;        Literal('blah').parseString('blahfooblah')  # -> ['blah']&#xa;        Literal('blah').parseString('bla')  # -> Exception: Expected ""blah""&#xa;    &#xa;    For case-insensitive matching, use L{CaselessLiteral}.&#xa;    &#xa;    For keyword matching (force word break before and after the matched string),&#xa;    use L{Keyword} or L{CaselessKeyword}.&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(Literal,self).__init__()&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Literal; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;            self.__class__ = Empty&#xa;        self.name = '""%s""' % _ustr(self.match)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;&#xa;    # Performance tuning: this routine gets called a *lot*&#xa;    # if this is a single character match string  and the first character matches,&#xa;    # short-circuit as quickly as possible, and avoid calling startswith&#xa;    #~ @profile&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if (instring[loc] == self.firstMatchChar and&#xa;            (self.matchLen==1 or instring.startswith(self.match,loc)) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;_L = Literal&#xa;ParserElement._literalStringClass = Literal&#xa;&#xa;class Keyword(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string as a keyword, that is, it must be&#xa;    immediately followed by a non-keyword character.  Compare with C{L{Literal}}:&#xa;     - C{Literal(""if"")} will match the leading C{'if'} in C{'ifAndOnlyIf'}.&#xa;     - C{Keyword(""if"")} will not; it will only match the leading C{'if'} in C{'if x=1'}, or C{'if(y==2)'}&#xa;    Accepts two optional constructor arguments in addition to the keyword string:&#xa;     - C{identChars} is a string of characters that would be valid identifier characters,&#xa;          defaulting to all alphanumerics + ""_"" and ""$""&#xa;     - C{caseless} allows case-insensitive matching, default is C{False}.&#xa;       &#xa;    Example::&#xa;        Keyword(""start"").parseString(""start"")  # -> ['start']&#xa;        Keyword(""start"").parseString(""starting"")  # -> Exception&#xa;&#xa;    For case-insensitive matching, use L{CaselessKeyword}.&#xa;    """"""&#xa;    DEFAULT_KEYWORD_CHARS = alphanums+""_$""&#xa;&#xa;    def __init__( self, matchString, identChars=None, caseless=False ):&#xa;        super(Keyword,self).__init__()&#xa;        if identChars is None:&#xa;            identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Keyword; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;        self.name = '""%s""' % self.match&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;        self.caseless = caseless&#xa;        if caseless:&#xa;            self.caselessmatch = matchString.upper()&#xa;            identChars = identChars.upper()&#xa;        self.identChars = set(identChars)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.caseless:&#xa;            if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;                 (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) and&#xa;                 (loc == 0 or instring[loc-1].upper() not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        else:&#xa;            if (instring[loc] == self.firstMatchChar and&#xa;                (self.matchLen==1 or instring.startswith(self.match,loc)) and&#xa;                (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen] not in self.identChars) and&#xa;                (loc == 0 or instring[loc-1] not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;    def copy(self):&#xa;        c = super(Keyword,self).copy()&#xa;        c.identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        return c&#xa;&#xa;    @staticmethod&#xa;    def setDefaultKeywordChars( chars ):&#xa;        """"""Overrides the default Keyword chars&#xa;        """"""&#xa;        Keyword.DEFAULT_KEYWORD_CHARS = chars&#xa;&#xa;class CaselessLiteral(Literal):&#xa;    """"""&#xa;    Token to match a specified string, ignoring case of letters.&#xa;    Note: the matched results will always be in the case of the given&#xa;    match string, NOT the case of the input text.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessLiteral(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessKeyword}.)&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(CaselessLiteral,self).__init__( matchString.upper() )&#xa;        # Preserve the defining literal.&#xa;        self.returnString = matchString&#xa;        self.name = ""'%s'"" % self.returnString&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[ loc:loc+self.matchLen ].upper() == self.match:&#xa;            return loc+self.matchLen, self.returnString&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CaselessKeyword(Keyword):&#xa;    """"""&#xa;    Caseless version of L{Keyword}.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessKeyword(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessLiteral}.)&#xa;    """"""&#xa;    def __init__( self, matchString, identChars=None ):&#xa;        super(CaselessKeyword,self).__init__( matchString, identChars, caseless=True )&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;             (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CloseMatch(Token):&#xa;    """"""&#xa;    A variation on L{Literal} which matches ""close"" matches, that is, &#xa;    strings with at most 'n' mismatching characters. C{CloseMatch} takes parameters:&#xa;     - C{match_string} - string to be matched&#xa;     - C{maxMismatches} - (C{default=1}) maximum number of mismatches allowed to count as a match&#xa;    &#xa;    The results from a successful parse will contain the matched text from the input string and the following named results:&#xa;     - C{mismatches} - a list of the positions within the match_string where mismatches were found&#xa;     - C{original} - the original match_string used to compare against the input string&#xa;    &#xa;    If C{mismatches} is an empty list, then the match was an exact match.&#xa;    &#xa;    Example::&#xa;        patt = CloseMatch(""ATCATCGAATGGA"")&#xa;        patt.parseString(""ATCATCGAAXGGA"") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)&#xa;&#xa;        # exact match&#xa;        patt.parseString(""ATCATCGAATGGA"") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})&#xa;&#xa;        # close match allowing up to 2 mismatches&#xa;        patt = CloseMatch(""ATCATCGAATGGA"", maxMismatches=2)&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})&#xa;    """"""&#xa;    def __init__(self, match_string, maxMismatches=1):&#xa;        super(CloseMatch,self).__init__()&#xa;        self.name = match_string&#xa;        self.match_string = match_string&#xa;        self.maxMismatches = maxMismatches&#xa;        self.errmsg = ""Expected %r (with up to %d mismatches)"" % (self.match_string, self.maxMismatches)&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        start = loc&#xa;        instrlen = len(instring)&#xa;        maxloc = start + len(self.match_string)&#xa;&#xa;        if maxloc <= instrlen:&#xa;            match_string = self.match_string&#xa;            match_stringloc = 0&#xa;            mismatches = []&#xa;            maxMismatches = self.maxMismatches&#xa;&#xa;            for match_stringloc,s_m in enumerate(zip(instring[loc:maxloc], self.match_string)):&#xa;                src,mat = s_m&#xa;                if src != mat:&#xa;                    mismatches.append(match_stringloc)&#xa;                    if len(mismatches) > maxMismatches:&#xa;                        break&#xa;            else:&#xa;                loc = match_stringloc + 1&#xa;                results = ParseResults([instring[start:loc]])&#xa;                results['original'] = self.match_string&#xa;                results['mismatches'] = mismatches&#xa;                return loc, results&#xa;&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Word(Token):&#xa;    """"""&#xa;    Token for matching words composed of allowed character sets.&#xa;    Defined with string containing all allowed initial characters,&#xa;    an optional string containing allowed body characters (if omitted,&#xa;    defaults to the initial character set), and an optional minimum,&#xa;    maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction. An optional&#xa;    C{excludeChars} parameter can list characters that might be found in &#xa;    the input C{bodyChars} string; useful to define a word of all printables&#xa;    except for one or two characters, for instance.&#xa;    &#xa;    L{srange} is useful for defining custom character set strings for defining &#xa;    C{Word} expressions, using range notation from regular expression character sets.&#xa;    &#xa;    A common mistake is to use C{Word} to match a specific literal string, as in &#xa;    C{Word(""Address"")}. Remember that C{Word} uses the string argument to define&#xa;    I{sets} of matchable characters. This expression would match ""Add"", ""AAA"",&#xa;    ""dAred"", or any other word made up of the characters 'A', 'd', 'r', 'e', and 's'.&#xa;    To match an exact literal string, use L{Literal} or L{Keyword}.&#xa;&#xa;    pyparsing includes helper strings for building Words:&#xa;     - L{alphas}&#xa;     - L{nums}&#xa;     - L{alphanums}&#xa;     - L{hexnums}&#xa;     - L{alphas8bit} (alphabetic characters in ASCII range 128-255 - accented, tilded, umlauted, etc.)&#xa;     - L{punc8bit} (non-alphabetic characters in ASCII range 128-255 - currency, symbols, superscripts, diacriticals, etc.)&#xa;     - L{printables} (any non-whitespace character)&#xa;&#xa;    Example::&#xa;        # a word composed of digits&#xa;        integer = Word(nums) # equivalent to Word(""0123456789"") or Word(srange(""0-9""))&#xa;        &#xa;        # a word with a leading capital, and zero or more lowercase&#xa;        capital_word = Word(alphas.upper(), alphas.lower())&#xa;&#xa;        # hostnames are alphanumeric, with leading alpha, and '-'&#xa;        hostname = Word(alphas, alphanums+'-')&#xa;        &#xa;        # roman numeral (not a strict parser, accepts invalid mix of characters)&#xa;        roman = Word(""IVXLCDM"")&#xa;        &#xa;        # any string of non-whitespace characters, except for ','&#xa;        csv_value = Word(printables, excludeChars="","")&#xa;    """"""&#xa;    def __init__( self, initChars, bodyChars=None, min=1, max=0, exact=0, asKeyword=False, excludeChars=None ):&#xa;        super(Word,self).__init__()&#xa;        if excludeChars:&#xa;            initChars = ''.join(c for c in initChars if c not in excludeChars)&#xa;            if bodyChars:&#xa;                bodyChars = ''.join(c for c in bodyChars if c not in excludeChars)&#xa;        self.initCharsOrig = initChars&#xa;        self.initChars = set(initChars)&#xa;        if bodyChars :&#xa;            self.bodyCharsOrig = bodyChars&#xa;            self.bodyChars = set(bodyChars)&#xa;        else:&#xa;            self.bodyCharsOrig = initChars&#xa;            self.bodyChars = set(initChars)&#xa;&#xa;        self.maxSpecified = max > 0&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(Word()) if zero-length word is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.asKeyword = asKeyword&#xa;&#xa;        if ' ' not in self.initCharsOrig+self.bodyCharsOrig and (min==1 and max==0 and exact==0):&#xa;            if self.bodyCharsOrig == self.initCharsOrig:&#xa;                self.reString = ""[%s]+"" % _escapeRegexRangeChars(self.initCharsOrig)&#xa;            elif len(self.initCharsOrig) == 1:&#xa;                self.reString = ""%s[%s]*"" % \&#xa;                                      (re.escape(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            else:&#xa;                self.reString = ""[%s][%s]*"" % \&#xa;                                      (_escapeRegexRangeChars(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            if self.asKeyword:&#xa;                self.reString = r""\b""+self.reString+r""\b""&#xa;            try:&#xa;                self.re = re.compile( self.reString )&#xa;            except Exception:&#xa;                self.re = None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.re:&#xa;            result = self.re.match(instring,loc)&#xa;            if not result:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;            loc = result.end()&#xa;            return loc, result.group()&#xa;&#xa;        if not(instring[ loc ] in self.initChars):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        instrlen = len(instring)&#xa;        bodychars = self.bodyChars&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, instrlen )&#xa;        while loc < maxloc and instring[loc] in bodychars:&#xa;            loc += 1&#xa;&#xa;        throwException = False&#xa;        if loc - start < self.minLen:&#xa;            throwException = True&#xa;        if self.maxSpecified and loc < instrlen and instring[loc] in bodychars:&#xa;            throwException = True&#xa;        if self.asKeyword:&#xa;            if (start>0 and instring[start-1] in bodychars) or (loc<instrlen and instring[loc] in bodychars):&#xa;                throwException = True&#xa;&#xa;        if throwException:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Word,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;&#xa;        if self.strRepr is None:&#xa;&#xa;            def charsAsStr(s):&#xa;                if len(s)>4:&#xa;                    return s[:4]+""...""&#xa;                else:&#xa;                    return s&#xa;&#xa;            if ( self.initCharsOrig != self.bodyCharsOrig ):&#xa;                self.strRepr = ""W:(%s,%s)"" % ( charsAsStr(self.initCharsOrig), charsAsStr(self.bodyCharsOrig) )&#xa;            else:&#xa;                self.strRepr = ""W:(%s)"" % charsAsStr(self.initCharsOrig)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Regex(Token):&#xa;    """"""&#xa;    Token for matching strings that match a given regular expression.&#xa;    Defined with string specifying the regular expression in a form recognized by the inbuilt Python re module.&#xa;    If the given regex contains named groups (defined using C{(?P<name>...)}), these will be preserved as &#xa;    named parse results.&#xa;&#xa;    Example::&#xa;        realnum = Regex(r""[+-]?\d+\.\d*"")&#xa;        date = Regex(r'(?P<year>\d{4})-(?P<month>\d\d?)-(?P<day>\d\d?)')&#xa;        # ref: http://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression&#xa;        roman = Regex(r""M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})"")&#xa;    """"""&#xa;    compiledREtype = type(re.compile(""[A-Z]""))&#xa;    def __init__( self, pattern, flags=0):&#xa;        """"""The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.""""""&#xa;        super(Regex,self).__init__()&#xa;&#xa;        if isinstance(pattern, basestring):&#xa;            if not pattern:&#xa;                warnings.warn(""null string passed to Regex; use Empty() instead"",&#xa;                        SyntaxWarning, stacklevel=2)&#xa;&#xa;            self.pattern = pattern&#xa;            self.flags = flags&#xa;&#xa;            try:&#xa;                self.re = re.compile(self.pattern, self.flags)&#xa;                self.reString = self.pattern&#xa;            except sre_constants.error:&#xa;                warnings.warn(""invalid pattern (%s) passed to Regex"" % pattern,&#xa;                    SyntaxWarning, stacklevel=2)&#xa;                raise&#xa;&#xa;        elif isinstance(pattern, Regex.compiledREtype):&#xa;            self.re = pattern&#xa;            self.pattern = \&#xa;            self.reString = str(pattern)&#xa;            self.flags = flags&#xa;            &#xa;        else:&#xa;            raise ValueError(""Regex may only be constructed with a string or a compiled RE object"")&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = self.re.match(instring,loc)&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        d = result.groupdict()&#xa;        ret = ParseResults(result.group())&#xa;        if d:&#xa;            for k in d:&#xa;                ret[k] = d[k]&#xa;        return loc,ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Regex,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""Re:(%s)"" % repr(self.pattern)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class QuotedString(Token):&#xa;    r""""""&#xa;    Token for matching strings that are delimited by quoting characters.&#xa;    &#xa;    Defined with the following parameters:&#xa;        - quoteChar - string of one or more characters defining the quote delimiting string&#xa;        - escChar - character to escape quotes, typically backslash (default=C{None})&#xa;        - escQuote - special quote sequence to escape an embedded quote string (such as SQL's """" to escape an embedded "") (default=C{None})&#xa;        - multiline - boolean indicating whether quotes can span multiple lines (default=C{False})&#xa;        - unquoteResults - boolean indicating whether the matched text should be unquoted (default=C{True})&#xa;        - endQuoteChar - string of one or more characters defining the end of the quote delimited string (default=C{None} => same as quoteChar)&#xa;        - convertWhitespaceEscapes - convert escaped whitespace (C{'\t'}, C{'\n'}, etc.) to actual whitespace (default=C{True})&#xa;&#xa;    Example::&#xa;        qs = QuotedString('""')&#xa;        print(qs.searchString('lsjdf ""This is the quote"" sldjf'))&#xa;        complex_qs = QuotedString('{{', endQuoteChar='}}')&#xa;        print(complex_qs.searchString('lsjdf {{This is the ""quote""}} sldjf'))&#xa;        sql_qs = QuotedString('""', escQuote='""""')&#xa;        print(sql_qs.searchString('lsjdf ""This is the quote with """"embedded"""" quotes"" sldjf'))&#xa;    prints::&#xa;        [['This is the quote']]&#xa;        [['This is the ""quote""']]&#xa;        [['This is the quote with ""embedded"" quotes']]&#xa;    """"""&#xa;    def __init__( self, quoteChar, escChar=None, escQuote=None, multiline=False, unquoteResults=True, endQuoteChar=None, convertWhitespaceEscapes=True):&#xa;        super(QuotedString,self).__init__()&#xa;&#xa;        # remove white space from quote chars - wont work anyway&#xa;        quoteChar = quoteChar.strip()&#xa;        if not quoteChar:&#xa;            warnings.warn(""quoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;            raise SyntaxError()&#xa;&#xa;        if endQuoteChar is None:&#xa;            endQuoteChar = quoteChar&#xa;        else:&#xa;            endQuoteChar = endQuoteChar.strip()&#xa;            if not endQuoteChar:&#xa;                warnings.warn(""endQuoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;                raise SyntaxError()&#xa;&#xa;        self.quoteChar = quoteChar&#xa;        self.quoteCharLen = len(quoteChar)&#xa;        self.firstQuoteChar = quoteChar[0]&#xa;        self.endQuoteChar = endQuoteChar&#xa;        self.endQuoteCharLen = len(endQuoteChar)&#xa;        self.escChar = escChar&#xa;        self.escQuote = escQuote&#xa;        self.unquoteResults = unquoteResults&#xa;        self.convertWhitespaceEscapes = convertWhitespaceEscapes&#xa;&#xa;        if multiline:&#xa;            self.flags = re.MULTILINE | re.DOTALL&#xa;            self.pattern = r'%s(?:[^%s%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        else:&#xa;            self.flags = 0&#xa;            self.pattern = r'%s(?:[^%s\n\r%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        if len(self.endQuoteChar) > 1:&#xa;            self.pattern += (&#xa;                '|(?:' + ')|(?:'.join(""%s[^%s]"" % (re.escape(self.endQuoteChar[:i]),&#xa;                                               _escapeRegexRangeChars(self.endQuoteChar[i]))&#xa;                                    for i in range(len(self.endQuoteChar)-1,0,-1)) + ')'&#xa;                )&#xa;        if escQuote:&#xa;            self.pattern += (r'|(?:%s)' % re.escape(escQuote))&#xa;        if escChar:&#xa;            self.pattern += (r'|(?:%s.)' % re.escape(escChar))&#xa;            self.escCharReplacePattern = re.escape(self.escChar)+""(.)""&#xa;        self.pattern += (r')*%s' % re.escape(self.endQuoteChar))&#xa;&#xa;        try:&#xa;            self.re = re.compile(self.pattern, self.flags)&#xa;            self.reString = self.pattern&#xa;        except sre_constants.error:&#xa;            warnings.warn(""invalid pattern (%s) passed to Regex"" % self.pattern,&#xa;                SyntaxWarning, stacklevel=2)&#xa;            raise&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = instring[loc] == self.firstQuoteChar and self.re.match(instring,loc) or None&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        ret = result.group()&#xa;&#xa;        if self.unquoteResults:&#xa;&#xa;            # strip off quotes&#xa;            ret = ret[self.quoteCharLen:-self.endQuoteCharLen]&#xa;&#xa;            if isinstance(ret,basestring):&#xa;                # replace escaped whitespace&#xa;                if '\\' in ret and self.convertWhitespaceEscapes:&#xa;                    ws_map = {&#xa;                        r'\t' : '\t',&#xa;                        r'\n' : '\n',&#xa;                        r'\f' : '\f',&#xa;                        r'\r' : '\r',&#xa;                    }&#xa;                    for wslit,wschar in ws_map.items():&#xa;                        ret = ret.replace(wslit, wschar)&#xa;&#xa;                # replace escaped characters&#xa;                if self.escChar:&#xa;                    ret = re.sub(self.escCharReplacePattern,""\g<1>"",ret)&#xa;&#xa;                # replace escaped quotes&#xa;                if self.escQuote:&#xa;                    ret = ret.replace(self.escQuote, self.endQuoteChar)&#xa;&#xa;        return loc, ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(QuotedString,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""quoted string, starting with %s ending with %s"" % (self.quoteChar, self.endQuoteChar)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class CharsNotIn(Token):&#xa;    """"""&#xa;    Token for matching words composed of characters I{not} in a given set (will&#xa;    include whitespace in matched characters if not listed in the provided exclusion set - see example).&#xa;    Defined with string containing all disallowed characters, and an optional&#xa;    minimum, maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction.&#xa;&#xa;    Example::&#xa;        # define a comma-separated-value as anything that is not a ','&#xa;        csv_value = CharsNotIn(',')&#xa;        print(delimitedList(csv_value).parseString(""dkls,lsdkjf,s12 34,@!#,213""))&#xa;    prints::&#xa;        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']&#xa;    """"""&#xa;    def __init__( self, notChars, min=1, max=0, exact=0 ):&#xa;        super(CharsNotIn,self).__init__()&#xa;        self.skipWhitespace = False&#xa;        self.notChars = notChars&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(CharsNotIn()) if zero-length char group is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = ( self.minLen == 0 )&#xa;        self.mayIndexError = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[loc] in self.notChars:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        notchars = self.notChars&#xa;        maxlen = min( start+self.maxLen, len(instring) )&#xa;        while loc < maxlen and \&#xa;              (instring[loc] not in notchars):&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(CharsNotIn, self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            if len(self.notChars) > 4:&#xa;                self.strRepr = ""!W:(%s...)"" % self.notChars[:4]&#xa;            else:&#xa;                self.strRepr = ""!W:(%s)"" % self.notChars&#xa;&#xa;        return self.strRepr&#xa;&#xa;class White(Token):&#xa;    """"""&#xa;    Special matching class for matching whitespace.  Normally, whitespace is ignored&#xa;    by pyparsing grammars.  This class is included when some whitespace structures&#xa;    are significant.  Define with a string containing the whitespace characters to be&#xa;    matched; default is C{"" \\t\\r\\n""}.  Also takes optional C{min}, C{max}, and C{exact} arguments,&#xa;    as defined for the C{L{Word}} class.&#xa;    """"""&#xa;    whiteStrs = {&#xa;        "" "" : ""<SPC>"",&#xa;        ""\t"": ""<TAB>"",&#xa;        ""\n"": ""<LF>"",&#xa;        ""\r"": ""<CR>"",&#xa;        ""\f"": ""<FF>"",&#xa;        }&#xa;    def __init__(self, ws="" \t\r\n"", min=1, max=0, exact=0):&#xa;        super(White,self).__init__()&#xa;        self.matchWhite = ws&#xa;        self.setWhitespaceChars( """".join(c for c in self.whiteChars if c not in self.matchWhite) )&#xa;        #~ self.leaveWhitespace()&#xa;        self.name = ("""".join(White.whiteStrs[c] for c in self.matchWhite))&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if not(instring[ loc ] in self.matchWhite):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        start = loc&#xa;        loc += 1&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, len(instring) )&#xa;        while loc < maxloc and instring[loc] in self.matchWhite:&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;&#xa;class _PositionToken(Token):&#xa;    def __init__( self ):&#xa;        super(_PositionToken,self).__init__()&#xa;        self.name=self.__class__.__name__&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;class GoToColumn(_PositionToken):&#xa;    """"""&#xa;    Token to advance to a specific column of input text; useful for tabular report scraping.&#xa;    """"""&#xa;    def __init__( self, colno ):&#xa;        super(GoToColumn,self).__init__()&#xa;        self.col = colno&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if col(loc,instring) != self.col:&#xa;            instrlen = len(instring)&#xa;            if self.ignoreExprs:&#xa;                loc = self._skipIgnorables( instring, loc )&#xa;            while loc < instrlen and instring[loc].isspace() and col( loc, instring ) != self.col :&#xa;                loc += 1&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        thiscol = col( loc, instring )&#xa;        if thiscol > self.col:&#xa;            raise ParseException( instring, loc, ""Text not in expected column"", self )&#xa;        newloc = loc + self.col - thiscol&#xa;        ret = instring[ loc: newloc ]&#xa;        return newloc, ret&#xa;&#xa;&#xa;class LineStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of a line within the parse string&#xa;    &#xa;    Example::&#xa;    &#xa;        test = '''\&#xa;        AAA this line&#xa;        AAA and this line&#xa;          AAA but not this one&#xa;        B AAA and definitely not this one&#xa;        '''&#xa;&#xa;        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):&#xa;            print(t)&#xa;    &#xa;    Prints::&#xa;        ['AAA', ' this line']&#xa;        ['AAA', ' and this line']    &#xa;&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineStart,self).__init__()&#xa;        self.errmsg = ""Expected start of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if col(loc, instring) == 1:&#xa;            return loc, []&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class LineEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of a line within the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineEnd,self).__init__()&#xa;        self.setWhitespaceChars( ParserElement.DEFAULT_WHITE_CHARS.replace(""\n"","""") )&#xa;        self.errmsg = ""Expected end of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc<len(instring):&#xa;            if instring[loc] == ""\n"":&#xa;                return loc+1, ""\n""&#xa;            else:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class StringStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringStart,self).__init__()&#xa;        self.errmsg = ""Expected start of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            # see if entire string up to here is just whitespace and ignoreables&#xa;            if loc != self.preParse( instring, 0 ):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class StringEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringEnd,self).__init__()&#xa;        self.errmsg = ""Expected end of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc < len(instring):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        elif loc > len(instring):&#xa;            return loc, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class WordStart(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the beginning of a Word, and&#xa;    is not preceded by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordStart(alphanums)}. C{WordStart} will also match at the beginning of&#xa;    the string being parsed, or at the beginning of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordStart,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.errmsg = ""Not at the start of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            if (instring[loc-1] in self.wordChars or&#xa;                instring[loc] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class WordEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the end of a Word, and&#xa;    is not followed by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordEnd(alphanums)}. C{WordEnd} will also match at the end of&#xa;    the string being parsed, or at the end of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordEnd,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.skipWhitespace = False&#xa;        self.errmsg = ""Not at the end of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        instrlen = len(instring)&#xa;        if instrlen>0 and loc<instrlen:&#xa;            if (instring[loc] in self.wordChars or&#xa;                instring[loc-1] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;&#xa;class ParseExpression(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of ParserElement, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(ParseExpression,self).__init__(savelist)&#xa;        if isinstance( exprs, _generatorType ):&#xa;            exprs = list(exprs)&#xa;&#xa;        if isinstance( exprs, basestring ):&#xa;            self.exprs = [ ParserElement._literalStringClass( exprs ) ]&#xa;        elif isinstance( exprs, collections.Iterable ):&#xa;            exprs = list(exprs)&#xa;            # if sequence of strings provided, wrap with Literal&#xa;            if all(isinstance(expr, basestring) for expr in exprs):&#xa;                exprs = map(ParserElement._literalStringClass, exprs)&#xa;            self.exprs = list(exprs)&#xa;        else:&#xa;            try:&#xa;                self.exprs = list( exprs )&#xa;            except TypeError:&#xa;                self.exprs = [ exprs ]&#xa;        self.callPreparse = False&#xa;&#xa;    def __getitem__( self, i ):&#xa;        return self.exprs[i]&#xa;&#xa;    def append( self, other ):&#xa;        self.exprs.append( other )&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""Extends C{leaveWhitespace} defined in base class, and also invokes C{leaveWhitespace} on&#xa;           all contained expressions.""""""&#xa;        self.skipWhitespace = False&#xa;        self.exprs = [ e.copy() for e in self.exprs ]&#xa;        for e in self.exprs:&#xa;            e.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseExpression, self).ignore( other )&#xa;                for e in self.exprs:&#xa;                    e.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseExpression, self).ignore( other )&#xa;            for e in self.exprs:&#xa;                e.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseExpression,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.exprs) )&#xa;        return self.strRepr&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseExpression,self).streamline()&#xa;&#xa;        for e in self.exprs:&#xa;            e.streamline()&#xa;&#xa;        # collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )&#xa;        # but only if there are no parse actions or resultsNames on the nested And's&#xa;        # (likewise for Or's and MatchFirst's)&#xa;        if ( len(self.exprs) == 2 ):&#xa;            other = self.exprs[0]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = other.exprs[:] + [ self.exprs[1] ]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;            other = self.exprs[-1]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = self.exprs[:-1] + other.exprs[:]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;        self.errmsg = ""Expected "" + _ustr(self)&#xa;        &#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        ret = super(ParseExpression,self).setResultsName(name,listAllMatches)&#xa;        return ret&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        for e in self.exprs:&#xa;            e.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;        &#xa;    def copy(self):&#xa;        ret = super(ParseExpression,self).copy()&#xa;        ret.exprs = [e.copy() for e in self.exprs]&#xa;        return ret&#xa;&#xa;class And(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found in the given order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'+'} operator.&#xa;    May also be constructed using the C{'-'} operator, which will suppress backtracking.&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        name_expr = OneOrMore(Word(alphas))&#xa;&#xa;        expr = And([integer(""id""),name_expr(""name""),integer(""age"")])&#xa;        # more easily written as:&#xa;        expr = integer(""id"") + name_expr(""name"") + integer(""age"")&#xa;    """"""&#xa;&#xa;    class _ErrorStop(Empty):&#xa;        def __init__(self, *args, **kwargs):&#xa;            super(And._ErrorStop,self).__init__(*args, **kwargs)&#xa;            self.name = '-'&#xa;            self.leaveWhitespace()&#xa;&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(And,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.setWhitespaceChars( self.exprs[0].whiteChars )&#xa;        self.skipWhitespace = self.exprs[0].skipWhitespace&#xa;        self.callPreparse = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        # pass False as last arg to _parse for first element, since we already&#xa;        # pre-parsed the string as part of our And pre-parsing&#xa;        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )&#xa;        errorStop = False&#xa;        for e in self.exprs[1:]:&#xa;            if isinstance(e, And._ErrorStop):&#xa;                errorStop = True&#xa;                continue&#xa;            if errorStop:&#xa;                try:&#xa;                    loc, exprtokens = e._parse( instring, loc, doActions )&#xa;                except ParseSyntaxException:&#xa;                    raise&#xa;                except ParseBaseException as pe:&#xa;                    pe.__traceback__ = None&#xa;                    raise ParseSyntaxException._from_exception(pe)&#xa;                except IndexError:&#xa;                    raise ParseSyntaxException(instring, len(instring), self.errmsg, self)&#xa;            else:&#xa;                loc, exprtokens = e._parse( instring, loc, doActions )&#xa;            if exprtokens or exprtokens.haskeys():&#xa;                resultlist += exprtokens&#xa;        return loc, resultlist&#xa;&#xa;    def __iadd__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #And( [ self, other ] )&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;            if not e.mayReturnEmpty:&#xa;                break&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Or(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the expression that matches the longest string will be used.&#xa;    May be constructed using the C{'^'} operator.&#xa;&#xa;    Example::&#xa;        # construct Or using '^' operator&#xa;        &#xa;        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789""))&#xa;    prints::&#xa;        [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(Or,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        matches = []&#xa;        for e in self.exprs:&#xa;            try:&#xa;                loc2 = e.tryParse( instring, loc )&#xa;            except ParseException as err:&#xa;                err.__traceback__ = None&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;            else:&#xa;                # save match among all matches, to retry longest to shortest&#xa;                matches.append((loc2, e))&#xa;&#xa;        if matches:&#xa;            matches.sort(key=lambda x: -x[0])&#xa;            for _,e in matches:&#xa;                try:&#xa;                    return e._parse( instring, loc, doActions )&#xa;                except ParseException as err:&#xa;                    err.__traceback__ = None&#xa;                    if err.loc > maxExcLoc:&#xa;                        maxException = err&#xa;                        maxExcLoc = err.loc&#xa;&#xa;        if maxException is not None:&#xa;            maxException.msg = self.errmsg&#xa;            raise maxException&#xa;        else:&#xa;            raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;&#xa;    def __ixor__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #Or( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" ^ "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class MatchFirst(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the first one listed is the one that will match.&#xa;    May be constructed using the C{'|'} operator.&#xa;&#xa;    Example::&#xa;        # construct MatchFirst using '|' operator&#xa;        &#xa;        # watch the order of expressions to match&#xa;        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789"")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]&#xa;&#xa;        # put more selective expression first&#xa;        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)&#xa;        print(number.searchString(""123 3.1416 789"")) #  Better -> [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(MatchFirst,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        for e in self.exprs:&#xa;            try:&#xa;                ret = e._parse( instring, loc, doActions )&#xa;                return ret&#xa;            except ParseException as err:&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;&#xa;        # only got here if no expression matched, raise exception for match that made it the furthest&#xa;        else:&#xa;            if maxException is not None:&#xa;                maxException.msg = self.errmsg&#xa;                raise maxException&#xa;            else:&#xa;                raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;    def __ior__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #MatchFirst( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" | "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class Each(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found, but in any order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'&'} operator.&#xa;&#xa;    Example::&#xa;        color = oneOf(""RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN"")&#xa;        shape_type = oneOf(""SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON"")&#xa;        integer = Word(nums)&#xa;        shape_attr = ""shape:"" + shape_type(""shape"")&#xa;        posn_attr = ""posn:"" + Group(integer(""x"") + ',' + integer(""y""))(""posn"")&#xa;        color_attr = ""color:"" + color(""color"")&#xa;        size_attr = ""size:"" + integer(""size"")&#xa;&#xa;        # use Each (using operator '&') to accept attributes in any order &#xa;        # (shape and posn are required, color and size are optional)&#xa;        shape_spec = shape_attr & posn_attr & Optional(color_attr) & Optional(size_attr)&#xa;&#xa;        shape_spec.runTests('''&#xa;            shape: SQUARE color: BLACK posn: 100, 120&#xa;            shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;            color:GREEN size:20 shape:TRIANGLE posn:20,40&#xa;            '''&#xa;            )&#xa;    prints::&#xa;        shape: SQUARE color: BLACK posn: 100, 120&#xa;        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]&#xa;        - color: BLACK&#xa;        - posn: ['100', ',', '120']&#xa;          - x: 100&#xa;          - y: 120&#xa;        - shape: SQUARE&#xa;&#xa;&#xa;        shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]&#xa;        - color: BLUE&#xa;        - posn: ['50', ',', '80']&#xa;          - x: 50&#xa;          - y: 80&#xa;        - shape: CIRCLE&#xa;        - size: 50&#xa;&#xa;&#xa;        color: GREEN size: 20 shape: TRIANGLE posn: 20,40&#xa;        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]&#xa;        - color: GREEN&#xa;        - posn: ['20', ',', '40']&#xa;          - x: 20&#xa;          - y: 40&#xa;        - shape: TRIANGLE&#xa;        - size: 20&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(Each,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.skipWhitespace = True&#xa;        self.initExprGroups = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.initExprGroups:&#xa;            self.opt1map = dict((id(e.expr),e) for e in self.exprs if isinstance(e,Optional))&#xa;            opt1 = [ e.expr for e in self.exprs if isinstance(e,Optional) ]&#xa;            opt2 = [ e for e in self.exprs if e.mayReturnEmpty and not isinstance(e,Optional)]&#xa;            self.optionals = opt1 + opt2&#xa;            self.multioptionals = [ e.expr for e in self.exprs if isinstance(e,ZeroOrMore) ]&#xa;            self.multirequired = [ e.expr for e in self.exprs if isinstance(e,OneOrMore) ]&#xa;            self.required = [ e for e in self.exprs if not isinstance(e,(Optional,ZeroOrMore,OneOrMore)) ]&#xa;            self.required += self.multirequired&#xa;            self.initExprGroups = False&#xa;        tmpLoc = loc&#xa;        tmpReqd = self.required[:]&#xa;        tmpOpt  = self.optionals[:]&#xa;        matchOrder = []&#xa;&#xa;        keepMatching = True&#xa;        while keepMatching:&#xa;            tmpExprs = tmpReqd + tmpOpt + self.multioptionals + self.multirequired&#xa;            failed = []&#xa;            for e in tmpExprs:&#xa;                try:&#xa;                    tmpLoc = e.tryParse( instring, tmpLoc )&#xa;                except ParseException:&#xa;                    failed.append(e)&#xa;                else:&#xa;                    matchOrder.append(self.opt1map.get(id(e),e))&#xa;                    if e in tmpReqd:&#xa;                        tmpReqd.remove(e)&#xa;                    elif e in tmpOpt:&#xa;                        tmpOpt.remove(e)&#xa;            if len(failed) == len(tmpExprs):&#xa;                keepMatching = False&#xa;&#xa;        if tmpReqd:&#xa;            missing = "", "".join(_ustr(e) for e in tmpReqd)&#xa;            raise ParseException(instring,loc,""Missing one or more required elements (%s)"" % missing )&#xa;&#xa;        # add any unmatched Optionals, in case they have default values defined&#xa;        matchOrder += [e for e in self.exprs if isinstance(e,Optional) and e.expr in tmpOpt]&#xa;&#xa;        resultlist = []&#xa;        for e in matchOrder:&#xa;            loc,results = e._parse(instring,loc,doActions)&#xa;            resultlist.append(results)&#xa;&#xa;        finalResults = sum(resultlist, ParseResults([]))&#xa;        return loc, finalResults&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" & "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class ParseElementEnhance(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of C{ParserElement}, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(ParseElementEnhance,self).__init__(savelist)&#xa;        if isinstance( expr, basestring ):&#xa;            if issubclass(ParserElement._literalStringClass, Token):&#xa;                expr = ParserElement._literalStringClass(expr)&#xa;            else:&#xa;                expr = ParserElement._literalStringClass(Literal(expr))&#xa;        self.expr = expr&#xa;        self.strRepr = None&#xa;        if expr is not None:&#xa;            self.mayIndexError = expr.mayIndexError&#xa;            self.mayReturnEmpty = expr.mayReturnEmpty&#xa;            self.setWhitespaceChars( expr.whiteChars )&#xa;            self.skipWhitespace = expr.skipWhitespace&#xa;            self.saveAsList = expr.saveAsList&#xa;            self.callPreparse = expr.callPreparse&#xa;            self.ignoreExprs.extend(expr.ignoreExprs)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr is not None:&#xa;            return self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        else:&#xa;            raise ParseException("""",loc,self.errmsg,self)&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        self.expr = self.expr.copy()&#xa;        if self.expr is not None:&#xa;            self.expr.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseElementEnhance, self).ignore( other )&#xa;                if self.expr is not None:&#xa;                    self.expr.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseElementEnhance, self).ignore( other )&#xa;            if self.expr is not None:&#xa;                self.expr.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseElementEnhance,self).streamline()&#xa;        if self.expr is not None:&#xa;            self.expr.streamline()&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        if self in parseElementList:&#xa;            raise RecursiveGrammarException( parseElementList+[self] )&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        if self.expr is not None:&#xa;            self.expr.checkRecursion( subRecCheckList )&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        if self.expr is not None:&#xa;            self.expr.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseElementEnhance,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None and self.expr is not None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.expr) )&#xa;        return self.strRepr&#xa;&#xa;&#xa;class FollowedBy(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead matching of the given parse expression.  C{FollowedBy}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression matches at the current&#xa;    position.  C{FollowedBy} always returns a null token list.&#xa;&#xa;    Example::&#xa;        # use FollowedBy to match a label only if it is followed by a ':'&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        OneOrMore(attr_expr).parseString(""shape: SQUARE color: BLACK posn: upper left"").pprint()&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(FollowedBy,self).__init__(expr)&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self.expr.tryParse( instring, loc )&#xa;        return loc, []&#xa;&#xa;&#xa;class NotAny(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead to disallow matching with the given parse expression.  C{NotAny}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression does I{not} match at the current&#xa;    position.  Also, C{NotAny} does I{not} skip over leading whitespace. C{NotAny}&#xa;    always returns a null token list.  May be constructed using the '~' operator.&#xa;&#xa;    Example::&#xa;        &#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(NotAny,self).__init__(expr)&#xa;        #~ self.leaveWhitespace()&#xa;        self.skipWhitespace = False  # do NOT use self.leaveWhitespace(), don't want to propagate to exprs&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Found unwanted token, ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr.canParseNext(instring, loc):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""~{"" + _ustr(self.expr) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _MultipleMatch(ParseElementEnhance):&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(_MultipleMatch, self).__init__(expr)&#xa;        self.saveAsList = True&#xa;        ender = stopOn&#xa;        if isinstance(ender, basestring):&#xa;            ender = ParserElement._literalStringClass(ender)&#xa;        self.not_ender = ~ender if ender is not None else None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self_expr_parse = self.expr._parse&#xa;        self_skip_ignorables = self._skipIgnorables&#xa;        check_ender = self.not_ender is not None&#xa;        if check_ender:&#xa;            try_not_ender = self.not_ender.tryParse&#xa;        &#xa;        # must be at least one (but first see if we are the stopOn sentinel;&#xa;        # if so, fail)&#xa;        if check_ender:&#xa;            try_not_ender(instring, loc)&#xa;        loc, tokens = self_expr_parse( instring, loc, doActions, callPreParse=False )&#xa;        try:&#xa;            hasIgnoreExprs = (not not self.ignoreExprs)&#xa;            while 1:&#xa;                if check_ender:&#xa;                    try_not_ender(instring, loc)&#xa;                if hasIgnoreExprs:&#xa;                    preloc = self_skip_ignorables( instring, loc )&#xa;                else:&#xa;                    preloc = loc&#xa;                loc, tmptokens = self_expr_parse( instring, preloc, doActions )&#xa;                if tmptokens or tmptokens.haskeys():&#xa;                    tokens += tmptokens&#xa;        except (ParseException,IndexError):&#xa;            pass&#xa;&#xa;        return loc, tokens&#xa;        &#xa;class OneOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Repetition of one or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match one or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: BLACK""&#xa;        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]&#xa;&#xa;        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]&#xa;        &#xa;        # could also be written as&#xa;        (attr_expr * (1,)).parseString(text).pprint()&#xa;    """"""&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + _ustr(self.expr) + ""}...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class ZeroOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Optional repetition of zero or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example: similar to L{OneOrMore}&#xa;    """"""&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(ZeroOrMore,self).__init__(expr, stopOn=stopOn)&#xa;        self.mayReturnEmpty = True&#xa;        &#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)&#xa;        except (ParseException,IndexError):&#xa;            return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _NullToken(object):&#xa;    def __bool__(self):&#xa;        return False&#xa;    __nonzero__ = __bool__&#xa;    def __str__(self):&#xa;        return """"&#xa;&#xa;_optionalNotMatched = _NullToken()&#xa;class Optional(ParseElementEnhance):&#xa;    """"""&#xa;    Optional matching of the given expression.&#xa;&#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - default (optional) - value to be returned if the optional expression is not found.&#xa;&#xa;    Example::&#xa;        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier&#xa;        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))&#xa;        zip.runTests('''&#xa;            # traditional ZIP code&#xa;            12345&#xa;            &#xa;            # ZIP+4 form&#xa;            12101-0001&#xa;            &#xa;            # invalid ZIP&#xa;            98765-&#xa;            ''')&#xa;    prints::&#xa;        # traditional ZIP code&#xa;        12345&#xa;        ['12345']&#xa;&#xa;        # ZIP+4 form&#xa;        12101-0001&#xa;        ['12101-0001']&#xa;&#xa;        # invalid ZIP&#xa;        98765-&#xa;             ^&#xa;        FAIL: Expected end of text (at char 5), (line:1, col:6)&#xa;    """"""&#xa;    def __init__( self, expr, default=_optionalNotMatched ):&#xa;        super(Optional,self).__init__( expr, savelist=False )&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.defaultValue = default&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        except (ParseException,IndexError):&#xa;            if self.defaultValue is not _optionalNotMatched:&#xa;                if self.expr.resultsName:&#xa;                    tokens = ParseResults([ self.defaultValue ])&#xa;                    tokens[self.expr.resultsName] = self.defaultValue&#xa;                else:&#xa;                    tokens = [ self.defaultValue ]&#xa;            else:&#xa;                tokens = []&#xa;        return loc, tokens&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class SkipTo(ParseElementEnhance):&#xa;    """"""&#xa;    Token for skipping over all undefined text until the matched expression is found.&#xa;&#xa;    Parameters:&#xa;     - expr - target expression marking the end of the data to be skipped&#xa;     - include - (default=C{False}) if True, the target expression is also parsed &#xa;          (the skipped text and target expression are returned as a 2-element list).&#xa;     - ignore - (default=C{None}) used to define grammars (typically quoted strings and &#xa;          comments) that might contain false matches to the target expression&#xa;     - failOn - (default=C{None}) define expressions that are not allowed to be &#xa;          included in the skipped test; if found before the target expression is found, &#xa;          the SkipTo is not a match&#xa;&#xa;    Example::&#xa;        report = '''&#xa;            Outstanding Issues Report - 1 Jan 2000&#xa;&#xa;               # | Severity | Description                               |  Days Open&#xa;            -----+----------+-------------------------------------------+-----------&#xa;             101 | Critical | Intermittent system crash                 |          6&#xa;              94 | Cosmetic | Spelling error on Login ('log|n')         |         14&#xa;              79 | Minor    | System slow when running too many reports |         47&#xa;            '''&#xa;        integer = Word(nums)&#xa;        SEP = Suppress('|')&#xa;        # use SkipTo to simply match everything up until the next SEP&#xa;        # - ignore quoted strings, so that a '|' character inside a quoted string does not match&#xa;        # - parse action will call token.strip() for each matched token, i.e., the description body&#xa;        string_data = SkipTo(SEP, ignore=quotedString)&#xa;        string_data.setParseAction(tokenMap(str.strip))&#xa;        ticket_expr = (integer(""issue_num"") + SEP &#xa;                      + string_data(""sev"") + SEP &#xa;                      + string_data(""desc"") + SEP &#xa;                      + integer(""days_open""))&#xa;        &#xa;        for tkt in ticket_expr.searchString(report):&#xa;            print tkt.dump()&#xa;    prints::&#xa;        ['101', 'Critical', 'Intermittent system crash', '6']&#xa;        - days_open: 6&#xa;        - desc: Intermittent system crash&#xa;        - issue_num: 101&#xa;        - sev: Critical&#xa;        ['94', 'Cosmetic', ""Spelling error on Login ('log|n')"", '14']&#xa;        - days_open: 14&#xa;        - desc: Spelling error on Login ('log|n')&#xa;        - issue_num: 94&#xa;        - sev: Cosmetic&#xa;        ['79', 'Minor', 'System slow when running too many reports', '47']&#xa;        - days_open: 47&#xa;        - desc: System slow when running too many reports&#xa;        - issue_num: 79&#xa;        - sev: Minor&#xa;    """"""&#xa;    def __init__( self, other, include=False, ignore=None, failOn=None ):&#xa;        super( SkipTo, self ).__init__( other )&#xa;        self.ignoreExpr = ignore&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.includeMatch = include&#xa;        self.asList = False&#xa;        if isinstance(failOn, basestring):&#xa;            self.failOn = ParserElement._literalStringClass(failOn)&#xa;        else:&#xa;            self.failOn = failOn&#xa;        self.errmsg = ""No match found for ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        startloc = loc&#xa;        instrlen = len(instring)&#xa;        expr = self.expr&#xa;        expr_parse = self.expr._parse&#xa;        self_failOn_canParseNext = self.failOn.canParseNext if self.failOn is not None else None&#xa;        self_ignoreExpr_tryParse = self.ignoreExpr.tryParse if self.ignoreExpr is not None else None&#xa;        &#xa;        tmploc = loc&#xa;        while tmploc <= instrlen:&#xa;            if self_failOn_canParseNext is not None:&#xa;                # break if failOn expression matches&#xa;                if self_failOn_canParseNext(instring, tmploc):&#xa;                    break&#xa;                    &#xa;            if self_ignoreExpr_tryParse is not None:&#xa;                # advance past ignore expressions&#xa;                while 1:&#xa;                    try:&#xa;                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)&#xa;                    except ParseBaseException:&#xa;                        break&#xa;            &#xa;            try:&#xa;                expr_parse(instring, tmploc, doActions=False, callPreParse=False)&#xa;            except (ParseException, IndexError):&#xa;                # no match, advance loc in string&#xa;                tmploc += 1&#xa;            else:&#xa;                # matched skipto expr, done&#xa;                break&#xa;&#xa;        else:&#xa;            # ran off the end of the input string without matching skipto expr, fail&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        # build up return values&#xa;        loc = tmploc&#xa;        skiptext = instring[startloc:loc]&#xa;        skipresult = ParseResults(skiptext)&#xa;        &#xa;        if self.includeMatch:&#xa;            loc, mat = expr_parse(instring,loc,doActions,callPreParse=False)&#xa;            skipresult += mat&#xa;&#xa;        return loc, skipresult&#xa;&#xa;class Forward(ParseElementEnhance):&#xa;    """"""&#xa;    Forward declaration of an expression to be defined later -&#xa;    used for recursive grammars, such as algebraic infix notation.&#xa;    When the expression is known, it is assigned to the C{Forward} variable using the '<<' operator.&#xa;&#xa;    Note: take care when assigning to C{Forward} not to overlook precedence of operators.&#xa;    Specifically, '|' has a lower precedence than '<<', so that::&#xa;        fwdExpr << a | b | c&#xa;    will actually be evaluated as::&#xa;        (fwdExpr << a) | b | c&#xa;    thereby leaving b and c out as parseable alternatives.  It is recommended that you&#xa;    explicitly group the values inserted into the C{Forward}::&#xa;        fwdExpr << (a | b | c)&#xa;    Converting to use the '<<=' operator instead will avoid this problem.&#xa;&#xa;    See L{ParseResults.pprint} for an example of a recursive parser created using&#xa;    C{Forward}.&#xa;    """"""&#xa;    def __init__( self, other=None ):&#xa;        super(Forward,self).__init__( other, savelist=False )&#xa;&#xa;    def __lshift__( self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass(other)&#xa;        self.expr = other&#xa;        self.strRepr = None&#xa;        self.mayIndexError = self.expr.mayIndexError&#xa;        self.mayReturnEmpty = self.expr.mayReturnEmpty&#xa;        self.setWhitespaceChars( self.expr.whiteChars )&#xa;        self.skipWhitespace = self.expr.skipWhitespace&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.ignoreExprs.extend(self.expr.ignoreExprs)&#xa;        return self&#xa;        &#xa;    def __ilshift__(self, other):&#xa;        return self << other&#xa;    &#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        if not self.streamlined:&#xa;            self.streamlined = True&#xa;            if self.expr is not None:&#xa;                self.expr.streamline()&#xa;        return self&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        if self not in validateTrace:&#xa;            tmp = validateTrace[:]+[self]&#xa;            if self.expr is not None:&#xa;                self.expr.validate(tmp)&#xa;        self.checkRecursion([])&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;        return self.__class__.__name__ + "": ...""&#xa;&#xa;        # stubbed out for now - creates awful memory and perf issues&#xa;        self._revertClass = self.__class__&#xa;        self.__class__ = _ForwardNoRecurse&#xa;        try:&#xa;            if self.expr is not None:&#xa;                retString = _ustr(self.expr)&#xa;            else:&#xa;                retString = ""None""&#xa;        finally:&#xa;            self.__class__ = self._revertClass&#xa;        return self.__class__.__name__ + "": "" + retString&#xa;&#xa;    def copy(self):&#xa;        if self.expr is not None:&#xa;            return super(Forward,self).copy()&#xa;        else:&#xa;            ret = Forward()&#xa;            ret <<= self&#xa;            return ret&#xa;&#xa;class _ForwardNoRecurse(Forward):&#xa;    def __str__( self ):&#xa;        return ""...""&#xa;&#xa;class TokenConverter(ParseElementEnhance):&#xa;    """"""&#xa;    Abstract subclass of C{ParseExpression}, for converting parsed results.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(TokenConverter,self).__init__( expr )#, savelist )&#xa;        self.saveAsList = False&#xa;&#xa;class Combine(TokenConverter):&#xa;    """"""&#xa;    Converter to concatenate all matching tokens to a single string.&#xa;    By default, the matching patterns must also be contiguous in the input string;&#xa;    this can be disabled by specifying C{'adjacent=False'} in the constructor.&#xa;&#xa;    Example::&#xa;        real = Word(nums) + '.' + Word(nums)&#xa;        print(real.parseString('3.1416')) # -> ['3', '.', '1416']&#xa;        # will also erroneously match the following&#xa;        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']&#xa;&#xa;        real = Combine(Word(nums) + '.' + Word(nums))&#xa;        print(real.parseString('3.1416')) # -> ['3.1416']&#xa;        # no match when there are internal spaces&#xa;        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)&#xa;    """"""&#xa;    def __init__( self, expr, joinString="""", adjacent=True ):&#xa;        super(Combine,self).__init__( expr )&#xa;        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself&#xa;        if adjacent:&#xa;            self.leaveWhitespace()&#xa;        self.adjacent = adjacent&#xa;        self.skipWhitespace = True&#xa;        self.joinString = joinString&#xa;        self.callPreparse = True&#xa;&#xa;    def ignore( self, other ):&#xa;        if self.adjacent:&#xa;            ParserElement.ignore(self, other)&#xa;        else:&#xa;            super( Combine, self).ignore( other )&#xa;        return self&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        retToks = tokenlist.copy()&#xa;        del retToks[:]&#xa;        retToks += ParseResults([ """".join(tokenlist._asStringList(self.joinString)) ], modal=self.modalResults)&#xa;&#xa;        if self.resultsName and retToks.haskeys():&#xa;            return [ retToks ]&#xa;        else:&#xa;            return retToks&#xa;&#xa;class Group(TokenConverter):&#xa;    """"""&#xa;    Converter to return the matched tokens as a list - useful for returning tokens of C{L{ZeroOrMore}} and C{L{OneOrMore}} expressions.&#xa;&#xa;    Example::&#xa;        ident = Word(alphas)&#xa;        num = Word(nums)&#xa;        term = ident | num&#xa;        func = ident + Optional(delimitedList(term))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', 'a', 'b', '100']&#xa;&#xa;        func = ident + Group(Optional(delimitedList(term)))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', ['a', 'b', '100']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Group,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return [ tokenlist ]&#xa;&#xa;class Dict(TokenConverter):&#xa;    """"""&#xa;    Converter to return a repetitive expression as a list, but also as a dictionary.&#xa;    Each element can also be referenced using the first token in the expression as its key.&#xa;    Useful for tabular report scraping when the first column can be used as a item key.&#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        # print attributes as plain groups&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names&#xa;        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)&#xa;        print(result.dump())&#xa;        &#xa;        # access named fields as dict entries, or output as dict&#xa;        print(result['shape'])        &#xa;        print(result.asDict())&#xa;    prints::&#xa;        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']&#xa;&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}&#xa;    See more examples at L{ParseResults} of accessing fields by results name.&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Dict,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        for i,tok in enumerate(tokenlist):&#xa;            if len(tok) == 0:&#xa;                continue&#xa;            ikey = tok[0]&#xa;            if isinstance(ikey,int):&#xa;                ikey = _ustr(tok[0]).strip()&#xa;            if len(tok)==1:&#xa;                tokenlist[ikey] = _ParseResultsWithOffset("""",i)&#xa;            elif len(tok)==2 and not isinstance(tok[1],ParseResults):&#xa;                tokenlist[ikey] = _ParseResultsWithOffset(tok[1],i)&#xa;            else:&#xa;                dictvalue = tok.copy() #ParseResults(i)&#xa;                del dictvalue[0]&#xa;                if len(dictvalue)!= 1 or (isinstance(dictvalue,ParseResults) and dictvalue.haskeys()):&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue,i)&#xa;                else:&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0],i)&#xa;&#xa;        if self.resultsName:&#xa;            return [ tokenlist ]&#xa;        else:&#xa;            return tokenlist&#xa;&#xa;&#xa;class Suppress(TokenConverter):&#xa;    """"""&#xa;    Converter for ignoring the results of a parsed expression.&#xa;&#xa;    Example::&#xa;        source = ""a, b, c,d""&#xa;        wd = Word(alphas)&#xa;        wd_list1 = wd + ZeroOrMore(',' + wd)&#xa;        print(wd_list1.parseString(source))&#xa;&#xa;        # often, delimiters that are useful during parsing are just in the&#xa;        # way afterward - use Suppress to keep them out of the parsed output&#xa;        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)&#xa;        print(wd_list2.parseString(source))&#xa;    prints::&#xa;        ['a', ',', 'b', ',', 'c', ',', 'd']&#xa;        ['a', 'b', 'c', 'd']&#xa;    (See also L{delimitedList}.)&#xa;    """"""&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return []&#xa;&#xa;    def suppress( self ):&#xa;        return self&#xa;&#xa;&#xa;class OnlyOnce(object):&#xa;    """"""&#xa;    Wrapper for parse actions, to ensure they are only called once.&#xa;    """"""&#xa;    def __init__(self, methodCall):&#xa;        self.callable = _trim_arity(methodCall)&#xa;        self.called = False&#xa;    def __call__(self,s,l,t):&#xa;        if not self.called:&#xa;            results = self.callable(s,l,t)&#xa;            self.called = True&#xa;            return results&#xa;        raise ParseException(s,l,"""")&#xa;    def reset(self):&#xa;        self.called = False&#xa;&#xa;def traceParseAction(f):&#xa;    """"""&#xa;    Decorator for debugging parse actions. &#xa;    &#xa;    When the parse action is called, this decorator will print C{"">> entering I{method-name}(line:I{current_source_line}, I{parse_location}, I{matched_tokens})"".}&#xa;    When the parse action completes, the decorator will print C{""<<""} followed by the returned value, or any exception that the parse action raised.&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;&#xa;        @traceParseAction&#xa;        def remove_duplicate_chars(tokens):&#xa;            return ''.join(sorted(set(''.join(tokens)))&#xa;&#xa;        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)&#xa;        print(wds.parseString(""slkdjs sld sldd sdlf sdljf""))&#xa;    prints::&#xa;        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))&#xa;        <<leaving remove_duplicate_chars (ret: 'dfjkls')&#xa;        ['dfjkls']&#xa;    """"""&#xa;    f = _trim_arity(f)&#xa;    def z(*paArgs):&#xa;        thisFunc = f.__name__&#xa;        s,l,t = paArgs[-3:]&#xa;        if len(paArgs)>3:&#xa;            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc&#xa;        sys.stderr.write( "">>entering %s(line: '%s', %d, %r)\n"" % (thisFunc,line(l,s),l,t) )&#xa;        try:&#xa;            ret = f(*paArgs)&#xa;        except Exception as exc:&#xa;            sys.stderr.write( ""<<leaving %s (exception: %s)\n"" % (thisFunc,exc) )&#xa;            raise&#xa;        sys.stderr.write( ""<<leaving %s (ret: %r)\n"" % (thisFunc,ret) )&#xa;        return ret&#xa;    try:&#xa;        z.__name__ = f.__name__&#xa;    except AttributeError:&#xa;        pass&#xa;    return z&#xa;&#xa;#&#xa;# global helpers&#xa;#&#xa;def delimitedList( expr, delim="","", combine=False ):&#xa;    """"""&#xa;    Helper to define a delimited list of expressions - the delimiter defaults to ','.&#xa;    By default, the list elements and delimiters can have intervening whitespace, and&#xa;    comments, but this can be overridden by passing C{combine=True} in the constructor.&#xa;    If C{combine} is set to C{True}, the matching tokens are returned as a single token&#xa;    string, with the delimiters included; otherwise, the matching tokens are returned&#xa;    as a list of tokens, with the delimiters suppressed.&#xa;&#xa;    Example::&#xa;        delimitedList(Word(alphas)).parseString(""aa,bb,cc"") # -> ['aa', 'bb', 'cc']&#xa;        delimitedList(Word(hexnums), delim=':', combine=True).parseString(""AA:BB:CC:DD:EE"") # -> ['AA:BB:CC:DD:EE']&#xa;    """"""&#xa;    dlName = _ustr(expr)+"" [""+_ustr(delim)+"" ""+_ustr(expr)+""]...""&#xa;    if combine:&#xa;        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)&#xa;    else:&#xa;        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)&#xa;&#xa;def countedArray( expr, intExpr=None ):&#xa;    """"""&#xa;    Helper to define a counted list of expressions.&#xa;    This helper defines a pattern of the form::&#xa;        integer expr expr expr...&#xa;    where the leading integer tells how many expr expressions follow.&#xa;    The matched tokens returns the array of expr tokens as a list - the leading count token is suppressed.&#xa;    &#xa;    If C{intExpr} is specified, it should be a pyparsing expression that produces an integer value.&#xa;&#xa;    Example::&#xa;        countedArray(Word(alphas)).parseString('2 ab cd ef')  # -> ['ab', 'cd']&#xa;&#xa;        # in this parser, the leading integer value is given in binary,&#xa;        # '10' indicating that 2 values are in the array&#xa;        binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))&#xa;        countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')  # -> ['ab', 'cd']&#xa;    """"""&#xa;    arrayExpr = Forward()&#xa;    def countFieldParseAction(s,l,t):&#xa;        n = t[0]&#xa;        arrayExpr << (n and Group(And([expr]*n)) or Group(empty))&#xa;        return []&#xa;    if intExpr is None:&#xa;        intExpr = Word(nums).setParseAction(lambda t:int(t[0]))&#xa;    else:&#xa;        intExpr = intExpr.copy()&#xa;    intExpr.setName(""arrayLen"")&#xa;    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)&#xa;    return ( intExpr + arrayExpr ).setName('(len) ' + _ustr(expr) + '...')&#xa;&#xa;def _flatten(L):&#xa;    ret = []&#xa;    for i in L:&#xa;        if isinstance(i,list):&#xa;            ret.extend(_flatten(i))&#xa;        else:&#xa;            ret.append(i)&#xa;    return ret&#xa;&#xa;def matchPreviousLiteral(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousLiteral(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches a&#xa;    previous literal, will also match the leading C{""1:1""} in C{""1:10""}.&#xa;    If this is not desired, use C{matchPreviousExpr}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        if t:&#xa;            if len(t) == 1:&#xa;                rep << t[0]&#xa;            else:&#xa;                # flatten t tokens&#xa;                tflat = _flatten(t.asList())&#xa;                rep << And(Literal(tt) for tt in tflat)&#xa;        else:&#xa;            rep << Empty()&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def matchPreviousExpr(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousExpr(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches by&#xa;    expressions, will I{not} match the leading C{""1:1""} in C{""1:10""};&#xa;    the expressions are evaluated first, and then compared, so&#xa;    C{""1""} is compared with C{""10""}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    e2 = expr.copy()&#xa;    rep <<= e2&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        matchTokens = _flatten(t.asList())&#xa;        def mustMatchTheseTokens(s,l,t):&#xa;            theseTokens = _flatten(t.asList())&#xa;            if  theseTokens != matchTokens:&#xa;                raise ParseException("""",0,"""")&#xa;        rep.setParseAction( mustMatchTheseTokens, callDuringTry=True )&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def _escapeRegexRangeChars(s):&#xa;    #~  escape these chars: ^-]&#xa;    for c in r""\^-]"":&#xa;        s = s.replace(c,_bslash+c)&#xa;    s = s.replace(""\n"",r""\n"")&#xa;    s = s.replace(""\t"",r""\t"")&#xa;    return _ustr(s)&#xa;&#xa;def oneOf( strs, caseless=False, useRegex=True ):&#xa;    """"""&#xa;    Helper to quickly define a set of alternative Literals, and makes sure to do&#xa;    longest-first testing when there is a conflict, regardless of the input order,&#xa;    but returns a C{L{MatchFirst}} for best performance.&#xa;&#xa;    Parameters:&#xa;     - strs - a string of space-delimited literals, or a collection of string literals&#xa;     - caseless - (default=C{False}) - treat all literals as caseless&#xa;     - useRegex - (default=C{True}) - as an optimization, will generate a Regex&#xa;          object; otherwise, will generate a C{MatchFirst} object (if C{caseless=True}, or&#xa;          if creating a C{Regex} raises an exception)&#xa;&#xa;    Example::&#xa;        comp_oper = oneOf(""< = > <= >= !="")&#xa;        var = Word(alphas)&#xa;        number = Word(nums)&#xa;        term = var | number&#xa;        comparison_expr = term + comp_oper + term&#xa;        print(comparison_expr.searchString(""B = 12  AA=23 B<=AA AA>12""))&#xa;    prints::&#xa;        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]&#xa;    """"""&#xa;    if caseless:&#xa;        isequal = ( lambda a,b: a.upper() == b.upper() )&#xa;        masks = ( lambda a,b: b.upper().startswith(a.upper()) )&#xa;        parseElementClass = CaselessLiteral&#xa;    else:&#xa;        isequal = ( lambda a,b: a == b )&#xa;        masks = ( lambda a,b: b.startswith(a) )&#xa;        parseElementClass = Literal&#xa;&#xa;    symbols = []&#xa;    if isinstance(strs,basestring):&#xa;        symbols = strs.split()&#xa;    elif isinstance(strs, collections.Iterable):&#xa;        symbols = list(strs)&#xa;    else:&#xa;        warnings.warn(""Invalid argument to oneOf, expected string or iterable"",&#xa;                SyntaxWarning, stacklevel=2)&#xa;    if not symbols:&#xa;        return NoMatch()&#xa;&#xa;    i = 0&#xa;    while i < len(symbols)-1:&#xa;        cur = symbols[i]&#xa;        for j,other in enumerate(symbols[i+1:]):&#xa;            if ( isequal(other, cur) ):&#xa;                del symbols[i+j+1]&#xa;                break&#xa;            elif ( masks(cur, other) ):&#xa;                del symbols[i+j+1]&#xa;                symbols.insert(i,other)&#xa;                cur = other&#xa;                break&#xa;        else:&#xa;            i += 1&#xa;&#xa;    if not caseless and useRegex:&#xa;        #~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))&#xa;        try:&#xa;            if len(symbols)==len("""".join(symbols)):&#xa;                return Regex( ""[%s]"" % """".join(_escapeRegexRangeChars(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;            else:&#xa;                return Regex( ""|"".join(re.escape(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;        except Exception:&#xa;            warnings.warn(""Exception creating Regex for oneOf, building MatchFirst"",&#xa;                    SyntaxWarning, stacklevel=2)&#xa;&#xa;&#xa;    # last resort, just use MatchFirst&#xa;    return MatchFirst(parseElementClass(sym) for sym in symbols).setName(' | '.join(symbols))&#xa;&#xa;def dictOf( key, value ):&#xa;    """"""&#xa;    Helper to easily and clearly define a dictionary by specifying the respective patterns&#xa;    for the key and value.  Takes care of defining the C{L{Dict}}, C{L{ZeroOrMore}}, and C{L{Group}} tokens&#xa;    in the proper order.  The key pattern can include delimiting markers or punctuation,&#xa;    as long as they are suppressed, thereby leaving the significant key text.  The value&#xa;    pattern can include named results, so that the C{Dict} results can include named token&#xa;    fields.&#xa;&#xa;    Example::&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        attr_label = label&#xa;        attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)&#xa;&#xa;        # similar to Dict, but simpler call format&#xa;        result = dictOf(attr_label, attr_value).parseString(text)&#xa;        print(result.dump())&#xa;        print(result['shape'])&#xa;        print(result.shape)  # object attribute access works too&#xa;        print(result.asDict())&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        SQUARE&#xa;        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}&#xa;    """"""&#xa;    return Dict( ZeroOrMore( Group ( key + value ) ) )&#xa;&#xa;def originalTextFor(expr, asString=True):&#xa;    """"""&#xa;    Helper to return the original, untokenized text for a given expression.  Useful to&#xa;    restore the parsed fields of an HTML start tag into the raw tag text itself, or to&#xa;    revert separate tokens with intervening whitespace back to the original matching&#xa;    input text. By default, returns astring containing the original parsed text.  &#xa;       &#xa;    If the optional C{asString} argument is passed as C{False}, then the return value is a &#xa;    C{L{ParseResults}} containing any results names that were originally matched, and a &#xa;    single token containing the original matched text from the input string.  So if &#xa;    the expression passed to C{L{originalTextFor}} contains expressions with defined&#xa;    results names, you must set C{asString} to C{False} if you want to preserve those&#xa;    results name values.&#xa;&#xa;    Example::&#xa;        src = ""this is test <b> bold <i>text</i> </b> normal text ""&#xa;        for tag in (""b"",""i""):&#xa;            opener,closer = makeHTMLTags(tag)&#xa;            patt = originalTextFor(opener + SkipTo(closer) + closer)&#xa;            print(patt.searchString(src)[0])&#xa;    prints::&#xa;        ['<b> bold <i>text</i> </b>']&#xa;        ['<i>text</i>']&#xa;    """"""&#xa;    locMarker = Empty().setParseAction(lambda s,loc,t: loc)&#xa;    endlocMarker = locMarker.copy()&#xa;    endlocMarker.callPreparse = False&#xa;    matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")&#xa;    if asString:&#xa;        extractText = lambda s,l,t: s[t._original_start:t._original_end]&#xa;    else:&#xa;        def extractText(s,l,t):&#xa;            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]&#xa;    matchExpr.setParseAction(extractText)&#xa;    matchExpr.ignoreExprs = expr.ignoreExprs&#xa;    return matchExpr&#xa;&#xa;def ungroup(expr): &#xa;    """"""&#xa;    Helper to undo pyparsing's default grouping of And expressions, even&#xa;    if all but one are non-empty.&#xa;    """"""&#xa;    return TokenConverter(expr).setParseAction(lambda t:t[0])&#xa;&#xa;def locatedExpr(expr):&#xa;    """"""&#xa;    Helper to decorate a returned token with its starting and ending locations in the input string.&#xa;    This helper adds the following results names:&#xa;     - locn_start = location where matched expression begins&#xa;     - locn_end = location where matched expression ends&#xa;     - value = the actual parsed results&#xa;&#xa;    Be careful if the input text contains C{<TAB>} characters, you may want to call&#xa;    C{L{ParserElement.parseWithTabs}}&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;        for match in locatedExpr(wd).searchString(""ljsdf123lksdjjf123lkkjj1222""):&#xa;            print(match)&#xa;    prints::&#xa;        [[0, 'ljsdf', 5]]&#xa;        [[8, 'lksdjjf', 15]]&#xa;        [[18, 'lkkjj', 23]]&#xa;    """"""&#xa;    locator = Empty().setParseAction(lambda s,l,t: l)&#xa;    return Group(locator(""locn_start"") + expr(""value"") + locator.copy().leaveWhitespace()(""locn_end""))&#xa;&#xa;&#xa;# convenience constants for positional expressions&#xa;empty       = Empty().setName(""empty"")&#xa;lineStart   = LineStart().setName(""lineStart"")&#xa;lineEnd     = LineEnd().setName(""lineEnd"")&#xa;stringStart = StringStart().setName(""stringStart"")&#xa;stringEnd   = StringEnd().setName(""stringEnd"")&#xa;&#xa;_escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])&#xa;_escapedHexChar = Regex(r""\\0?[xX][0-9a-fA-F]+"").setParseAction(lambda s,l,t:unichr(int(t[0].lstrip(r'\0x'),16)))&#xa;_escapedOctChar = Regex(r""\\0[0-7]+"").setParseAction(lambda s,l,t:unichr(int(t[0][1:],8)))&#xa;_singleChar = _escapedPunc | _escapedHexChar | _escapedOctChar | Word(printables, excludeChars=r'\]', exact=1) | Regex(r""\w"", re.UNICODE)&#xa;_charRange = Group(_singleChar + Suppress(""-"") + _singleChar)&#xa;_reBracketExpr = Literal(""["") + Optional(""^"").setResultsName(""negate"") + Group( OneOrMore( _charRange | _singleChar ) ).setResultsName(""body"") + ""]""&#xa;&#xa;def srange(s):&#xa;    r""""""&#xa;    Helper to easily define string ranges for use in Word construction.  Borrows&#xa;    syntax from regexp '[]' string range definitions::&#xa;        srange(""[0-9]"")   -> ""0123456789""&#xa;        srange(""[a-z]"")   -> ""abcdefghijklmnopqrstuvwxyz""&#xa;        srange(""[a-z$_]"") -> ""abcdefghijklmnopqrstuvwxyz$_""&#xa;    The input string must be enclosed in []'s, and the returned string is the expanded&#xa;    character set joined into a single string.&#xa;    The values enclosed in the []'s may be:&#xa;     - a single character&#xa;     - an escaped character with a leading backslash (such as C{\-} or C{\]})&#xa;     - an escaped hex character with a leading C{'\x'} (C{\x21}, which is a C{'!'} character) &#xa;         (C{\0x##} is also supported for backwards compatibility) &#xa;     - an escaped octal character with a leading C{'\0'} (C{\041}, which is a C{'!'} character)&#xa;     - a range of any of the above, separated by a dash (C{'a-z'}, etc.)&#xa;     - any combination of the above (C{'aeiouy'}, C{'a-zA-Z0-9_$'}, etc.)&#xa;    """"""&#xa;    _expanded = lambda p: p if not isinstance(p,ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]),ord(p[1])+1))&#xa;    try:&#xa;        return """".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)&#xa;    except Exception:&#xa;        return """"&#xa;&#xa;def matchOnlyAtCol(n):&#xa;    """"""&#xa;    Helper method for defining parse actions that require matching at a specific&#xa;    column in the input text.&#xa;    """"""&#xa;    def verifyCol(strg,locn,toks):&#xa;        if col(locn,strg) != n:&#xa;            raise ParseException(strg,locn,""matched token not at column %d"" % n)&#xa;    return verifyCol&#xa;&#xa;def replaceWith(replStr):&#xa;    """"""&#xa;    Helper method for common parse actions that simply return a literal value.  Especially&#xa;    useful when used with C{L{transformString<ParserElement.transformString>}()}.&#xa;&#xa;    Example::&#xa;        num = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;        na = oneOf(""N/A NA"").setParseAction(replaceWith(math.nan))&#xa;        term = na | num&#xa;        &#xa;        OneOrMore(term).parseString(""324 234 N/A 234"") # -> [324, 234, nan, 234]&#xa;    """"""&#xa;    return lambda s,l,t: [replStr]&#xa;&#xa;def removeQuotes(s,l,t):&#xa;    """"""&#xa;    Helper parse action for removing quotation marks from parsed quoted strings.&#xa;&#xa;    Example::&#xa;        # by default, quotation marks are included in parsed results&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""'Now is the Winter of our Discontent'""]&#xa;&#xa;        # use removeQuotes to strip quotation marks from parsed results&#xa;        quotedString.setParseAction(removeQuotes)&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""Now is the Winter of our Discontent""]&#xa;    """"""&#xa;    return t[0][1:-1]&#xa;&#xa;def tokenMap(func, *args):&#xa;    """"""&#xa;    Helper to define a parse action by mapping a function to all elements of a ParseResults list.If any additional &#xa;    args are passed, they are forwarded to the given function as additional arguments after&#xa;    the token, as in C{hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))}, which will convert the&#xa;    parsed data to an integer using base 16.&#xa;&#xa;    Example (compare the last to example in L{ParserElement.transformString}::&#xa;        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))&#xa;        hex_ints.runTests('''&#xa;            00 11 22 aa FF 0a 0d 1a&#xa;            ''')&#xa;        &#xa;        upperword = Word(alphas).setParseAction(tokenMap(str.upper))&#xa;        OneOrMore(upperword).runTests('''&#xa;            my kingdom for a horse&#xa;            ''')&#xa;&#xa;        wd = Word(alphas).setParseAction(tokenMap(str.title))&#xa;        OneOrMore(wd).setParseAction(' '.join).runTests('''&#xa;            now is the winter of our discontent made glorious summer by this sun of york&#xa;            ''')&#xa;    prints::&#xa;        00 11 22 aa FF 0a 0d 1a&#xa;        [0, 17, 34, 170, 255, 10, 13, 26]&#xa;&#xa;        my kingdom for a horse&#xa;        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']&#xa;&#xa;        now is the winter of our discontent made glorious summer by this sun of york&#xa;        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']&#xa;    """"""&#xa;    def pa(s,l,t):&#xa;        return [func(tokn, *args) for tokn in t]&#xa;&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    pa.__name__ = func_name&#xa;&#xa;    return pa&#xa;&#xa;upcaseTokens = tokenMap(lambda t: _ustr(t).upper())&#xa;""""""(Deprecated) Helper parse action to convert tokens to upper case. Deprecated in favor of L{pyparsing_common.upcaseTokens}""""""&#xa;&#xa;downcaseTokens = tokenMap(lambda t: _ustr(t).lower())&#xa;""""""(Deprecated) Helper parse action to convert tokens to lower case. Deprecated in favor of L{pyparsing_common.downcaseTokens}""""""&#xa;    &#xa;def _makeTags(tagStr, xml):&#xa;    """"""Internal helper to construct opening and closing tag expressions, given a tag name""""""&#xa;    if isinstance(tagStr,basestring):&#xa;        resname = tagStr&#xa;        tagStr = Keyword(tagStr, caseless=not xml)&#xa;    else:&#xa;        resname = tagStr.name&#xa;&#xa;    tagAttrName = Word(alphas,alphanums+""_-:"")&#xa;    if (xml):&#xa;        tagAttrValue = dblQuotedString.copy().setParseAction( removeQuotes )&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName + Suppress(""="") + tagAttrValue ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    else:&#xa;        printablesLessRAbrack = """".join(c for c in printables if c not in "">"")&#xa;        tagAttrValue = quotedString.copy().setParseAction( removeQuotes ) | Word(printablesLessRAbrack)&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \&#xa;                Optional( Suppress(""="") + tagAttrValue ) ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    closeTag = Combine(_L(""</"") + tagStr + "">"")&#xa;&#xa;    openTag = openTag.setResultsName(""start""+"""".join(resname.replace("":"","" "").title().split())).setName(""<%s>"" % resname)&#xa;    closeTag = closeTag.setResultsName(""end""+"""".join(resname.replace("":"","" "").title().split())).setName(""</%s>"" % resname)&#xa;    openTag.tag = resname&#xa;    closeTag.tag = resname&#xa;    return openTag, closeTag&#xa;&#xa;def makeHTMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for HTML, given a tag name. Matches&#xa;    tags in either upper or lower case, attributes with namespaces and with quoted or unquoted values.&#xa;&#xa;    Example::&#xa;        text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;        # makeHTMLTags returns pyparsing expressions for the opening and closing tags as a 2-tuple&#xa;        a,a_end = makeHTMLTags(""A"")&#xa;        link_expr = a + SkipTo(a_end)(""link_text"") + a_end&#xa;        &#xa;        for link in link_expr.searchString(text):&#xa;            # attributes in the <A> tag (like ""href"" shown here) are also accessible as named results&#xa;            print(link.link_text, '->', link.href)&#xa;    prints::&#xa;        pyparsing -> http://pyparsing.wikispaces.com&#xa;    """"""&#xa;    return _makeTags( tagStr, False )&#xa;&#xa;def makeXMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for XML, given a tag name. Matches&#xa;    tags only in the given upper/lower case.&#xa;&#xa;    Example: similar to L{makeHTMLTags}&#xa;    """"""&#xa;    return _makeTags( tagStr, True )&#xa;&#xa;def withAttribute(*args,**attrDict):&#xa;    """"""&#xa;    Helper to create a validating parse action to be used with start tags created&#xa;    with C{L{makeXMLTags}} or C{L{makeHTMLTags}}. Use C{withAttribute} to qualify a starting tag&#xa;    with a required attribute value, to avoid false matches on common tags such as&#xa;    C{<TD>} or C{<DIV>}.&#xa;&#xa;    Call C{withAttribute} with a series of attribute names and values. Specify the list&#xa;    of filter attributes names and values as:&#xa;     - keyword arguments, as in C{(align=""right"")}, or&#xa;     - as an explicit dict with C{**} operator, when an attribute name is also a Python&#xa;          reserved word, as in C{**{""class"":""Customer"", ""align"":""right""}}&#xa;     - a list of name-value tuples, as in ( (""ns1:class"", ""Customer""), (""ns2:align"",""right"") )&#xa;    For attribute names with a namespace prefix, you must use the second form.  Attribute&#xa;    names are matched insensitive to upper/lower case.&#xa;       &#xa;    If just testing for C{class} (with or without a namespace), use C{L{withClass}}.&#xa;&#xa;    To verify that the attribute exists, but without specifying a value, pass&#xa;    C{withAttribute.ANY_VALUE} as the value.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div type=""grid"">1 4 0 1 0</div>&#xa;            <div type=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this has no type</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;&#xa;        # only match div tag having a type attribute with value ""grid""&#xa;        div_grid = div().setParseAction(withAttribute(type=""grid""))&#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        # construct a match with any div tag having a type attribute, regardless of the value&#xa;        div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    if args:&#xa;        attrs = args[:]&#xa;    else:&#xa;        attrs = attrDict.items()&#xa;    attrs = [(k,v) for k,v in attrs]&#xa;    def pa(s,l,tokens):&#xa;        for attrName,attrValue in attrs:&#xa;            if attrName not in tokens:&#xa;                raise ParseException(s,l,""no matching attribute "" + attrName)&#xa;            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:&#xa;                raise ParseException(s,l,""attribute '%s' has value '%s', must be '%s'"" %&#xa;                                            (attrName, tokens[attrName], attrValue))&#xa;    return pa&#xa;withAttribute.ANY_VALUE = object()&#xa;&#xa;def withClass(classname, namespace=''):&#xa;    """"""&#xa;    Simplified version of C{L{withAttribute}} when matching on a div class - made&#xa;    difficult because C{class} is a reserved word in Python.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div class=""grid"">1 4 0 1 0</div>&#xa;            <div class=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this &lt;div&gt; has no class</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;        div_grid = div().setParseAction(withClass(""grid""))&#xa;        &#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    classattr = ""%s:class"" % namespace if namespace else ""class""&#xa;    return withAttribute(**{classattr : classname})        &#xa;&#xa;opAssoc = _Constants()&#xa;opAssoc.LEFT = object()&#xa;opAssoc.RIGHT = object()&#xa;&#xa;def infixNotation( baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')') ):&#xa;    """"""&#xa;    Helper method for constructing grammars of expressions made up of&#xa;    operators working in a precedence hierarchy.  Operators may be unary or&#xa;    binary, left- or right-associative.  Parse actions can also be attached&#xa;    to operator expressions. The generated parser will also recognize the use &#xa;    of parentheses to override operator precedences (see example below).&#xa;    &#xa;    Note: if you define a deep operator list, you may see performance issues&#xa;    when using infixNotation. See L{ParserElement.enablePackrat} for a&#xa;    mechanism to potentially improve your parser performance.&#xa;&#xa;    Parameters:&#xa;     - baseExpr - expression representing the most basic element for the nested&#xa;     - opList - list of tuples, one for each operator precedence level in the&#xa;      expression grammar; each tuple is of the form&#xa;      (opExpr, numTerms, rightLeftAssoc, parseAction), where:&#xa;       - opExpr is the pyparsing expression for the operator;&#xa;          may also be a string, which will be converted to a Literal;&#xa;          if numTerms is 3, opExpr is a tuple of two expressions, for the&#xa;          two operators separating the 3 terms&#xa;       - numTerms is the number of terms for this operator (must&#xa;          be 1, 2, or 3)&#xa;       - rightLeftAssoc is the indicator whether the operator is&#xa;          right or left associative, using the pyparsing-defined&#xa;          constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}.&#xa;       - parseAction is the parse action to be associated with&#xa;          expressions matching this operator expression (the&#xa;          parse action tuple member may be omitted)&#xa;     - lpar - expression for matching left-parentheses (default=C{Suppress('(')})&#xa;     - rpar - expression for matching right-parentheses (default=C{Suppress(')')})&#xa;&#xa;    Example::&#xa;        # simple example of four-function arithmetic with ints and variable names&#xa;        integer = pyparsing_common.signed_integer&#xa;        varname = pyparsing_common.identifier &#xa;        &#xa;        arith_expr = infixNotation(integer | varname,&#xa;            [&#xa;            ('-', 1, opAssoc.RIGHT),&#xa;            (oneOf('* /'), 2, opAssoc.LEFT),&#xa;            (oneOf('+ -'), 2, opAssoc.LEFT),&#xa;            ])&#xa;        &#xa;        arith_expr.runTests('''&#xa;            5+3*6&#xa;            (5+3)*6&#xa;            -2--11&#xa;            ''', fullDump=False)&#xa;    prints::&#xa;        5+3*6&#xa;        [[5, '+', [3, '*', 6]]]&#xa;&#xa;        (5+3)*6&#xa;        [[[5, '+', 3], '*', 6]]&#xa;&#xa;        -2--11&#xa;        [[['-', 2], '-', ['-', 11]]]&#xa;    """"""&#xa;    ret = Forward()&#xa;    lastExpr = baseExpr | ( lpar + ret + rpar )&#xa;    for i,operDef in enumerate(opList):&#xa;        opExpr,arity,rightLeftAssoc,pa = (operDef + (None,))[:4]&#xa;        termName = ""%s term"" % opExpr if arity < 3 else ""%s%s term"" % opExpr&#xa;        if arity == 3:&#xa;            if opExpr is None or len(opExpr) != 2:&#xa;                raise ValueError(""if numterms=3, opExpr must be a tuple or list of two expressions"")&#xa;            opExpr1, opExpr2 = opExpr&#xa;        thisExpr = Forward().setName(termName)&#xa;        if rightLeftAssoc == opAssoc.LEFT:&#xa;            if arity == 1:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr) + Group( lastExpr + OneOrMore( opExpr ) )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + lastExpr) + Group( lastExpr + OneOrMore( opExpr + lastExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr+lastExpr) + Group( lastExpr + OneOrMore(lastExpr) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr) + \&#xa;                            Group( lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        elif rightLeftAssoc == opAssoc.RIGHT:&#xa;            if arity == 1:&#xa;                # try to avoid LR with this extra test&#xa;                if not isinstance(opExpr, Optional):&#xa;                    opExpr = Optional(opExpr)&#xa;                matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + thisExpr) + Group( lastExpr + OneOrMore( opExpr + thisExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr + thisExpr) + Group( lastExpr + OneOrMore( thisExpr ) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr) + \&#xa;                            Group( lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        else:&#xa;            raise ValueError(""operator must indicate right or left associativity"")&#xa;        if pa:&#xa;            matchExpr.setParseAction( pa )&#xa;        thisExpr <<= ( matchExpr.setName(termName) | lastExpr )&#xa;        lastExpr = thisExpr&#xa;    ret <<= lastExpr&#xa;    return ret&#xa;&#xa;operatorPrecedence = infixNotation&#xa;""""""(Deprecated) Former name of C{L{infixNotation}}, will be dropped in a future release.""""""&#xa;&#xa;dblQuotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""').setName(""string enclosed in double quotes"")&#xa;sglQuotedString = Combine(Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""string enclosed in single quotes"")&#xa;quotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""'|&#xa;                       Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""quotedString using single or double quotes"")&#xa;unicodeString = Combine(_L('u') + quotedString.copy()).setName(""unicode string literal"")&#xa;&#xa;def nestedExpr(opener=""("", closer="")"", content=None, ignoreExpr=quotedString.copy()):&#xa;    """"""&#xa;    Helper method for defining nested lists enclosed in opening and closing&#xa;    delimiters (""("" and "")"" are the default).&#xa;&#xa;    Parameters:&#xa;     - opener - opening character for a nested list (default=C{""(""}); can also be a pyparsing expression&#xa;     - closer - closing character for a nested list (default=C{"")""}); can also be a pyparsing expression&#xa;     - content - expression for items within the nested lists (default=C{None})&#xa;     - ignoreExpr - expression for ignoring opening and closing delimiters (default=C{quotedString})&#xa;&#xa;    If an expression is not provided for the content argument, the nested&#xa;    expression will capture all whitespace-delimited content between delimiters&#xa;    as a list of separate values.&#xa;&#xa;    Use the C{ignoreExpr} argument to define expressions that may contain&#xa;    opening or closing characters that should not be treated as opening&#xa;    or closing characters for nesting, such as quotedString or a comment&#xa;    expression.  Specify multiple expressions using an C{L{Or}} or C{L{MatchFirst}}.&#xa;    The default is L{quotedString}, but if no expressions are to be ignored,&#xa;    then pass C{None} for this argument.&#xa;&#xa;    Example::&#xa;        data_type = oneOf(""void int short long char float double"")&#xa;        decl_data_type = Combine(data_type + Optional(Word('*')))&#xa;        ident = Word(alphas+'_', alphanums+'_')&#xa;        number = pyparsing_common.number&#xa;        arg = Group(decl_data_type + ident)&#xa;        LPAR,RPAR = map(Suppress, ""()"")&#xa;&#xa;        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))&#xa;&#xa;        c_function = (decl_data_type(""type"") &#xa;                      + ident(""name"")&#xa;                      + LPAR + Optional(delimitedList(arg), [])(""args"") + RPAR &#xa;                      + code_body(""body""))&#xa;        c_function.ignore(cStyleComment)&#xa;        &#xa;        source_code = '''&#xa;            int is_odd(int x) { &#xa;                return (x%2); &#xa;            }&#xa;                &#xa;            int dec_to_hex(char hchar) { &#xa;                if (hchar >= '0' && hchar <= '9') { &#xa;                    return (ord(hchar)-ord('0')); &#xa;                } else { &#xa;                    return (10+ord(hchar)-ord('A'));&#xa;                } &#xa;            }&#xa;        '''&#xa;        for func in c_function.searchString(source_code):&#xa;            print(""%(name)s (%(type)s) args: %(args)s"" % func)&#xa;&#xa;    prints::&#xa;        is_odd (int) args: [['int', 'x']]&#xa;        dec_to_hex (int) args: [['char', 'hchar']]&#xa;    """"""&#xa;    if opener == closer:&#xa;        raise ValueError(""opening and closing strings cannot be the same"")&#xa;    if content is None:&#xa;        if isinstance(opener,basestring) and isinstance(closer,basestring):&#xa;            if len(opener) == 1 and len(closer)==1:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr +&#xa;                                    CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (empty.copy()+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;            else:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr + &#xa;                                    ~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (Combine(OneOrMore(~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;        else:&#xa;            raise ValueError(""opening and closing arguments must be strings if no content expression is given"")&#xa;    ret = Forward()&#xa;    if ignoreExpr is not None:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )&#xa;    else:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )&#xa;    ret.setName('nested %s%s expression' % (opener,closer))&#xa;    return ret&#xa;&#xa;def indentedBlock(blockStatementExpr, indentStack, indent=True):&#xa;    """"""&#xa;    Helper method for defining space-delimited indentation blocks, such as&#xa;    those used to define block statements in Python source code.&#xa;&#xa;    Parameters:&#xa;     - blockStatementExpr - expression defining syntax of statement that&#xa;            is repeated within the indented block&#xa;     - indentStack - list created by caller to manage indentation stack&#xa;            (multiple statementWithIndentedBlock expressions within a single grammar&#xa;            should share a common indentStack)&#xa;     - indent - boolean indicating whether block must be indented beyond the&#xa;            the current level; set to False for block of left-most statements&#xa;            (default=C{True})&#xa;&#xa;    A valid block must contain at least one C{blockStatement}.&#xa;&#xa;    Example::&#xa;        data = '''&#xa;        def A(z):&#xa;          A1&#xa;          B = 100&#xa;          G = A2&#xa;          A2&#xa;          A3&#xa;        B&#xa;        def BB(a,b,c):&#xa;          BB1&#xa;          def BBA():&#xa;            bba1&#xa;            bba2&#xa;            bba3&#xa;        C&#xa;        D&#xa;        def spam(x,y):&#xa;             def eggs(z):&#xa;                 pass&#xa;        '''&#xa;&#xa;&#xa;        indentStack = [1]&#xa;        stmt = Forward()&#xa;&#xa;        identifier = Word(alphas, alphanums)&#xa;        funcDecl = (""def"" + identifier + Group( ""("" + Optional( delimitedList(identifier) ) + "")"" ) + "":"")&#xa;        func_body = indentedBlock(stmt, indentStack)&#xa;        funcDef = Group( funcDecl + func_body )&#xa;&#xa;        rvalue = Forward()&#xa;        funcCall = Group(identifier + ""("" + Optional(delimitedList(rvalue)) + "")"")&#xa;        rvalue << (funcCall | identifier | Word(nums))&#xa;        assignment = Group(identifier + ""="" + rvalue)&#xa;        stmt << ( funcDef | assignment | identifier )&#xa;&#xa;        module_body = OneOrMore(stmt)&#xa;&#xa;        parseTree = module_body.parseString(data)&#xa;        parseTree.pprint()&#xa;    prints::&#xa;        [['def',&#xa;          'A',&#xa;          ['(', 'z', ')'],&#xa;          ':',&#xa;          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],&#xa;         'B',&#xa;         ['def',&#xa;          'BB',&#xa;          ['(', 'a', 'b', 'c', ')'],&#xa;          ':',&#xa;          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],&#xa;         'C',&#xa;         'D',&#xa;         ['def',&#xa;          'spam',&#xa;          ['(', 'x', 'y', ')'],&#xa;          ':',&#xa;          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]] &#xa;    """"""&#xa;    def checkPeerIndent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if curCol != indentStack[-1]:&#xa;            if curCol > indentStack[-1]:&#xa;                raise ParseFatalException(s,l,""illegal nesting"")&#xa;            raise ParseException(s,l,""not a peer entry"")&#xa;&#xa;    def checkSubIndent(s,l,t):&#xa;        curCol = col(l,s)&#xa;        if curCol > indentStack[-1]:&#xa;            indentStack.append( curCol )&#xa;        else:&#xa;            raise ParseException(s,l,""not a subentry"")&#xa;&#xa;    def checkUnindent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if not(indentStack and curCol < indentStack[-1] and curCol <= indentStack[-2]):&#xa;            raise ParseException(s,l,""not an unindent"")&#xa;        indentStack.pop()&#xa;&#xa;    NL = OneOrMore(LineEnd().setWhitespaceChars(""\t "").suppress())&#xa;    INDENT = (Empty() + Empty().setParseAction(checkSubIndent)).setName('INDENT')&#xa;    PEER   = Empty().setParseAction(checkPeerIndent).setName('')&#xa;    UNDENT = Empty().setParseAction(checkUnindent).setName('UNINDENT')&#xa;    if indent:&#xa;        smExpr = Group( Optional(NL) +&#xa;            #~ FollowedBy(blockStatementExpr) +&#xa;            INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT)&#xa;    else:&#xa;        smExpr = Group( Optional(NL) +&#xa;            (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) )&#xa;    blockStatementExpr.ignore(_bslash + LineEnd())&#xa;    return smExpr.setName('indented block')&#xa;&#xa;alphas8bit = srange(r""[\0xc0-\0xd6\0xd8-\0xf6\0xf8-\0xff]"")&#xa;punc8bit = srange(r""[\0xa1-\0xbf\0xd7\0xf7]"")&#xa;&#xa;anyOpenTag,anyCloseTag = makeHTMLTags(Word(alphas,alphanums+""_:"").setName('any tag'))&#xa;_htmlEntityMap = dict(zip(""gt lt amp nbsp quot apos"".split(),'><& ""\''))&#xa;commonHTMLEntity = Regex('&(?P<entity>' + '|'.join(_htmlEntityMap.keys()) +"");"").setName(""common HTML entity"")&#xa;def replaceHTMLEntity(t):&#xa;    """"""Helper parser action to replace common HTML entities with their special characters""""""&#xa;    return _htmlEntityMap.get(t.entity)&#xa;&#xa;# it's easy to get these comment structures wrong - they're very common, so may as well make them available&#xa;cStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/').setName(""C style comment"")&#xa;""Comment of the form C{/* ... */}""&#xa;&#xa;htmlComment = Regex(r""<!--[\s\S]*?-->"").setName(""HTML comment"")&#xa;""Comment of the form C{<!-- ... -->}""&#xa;&#xa;restOfLine = Regex(r"".*"").leaveWhitespace().setName(""rest of line"")&#xa;dblSlashComment = Regex(r""//(?:\\\n|[^\n])*"").setName(""// comment"")&#xa;""Comment of the form C{// ... (to end of line)}""&#xa;&#xa;cppStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/'| dblSlashComment).setName(""C++ style comment"")&#xa;""Comment of either form C{L{cStyleComment}} or C{L{dblSlashComment}}""&#xa;&#xa;javaStyleComment = cppStyleComment&#xa;""Same as C{L{cppStyleComment}}""&#xa;&#xa;pythonStyleComment = Regex(r""#.*"").setName(""Python style comment"")&#xa;""Comment of the form C{# ... (to end of line)}""&#xa;&#xa;_commasepitem = Combine(OneOrMore(Word(printables, excludeChars=',') +&#xa;                                  Optional( Word("" \t"") +&#xa;                                            ~Literal("","") + ~LineEnd() ) ) ).streamline().setName(""commaItem"")&#xa;commaSeparatedList = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""commaSeparatedList"")&#xa;""""""(Deprecated) Predefined expression of 1 or more printable words or quoted strings, separated by commas.&#xa;   This expression is deprecated in favor of L{pyparsing_common.comma_separated_list}.""""""&#xa;&#xa;# some other useful expressions - using lower-case class name since we are really using this as a namespace&#xa;class pyparsing_common:&#xa;    """"""&#xa;    Here are some common low-level expressions that may be useful in jump-starting parser development:&#xa;     - numeric forms (L{integers<integer>}, L{reals<real>}, L{scientific notation<sci_real>})&#xa;     - common L{programming identifiers<identifier>}&#xa;     - network addresses (L{MAC<mac_address>}, L{IPv4<ipv4_address>}, L{IPv6<ipv6_address>})&#xa;     - ISO8601 L{dates<iso8601_date>} and L{datetime<iso8601_datetime>}&#xa;     - L{UUID<uuid>}&#xa;     - L{comma-separated list<comma_separated_list>}&#xa;    Parse actions:&#xa;     - C{L{convertToInteger}}&#xa;     - C{L{convertToFloat}}&#xa;     - C{L{convertToDate}}&#xa;     - C{L{convertToDatetime}}&#xa;     - C{L{stripHTMLTags}}&#xa;     - C{L{upcaseTokens}}&#xa;     - C{L{downcaseTokens}}&#xa;&#xa;    Example::&#xa;        pyparsing_common.number.runTests('''&#xa;            # any int or real number, returned as the appropriate type&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.fnumber.runTests('''&#xa;            # any int or real number, returned as float&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.hex_integer.runTests('''&#xa;            # hex numbers&#xa;            100&#xa;            FF&#xa;            ''')&#xa;&#xa;        pyparsing_common.fraction.runTests('''&#xa;            # fractions&#xa;            1/2&#xa;            -3/4&#xa;            ''')&#xa;&#xa;        pyparsing_common.mixed_integer.runTests('''&#xa;            # mixed fractions&#xa;            1&#xa;            1/2&#xa;            -3/4&#xa;            1-3/4&#xa;            ''')&#xa;&#xa;        import uuid&#xa;        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;        pyparsing_common.uuid.runTests('''&#xa;            # uuid&#xa;            12345678-1234-5678-1234-567812345678&#xa;            ''')&#xa;    prints::&#xa;        # any int or real number, returned as the appropriate type&#xa;        100&#xa;        [100]&#xa;&#xa;        -100&#xa;        [-100]&#xa;&#xa;        +100&#xa;        [100]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # any int or real number, returned as float&#xa;        100&#xa;        [100.0]&#xa;&#xa;        -100&#xa;        [-100.0]&#xa;&#xa;        +100&#xa;        [100.0]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # hex numbers&#xa;        100&#xa;        [256]&#xa;&#xa;        FF&#xa;        [255]&#xa;&#xa;        # fractions&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        # mixed fractions&#xa;        1&#xa;        [1]&#xa;&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        1-3/4&#xa;        [1.75]&#xa;&#xa;        # uuid&#xa;        12345678-1234-5678-1234-567812345678&#xa;        [UUID('12345678-1234-5678-1234-567812345678')]&#xa;    """"""&#xa;&#xa;    convertToInteger = tokenMap(int)&#xa;    """"""&#xa;    Parse action for converting parsed integers to Python int&#xa;    """"""&#xa;&#xa;    convertToFloat = tokenMap(float)&#xa;    """"""&#xa;    Parse action for converting parsed numbers to Python float&#xa;    """"""&#xa;&#xa;    integer = Word(nums).setName(""integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an unsigned integer, returns an int""""""&#xa;&#xa;    hex_integer = Word(hexnums).setName(""hex integer"").setParseAction(tokenMap(int,16))&#xa;    """"""expression that parses a hexadecimal integer, returns an int""""""&#xa;&#xa;    signed_integer = Regex(r'[+-]?\d+').setName(""signed integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an integer with optional leading sign, returns an int""""""&#xa;&#xa;    fraction = (signed_integer().setParseAction(convertToFloat) + '/' + signed_integer().setParseAction(convertToFloat)).setName(""fraction"")&#xa;    """"""fractional expression of an integer divided by an integer, returns a float""""""&#xa;    fraction.addParseAction(lambda t: t[0]/t[-1])&#xa;&#xa;    mixed_integer = (fraction | signed_integer + Optional(Optional('-').suppress() + fraction)).setName(""fraction or mixed integer-fraction"")&#xa;    """"""mixed integer of the form 'integer - fraction', with optional leading integer, returns float""""""&#xa;    mixed_integer.addParseAction(sum)&#xa;&#xa;    real = Regex(r'[+-]?\d+\.\d*').setName(""real number"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number and returns a float""""""&#xa;&#xa;    sci_real = Regex(r'[+-]?\d+([eE][+-]?\d+|\.\d*([eE][+-]?\d+)?)').setName(""real number with scientific notation"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number with optional scientific notation and returns a float""""""&#xa;&#xa;    # streamlining this expression makes the docs nicer-looking&#xa;    number = (sci_real | real | signed_integer).streamline()&#xa;    """"""any numeric expression, returns the corresponding Python type""""""&#xa;&#xa;    fnumber = Regex(r'[+-]?\d+\.?\d*([eE][+-]?\d+)?').setName(""fnumber"").setParseAction(convertToFloat)&#xa;    """"""any int or real number, returned as float""""""&#xa;    &#xa;    identifier = Word(alphas+'_', alphanums+'_').setName(""identifier"")&#xa;    """"""typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')""""""&#xa;    &#xa;    ipv4_address = Regex(r'(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}').setName(""IPv4 address"")&#xa;    ""IPv4 address (C{0.0.0.0 - 255.255.255.255})""&#xa;&#xa;    _ipv6_part = Regex(r'[0-9a-fA-F]{1,4}').setName(""hex_integer"")&#xa;    _full_ipv6_address = (_ipv6_part + (':' + _ipv6_part)*7).setName(""full IPv6 address"")&#xa;    _short_ipv6_address = (Optional(_ipv6_part + (':' + _ipv6_part)*(0,6)) + ""::"" + Optional(_ipv6_part + (':' + _ipv6_part)*(0,6))).setName(""short IPv6 address"")&#xa;    _short_ipv6_address.addCondition(lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8)&#xa;    _mixed_ipv6_address = (""::ffff:"" + ipv4_address).setName(""mixed IPv6 address"")&#xa;    ipv6_address = Combine((_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).setName(""IPv6 address"")).setName(""IPv6 address"")&#xa;    ""IPv6 address (long, short, or mixed form)""&#xa;    &#xa;    mac_address = Regex(r'[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\1[0-9a-fA-F]{2}){4}').setName(""MAC address"")&#xa;    ""MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)""&#xa;&#xa;    @staticmethod&#xa;    def convertToDate(fmt=""%Y-%m-%d""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed date string to Python datetime.date&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%d""})&#xa;&#xa;        Example::&#xa;            date_expr = pyparsing_common.iso8601_date.copy()&#xa;            date_expr.setParseAction(pyparsing_common.convertToDate())&#xa;            print(date_expr.parseString(""1999-12-31""))&#xa;        prints::&#xa;            [datetime.date(1999, 12, 31)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt).date()&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    @staticmethod&#xa;    def convertToDatetime(fmt=""%Y-%m-%dT%H:%M:%S.%f""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed datetime string to Python datetime.datetime&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%dT%H:%M:%S.%f""})&#xa;&#xa;        Example::&#xa;            dt_expr = pyparsing_common.iso8601_datetime.copy()&#xa;            dt_expr.setParseAction(pyparsing_common.convertToDatetime())&#xa;            print(dt_expr.parseString(""1999-12-31T23:59:59.999""))&#xa;        prints::&#xa;            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt)&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    iso8601_date = Regex(r'(?P<year>\d{4})(?:-(?P<month>\d\d)(?:-(?P<day>\d\d))?)?').setName(""ISO8601 date"")&#xa;    ""ISO8601 date (C{yyyy-mm-dd})""&#xa;&#xa;    iso8601_datetime = Regex(r'(?P<year>\d{4})-(?P<month>\d\d)-(?P<day>\d\d)[T ](?P<hour>\d\d):(?P<minute>\d\d)(:(?P<second>\d\d(\.\d*)?)?)?(?P<tz>Z|[+-]\d\d:?\d\d)?').setName(""ISO8601 datetime"")&#xa;    ""ISO8601 datetime (C{yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)}) - trailing seconds, milliseconds, and timezone optional; accepts separating C{'T'} or C{' '}""&#xa;&#xa;    uuid = Regex(r'[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}').setName(""UUID"")&#xa;    ""UUID (C{xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx})""&#xa;&#xa;    _html_stripper = anyOpenTag.suppress() | anyCloseTag.suppress()&#xa;    @staticmethod&#xa;    def stripHTMLTags(s, l, tokens):&#xa;        """"""&#xa;        Parse action to remove HTML tags from web page HTML source&#xa;&#xa;        Example::&#xa;            # strip HTML links from normal text &#xa;            text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;            td,td_end = makeHTMLTags(""TD"")&#xa;            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)(""body"") + td_end&#xa;            &#xa;            print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'&#xa;        """"""&#xa;        return pyparsing_common._html_stripper.transformString(tokens[0])&#xa;&#xa;    _commasepitem = Combine(OneOrMore(~Literal("","") + ~LineEnd() + Word(printables, excludeChars=',') &#xa;                                        + Optional( White("" \t"") ) ) ).streamline().setName(""commaItem"")&#xa;    comma_separated_list = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""comma separated list"")&#xa;    """"""Predefined expression of 1 or more printable words or quoted strings, separated by commas.""""""&#xa;&#xa;    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))&#xa;    """"""Parse action to convert tokens to upper case.""""""&#xa;&#xa;    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))&#xa;    """"""Parse action to convert tokens to lower case.""""""&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    selectToken    = CaselessLiteral(""select"")&#xa;    fromToken      = CaselessLiteral(""from"")&#xa;&#xa;    ident          = Word(alphas, alphanums + ""_$"")&#xa;&#xa;    columnName     = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    columnNameList = Group(delimitedList(columnName)).setName(""columns"")&#xa;    columnSpec     = ('*' | columnNameList)&#xa;&#xa;    tableName      = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    tableNameList  = Group(delimitedList(tableName)).setName(""tables"")&#xa;    &#xa;    simpleSQL      = selectToken(""command"") + columnSpec(""columns"") + fromToken + tableNameList(""tables"")&#xa;&#xa;    # demo runTests method, including embedded comments in test string&#xa;    simpleSQL.runTests(""""""&#xa;        # '*' as column list and dotted table name&#xa;        select * from SYS.XYZZY&#xa;&#xa;        # caseless match on ""SELECT"", and casts back to ""select""&#xa;        SELECT * from XYZZY, ABC&#xa;&#xa;        # list of column names, and mixed case SELECT keyword&#xa;        Select AA,BB,CC from Sys.dual&#xa;&#xa;        # multiple tables&#xa;        Select A, B, C from Sys.dual, Table2&#xa;&#xa;        # invalid SELECT keyword - should fail&#xa;        Xelect A, B, C from Sys.dual&#xa;&#xa;        # incomplete command - should fail&#xa;        Select&#xa;&#xa;        # invalid column name - should fail&#xa;        Select ^^^ frox Sys.dual&#xa;&#xa;        """""")&#xa;&#xa;    pyparsing_common.number.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    # any int or real number, returned as float&#xa;    pyparsing_common.fnumber.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    pyparsing_common.hex_integer.runTests(""""""&#xa;        100&#xa;        FF&#xa;        """""")&#xa;&#xa;    import uuid&#xa;    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;    pyparsing_common.uuid.runTests(""""""&#xa;        12345678-1234-5678-1234-567812345678&#xa;        """""")&#xa;"
764360|"# (c) 2014, Chris Church <chris@ninemoreminutes.com>&#xa;#&#xa;# This file is part of Ansible.&#xa;#&#xa;# Ansible is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Ansible is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.&#xa;from __future__ import (absolute_import, division, print_function)&#xa;__metaclass__ = type&#xa;&#xa;import base64&#xa;import os&#xa;import re&#xa;import shlex&#xa;&#xa;from ansible.errors import AnsibleError&#xa;from ansible.module_utils._text import to_bytes, to_text&#xa;&#xa;&#xa;_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']&#xa;&#xa;# Primarily for testing, allow explicitly specifying PowerShell version via&#xa;# an environment variable.&#xa;_powershell_version = os.environ.get('POWERSHELL_VERSION', None)&#xa;if _powershell_version:&#xa;    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]&#xa;&#xa;&#xa;class ShellModule(object):&#xa;&#xa;    # Common shell filenames that this plugin handles&#xa;    # Powershell is handled differently.  It's selected when winrm is the&#xa;    # connection&#xa;    COMPATIBLE_SHELLS = frozenset()&#xa;    # Family of shells this has.  Must match the filename without extension&#xa;    SHELL_FAMILY = 'powershell'&#xa;&#xa;    env = dict()&#xa;&#xa;    # We're being overly cautious about which keys to accept (more so than&#xa;    # the Windows environment is capable of doing), since the powershell&#xa;    # env provider's limitations don't appear to be documented.&#xa;    safe_envkey = re.compile(r'^[\d\w_]{1,255}$')&#xa;&#xa;    def assert_safe_env_key(self, key):&#xa;        if not self.safe_envkey.match(key):&#xa;            raise AnsibleError(""Invalid PowerShell environment key: %s"" % key)&#xa;        return key&#xa;&#xa;    def safe_env_value(self, key, value):&#xa;        if len(value) > 32767:&#xa;            raise AnsibleError(""PowerShell environment value for key '%s' exceeds 32767 characters in length"" % key)&#xa;        # powershell single quoted literals need single-quote doubling as their only escaping&#xa;        value = value.replace(""'"", ""''"")&#xa;        return to_text(value, errors='surrogate_or_strict')&#xa;&#xa;    def env_prefix(self, **kwargs):&#xa;        env = self.env.copy()&#xa;        env.update(kwargs)&#xa;        return ';'.join([""$env:%s='%s'"" % (self.assert_safe_env_key(k), self.safe_env_value(k,v)) for k,v in env.items()])&#xa;&#xa;    def join_path(self, *args):&#xa;        parts = []&#xa;        for arg in args:&#xa;            arg = self._unquote(arg).replace('/', '\\')&#xa;            parts.extend([a for a in arg.split('\\') if a])&#xa;        path = '\\'.join(parts)&#xa;        if path.startswith('~'):&#xa;            return path&#xa;        return '\'%s\'' % path&#xa;&#xa;    def get_remote_filename(self, pathname):&#xa;        # powershell requires that script files end with .ps1&#xa;        base_name = os.path.basename(pathname.strip())&#xa;        name, ext = os.path.splitext(base_name.strip())&#xa;        if ext.lower() not in ['.ps1', '.exe']:&#xa;            return name + '.ps1'&#xa;&#xa;        return base_name.strip()&#xa;&#xa;    def path_has_trailing_slash(self, path):&#xa;        # Allow Windows paths to be specified using either slash.&#xa;        path = self._unquote(path)&#xa;        return path.endswith('/') or path.endswith('\\')&#xa;&#xa;    def chmod(self, paths, mode):&#xa;        raise NotImplementedError('chmod is not implemented for Powershell')&#xa;&#xa;    def chown(self, paths, user):&#xa;        raise NotImplementedError('chown is not implemented for Powershell')&#xa;&#xa;    def set_user_facl(self, paths, user, mode):&#xa;        raise NotImplementedError('set_user_facl is not implemented for Powershell')&#xa;&#xa;    def remove(self, path, recurse=False):&#xa;        path = self._escape(self._unquote(path))&#xa;        if recurse:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force -Recurse;''' % path)&#xa;        else:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force;''' % path)&#xa;&#xa;    def mkdtemp(self, basefile, system=False, mode=None):&#xa;        basefile = self._escape(self._unquote(basefile))&#xa;        # FIXME: Support system temp path!&#xa;        return self._encode_script('''(New-Item -Type Directory -Path $env:temp -Name ""%s"").FullName | Write-Host -Separator '';''' % basefile)&#xa;&#xa;    def expand_user(self, user_home_path):&#xa;        # PowerShell only supports ""~"" (not ""~username"").  Resolve-Path ~ does&#xa;        # not seem to work remotely, though by default we are always starting&#xa;        # in the user's home directory.&#xa;        user_home_path = self._unquote(user_home_path)&#xa;        if user_home_path == '~':&#xa;            script = 'Write-Host (Get-Location).Path'&#xa;        elif user_home_path.startswith('~\\'):&#xa;            script = 'Write-Host ((Get-Location).Path + ""%s"")' % self._escape(user_home_path[1:])&#xa;        else:&#xa;            script = 'Write-Host ""%s""' % self._escape(user_home_path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def exists(self, path):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path ""%s"")&#xa;            {&#xa;                $res = 0;&#xa;            }&#xa;            Else&#xa;            {&#xa;                $res = 1;&#xa;            }&#xa;            Write-Host ""$res"";&#xa;            Exit $res;&#xa;         ''' % path&#xa;        return self._encode_script(script)&#xa;&#xa;    def checksum(self, path, *args, **kwargs):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path -PathType Leaf ""%(path)s"")&#xa;            {&#xa;                $sp = new-object -TypeName System.Security.Cryptography.SHA1CryptoServiceProvider;&#xa;                $fp = [System.IO.File]::Open(""%(path)s"", [System.IO.Filemode]::Open, [System.IO.FileAccess]::Read);&#xa;                [System.BitConverter]::ToString($sp.ComputeHash($fp)).Replace(""-"", """").ToLower();&#xa;                $fp.Dispose();&#xa;            }&#xa;            ElseIf (Test-Path -PathType Container ""%(path)s"")&#xa;            {&#xa;                Write-Host ""3"";&#xa;            }&#xa;            Else&#xa;            {&#xa;                Write-Host ""1"";&#xa;            }&#xa;        ''' % dict(path=path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def build_module_command(self, env_string, shebang, cmd, arg_path=None, rm_tmp=None):&#xa;        cmd_parts = shlex.split(to_bytes(cmd), posix=False)&#xa;        cmd_parts = map(to_text, cmd_parts)&#xa;        if shebang and shebang.lower() == '#!powershell':&#xa;            if not self._unquote(cmd_parts[0]).lower().endswith('.ps1'):&#xa;                cmd_parts[0] = '""%s.ps1""' % self._unquote(cmd_parts[0])&#xa;            cmd_parts.insert(0, '&')&#xa;        elif shebang and shebang.startswith('#!'):&#xa;            cmd_parts.insert(0, shebang[2:])&#xa;        elif not shebang:&#xa;            # The module is assumed to be a binary&#xa;            cmd_parts[0] = self._unquote(cmd_parts[0])&#xa;            cmd_parts.append(arg_path)&#xa;        script = '''&#xa;            Try&#xa;            {&#xa;                %s&#xa;                %s&#xa;            }&#xa;            Catch&#xa;            {&#xa;                $_obj = @{ failed = $true }&#xa;                If ($_.Exception.GetType)&#xa;                {&#xa;                    $_obj.Add('msg', $_.Exception.Message)&#xa;                }&#xa;                Else&#xa;                {&#xa;                    $_obj.Add('msg', $_.ToString())&#xa;                }&#xa;                If ($_.InvocationInfo.PositionMessage)&#xa;                {&#xa;                    $_obj.Add('exception', $_.InvocationInfo.PositionMessage)&#xa;                }&#xa;                ElseIf ($_.ScriptStackTrace)&#xa;                {&#xa;                    $_obj.Add('exception', $_.ScriptStackTrace)&#xa;                }&#xa;                Try&#xa;                {&#xa;                    $_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))&#xa;                }&#xa;                Catch&#xa;                {&#xa;                }&#xa;                Echo $_obj | ConvertTo-Json -Compress -Depth 99&#xa;                Exit 1&#xa;            }&#xa;        ''' % (env_string, ' '.join(cmd_parts))&#xa;        if rm_tmp:&#xa;            rm_tmp = self._escape(self._unquote(rm_tmp))&#xa;            rm_cmd = 'Remove-Item ""%s"" -Force -Recurse -ErrorAction SilentlyContinue' % rm_tmp&#xa;            script = '%s\nFinally { %s }' % (script, rm_cmd)&#xa;        return self._encode_script(script)&#xa;&#xa;    def _unquote(self, value):&#xa;        '''Remove any matching quotes that wrap the given value.'''&#xa;        value = to_text(value or '')&#xa;        m = re.match(r'^\s*?\'(.*?)\'\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        m = re.match(r'^\s*?""(.*?)""\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        return value&#xa;&#xa;    def _escape(self, value, include_vars=False):&#xa;        '''Return value escaped for use in PowerShell command.'''&#xa;        # http://www.techotopia.com/index.php/Windows_PowerShell_1.0_String_Quoting_and_Escape_Sequences&#xa;        # http://stackoverflow.com/questions/764360/a-list-of-string-replacements-in-python&#xa;        subs = [('\n', '`n'), ('\r', '`r'), ('\t', '`t'), ('\a', '`a'),&#xa;                ('\b', '`b'), ('\f', '`f'), ('\v', '`v'), ('""', '`""'),&#xa;                ('\'', '`\''), ('`', '``'), ('\x00', '`0')]&#xa;        if include_vars:&#xa;            subs.append(('$', '`$'))&#xa;        pattern = '|'.join('(%s)' % re.escape(p) for p, s in subs)&#xa;        substs = [s for p, s in subs]&#xa;        replace = lambda m: substs[m.lastindex - 1]&#xa;        return re.sub(pattern, replace, value)&#xa;&#xa;    def _encode_script(self, script, as_list=False, strict_mode=True):&#xa;        '''Convert a PowerShell script to a single base64-encoded command.'''&#xa;        script = to_text(script)&#xa;        if strict_mode:&#xa;            script = u'Set-StrictMode -Version Latest\r\n%s' % script&#xa;        script = '\n'.join([x.strip() for x in script.splitlines() if x.strip()])&#xa;        encoded_script = base64.b64encode(script.encode('utf-16-le'))&#xa;        cmd_parts = _common_args + ['-EncodedCommand', encoded_script]&#xa;        if as_list:&#xa;            return cmd_parts&#xa;        return ' '.join(cmd_parts)&#xa;"
15785719|"from contacts import CompanyContacts&#xa;import json&#xa;from json import JSONEncoder&#xa;&#xa;&#xa;# - Functions&#xa;def store_contact(contact):&#xa;    class ContactEncoder(JSONEncoder):&#xa;        def default(self, contact):&#xa;            return contact.__dict__&#xa;&#xa;    # Open Issue -&#xa;    # Multi-line JSON printing - http://stackoverflow.com/questions/2392766/multiline-strings-in-json&#xa;    test_dict = {""18509827871"":&#xa;                 {&#xa;                     ""first_name"": ""Joe"",&#xa;                     ""last_name"": ""Flack""&#xa;                 }&#xa;             }&#xa;&#xa;    # json_contact = (ContactEncoder().encode(contact))&#xa;    # json_contact = (ContactEncoder().encode(test_dict))&#xa;    json_contact = test_dict&#xa;    with open(""contacts.json"", ""w"") as outfile:&#xa;        # test = [{""lastname"": ""lkjskdfj"", ""name"": ""ruby""}]&#xa;        # print(json_contact)&#xa;        # print(type(json_contact))&#xa;        formatted_json_contact = json_contact&#xa;        for value in json_contact:&#xa;            test = 1&#xa;&#xa;        print(formatted_json_contact)&#xa;        json.dump(formatted_json_contact, outfile, sort_keys=True, indent=4, separators=(',', ': '))&#xa;&#xa;def register_contact(first_name, last_name, primary_phone):&#xa;    contact = CompanyContacts(first_name, last_name, primary_phone)&#xa;    print(""contact: "", contact.first_name)&#xa;    store_contact(contact)&#xa;    return contact&#xa;&#xa;if __name__ == '__main__':&#xa;    # Input&#xa;    print(""Welcome to contact registration."")&#xa;    print("""")&#xa;    first_name_input = input(""Enter first name: "")&#xa;    last_name_input = input(""Enter last name: "")&#xa;    primary_phone_input = input(""Enter primary phone #: "")&#xa;&#xa;    # Write&#xa;    new_contact = register_contact(first_name_input, last_name_input, primary_phone_input)&#xa;&#xa;    # Confirmation&#xa;    # Open Issue -&#xa;    # http://stackoverflow.com/questions/15785719/how-to-print-a-dictionary-line-by-line-in-pythonhttp://stackoverflow.com/questions/15785719/how-to-print-a-dictionary-line-by-line-in-python&#xa;    print(""Thank you for registering, "", new_contact.first_name, ""."")&#xa;"
6957016|"from contextlib import contextmanager&#xa;from django.conf import settings&#xa;from django.db import connection&#xa;from django.db.models.loading import get_model&#xa;from django.core import mail&#xa;&#xa;&#xa;@contextmanager&#xa;def schema_context(schema_name):&#xa;    previous_tenant = connection.tenant&#xa;    try:&#xa;        connection.set_schema(schema_name)&#xa;        yield&#xa;    finally:&#xa;        if previous_tenant is None:&#xa;            connection.set_schema_to_public()&#xa;        else:&#xa;            connection.set_tenant(previous_tenant)&#xa;&#xa;&#xa;@contextmanager&#xa;def tenant_context(tenant):&#xa;    previous_tenant = connection.tenant&#xa;    try:&#xa;        connection.set_tenant(tenant)&#xa;        yield&#xa;    finally:&#xa;        if previous_tenant is None:&#xa;            connection.set_schema_to_public()&#xa;        else:&#xa;            connection.set_tenant(previous_tenant)&#xa;&#xa;&#xa;def get_tenant_model():&#xa;    return get_model(*settings.TENANT_MODEL.split("".""))&#xa;&#xa;&#xa;def get_public_schema_name():&#xa;    return getattr(settings, 'PUBLIC_SCHEMA_NAME', 'public')&#xa;&#xa;&#xa;def get_limit_set_calls():&#xa;    return getattr(settings, 'TENANT_LIMIT_SET_CALLS', False)&#xa;&#xa;&#xa;def clean_tenant_url(url_string):&#xa;    """"""&#xa;    Removes the TENANT_TOKEN from a particular string&#xa;    """"""&#xa;    if hasattr(settings, 'PUBLIC_SCHEMA_URLCONF'):&#xa;        if (settings.PUBLIC_SCHEMA_URLCONF&#xa;                and url_string.startswith(settings.PUBLIC_SCHEMA_URLCONF)):&#xa;            url_string = url_string[len(settings.PUBLIC_SCHEMA_URLCONF):]&#xa;    return url_string&#xa;&#xa;&#xa;def remove_www_and_dev(hostname):&#xa;    """"""&#xa;    Legacy function - just in case someone is still using the old name&#xa;    """"""&#xa;    return remove_www(hostname)&#xa;&#xa;&#xa;def remove_www(hostname):&#xa;    """"""&#xa;    Removes www. from the beginning of the address. Only for&#xa;    routing purposes. www.test.com/login/ and test.com/login/ should&#xa;    find the same tenant.&#xa;    """"""&#xa;    if hostname.startswith(""www.""):&#xa;        return hostname[4:]&#xa;&#xa;    return hostname&#xa;&#xa;&#xa;def django_is_in_test_mode():&#xa;    """"""&#xa;    I know this is very ugly! I'm looking for more elegant solutions.&#xa;    See: http://stackoverflow.com/questions/6957016/detect-django-testing-mode&#xa;    """"""&#xa;    return hasattr(mail, 'outbox')&#xa;&#xa;&#xa;def schema_exists(schema_name):&#xa;    cursor = connection.cursor()&#xa;&#xa;    # check if this schema already exists in the db&#xa;    sql = 'SELECT EXISTS(SELECT 1 FROM pg_catalog.pg_namespace WHERE LOWER(nspname) = LOWER(%s))'&#xa;    cursor.execute(sql, (schema_name, ))&#xa;&#xa;    row = cursor.fetchone()&#xa;    if row:&#xa;        exists = row[0]&#xa;    else:&#xa;        exists = False&#xa;&#xa;    cursor.close()&#xa;&#xa;    return exists&#xa;&#xa;&#xa;def app_labels(apps_list):&#xa;    """"""&#xa;    Returns a list of app labels of the given apps_list&#xa;    """"""&#xa;    return [app.split('.')[-1] for app in apps_list]&#xa;"
25107697|"""""""&#xa;Installs cx_freeze from source, but first patching&#xa;setup.py as described here:&#xa;&#xa;http://stackoverflow.com/questions/25107697/compiling-cx-freeze-under-ubuntu&#xa;""""""&#xa;import glob&#xa;import tarfile&#xa;import os&#xa;import sys&#xa;import platform&#xa;import py&#xa;&#xa;if __name__ == '__main__':&#xa;    if 'ubuntu' not in platform.version().lower():&#xa;&#xa;        print('Not Ubuntu, installing using pip. (platform.version() is %r)' %&#xa;              platform.version())&#xa;        res = os.system('pip install cx_freeze')&#xa;        if res != 0:&#xa;            sys.exit(res)&#xa;        sys.exit(0)&#xa;&#xa;    rootdir = py.path.local.make_numbered_dir(prefix='cx_freeze')&#xa;&#xa;    res = os.system('pip install --download %s --no-use-wheel '&#xa;                    'cx_freeze' % rootdir)&#xa;    if res != 0:&#xa;        sys.exit(res)&#xa;&#xa;    packages = glob.glob('%s/*.tar.gz' % rootdir)&#xa;    assert len(packages) == 1&#xa;    tar_filename = packages[0]&#xa;&#xa;    tar_file = tarfile.open(tar_filename)&#xa;    try:&#xa;        tar_file.extractall(path=str(rootdir))&#xa;    finally:&#xa;        tar_file.close()&#xa;&#xa;    basename = os.path.basename(tar_filename).replace('.tar.gz', '')&#xa;    setup_py_filename = '%s/%s/setup.py' % (rootdir, basename)&#xa;    with open(setup_py_filename) as f:&#xa;        lines = f.readlines()&#xa;&#xa;    line_to_patch = 'if not vars.get(""Py_ENABLE_SHARED"", 0):'&#xa;    for index, line in enumerate(lines):&#xa;        if line_to_patch in line:&#xa;            indent = line[:line.index(line_to_patch)]&#xa;            lines[index] = indent + 'if True:\n'&#xa;            print('Patched line %d' % (index + 1))&#xa;            break&#xa;    else:&#xa;        sys.exit('Could not find line in setup.py to patch!')&#xa;&#xa;    with open(setup_py_filename, 'w') as f:&#xa;        f.writelines(lines)&#xa;&#xa;    os.chdir('%s/%s' % (rootdir, basename))&#xa;    res = os.system('python setup.py install')&#xa;    if res != 0:&#xa;        sys.exit(res)&#xa;&#xa;    sys.exit(0)&#xa;"
29465468|"# -*- coding: utf-8 -*-&#xa;""""""&#xa;Created on Tues Jun 23 16:32:27 2015&#xa;&#xa;@author: boland&#xa;&#xa;help for interesections function came from:&#xa;http://stackoverflow.com/questions/29465468\&#xa;python-intersection-point-of-two-great-circles-lat-long&#xa;&#xa;CODE DESCRIPTION:&#xa;The following python script takes a set of four lat-lon coordinates and&#xa;returns which two points intersect the two great-circle lines between these&#xa;two stations. Note that two points are given as output, NOT the closest &#xa;intersection to both stations. This could be determined in future. &#xa;""""""&#xa;&#xa;import numpy as np&#xa;import math&#xa;&#xa;# Define points in great circle 1&#xa;#p1_lat1 = 32.498520&#xa;#p1_long1 = -106.816846&#xa;#p1_lat2 = 38.199999&#xa;#p1_long2 = -102.371389&#xa;&#xa;# Define points in great circle 2&#xa;#p2_lat1 = 34.086771&#xa;#p2_long1 = -107.313379&#xa;#p2_lat2 = 34.910553&#xa;#p2_long2 = -98.711786&#xa;&#xa;&#xa;def intersect_paths(coord1, coord2, coord3, coord4):&#xa;	""""""&#xa;	Function that returns the two possible intersection&#xa;	lat-lon points between two great circle paths given&#xa;	there four initial coordinates!&#xa;	""""""&#xa;&#xa;	# Define points in great circle 1&#xa;	p1_long1 = coord1[0]&#xa;	p1_lat1 = coord1[1]&#xa;	p1_long2 = coord2[0]&#xa;	p1_lat2 = coord2[1]&#xa;&#xa;	# Define points in great circle 2&#xa;	p2_long1 = coord3[0]&#xa;	p2_lat1 = coord3[1]&#xa;	p2_long2 = coord4[0]&#xa;	p2_lat2 = coord4[1]&#xa;&#xa;	# Convert points in great circle 1, degrees to radians&#xa;	p1_lat1_rad = ((math.pi * p1_lat1) / 180.0)&#xa;	p1_long1_rad = ((math.pi * p1_long1) / 180.0)&#xa;	p1_lat2_rad = ((math.pi * p1_lat2) / 180.0)&#xa;	p1_long2_rad = ((math.pi * p1_long2) / 180.0)&#xa;&#xa;	# Convert points in great circle 2, degrees to radians&#xa;	p2_lat1_rad = ((math.pi * p2_lat1) / 180.0)&#xa;	p2_long1_rad = ((math.pi * p2_long1) / 180.0)&#xa;	p2_lat2_rad = ((math.pi * p2_lat2) / 180.0)&#xa;	p2_long2_rad = ((math.pi * p2_long2) / 180.0)&#xa;&#xa;	# Put in polar coordinates&#xa;	x1 = math.cos(p1_lat1_rad) * math.cos(p1_long1_rad)&#xa;	y1 = math.cos(p1_lat1_rad) * math.sin(p1_long1_rad)&#xa;	z1 = math.sin(p1_lat1_rad)&#xa;	x2 = math.cos(p1_lat2_rad) * math.cos(p1_long2_rad)&#xa;	y2 = math.cos(p1_lat2_rad) * math.sin(p1_long2_rad)&#xa;	z2 = math.sin(p1_lat2_rad)&#xa;	cx1 = math.cos(p2_lat1_rad) * math.cos(p2_long1_rad)&#xa;	cy1 = math.cos(p2_lat1_rad) * math.sin(p2_long1_rad)&#xa;	cz1 = math.sin(p2_lat1_rad)&#xa;	cx2 = math.cos(p2_lat2_rad) * math.cos(p2_long2_rad)&#xa;	cy2 = math.cos(p2_lat2_rad) * math.sin(p2_long2_rad)&#xa;	cz2 = math.sin(p2_lat2_rad)&#xa;&#xa;	# Get normal to planes containing great circles&#xa;	# np.cross product of vector to each point from the origin&#xa;	N1 = np.cross([x1, y1, z1], [x2, y2, z2])&#xa;	N2 = np.cross([cx1, cy1, cz1], [cx2, cy2, cz2])&#xa;&#xa;	# Find line of intersection between two planes&#xa;	L = np.cross(N1, N2)&#xa;&#xa;	# Find two intersection points&#xa;	X1 = L / np.sqrt(L[0]**2 + L[1]**2 + L[2]**2)&#xa;	X2 = -X1&#xa;	i_lat1 = math.asin(X1[2]) * 180./np.pi&#xa;	i_long1 = math.atan2(X1[1], X1[0]) * 180./np.pi&#xa;	i_lat2 = math.asin(X2[2]) * 180./np.pi&#xa;	i_long2 = math.atan2(X2[1], X2[0]) * 180./np.pi&#xa;	&#xa;	int_coord1 = [i_lat1, i_long1]; int_coord2 = [i_lat2, i_long2]&#xa;	# Print results&#xa;	return [int_coord1, int_coord2]&#xa;&#xa;&#xa;&#xa;&#xa;"
35817|"# Copyright (c) 2012 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.SCons as SCons&#xa;import os.path&#xa;import pprint&#xa;import re&#xa;import subprocess&#xa;&#xa;&#xa;# TODO:  remove when we delete the last WriteList() call in this module&#xa;WriteList = SCons.WriteList&#xa;&#xa;&#xa;generator_default_variables = {&#xa;    'EXECUTABLE_PREFIX': '',&#xa;    'EXECUTABLE_SUFFIX': '',&#xa;    'STATIC_LIB_PREFIX': '${LIBPREFIX}',&#xa;    'SHARED_LIB_PREFIX': '${SHLIBPREFIX}',&#xa;    'STATIC_LIB_SUFFIX': '${LIBSUFFIX}',&#xa;    'SHARED_LIB_SUFFIX': '${SHLIBSUFFIX}',&#xa;    'INTERMEDIATE_DIR': '${INTERMEDIATE_DIR}',&#xa;    'SHARED_INTERMEDIATE_DIR': '${SHARED_INTERMEDIATE_DIR}',&#xa;    'OS': 'linux',&#xa;    'PRODUCT_DIR': '$TOP_BUILDDIR',&#xa;    'SHARED_LIB_DIR': '$LIB_DIR',&#xa;    'LIB_DIR': '$LIB_DIR',&#xa;    'RULE_INPUT_ROOT': '${SOURCE.filebase}',&#xa;    'RULE_INPUT_DIRNAME': '${SOURCE.dir}',&#xa;    'RULE_INPUT_EXT': '${SOURCE.suffix}',&#xa;    'RULE_INPUT_NAME': '${SOURCE.file}',&#xa;    'RULE_INPUT_PATH': '${SOURCE.abspath}',&#xa;    'CONFIGURATION_NAME': '${CONFIG_NAME}',&#xa;}&#xa;&#xa;# Tell GYP how to process the input for us.&#xa;generator_handles_variants = True&#xa;generator_wants_absolute_build_file_paths = True&#xa;&#xa;&#xa;def FixPath(path, prefix):&#xa;  if not os.path.isabs(path) and not path[0] == '$':&#xa;    path = prefix + path&#xa;  return path&#xa;&#xa;&#xa;header = """"""\&#xa;# This file is generated; do not edit.&#xa;""""""&#xa;&#xa;&#xa;_alias_template = """"""&#xa;if GetOption('verbose'):&#xa;  _action = Action([%(action)s])&#xa;else:&#xa;  _action = Action([%(action)s], %(message)s)&#xa;_outputs = env.Alias(&#xa;  ['_%(target_name)s_action'],&#xa;  %(inputs)s,&#xa;  _action&#xa;)&#xa;env.AlwaysBuild(_outputs)&#xa;""""""&#xa;&#xa;_run_as_template = """"""&#xa;if GetOption('verbose'):&#xa;  _action = Action([%(action)s])&#xa;else:&#xa;  _action = Action([%(action)s], %(message)s)&#xa;""""""&#xa;&#xa;_run_as_template_suffix = """"""&#xa;_run_as_target = env.Alias('run_%(target_name)s', target_files, _action)&#xa;env.Requires(_run_as_target, [&#xa;    Alias('%(target_name)s'),&#xa;])&#xa;env.AlwaysBuild(_run_as_target)&#xa;""""""&#xa;&#xa;_command_template = """"""&#xa;if GetOption('verbose'):&#xa;  _action = Action([%(action)s])&#xa;else:&#xa;  _action = Action([%(action)s], %(message)s)&#xa;_outputs = env.Command(&#xa;  %(outputs)s,&#xa;  %(inputs)s,&#xa;  _action&#xa;)&#xa;""""""&#xa;&#xa;# This is copied from the default SCons action, updated to handle symlinks.&#xa;_copy_action_template = """"""&#xa;import shutil&#xa;import SCons.Action&#xa;&#xa;def _copy_files_or_dirs_or_symlinks(dest, src):&#xa;  SCons.Node.FS.invalidate_node_memos(dest)&#xa;  if SCons.Util.is_List(src) and os.path.isdir(dest):&#xa;    for file in src:&#xa;      shutil.copy2(file, dest)&#xa;    return 0&#xa;  elif os.path.islink(src):&#xa;    linkto = os.readlink(src)&#xa;    os.symlink(linkto, dest)&#xa;    return 0&#xa;  elif os.path.isfile(src):&#xa;    return shutil.copy2(src, dest)&#xa;  else:&#xa;    return shutil.copytree(src, dest, 1)&#xa;&#xa;def _copy_files_or_dirs_or_symlinks_str(dest, src):&#xa;  return 'Copying %s to %s ...' % (src, dest)&#xa;&#xa;GYPCopy = SCons.Action.ActionFactory(_copy_files_or_dirs_or_symlinks,&#xa;                                     _copy_files_or_dirs_or_symlinks_str,&#xa;                                     convert=str)&#xa;""""""&#xa;&#xa;_rule_template = """"""&#xa;%(name)s_additional_inputs = %(inputs)s&#xa;%(name)s_outputs = %(outputs)s&#xa;def %(name)s_emitter(target, source, env):&#xa;  return (%(name)s_outputs, source + %(name)s_additional_inputs)&#xa;if GetOption('verbose'):&#xa;  %(name)s_action = Action([%(action)s])&#xa;else:&#xa;  %(name)s_action = Action([%(action)s], %(message)s)&#xa;env['BUILDERS']['%(name)s'] = Builder(action=%(name)s_action,&#xa;                                      emitter=%(name)s_emitter)&#xa;&#xa;_outputs = []&#xa;_processed_input_files = []&#xa;for infile in input_files:&#xa;  if (type(infile) == type('')&#xa;      and not os.path.isabs(infile)&#xa;      and not infile[0] == '$'):&#xa;    infile = %(src_dir)r + infile&#xa;  if str(infile).endswith('.%(extension)s'):&#xa;    _generated = env.%(name)s(infile)&#xa;    env.Precious(_generated)&#xa;    _outputs.append(_generated)&#xa;    %(process_outputs_as_sources_line)s&#xa;  else:&#xa;    _processed_input_files.append(infile)&#xa;prerequisites.extend(_outputs)&#xa;input_files = _processed_input_files&#xa;""""""&#xa;&#xa;_spawn_hack = """"""&#xa;import re&#xa;import SCons.Platform.posix&#xa;needs_shell = re.compile('[""\\'><!^&]')&#xa;def gyp_spawn(sh, escape, cmd, args, env):&#xa;  def strip_scons_quotes(arg):&#xa;    if arg[0] == '""' and arg[-1] == '""':&#xa;      return arg[1:-1]&#xa;    return arg&#xa;  stripped_args = [strip_scons_quotes(a) for a in args]&#xa;  if needs_shell.search(' '.join(stripped_args)):&#xa;    return SCons.Platform.posix.exec_spawnvpe([sh, '-c', ' '.join(args)], env)&#xa;  else:&#xa;    return SCons.Platform.posix.exec_spawnvpe(stripped_args, env)&#xa;""""""&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def InvertNaiveSConsQuoting(s):&#xa;  """"""SCons tries to ""help"" with quoting by naively putting double-quotes around&#xa;     command-line arguments containing space or tab, which is broken for all&#xa;     but trivial cases, so we undo it. (See quote_spaces() in Subst.py)""""""&#xa;  if ' ' in s or '\t' in s:&#xa;    # Then SCons will put double-quotes around this, so add our own quotes&#xa;    # to close its quotes at the beginning and end.&#xa;    s = '""' + s + '""'&#xa;  return s&#xa;&#xa;&#xa;def EscapeSConsVariableExpansion(s):&#xa;  """"""SCons has its own variable expansion syntax using $. We must escape it for&#xa;    strings to be interpreted literally. For some reason this requires four&#xa;    dollar signs, not two, even without the shell involved.""""""&#xa;  return s.replace('$', '$$$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = InvertNaiveSConsQuoting(s)&#xa;  s = EscapeSConsVariableExpansion(s)&#xa;  return s&#xa;&#xa;&#xa;def GenerateConfig(fp, config, indent='', src_dir=''):&#xa;  """"""&#xa;  Generates SCons dictionary items for a gyp configuration.&#xa;&#xa;  This provides the main translation between the (lower-case) gyp settings&#xa;  keywords and the (upper-case) SCons construction variables.&#xa;  """"""&#xa;  var_mapping = {&#xa;      'ASFLAGS' : 'asflags',&#xa;      'CCFLAGS' : 'cflags',&#xa;      'CFLAGS' : 'cflags_c',&#xa;      'CXXFLAGS' : 'cflags_cc',&#xa;      'CPPDEFINES' : 'defines',&#xa;      'CPPPATH' : 'include_dirs',&#xa;      # Add the ldflags value to $LINKFLAGS, but not $SHLINKFLAGS.&#xa;      # SCons defines $SHLINKFLAGS to incorporate $LINKFLAGS, so&#xa;      # listing both here would case 'ldflags' to get appended to&#xa;      # both, and then have it show up twice on the command line.&#xa;      'LINKFLAGS' : 'ldflags',&#xa;  }&#xa;  postamble='\n%s],\n' % indent&#xa;  for scons_var in sorted(var_mapping.keys()):&#xa;      gyp_var = var_mapping[scons_var]&#xa;      value = config.get(gyp_var)&#xa;      if value:&#xa;        if gyp_var in ('defines',):&#xa;          value = [EscapeCppDefine(v) for v in value]&#xa;        if gyp_var in ('include_dirs',):&#xa;          if src_dir and not src_dir.endswith('/'):&#xa;            src_dir += '/'&#xa;          result = []&#xa;          for v in value:&#xa;            v = FixPath(v, src_dir)&#xa;            # Force SCons to evaluate the CPPPATH directories at&#xa;            # SConscript-read time, so delayed evaluation of $SRC_DIR&#xa;            # doesn't point it to the --generator-output= directory.&#xa;            result.append('env.Dir(%r)' % v)&#xa;          value = result&#xa;        else:&#xa;          value = map(repr, value)&#xa;        WriteList(fp,&#xa;                  value,&#xa;                  prefix=indent,&#xa;                  preamble='%s%s = [\n    ' % (indent, scons_var),&#xa;                  postamble=postamble)&#xa;&#xa;&#xa;def GenerateSConscript(output_filename, spec, build_file, build_file_data):&#xa;  """"""&#xa;  Generates a SConscript file for a specific target.&#xa;&#xa;  This generates a SConscript file suitable for building any or all of&#xa;  the target's configurations.&#xa;&#xa;  A SConscript file may be called multiple times to generate targets for&#xa;  multiple configurations.  Consequently, it needs to be ready to build&#xa;  the target for any requested configuration, and therefore contains&#xa;  information about the settings for all configurations (generated into&#xa;  the SConscript file at gyp configuration time) as well as logic for&#xa;  selecting (at SCons build time) the specific configuration being built.&#xa;&#xa;  The general outline of a generated SConscript file is:&#xa;&#xa;    --  Header&#xa;&#xa;    --  Import 'env'.  This contains a $CONFIG_NAME construction&#xa;        variable that specifies what configuration to build&#xa;        (e.g. Debug, Release).&#xa;&#xa;    --  Configurations.  This is a dictionary with settings for&#xa;        the different configurations (Debug, Release) under which this&#xa;        target can be built.  The values in the dictionary are themselves&#xa;        dictionaries specifying what construction variables should added&#xa;        to the local copy of the imported construction environment&#xa;        (Append), should be removed (FilterOut), and should outright&#xa;        replace the imported values (Replace).&#xa;&#xa;    --  Clone the imported construction environment and update&#xa;        with the proper configuration settings.&#xa;&#xa;    --  Initialize the lists of the targets' input files and prerequisites.&#xa;&#xa;    --  Target-specific actions and rules.  These come after the&#xa;        input file and prerequisite initializations because the&#xa;        outputs of the actions and rules may affect the input file&#xa;        list (process_outputs_as_sources) and get added to the list of&#xa;        prerequisites (so that they're guaranteed to be executed before&#xa;        building the target).&#xa;&#xa;    --  Call the Builder for the target itself.&#xa;&#xa;    --  Arrange for any copies to be made into installation directories.&#xa;&#xa;    --  Set up the {name} Alias (phony Node) for the target as the&#xa;        primary handle for building all of the target's pieces.&#xa;&#xa;    --  Use env.Require() to make sure the prerequisites (explicitly&#xa;        specified, but also including the actions and rules) are built&#xa;        before the target itself.&#xa;&#xa;    --  Return the {name} Alias to the calling SConstruct file&#xa;        so it can be added to the list of default targets.&#xa;  """"""&#xa;  scons_target = SCons.Target(spec)&#xa;&#xa;  gyp_dir = os.path.dirname(output_filename)&#xa;  if not gyp_dir:&#xa;      gyp_dir = '.'&#xa;  gyp_dir = os.path.abspath(gyp_dir)&#xa;&#xa;  output_dir = os.path.dirname(output_filename)&#xa;  src_dir = build_file_data['_DEPTH']&#xa;  src_dir_rel = gyp.common.RelativePath(src_dir, output_dir)&#xa;  subdir = gyp.common.RelativePath(os.path.dirname(build_file), src_dir)&#xa;  src_subdir = '$SRC_DIR/' + subdir&#xa;  src_subdir_ = src_subdir + '/'&#xa;&#xa;  component_name = os.path.splitext(os.path.basename(build_file))[0]&#xa;  target_name = spec['target_name']&#xa;&#xa;  if not os.path.exists(gyp_dir):&#xa;    os.makedirs(gyp_dir)&#xa;  fp = open(output_filename, 'w')&#xa;  fp.write(header)&#xa;&#xa;  fp.write('\nimport os\n')&#xa;  fp.write('\nImport(""env"")\n')&#xa;&#xa;  #&#xa;  fp.write('\n')&#xa;  fp.write('env = env.Clone(COMPONENT_NAME=%s,\n' % repr(component_name))&#xa;  fp.write('                TARGET_NAME=%s)\n' % repr(target_name))&#xa;&#xa;  #&#xa;  for config in spec['configurations'].itervalues():&#xa;    if config.get('scons_line_length'):&#xa;      fp.write(_spawn_hack)&#xa;      break&#xa;&#xa;  #&#xa;  indent = ' ' * 12&#xa;  fp.write('\n')&#xa;  fp.write('configurations = {\n')&#xa;  for config_name, config in spec['configurations'].iteritems():&#xa;    fp.write('    \'%s\' : {\n' % config_name)&#xa;&#xa;    fp.write('        \'Append\' : dict(\n')&#xa;    GenerateConfig(fp, config, indent, src_subdir)&#xa;    libraries = spec.get('libraries')&#xa;    if libraries:&#xa;      WriteList(fp,&#xa;                map(repr, libraries),&#xa;                prefix=indent,&#xa;                preamble='%sLIBS = [\n    ' % indent,&#xa;                postamble='\n%s],\n' % indent)&#xa;    fp.write('        ),\n')&#xa;&#xa;    fp.write('        \'FilterOut\' : dict(\n' )&#xa;    for key, var in config.get('scons_remove', {}).iteritems():&#xa;      fp.write('             %s = %s,\n' % (key, repr(var)))&#xa;    fp.write('        ),\n')&#xa;&#xa;    fp.write('        \'Replace\' : dict(\n' )&#xa;    scons_settings = config.get('scons_variable_settings', {})&#xa;    for key in sorted(scons_settings.keys()):&#xa;      val = pprint.pformat(scons_settings[key])&#xa;      fp.write('             %s = %s,\n' % (key, val))&#xa;    if 'c++' in spec.get('link_languages', []):&#xa;      fp.write('             %s = %s,\n' % ('LINK', repr('$CXX')))&#xa;    if config.get('scons_line_length'):&#xa;      fp.write('             SPAWN = gyp_spawn,\n')&#xa;    fp.write('        ),\n')&#xa;&#xa;    fp.write('        \'ImportExternal\' : [\n' )&#xa;    for var in config.get('scons_import_variables', []):&#xa;      fp.write('             %s,\n' % repr(var))&#xa;    fp.write('        ],\n')&#xa;&#xa;    fp.write('        \'PropagateExternal\' : [\n' )&#xa;    for var in config.get('scons_propagate_variables', []):&#xa;      fp.write('             %s,\n' % repr(var))&#xa;    fp.write('        ],\n')&#xa;&#xa;    fp.write('    },\n')&#xa;  fp.write('}\n')&#xa;&#xa;  fp.write('\n'&#xa;           'config = configurations[env[\'CONFIG_NAME\']]\n'&#xa;           'env.Append(**config[\'Append\'])\n'&#xa;           'env.FilterOut(**config[\'FilterOut\'])\n'&#xa;           'env.Replace(**config[\'Replace\'])\n')&#xa;&#xa;  fp.write('\n'&#xa;           '# Scons forces -fPIC for SHCCFLAGS on some platforms.\n'&#xa;           '# Disable that so we can control it from cflags in gyp.\n'&#xa;           '# Note that Scons itself is inconsistent with its -fPIC\n'&#xa;           '# setting. SHCCFLAGS forces -fPIC, and SHCFLAGS does not.\n'&#xa;           '# This will make SHCCFLAGS consistent with SHCFLAGS.\n'&#xa;           'env[\'SHCCFLAGS\'] = [\'$CCFLAGS\']\n')&#xa;&#xa;  fp.write('\n'&#xa;           'for _var in config[\'ImportExternal\']:\n'&#xa;           '  if _var in ARGUMENTS:\n'&#xa;           '    env[_var] = ARGUMENTS[_var]\n'&#xa;           '  elif _var in os.environ:\n'&#xa;           '    env[_var] = os.environ[_var]\n'&#xa;           'for _var in config[\'PropagateExternal\']:\n'&#xa;           '  if _var in ARGUMENTS:\n'&#xa;           '    env[_var] = ARGUMENTS[_var]\n'&#xa;           '  elif _var in os.environ:\n'&#xa;           '    env[\'ENV\'][_var] = os.environ[_var]\n')&#xa;&#xa;  fp.write('\n'&#xa;           ""env['ENV']['LD_LIBRARY_PATH'] = env.subst('$LIB_DIR')\n"")&#xa;&#xa;  #&#xa;  #fp.write(""\nif env.has_key('CPPPATH'):\n"")&#xa;  #fp.write(""  env['CPPPATH'] = map(env.Dir, env['CPPPATH'])\n"")&#xa;&#xa;  variants = spec.get('variants', {})&#xa;  for setting in sorted(variants.keys()):&#xa;    if_fmt = 'if ARGUMENTS.get(%s) not in (None, \'0\'):\n'&#xa;    fp.write('\n')&#xa;    fp.write(if_fmt % repr(setting.upper()))&#xa;    fp.write('  env.AppendUnique(\n')&#xa;    GenerateConfig(fp, variants[setting], indent, src_subdir)&#xa;    fp.write('  )\n')&#xa;&#xa;  #&#xa;  scons_target.write_input_files(fp)&#xa;&#xa;  fp.write('\n')&#xa;  fp.write('target_files = []\n')&#xa;  prerequisites = spec.get('scons_prerequisites', [])&#xa;  fp.write('prerequisites = %s\n' % pprint.pformat(prerequisites))&#xa;&#xa;  actions = spec.get('actions', [])&#xa;  for action in actions:&#xa;    a = ['cd', src_subdir, '&&'] + action['action']&#xa;    message = action.get('message')&#xa;    if message:&#xa;      message = repr(message)&#xa;    inputs = [FixPath(f, src_subdir_) for f in action.get('inputs', [])]&#xa;    outputs = [FixPath(f, src_subdir_) for f in action.get('outputs', [])]&#xa;    if outputs:&#xa;      template = _command_template&#xa;    else:&#xa;      template = _alias_template&#xa;    fp.write(template % {&#xa;                 'inputs' : pprint.pformat(inputs),&#xa;                 'outputs' : pprint.pformat(outputs),&#xa;                 'action' : pprint.pformat(a),&#xa;                 'message' : message,&#xa;                 'target_name': target_name,&#xa;             })&#xa;    if int(action.get('process_outputs_as_sources', 0)):&#xa;      fp.write('input_files.extend(_outputs)\n')&#xa;    fp.write('prerequisites.extend(_outputs)\n')&#xa;    fp.write('target_files.extend(_outputs)\n')&#xa;&#xa;  rules = spec.get('rules', [])&#xa;  for rule in rules:&#xa;    name = re.sub('[^a-zA-Z0-9_]', '_', rule['rule_name'])&#xa;    message = rule.get('message')&#xa;    if message:&#xa;        message = repr(message)&#xa;    if int(rule.get('process_outputs_as_sources', 0)):&#xa;      poas_line = '_processed_input_files.extend(_generated)'&#xa;    else:&#xa;      poas_line = '_processed_input_files.append(infile)'&#xa;    inputs = [FixPath(f, src_subdir_) for f in rule.get('inputs', [])]&#xa;    outputs = [FixPath(f, src_subdir_) for f in rule.get('outputs', [])]&#xa;    # Skip a rule with no action and no inputs.&#xa;    if 'action' not in rule and not rule.get('rule_sources', []):&#xa;      continue&#xa;    a = ['cd', src_subdir, '&&'] + rule['action']&#xa;    fp.write(_rule_template % {&#xa;                 'inputs' : pprint.pformat(inputs),&#xa;                 'outputs' : pprint.pformat(outputs),&#xa;                 'action' : pprint.pformat(a),&#xa;                 'extension' : rule['extension'],&#xa;                 'name' : name,&#xa;                 'message' : message,&#xa;                 'process_outputs_as_sources_line' : poas_line,&#xa;                 'src_dir' : src_subdir_,&#xa;             })&#xa;&#xa;  scons_target.write_target(fp, src_subdir)&#xa;&#xa;  copies = spec.get('copies', [])&#xa;  if copies:&#xa;    fp.write(_copy_action_template)&#xa;  for copy in copies:&#xa;    destdir = None&#xa;    files = None&#xa;    try:&#xa;      destdir = copy['destination']&#xa;    except KeyError, e:&#xa;      gyp.common.ExceptionAppend(&#xa;        e,&#xa;        ""Required 'destination' key missing for 'copies' in %s."" % build_file)&#xa;      raise&#xa;    try:&#xa;      files = copy['files']&#xa;    except KeyError, e:&#xa;      gyp.common.ExceptionAppend(&#xa;        e, ""Required 'files' key missing for 'copies' in %s."" % build_file)&#xa;      raise&#xa;    if not files:&#xa;      # TODO:  should probably add a (suppressible) warning;&#xa;      # a null file list may be unintentional.&#xa;      continue&#xa;    if not destdir:&#xa;      raise Exception(&#xa;        ""Required 'destination' key is empty for 'copies' in %s."" % build_file)&#xa;&#xa;    fmt = ('\n'&#xa;           '_outputs = env.Command(%s,\n'&#xa;           '    %s,\n'&#xa;           '    GYPCopy(\'$TARGET\', \'$SOURCE\'))\n')&#xa;    for f in copy['files']:&#xa;      # Remove trailing separators so basename() acts like Unix basename and&#xa;      # always returns the last element, whether a file or dir. Without this,&#xa;      # only the contents, not the directory itself, are copied (and nothing&#xa;      # might be copied if dest already exists, since scons thinks nothing needs&#xa;      # to be done).&#xa;      dest = os.path.join(destdir, os.path.basename(f.rstrip(os.sep)))&#xa;      f = FixPath(f, src_subdir_)&#xa;      dest = FixPath(dest, src_subdir_)&#xa;      fp.write(fmt % (repr(dest), repr(f)))&#xa;      fp.write('target_files.extend(_outputs)\n')&#xa;&#xa;  run_as = spec.get('run_as')&#xa;  if run_as:&#xa;    action = run_as.get('action', [])&#xa;    working_directory = run_as.get('working_directory')&#xa;    if not working_directory:&#xa;      working_directory = gyp_dir&#xa;    else:&#xa;      if not os.path.isabs(working_directory):&#xa;        working_directory = os.path.normpath(os.path.join(gyp_dir,&#xa;                                                          working_directory))&#xa;    if run_as.get('environment'):&#xa;      for (key, val) in run_as.get('environment').iteritems():&#xa;        action = ['%s=""%s""' % (key, val)] + action&#xa;    action = ['cd', '""%s""' % working_directory, '&&'] + action&#xa;    fp.write(_run_as_template % {&#xa;      'action' : pprint.pformat(action),&#xa;      'message' : run_as.get('message', ''),&#xa;    })&#xa;&#xa;  fmt = ""\ngyp_target = env.Alias('%s', target_files)\n""&#xa;  fp.write(fmt % target_name)&#xa;&#xa;  dependencies = spec.get('scons_dependencies', [])&#xa;  if dependencies:&#xa;    WriteList(fp, dependencies, preamble='dependencies = [\n    ',&#xa;                                postamble='\n]\n')&#xa;    fp.write('env.Requires(target_files, dependencies)\n')&#xa;    fp.write('env.Requires(gyp_target, dependencies)\n')&#xa;    fp.write('for prerequisite in prerequisites:\n')&#xa;    fp.write('  env.Requires(prerequisite, dependencies)\n')&#xa;  fp.write('env.Requires(gyp_target, prerequisites)\n')&#xa;&#xa;  if run_as:&#xa;    fp.write(_run_as_template_suffix % {&#xa;      'target_name': target_name,&#xa;    })&#xa;&#xa;  fp.write('Return(""gyp_target"")\n')&#xa;&#xa;  fp.close()&#xa;&#xa;&#xa;#############################################################################&#xa;# TEMPLATE BEGIN&#xa;&#xa;_wrapper_template = """"""\&#xa;&#xa;__doc__ = '''&#xa;Wrapper configuration for building this entire ""solution,""&#xa;including all the specific targets in various *.scons files.&#xa;'''&#xa;&#xa;import os&#xa;import sys&#xa;&#xa;import SCons.Environment&#xa;import SCons.Util&#xa;&#xa;def GetProcessorCount():&#xa;  '''&#xa;  Detects the number of CPUs on the system. Adapted form:&#xa;  http://codeliberates.blogspot.com/2008/05/detecting-cpuscores-in-python.html&#xa;  '''&#xa;  # Linux, Unix and Mac OS X:&#xa;  if hasattr(os, 'sysconf'):&#xa;    if os.sysconf_names.has_key('SC_NPROCESSORS_ONLN'):&#xa;      # Linux and Unix or Mac OS X with python >= 2.5:&#xa;      return os.sysconf('SC_NPROCESSORS_ONLN')&#xa;    else:  # Mac OS X with Python < 2.5:&#xa;      return int(os.popen2(""sysctl -n hw.ncpu"")[1].read())&#xa;  # Windows:&#xa;  if os.environ.has_key('NUMBER_OF_PROCESSORS'):&#xa;    return max(int(os.environ.get('NUMBER_OF_PROCESSORS', '1')), 1)&#xa;  return 1  # Default&#xa;&#xa;# Support PROGRESS= to show progress in different ways.&#xa;p = ARGUMENTS.get('PROGRESS')&#xa;if p == 'spinner':&#xa;  Progress(['/\\r', '|\\r', '\\\\\\r', '-\\r'],&#xa;           interval=5,&#xa;           file=open('/dev/tty', 'w'))&#xa;elif p == 'name':&#xa;  Progress('$TARGET\\r', overwrite=True, file=open('/dev/tty', 'w'))&#xa;&#xa;# Set the default -j value based on the number of processors.&#xa;SetOption('num_jobs', GetProcessorCount() + 1)&#xa;&#xa;# Have SCons use its cached dependency information.&#xa;SetOption('implicit_cache', 1)&#xa;&#xa;# Only re-calculate MD5 checksums if a timestamp has changed.&#xa;Decider('MD5-timestamp')&#xa;&#xa;# Since we set the -j value by default, suppress SCons warnings about being&#xa;# unable to support parallel build on versions of Python with no threading.&#xa;default_warnings = ['no-no-parallel-support']&#xa;SetOption('warn', default_warnings + GetOption('warn'))&#xa;&#xa;AddOption('--mode', nargs=1, dest='conf_list', default=[],&#xa;          action='append', help='Configuration to build.')&#xa;&#xa;AddOption('--verbose', dest='verbose', default=False,&#xa;          action='store_true', help='Verbose command-line output.')&#xa;&#xa;&#xa;#&#xa;sconscript_file_map = %(sconscript_files)s&#xa;&#xa;class LoadTarget:&#xa;  '''&#xa;  Class for deciding if a given target sconscript is to be included&#xa;  based on a list of included target names, optionally prefixed with '-'&#xa;  to exclude a target name.&#xa;  '''&#xa;  def __init__(self, load):&#xa;    '''&#xa;    Initialize a class with a list of names for possible loading.&#xa;&#xa;    Arguments:&#xa;      load:  list of elements in the LOAD= specification&#xa;    '''&#xa;    self.included = set([c for c in load if not c.startswith('-')])&#xa;    self.excluded = set([c[1:] for c in load if c.startswith('-')])&#xa;&#xa;    if not self.included:&#xa;      self.included = set(['all'])&#xa;&#xa;  def __call__(self, target):&#xa;    '''&#xa;    Returns True if the specified target's sconscript file should be&#xa;    loaded, based on the initialized included and excluded lists.&#xa;    '''&#xa;    return (target in self.included or&#xa;            ('all' in self.included and not target in self.excluded))&#xa;&#xa;if 'LOAD' in ARGUMENTS:&#xa;  load = ARGUMENTS['LOAD'].split(',')&#xa;else:&#xa;  load = []&#xa;load_target = LoadTarget(load)&#xa;&#xa;sconscript_files = []&#xa;for target, sconscript in sconscript_file_map.iteritems():&#xa;  if load_target(target):&#xa;    sconscript_files.append(sconscript)&#xa;&#xa;&#xa;target_alias_list= []&#xa;&#xa;conf_list = GetOption('conf_list')&#xa;if conf_list:&#xa;    # In case the same --mode= value was specified multiple times.&#xa;    conf_list = list(set(conf_list))&#xa;else:&#xa;    conf_list = [%(default_configuration)r]&#xa;&#xa;sconsbuild_dir = Dir(%(sconsbuild_dir)s)&#xa;&#xa;&#xa;def FilterOut(self, **kw):&#xa;  kw = SCons.Environment.copy_non_reserved_keywords(kw)&#xa;  for key, val in kw.items():&#xa;    envval = self.get(key, None)&#xa;    if envval is None:&#xa;      # No existing variable in the environment, so nothing to delete.&#xa;      continue&#xa;&#xa;    for vremove in val:&#xa;      # Use while not if, so we can handle duplicates.&#xa;      while vremove in envval:&#xa;        envval.remove(vremove)&#xa;&#xa;    self[key] = envval&#xa;&#xa;    # TODO(sgk): SCons.Environment.Append() has much more logic to deal&#xa;    # with various types of values.  We should handle all those cases in here&#xa;    # too.  (If variable is a dict, etc.)&#xa;&#xa;&#xa;non_compilable_suffixes = {&#xa;    'LINUX' : set([&#xa;        '.bdic',&#xa;        '.css',&#xa;        '.dat',&#xa;        '.fragment',&#xa;        '.gperf',&#xa;        '.h',&#xa;        '.hh',&#xa;        '.hpp',&#xa;        '.html',&#xa;        '.hxx',&#xa;        '.idl',&#xa;        '.in',&#xa;        '.in0',&#xa;        '.in1',&#xa;        '.js',&#xa;        '.mk',&#xa;        '.rc',&#xa;        '.sigs',&#xa;        '',&#xa;    ]),&#xa;    'WINDOWS' : set([&#xa;        '.h',&#xa;        '.hh',&#xa;        '.hpp',&#xa;        '.dat',&#xa;        '.idl',&#xa;        '.in',&#xa;        '.in0',&#xa;        '.in1',&#xa;    ]),&#xa;}&#xa;&#xa;def compilable(env, file):&#xa;  base, ext = os.path.splitext(str(file))&#xa;  if ext in non_compilable_suffixes[env['TARGET_PLATFORM']]:&#xa;    return False&#xa;  return True&#xa;&#xa;def compilable_files(env, sources):&#xa;  return [x for x in sources if compilable(env, x)]&#xa;&#xa;def GypProgram(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.Program(target, source, *args, **kw)&#xa;  if env.get('INCREMENTAL'):&#xa;    env.Precious(result)&#xa;  return result&#xa;&#xa;def GypTestProgram(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.Program(target, source, *args, **kw)&#xa;  if env.get('INCREMENTAL'):&#xa;    env.Precious(*result)&#xa;  return result&#xa;&#xa;def GypLibrary(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.Library(target, source, *args, **kw)&#xa;  return result&#xa;&#xa;def GypLoadableModule(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.LoadableModule(target, source, *args, **kw)&#xa;  return result&#xa;&#xa;def GypStaticLibrary(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.StaticLibrary(target, source, *args, **kw)&#xa;  return result&#xa;&#xa;def GypSharedLibrary(env, target, source, *args, **kw):&#xa;  source = compilable_files(env, source)&#xa;  result = env.SharedLibrary(target, source, *args, **kw)&#xa;  if env.get('INCREMENTAL'):&#xa;    env.Precious(result)&#xa;  return result&#xa;&#xa;def add_gyp_methods(env):&#xa;  env.AddMethod(GypProgram)&#xa;  env.AddMethod(GypTestProgram)&#xa;  env.AddMethod(GypLibrary)&#xa;  env.AddMethod(GypLoadableModule)&#xa;  env.AddMethod(GypStaticLibrary)&#xa;  env.AddMethod(GypSharedLibrary)&#xa;&#xa;  env.AddMethod(FilterOut)&#xa;&#xa;  env.AddMethod(compilable)&#xa;&#xa;&#xa;base_env = Environment(&#xa;    tools = %(scons_tools)s,&#xa;    INTERMEDIATE_DIR='$OBJ_DIR/${COMPONENT_NAME}/_${TARGET_NAME}_intermediate',&#xa;    LIB_DIR='$TOP_BUILDDIR/lib',&#xa;    OBJ_DIR='$TOP_BUILDDIR/obj',&#xa;    SCONSBUILD_DIR=sconsbuild_dir.abspath,&#xa;    SHARED_INTERMEDIATE_DIR='$OBJ_DIR/_global_intermediate',&#xa;    SRC_DIR=Dir(%(src_dir)r),&#xa;    TARGET_PLATFORM='LINUX',&#xa;    TOP_BUILDDIR='$SCONSBUILD_DIR/$CONFIG_NAME',&#xa;    LIBPATH=['$LIB_DIR'],&#xa;)&#xa;&#xa;if not GetOption('verbose'):&#xa;  base_env.SetDefault(&#xa;      ARCOMSTR='Creating library $TARGET',&#xa;      ASCOMSTR='Assembling $TARGET',&#xa;      CCCOMSTR='Compiling $TARGET',&#xa;      CONCATSOURCECOMSTR='ConcatSource $TARGET',&#xa;      CXXCOMSTR='Compiling $TARGET',&#xa;      LDMODULECOMSTR='Building loadable module $TARGET',&#xa;      LINKCOMSTR='Linking $TARGET',&#xa;      MANIFESTCOMSTR='Updating manifest for $TARGET',&#xa;      MIDLCOMSTR='Compiling IDL $TARGET',&#xa;      PCHCOMSTR='Precompiling $TARGET',&#xa;      RANLIBCOMSTR='Indexing $TARGET',&#xa;      RCCOMSTR='Compiling resource $TARGET',&#xa;      SHCCCOMSTR='Compiling $TARGET',&#xa;      SHCXXCOMSTR='Compiling $TARGET',&#xa;      SHLINKCOMSTR='Linking $TARGET',&#xa;      SHMANIFESTCOMSTR='Updating manifest for $TARGET',&#xa;  )&#xa;&#xa;add_gyp_methods(base_env)&#xa;&#xa;for conf in conf_list:&#xa;  env = base_env.Clone(CONFIG_NAME=conf)&#xa;  SConsignFile(env.File('$TOP_BUILDDIR/.sconsign').abspath)&#xa;  for sconscript in sconscript_files:&#xa;    target_alias = env.SConscript(sconscript, exports=['env'])&#xa;    if target_alias:&#xa;      target_alias_list.extend(target_alias)&#xa;&#xa;Default(Alias('all', target_alias_list))&#xa;&#xa;help_fmt = '''&#xa;Usage: hammer [SCONS_OPTIONS] [VARIABLES] [TARGET] ...&#xa;&#xa;Local command-line build options:&#xa;  --mode=CONFIG             Configuration to build:&#xa;                              --mode=Debug [default]&#xa;                              --mode=Release&#xa;  --verbose                 Print actual executed command lines.&#xa;&#xa;Supported command-line build variables:&#xa;  LOAD=[module,...]         Comma-separated list of components to load in the&#xa;                              dependency graph ('-' prefix excludes)&#xa;  PROGRESS=type             Display a progress indicator:&#xa;                              name:  print each evaluated target name&#xa;                              spinner:  print a spinner every 5 targets&#xa;&#xa;The following TARGET names can also be used as LOAD= module names:&#xa;&#xa;%%s&#xa;'''&#xa;&#xa;if GetOption('help'):&#xa;  def columnar_text(items, width=78, indent=2, sep=2):&#xa;    result = []&#xa;    colwidth = max(map(len, items)) + sep&#xa;    cols = (width - indent) / colwidth&#xa;    if cols < 1:&#xa;      cols = 1&#xa;    rows = (len(items) + cols - 1) / cols&#xa;    indent = '%%*s' %% (indent, '')&#xa;    sep = indent&#xa;    for row in xrange(0, rows):&#xa;      result.append(sep)&#xa;      for i in xrange(row, len(items), rows):&#xa;        result.append('%%-*s' %% (colwidth, items[i]))&#xa;      sep = '\\n' + indent&#xa;    result.append('\\n')&#xa;    return ''.join(result)&#xa;&#xa;  load_list = set(sconscript_file_map.keys())&#xa;  target_aliases = set(map(str, target_alias_list))&#xa;&#xa;  common = load_list and target_aliases&#xa;  load_only = load_list - common&#xa;  target_only = target_aliases - common&#xa;  help_text = [help_fmt %% columnar_text(sorted(list(common)))]&#xa;  if target_only:&#xa;    fmt = ""The following are additional TARGET names:\\n\\n%%s\\n""&#xa;    help_text.append(fmt %% columnar_text(sorted(list(target_only))))&#xa;  if load_only:&#xa;    fmt = ""The following are additional LOAD= module names:\\n\\n%%s\\n""&#xa;    help_text.append(fmt %% columnar_text(sorted(list(load_only))))&#xa;  Help(''.join(help_text))&#xa;""""""&#xa;&#xa;# TEMPLATE END&#xa;#############################################################################&#xa;&#xa;&#xa;def GenerateSConscriptWrapper(build_file, build_file_data, name,&#xa;                              output_filename, sconscript_files,&#xa;                              default_configuration):&#xa;  """"""&#xa;  Generates the ""wrapper"" SConscript file (analogous to the Visual Studio&#xa;  solution) that calls all the individual target SConscript files.&#xa;  """"""&#xa;  output_dir = os.path.dirname(output_filename)&#xa;  src_dir = build_file_data['_DEPTH']&#xa;  src_dir_rel = gyp.common.RelativePath(src_dir, output_dir)&#xa;  if not src_dir_rel:&#xa;    src_dir_rel = '.'&#xa;  scons_settings = build_file_data.get('scons_settings', {})&#xa;  sconsbuild_dir = scons_settings.get('sconsbuild_dir', '#')&#xa;  scons_tools = scons_settings.get('tools', ['default'])&#xa;&#xa;  sconscript_file_lines = ['dict(']&#xa;  for target in sorted(sconscript_files.keys()):&#xa;    sconscript = sconscript_files[target]&#xa;    sconscript_file_lines.append('    %s = %r,' % (target, sconscript))&#xa;  sconscript_file_lines.append(')')&#xa;&#xa;  fp = open(output_filename, 'w')&#xa;  fp.write(header)&#xa;  fp.write(_wrapper_template % {&#xa;               'default_configuration' : default_configuration,&#xa;               'name' : name,&#xa;               'scons_tools' : repr(scons_tools),&#xa;               'sconsbuild_dir' : repr(sconsbuild_dir),&#xa;               'sconscript_files' : '\n'.join(sconscript_file_lines),&#xa;               'src_dir' : src_dir_rel,&#xa;           })&#xa;  fp.close()&#xa;&#xa;  # Generate the SConstruct file that invokes the wrapper SConscript.&#xa;  dir, fname = os.path.split(output_filename)&#xa;  SConstruct = os.path.join(dir, 'SConstruct')&#xa;  fp = open(SConstruct, 'w')&#xa;  fp.write(header)&#xa;  fp.write('SConscript(%s)\n' % repr(fname))&#xa;  fp.close()&#xa;&#xa;&#xa;def TargetFilename(target, build_file=None, output_suffix=''):&#xa;  """"""Returns the .scons file name for the specified target.&#xa;  """"""&#xa;  if build_file is None:&#xa;    build_file, target = gyp.common.ParseQualifiedTarget(target)[:2]&#xa;  output_file = os.path.join(os.path.dirname(build_file),&#xa;                             target + output_suffix + '.scons')&#xa;  return output_file&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;&#xa;  # Due to the way we test gyp on the chromium typbots&#xa;  # we need to look for 'scons.py' as well as the more common 'scons'&#xa;  # TODO(sbc): update the trybots to have a more normal install&#xa;  # of scons.&#xa;  scons = 'scons'&#xa;  paths = os.environ['PATH'].split(os.pathsep)&#xa;  for scons_name in ['scons', 'scons.py']:&#xa;    for path in paths:&#xa;      test_scons = os.path.join(path, scons_name)&#xa;      print 'looking for: %s' % test_scons&#xa;      if os.path.exists(test_scons):&#xa;        print ""found scons: %s"" % scons&#xa;        scons = test_scons&#xa;        break&#xa;&#xa;  for config in configurations:&#xa;    arguments = [scons, '-C', options.toplevel_dir, '--mode=%s' % config]&#xa;    print ""Building [%s]: %s"" % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  """"""&#xa;  Generates all the output files for the specified targets.&#xa;  """"""&#xa;  options = params['options']&#xa;&#xa;  if options.generator_output:&#xa;    def output_path(filename):&#xa;      return filename.replace(params['cwd'], options.generator_output)&#xa;  else:&#xa;    def output_path(filename):&#xa;      return filename&#xa;&#xa;  default_configuration = None&#xa;&#xa;  for qualified_target in target_list:&#xa;    spec = target_dicts[qualified_target]&#xa;    if spec['toolset'] != 'target':&#xa;      raise Exception(&#xa;          'Multiple toolsets not supported in scons build (target %s)' %&#xa;          qualified_target)&#xa;    scons_target = SCons.Target(spec)&#xa;    if scons_target.is_ignored:&#xa;      continue&#xa;&#xa;    # TODO:  assumes the default_configuration of the first target&#xa;    # non-Default target is the correct default for all targets.&#xa;    # Need a better model for handle variation between targets.&#xa;    if (not default_configuration and&#xa;        spec['default_configuration'] != 'Default'):&#xa;      default_configuration = spec['default_configuration']&#xa;&#xa;    build_file, target = gyp.common.ParseQualifiedTarget(qualified_target)[:2]&#xa;    output_file = TargetFilename(target, build_file, options.suffix)&#xa;    if options.generator_output:&#xa;      output_file = output_path(output_file)&#xa;&#xa;    if not spec.has_key('libraries'):&#xa;      spec['libraries'] = []&#xa;&#xa;    # Add dependent static library targets to the 'libraries' value.&#xa;    deps = spec.get('dependencies', [])&#xa;    spec['scons_dependencies'] = []&#xa;    for d in deps:&#xa;      td = target_dicts[d]&#xa;      target_name = td['target_name']&#xa;      spec['scons_dependencies'].append(""Alias('%s')"" % target_name)&#xa;      if td['type'] in ('static_library', 'shared_library'):&#xa;        libname = td.get('product_name', target_name)&#xa;        spec['libraries'].append('lib' + libname)&#xa;      if td['type'] == 'loadable_module':&#xa;        prereqs = spec.get('scons_prerequisites', [])&#xa;        # TODO:  parameterize with <(SHARED_LIBRARY_*) variables?&#xa;        td_target = SCons.Target(td)&#xa;        td_target.target_prefix = '${SHLIBPREFIX}'&#xa;        td_target.target_suffix = '${SHLIBSUFFIX}'&#xa;&#xa;    GenerateSConscript(output_file, spec, build_file, data[build_file])&#xa;&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  for build_file in sorted(data.keys()):&#xa;    path, ext = os.path.splitext(build_file)&#xa;    if ext != '.gyp':&#xa;      continue&#xa;    output_dir, basename = os.path.split(path)&#xa;    output_filename  = path + '_main' + options.suffix + '.scons'&#xa;&#xa;    all_targets = gyp.common.AllTargets(target_list, target_dicts, build_file)&#xa;    sconscript_files = {}&#xa;    for t in all_targets:&#xa;      scons_target = SCons.Target(target_dicts[t])&#xa;      if scons_target.is_ignored:&#xa;        continue&#xa;      bf, target = gyp.common.ParseQualifiedTarget(t)[:2]&#xa;      target_filename = TargetFilename(target, bf, options.suffix)&#xa;      tpath = gyp.common.RelativePath(target_filename, output_dir)&#xa;      sconscript_files[target] = tpath&#xa;&#xa;    output_filename = output_path(output_filename)&#xa;    if sconscript_files:&#xa;      GenerateSConscriptWrapper(build_file, data[build_file], basename,&#xa;                                output_filename, sconscript_files,&#xa;                                default_configuration)&#xa;"
12772927|"# -*- coding: utf-8 -*-&#xa;#&#xa;# Luigi documentation build configuration file, created by&#xa;# sphinx-quickstart on Sat Feb  8 00:56:43 2014.&#xa;#&#xa;# This file is execfile()d with the current directory set to its&#xa;# containing dir.&#xa;#&#xa;# Note that not all possible configuration values are present in this&#xa;# autogenerated file.&#xa;#&#xa;# All configuration values have a default; values that are commented out&#xa;# serve to show the default.&#xa;&#xa;import sys&#xa;import os&#xa;import datetime&#xa;import sphinx.environment&#xa;from docutils.utils import get_source_line&#xa;from pkg_resources import get_distribution&#xa;&#xa;&#xa;try:&#xa;    import luigi&#xa;    import luigi.parameter&#xa;&#xa;    def parameter_repr(self):&#xa;        """"""&#xa;        When building documentation, we want Parameter objects to show their&#xa;        description in a nice way&#xa;        """"""&#xa;        significance = 'Insignificant ' if not self.significant else ''&#xa;        class_name = self.__class__.__name__&#xa;        has_default = self._default != luigi.parameter._no_value&#xa;        default = ' (defaults to {})'.format(self._default) if has_default else ''&#xa;        description = (': ' + self.description if self.description else '')&#xa;        return significance + class_name + default + description&#xa;&#xa;    luigi.parameter.Parameter.__repr__ = parameter_repr&#xa;&#xa;    def assertIn(needle, haystack):&#xa;        """"""&#xa;        We test repr of Parameter objects, since it'll be used for readthedocs&#xa;        """"""&#xa;        assert needle in haystack&#xa;&#xa;    # TODO: find a better place to put this!&#xa;    assertIn('IntParameter', repr(luigi.IntParameter()))&#xa;    assertIn('defaults to 37', repr(luigi.IntParameter(default=37)))&#xa;    assertIn('hi mom', repr(luigi.IntParameter(description='hi mom')))&#xa;    assertIn('Insignificant BoolParameter', repr(luigi.BoolParameter(significant=False)))&#xa;except ImportError:&#xa;    pass&#xa;&#xa;&#xa;def _warn_node(self, msg, node):&#xa;    """"""&#xa;    Mute warnings that are like ``WARNING: nonlocal image URI found: https://img. ...``&#xa;&#xa;    Solution was found by googling, copied it from SO:&#xa;&#xa;    http://stackoverflow.com/questions/12772927/specifying-an-online-image-in-sphinx-restructuredtext-format&#xa;    """"""&#xa;    if not msg.startswith('nonlocal image URI found:'):&#xa;        self._warnfunc(msg, '%s:%s' % get_source_line(node))&#xa;&#xa;sphinx.environment.BuildEnvironment.warn_node = _warn_node&#xa;&#xa;if os.environ.get('READTHEDOCS', None) == 'True':&#xa;    # Run sphinx-apidoc automatically in readthedocs&#xa;    # Taken from this: https://lists.torproject.org/pipermail/tor-commits/2012-September/046695.html&#xa;    os.system('sphinx-apidoc -o api -T ../luigi --separate')&#xa;&#xa;# If extensions (or modules to document with autodoc) are in another directory,&#xa;# add these directories to sys.path here. If the directory is relative to the&#xa;# documentation root, use os.path.abspath to make it absolute, like shown here.&#xa;sys.path.insert(0, os.path.abspath(os.path.pardir))&#xa;&#xa;# append the __init__ to class definitions&#xa;autoclass_content = 'both'&#xa;&#xa;# -- General configuration ------------------------------------------------&#xa;&#xa;# If your documentation needs a minimal Sphinx version, state it here.&#xa;#needs_sphinx = '1.0'&#xa;&#xa;# Add any Sphinx extension module names here, as strings. They can be&#xa;# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom&#xa;# ones.&#xa;extensions = [&#xa;    'sphinx.ext.autodoc',&#xa;    'sphinx.ext.viewcode',&#xa;    'sphinx.ext.autosummary',&#xa;]&#xa;&#xa;# Add any paths that contain templates here, relative to this directory.&#xa;templates_path = ['_templates']&#xa;&#xa;# The suffix of source filenames.&#xa;source_suffix = '.rst'&#xa;&#xa;# The encoding of source files.&#xa;#source_encoding = 'utf-8-sig'&#xa;&#xa;# The master toctree document.&#xa;master_doc = 'index'&#xa;&#xa;# General information about the project.&#xa;project = u'Luigi'&#xa;copyright = u""2011-{}, Erik Bernhardsson and Elias Freider"".format(datetime.datetime.now().year)&#xa;&#xa;# The version info for the project you're documenting, acts as replacement for&#xa;# |version| and |release|, also used in various other places throughout the&#xa;# built documents.&#xa;#&#xa;__version__ = get_distribution('luigi').version  # assume luigi is already installed&#xa;# The short X.Y version.&#xa;version = ""."".join(__version__.split(""."")[0:2])&#xa;# The full version, including alpha/beta/rc tags.&#xa;release = __version__&#xa;&#xa;# The language for content autogenerated by Sphinx. Refer to documentation&#xa;# for a list of supported languages.&#xa;#language = None&#xa;&#xa;# There are two options for replacing |today|: either, you set today to some&#xa;# non-false value, then it is used:&#xa;#today = ''&#xa;# Else, today_fmt is used as the format for a strftime call.&#xa;#today_fmt = '%B %d, %Y'&#xa;&#xa;# List of patterns, relative to source directory, that match files and&#xa;# directories to ignore when looking for source files.&#xa;exclude_patterns = ['_build', 'README.rst']&#xa;&#xa;# The reST default role (used for this markup: `text`) to use for all&#xa;# documents.&#xa;#default_role = None&#xa;&#xa;# If true, '()' will be appended to :func: etc. cross-reference text.&#xa;#add_function_parentheses = True&#xa;&#xa;# If true, the current module name will be prepended to all description&#xa;# unit titles (such as .. function::).&#xa;#add_module_names = True&#xa;&#xa;# If true, sectionauthor and moduleauthor directives will be shown in the&#xa;# output. They are ignored by default.&#xa;#show_authors = False&#xa;&#xa;# The name of the Pygments (syntax highlighting) style to use.&#xa;pygments_style = 'sphinx'&#xa;&#xa;# A list of ignored prefixes for module index sorting.&#xa;#modindex_common_prefix = []&#xa;&#xa;# If true, keep warnings as ""system message"" paragraphs in the built documents.&#xa;#keep_warnings = False&#xa;&#xa;autodoc_default_flags = ['members', 'undoc-members']&#xa;autodoc_member_order = 'bysource'&#xa;&#xa;# -- Options for HTML output ----------------------------------------------&#xa;&#xa;# The theme to use for HTML and HTML Help pages.  See the documentation for&#xa;# a list of builtin themes.&#xa;&#xa;# on_rtd is whether we are on readthedocs.org, this line of code grabbed from docs.readthedocs.org&#xa;on_rtd = os.environ.get('READTHEDOCS', None) == 'True'&#xa;&#xa;if not on_rtd:  # only import and set the theme if we're building docs locally&#xa;    try:&#xa;        import sphinx_rtd_theme&#xa;    except ImportError:&#xa;        raise Exception(""You must `pip install sphinx_rtd_theme` to build docs locally."")&#xa;&#xa;    html_theme = 'sphinx_rtd_theme'&#xa;    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]&#xa;&#xa;# otherwise, readthedocs.org uses their theme by default, so no need to specify it&#xa;&#xa;# Theme options are theme-specific and customize the look and feel of a theme&#xa;# further.  For a list of options available for each theme, see the&#xa;# documentation.&#xa;#html_theme_options = {}&#xa;&#xa;# Add any paths that contain custom themes here, relative to this directory.&#xa;#html_theme_path = []&#xa;&#xa;# The name for this set of Sphinx documents.  If None, it defaults to&#xa;# ""<project> v<release> documentation"".&#xa;#html_title = None&#xa;&#xa;# A shorter title for the navigation bar.  Default is the same as html_title.&#xa;#html_short_title = None&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top&#xa;# of the sidebar.&#xa;html_logo = 'luigi.png'&#xa;&#xa;# The name of an image file (within the static path) to use as favicon of the&#xa;# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32&#xa;# pixels large.&#xa;#html_favicon = None&#xa;&#xa;# Add any paths that contain custom static files (such as style sheets) here,&#xa;# relative to this directory. They are copied after the builtin static files,&#xa;# so a file named ""default.css"" will overwrite the builtin ""default.css"".&#xa;#html_static_path = ['_static']&#xa;&#xa;# Add any extra paths that contain custom files (such as robots.txt or&#xa;# .htaccess) here, relative to this directory. These files are copied&#xa;# directly to the root of the documentation.&#xa;#html_extra_path = []&#xa;&#xa;# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,&#xa;# using the given strftime format.&#xa;#html_last_updated_fmt = '%b %d, %Y'&#xa;&#xa;# If true, SmartyPants will be used to convert quotes and dashes to&#xa;# typographically correct entities.&#xa;#html_use_smartypants = True&#xa;&#xa;# Custom sidebar templates, maps document names to template names.&#xa;#html_sidebars = {}&#xa;&#xa;# Additional templates that should be rendered to pages, maps page names to&#xa;# template names.&#xa;#html_additional_pages = {}&#xa;&#xa;# If false, no module index is generated.&#xa;#html_domain_indices = True&#xa;&#xa;# If false, no index is generated.&#xa;#html_use_index = True&#xa;&#xa;# If true, the index is split into individual pages for each letter.&#xa;#html_split_index = False&#xa;&#xa;# If true, links to the reST sources are added to the pages.&#xa;#html_show_sourcelink = True&#xa;&#xa;# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.&#xa;#html_show_sphinx = True&#xa;&#xa;# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.&#xa;#html_show_copyright = True&#xa;&#xa;# If true, an OpenSearch description file will be output, and all pages will&#xa;# contain a <link> tag referring to it.  The value of this option must be the&#xa;# base URL from which the finished HTML is served.&#xa;#html_use_opensearch = ''&#xa;&#xa;# This is the file name suffix for HTML files (e.g. "".xhtml"").&#xa;#html_file_suffix = None&#xa;&#xa;# Output file base name for HTML help builder.&#xa;htmlhelp_basename = 'Luigidoc'&#xa;&#xa;&#xa;# -- Options for LaTeX output ---------------------------------------------&#xa;&#xa;latex_elements = {&#xa;    # The paper size ('letterpaper' or 'a4paper').&#xa;    #'papersize': 'letterpaper',&#xa;&#xa;    # The font size ('10pt', '11pt' or '12pt').&#xa;    #'pointsize': '10pt',&#xa;&#xa;    # Additional stuff for the LaTeX preamble.&#xa;    #'preamble': '',&#xa;}&#xa;&#xa;# Grouping the document tree into LaTeX files. List of tuples&#xa;# (source start file, target name, title,&#xa;#  author, documentclass [howto, manual, or own class]).&#xa;latex_documents = [&#xa;    ('index', 'Luigi.tex', u'Luigi Documentation',&#xa;     u'Erik Bernhardsson and Elias Freider', 'manual'),&#xa;]&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top of&#xa;# the title page.&#xa;#latex_logo = None&#xa;&#xa;# For ""manual"" documents, if this is true, then toplevel headings are parts,&#xa;# not chapters.&#xa;#latex_use_parts = False&#xa;&#xa;# If true, show page references after internal links.&#xa;#latex_show_pagerefs = False&#xa;&#xa;# If true, show URL addresses after external links.&#xa;#latex_show_urls = False&#xa;&#xa;# Documents to append as an appendix to all manuals.&#xa;#latex_appendices = []&#xa;&#xa;# If false, no module index is generated.&#xa;#latex_domain_indices = True&#xa;&#xa;&#xa;# -- Options for manual page output ---------------------------------------&#xa;&#xa;# One entry per manual page. List of tuples&#xa;# (source start file, name, description, authors, manual section).&#xa;man_pages = [&#xa;    ('index', 'luigi', u'Luigi Documentation',&#xa;     [u'Erik Bernhardsson and Elias Freider'], 1)&#xa;]&#xa;&#xa;# If true, show URL addresses after external links.&#xa;#man_show_urls = False&#xa;&#xa;&#xa;# -- Options for Texinfo output -------------------------------------------&#xa;&#xa;# Grouping the document tree into Texinfo files. List of tuples&#xa;# (source start file, target name, title, author,&#xa;#  dir menu entry, description, category)&#xa;texinfo_documents = [&#xa;    ('index', 'Luigi', u'Luigi Documentation',&#xa;     u'Erik Bernhardsson and Elias Freider', 'Luigi', 'One line description of project.',&#xa;     'Miscellaneous'),&#xa;]&#xa;&#xa;# Documents to append as an appendix to all manuals.&#xa;#texinfo_appendices = []&#xa;&#xa;# If false, no module index is generated.&#xa;#texinfo_domain_indices = True&#xa;&#xa;# How to display URL addresses: 'footnote', 'no', or 'inline'.&#xa;#texinfo_show_urls = 'footnote'&#xa;&#xa;# If true, do not generate a @detailmenu in the ""Top"" node's menu.&#xa;#texinfo_no_detailmenu = False&#xa;&#xa;# Some regression introduced&#xa;# https://github.com/sphinx-doc/sphinx/issues/2330&#xa;# https://github.com/spotify/luigi/pull/1555&#xa;highlight_language = ""python""&#xa;"
4494404|"# -*- coding: utf-8 -*-&#xa;""""""&#xa;Created on Thu Feb 18 18:46:36 2016&#xa;&#xa;@author: hjalmar&#xa;""""""&#xa;import numpy as np&#xa;import sys&#xa;from video_tools import VideoReader&#xa;from scipy.interpolate import UnivariateSpline&#xa;from scipy.ndimage.filters import gaussian_filter1d&#xa;from sklearn.metrics import roc_auc_score, matthews_corrcoef, brier_score_loss&#xa;&#xa;&#xa;def circmedian(angs, units='deg'):&#xa;    """"""&#xa;    From: https://github.com/scipy/scipy/issues/6644&#xa;    """"""    &#xa;    &#xa;    if units == 'deg':&#xa;        angs = np.deg2rad(angs)&#xa;        &#xa;    pdists = angs[np.newaxis, :] - angs[:, np.newaxis]&#xa;    pdists = (pdists + np.pi) % (2 * np.pi) - np.pi&#xa;    pdists = np.abs(pdists).sum(1)&#xa;    &#xa;    if units == 'deg':&#xa;        return np.rad2deg(angs[np.argmin(pdists)])&#xa;    else:&#xa;        return angs[np.argmin(pdists)]&#xa;&#xa;&#xa;def get_error_OLD(est_track, true_track):&#xa;    """"""&#xa;    """"""&#xa;    &#xa;    if est_track.ndim > 1:&#xa;        true_track = true_track.reshape((true_track.shape[0],1))&#xa;    &#xa;    error = np.recarray(shape=est_track.shape,&#xa;                        dtype=[('position', float),&#xa;                               ('orientation', float),&#xa;                               ('orientation_weighted', float)])&#xa;    &#xa;    # Position error&#xa;    pos_err = (true_track.x - est_track.x)**2 + (true_track.y - est_track.y)**2&#xa;    error.position = np.sqrt(pos_err)&#xa;    &#xa;    # Orientation error&#xa;    error.orientation = anglediff(true_track.angle, est_track.angle, units='deg')    &#xa;    error.orientation_weighted = anglediff(true_track.angle, est_track.angle_w, units='deg')&#xa;    # no angle&#xa;    true_angle_not_ok = np.isnan(true_track.angle)&#xa;    est_angle_not_ok = np.isnan(est_track.angle)&#xa;    agree = np.logical_and(true_angle_not_ok, est_angle_not_ok)&#xa;    disagree = np.logical_xor(true_angle_not_ok, est_angle_not_ok)&#xa;    error.orientation[agree] = 0.  # if both true orientation and predicted orientation not ok -> error zero&#xa;    error.orientation_weighted[agree] = 0.&#xa;    error.orientation[disagree] = 180. # a missclassification of whether head orientation is discernible is considered 180 degree error.&#xa;    error.orientation_weighted[disagree] = 180. &#xa;&#xa;    return error     &#xa;&#xa;    &#xa;def get_error(est_track, true_track):&#xa;    """"""&#xa;    """"""&#xa;    &#xa;    if est_track.ndim > 1:&#xa;        true_track = true_track.reshape((true_track.shape[0],1))&#xa;    &#xa;    error = np.recarray(shape=est_track.shape,&#xa;                        dtype=[('position', float),&#xa;                               ('orientation', float),&#xa;                               ('orientation_weighted', float)])&#xa;    &#xa;    # Position error&#xa;    pos_err = (true_track.x - est_track.x)**2 + (true_track.y - est_track.y)**2&#xa;    error.position = np.sqrt(pos_err)&#xa;    &#xa;    # Orientation error&#xa;    error.orientation = anglediff(true_track.angle, est_track.angle, units='deg')    &#xa;    error.orientation_weighted = anglediff(true_track.angle, est_track.angle_w, units='deg')&#xa;    &#xa;    descr = {}&#xa;    bix = np.logical_not(np.isnan(error.orientation))&#xa;    descr['orientation_median'] = np.median(np.abs(error.orientation[bix]))&#xa;    descr['orientation_mean'] = np.mean(np.abs(error.orientation[bix]))&#xa;    bix = np.logical_not(np.isnan(error.orientation_weighted))&#xa;    descr['orientation_weighted_median'] = np.nanmedian(np.abs(error.orientation_weighted[bix]))&#xa;    descr['orientation_weighted_mean'] = np.nanmean(np.abs(error.orientation_weighted[bix]))&#xa;    # no angle&#xa;    true_no_angle = np.isnan(true_track.angle)&#xa;    est_no_angle = np.isnan(est_track.angle)&#xa;    agree = np.logical_and(true_no_angle, est_no_angle)&#xa;    disagree = np.logical_xor(true_no_angle, est_no_angle)&#xa;    both = np.logical_or(true_no_angle, est_no_angle)&#xa;    #ipdb.set_trace()&#xa;    descr['no_angle_auc'] = roc_auc_score(true_no_angle, est_no_angle)&#xa;    descr['no_angle_mcc'] = matthews_corrcoef(true_no_angle, est_no_angle)&#xa;    descr['no_angle_brier'] = brier_score_loss(true_no_angle, est_no_angle)    &#xa;    descr['no_angle_acc'] = agree.sum()/both.sum()&#xa;    descr['no_angle_p_per_frame'] = disagree.sum()/disagree.shape[0]&#xa;    descr['position_median'] = np.median(error.position)&#xa;    descr['position_mean'] = np.mean(error.position)&#xa;    &#xa;    #print('True frequency of angle-does-not-apply:',&#xa;     #     true_no_angle.sum()/true_no_angle.shape[0])&#xa;    &#xa;    #print('Estimated frequency of angle-does-not-apply:',&#xa;     #     est_no_angle.sum()/est_no_angle.shape[0])    &#xa;&#xa;    return error, descr&#xa;    &#xa;&#xa;def contiguous_regions(b, minlen=1):&#xa;    """"""&#xa;    Finds contiguous True regions of the boolean array ""b"". Returns&#xa;    a 2D array where the first column is the start index of the region and the&#xa;    second column is the end index.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    b       - an array of booleans&#xa;    minlen  - minimum length of contigous region to accept&#xa;&#xa;    Returns&#xa;    -------&#xa;    idx     - 2D array where the first column is the start index of the region&#xa;              and the second column is the end index&#xa;&#xa;    From Stackoverflow:&#xa;        http://stackoverflow.com/questions/4494404/&#xa;        find-large-number-of-consecutive-values-fulfilling-&#xa;        condition-in-a-numpy-array&#xa;    """"""&#xa;    # Find the indicies of changes in ""b""&#xa;    d = np.diff(b)&#xa;    idx, = d.nonzero()&#xa;    # We need to start things after the change in ""b"".&#xa;    # Therefore, we'll shift the index by 1 to the right.&#xa;    idx += 1&#xa;    # If the start of condition is True prepend a 0&#xa;    if b[0]:&#xa;        idx = np.r_[0, idx]&#xa;    # If the end of condition is True, append the length of the array&#xa;    if b[-1]:&#xa;        idx = np.r_[idx, b.size]&#xa;    # Reshape the result into two columns&#xa;    idx.shape = (-1, 2)&#xa;    # Remove indeci for contigous regions shorter than minlen&#xa;    bix = (np.diff(idx) >= (minlen - 1)).flatten()&#xa;    # shift the end indecei to refer to the last position in the regions&#xa;    # otherwise the region length for regions what run until the end of ""b"" &#xa;    # would be counted as 1 element shorter than those fully within ""b""&#xa;    idx[:, 1] -= 1&#xa;    return idx[bix]    &#xa;    &#xa;    &#xa;def smooth(y, sigma, axis=-1, interpolation='spline'):&#xa;    """"""&#xa;    Does spline interpolation of missing values (NaNs) before gaussian smoothing.&#xa;    &#xa;    interpolattion   -- ""spline"" | ""linear""&#xa;    """"""&#xa;    &#xa;    if axis == -1:&#xa;        axis = y.ndim - 1&#xa;    elif axis == 0 or axis == 1:&#xa;        pass&#xa;    else:&#xa;        raise ValueError('axis has to be 0, 1 or -1')&#xa;        &#xa;    y = y.copy()&#xa;    x = np.arange(y.shape[axis])    &#xa;&#xa;    if y.ndim == 1:&#xa;        w = np.isnan(y)&#xa;    &#xa;        if w.any():&#xa;            &#xa;            if interpolation == 'spline':&#xa;                y[w] = 0.&#xa;                spl = UnivariateSpline(x, y, w=np.logical_not(w), k=3)&#xa;                y[w] = spl(x[w])&#xa;            elif interpolation == 'linear':&#xa;                cregs = contiguous_regions(w, minlen=0)&#xa;                for cr in cregs:&#xa;                    if cr[0] > 0:&#xa;                        y0 = y[cr[0]-1]&#xa;                        &#xa;                        if cr[1] < y.shape[axis]-1:&#xa;                            y1 = y[cr[1]+1]&#xa;                            ynew = np.linspace(y0, y1, cr[1]+1-cr[0]+2, endpoint=True)&#xa;                            y[cr[0]: cr[1]+1] = ynew[1:-1]                            &#xa;&#xa;                        else:  # cr[1] is last value&#xa;                            y[cr[0]:] = y0                            &#xa;                    else: # cr[0] is first value&#xa;                        y[:cr[1]+1] = y[cr[1]+1]&#xa;&#xa;    elif y.ndim == 2:&#xa;        &#xa;        if axis == 0:&#xa;            y = y.T&#xa;                    &#xa;        for i in range(y.shape[0]):&#xa;            w = np.isnan(y[i])&#xa;            if w.any():&#xa;                if interpolation == 'spline':&#xa;                    y[i, w] = 0.&#xa;                    spl = UnivariateSpline(x, y[i], w=(np.logical_not(w)).astype(int), k=3)&#xa;                    y[i, w] = spl(x[w])&#xa;                elif interpolation == 'linear':&#xa;                    cregs = contiguous_regions(w, minlen=0)&#xa;                    for cr in cregs:&#xa;                        if cr[0] > 0:&#xa;                            y0 = y[i, cr[0]-1]&#xa;                            &#xa;                            if cr[1] < y.shape[1]-1:&#xa;                                y1 = y[i, cr[1]+1]&#xa;                                ynew = np.linspace(y0, y1, cr[1]+1-cr[0]+2, endpoint=True)&#xa;                                y[i, cr[0]: cr[1]+1] = ynew[1:-1]&#xa;                            else:  # cr[1] is last value&#xa;                                y[i, cr[0]:] = y0                            &#xa;                        else: # cr[0] is first value&#xa;                            y[i, :cr[1]+1] = y[i, cr[1]+1]&#xa;&#xa;    else:&#xa;        raise ValueError('Only 1 or 2 dimensional input arrays are supported.')&#xa;                                                    &#xa;    return gaussian_filter1d(y, sigma, axis=axis)&#xa;&#xa;&#xa;def HeSD(layer_shape):&#xa;    """"""&#xa;    Initialize weights according to He, Zang, Ren & Sun (2015).&#xa;&#xa;    Parameters&#xa;    ----------------&#xa;    layer_shape  :  shape of the weight layer to initialize&#xa;    &#xa;    Returns&#xa;    -----------&#xa;    &#xa;    Usage&#xa;    ---------&#xa;    1st layer:&#xa;    shape = (patch_sz, patch_sz, 1, Nflt1)&#xa;    tf.Variable(tf.truncated_normal(shape, stddev=HeSD(shape)))&#xa;&#xa;    Should be same as:&#xa;        tf.contrib.layers.variance_scaling_initializer(factor=2.0, mode='FANIN', uniform=False)&#xa;    """"""&#xa;    nl = np.prod(layer_shape[:-1])&#xa;    return np.sqrt(2 / nl)&#xa;    &#xa;    &#xa;def softmax(logits):&#xa;    """"""&#xa;    Temporary replacement for tf.nn.softmax()&#xa;    See issue #2810 on github.com/tensorflow/tensorflow/issues  &#xa;    """"""&#xa;    e = np.exp(logits)&#xa;    return e / e.sum(axis=1,  keepdims=True)&#xa;    &#xa;&#xa;class CountdownPrinter:&#xa;    &#xa;    def __init__(self, N):&#xa;        self.N = int(N)&#xa;        self.ndigits = len(str(N))&#xa;        self.fmt = '%' + '0%dd' % self.ndigits&#xa;        self.clear = '\b'*self.ndigits&#xa;    def print(self, i):&#xa;        sys.stdout.flush()&#xa;        sys.stdout.write(self.fmt % (self.N - i))&#xa;        sys.stdout.flush()&#xa;        sys.stdout.write(self.clear)&#xa;&#xa;&#xa;def get_input(q_str, ans_type, default=None, check=False):&#xa;    """"""&#xa;    Parameters&#xa;    ----------&#xa;    q_str       -- Question/input string, e.g.:&#xa;                   ' Are the stimulus positions OK'&#xa;                   ' 1st scan --- Enter diameter of the UV stimulation spot (micron)'&#xa;    ans_type    -- Type of the returned answer, e.g. int, float, str, bool&#xa;    default     -- Default value for output, if given it should have the same&#xa;                   type as ans_type. For not defalut value enter None.&#xa;    check       -- Whether or not to ask if the input value is ok?&#xa;&#xa;    Return&#xa;    ------&#xa;    ans         -- Reply to asked question, same type as ans_type&#xa;&#xa;    Example&#xa;    -------&#xa;    ans = get_input( ' Enter duration of UV stimulation (ms)', int, 50, True )&#xa;    """"""&#xa;&#xa;    q_str = ' ' + q_str&#xa;&#xa;    if type(default) in [float, int, str]:&#xa;        q_str += ': [' + str(default) + ']\n'&#xa;    elif type(default) is bool:&#xa;        if default:&#xa;            q_str += ' ([y]/n)?\n'&#xa;        else:&#xa;            q_str += ' (y/[n])?\n'&#xa;    elif default is None:&#xa;        q_str += ':\n'&#xa;&#xa;    OK = False&#xa;    while not OK:&#xa;&#xa;        s = input(q_str)&#xa;        if not s:&#xa;            ans = default&#xa;            OK = True&#xa;        else:&#xa;            if ans_type is bool:&#xa;                if s.lower() == 'n':&#xa;                    ans, OK = False, True&#xa;                elif s.lower() == 'y':&#xa;                    ans, OK = True, True&#xa;                else:&#xa;                    OK = False&#xa;            elif ans_type is str:&#xa;                if s.isalpha():&#xa;                    ans, OK = s, True&#xa;                else:&#xa;                    print(' Invalid input')&#xa;                    OK = False&#xa;            else:&#xa;                try:&#xa;                    ans = ans_type(s)&#xa;                    OK = True&#xa;                except:&#xa;                    print(' Invalid input')&#xa;                    OK = False&#xa;&#xa;        if check:&#xa;            ok = 'x'&#xa;            while not ok.lower() in ['y', 'n', '']:&#xa;                ok = input(str(ans) + ' OK ([y]/n)?\n')&#xa;            if ok.lower() == 'n':&#xa;                OK = False&#xa;&#xa;    return ans&#xa;&#xa;&#xa;class FrameStepper:&#xa;&#xa;    def __init__(self, fname):&#xa;        """"""&#xa;        """"""&#xa;        self.vr = VideoReader(fname)&#xa;        self.dt = 1/self.vr.fps&#xa;        self.frame = self.vr.get_frame(0.0)&#xa;        self.t = self.vr.get_current_position(fmt='time')&#xa;        self.n = self.vr.get_current_position(fmt='frame_number')&#xa;        self.tot_n = self.vr.duration_nframes&#xa;        self.duration = self.vr.duration_seconds&#xa;&#xa;    def read_t(self, t):&#xa;        """"""&#xa;        Read a frame at t. t has to be >= to 0,&#xa;        and <= the total video duration.&#xa;        """"""&#xa;        self.frame = self.vr.get_frame(t)&#xa;        &#xa;        if self.frame is None:&#xa;            return False&#xa;        &#xa;        else:&#xa;            &#xa;            self.t = self.vr.get_current_position(fmt='time')&#xa;            self.n = self.vr.get_current_position(fmt='frame_number')&#xa;            if self.n is None:&#xa;                self.n = self.t*self.vr.fps&#xa;                &#xa;            return True&#xa;&#xa;    def read_frame(self, frame_num):&#xa;        """"""&#xa;        Read frame number frame_num. frame_num has to be >= to 0,&#xa;        and <= the total number of frames in video.&#xa;        """"""&#xa;        self.read_t(frame_num * self.dt)&#xa;&#xa;    def next(self):&#xa;        """"""&#xa;        Reads next frame.&#xa;        Cannot be last frame.&#xa;        """"""&#xa;        self.frame = self.vr.get_next_frame()&#xa;        if self.frame is None:&#xa;            return False&#xa;        else:&#xa;            self.t = self.vr.get_current_position(fmt='time')&#xa;            self.n = self.vr.get_current_position(fmt='frame_number')&#xa;    &#xa;            if self.n is None:&#xa;                self.n = self.t*self.vr.fps&#xa;                &#xa;            return True&#xa;&#xa;    def previous(self):&#xa;        """"""&#xa;        Reads previous frame.&#xa;        Current frame cannot the first (i.e. t = 0 and n = 1).&#xa;        """"""&#xa;        if (self.t - self.dt < 0) or (self.n == 1):&#xa;            print('Frame NOT updated!\n'&#xa;                  'The current frame is already the first.\n'&#xa;                  'Previous frame does not exist.')&#xa;            return False&#xa;            &#xa;        else:&#xa;            self.frame = self.vr.get_frame(self.t - self.dt)&#xa;            self.t = self.vr.get_current_position(fmt='time')&#xa;            self.n = self.vr.get_current_position(fmt='frame_number')&#xa;            if self.n is None:&#xa;                self.n = self.t*self.vr.fps&#xa;                &#xa;            return True&#xa;&#xa;    def jump_t(self, jump_dur):&#xa;        """"""&#xa;        Jumps some duration of time from current time.&#xa;        The jump duration plus the current time has to be less than the&#xa;        total video duration.&#xa;        """"""&#xa;        t1 = self.vr.get_current_position(fmt='time')&#xa;        self.frame = self.vr.get_frame(t1 + jump_dur)&#xa;        &#xa;        if self.frame is None:&#xa;&#xa;            return False&#xa;            &#xa;        else:            &#xa;            self.t = self.vr.get_current_position(fmt='time')&#xa;            self.n = self.vr.get_current_position(fmt='frame_number')&#xa;            if self.n is None:&#xa;                self.n = self.t*self.vr.fps&#xa;            &#xa;            return True&#xa;        &#xa;&#xa;    def jump_nf(self, nframes):&#xa;        """"""&#xa;        Jump some number of frames from current frame.&#xa;        The number of frames to jump plus the current frame number needs to be&#xa;        less than the total number of frames in the video.&#xa;        """"""&#xa;        jump_dur = nframes*self.dt&#xa;        return self.jump_t(jump_dur)&#xa;&#xa;    def close(self):&#xa;        """"""&#xa;        Close the video.&#xa;        """"""&#xa;        self.vr.close()&#xa;&#xa;&#xa;def get_max_gaze_line(angle, x, y, im_w, im_h, margin=10, units='deg'):&#xa;    """"""&#xa;    Returns the end positions of a line starting at x, y and with an angle of&#xa;    angle within a rectangular image with width im_w and height im_h.&#xa;    &#xa;    Use to draw the line of gaze in an image.&#xa;    &#xa;    Bottom of image is y = 0, ie not y is not equal row in an array.&#xa;    Use imshow origin='lower'&#xa;    &#xa;    Parameters&#xa;    ----------&#xa;    angle   : angle of line&#xa;    x       : x-coordinate of the line's origin&#xa;    y       : y-coordinate of the line's origin&#xa;    im_w    : width of limiting rectangle/image&#xa;    im_h    : height of limiting rectangle/image&#xa;    units   : 'deg' or 'rad'&#xa;    margin  : gazeline stops at margin&#xa;&#xa;    Returns&#xa;    -------&#xa;    x1      : x-coordinate of the line's end point&#xa;    y1      : y coordinate of the line's end point&#xa;    """"""&#xa;        &#xa;    if units == 'deg':&#xa;        angle = np.deg2rad(angle)&#xa;&#xa;    # make sure the angle stays between -pi and pi&#xa;    angle = np.arctan2(np.sin(angle), np.cos(angle))        &#xa;    &#xa;    if np.abs(angle) > np.pi/2:&#xa;        dx = x - margin&#xa;    else:&#xa;        dx = im_w - margin - x&#xa;        &#xa;    if angle > 0.0:&#xa;        dy = im_h - margin - y&#xa;    else:&#xa;        dy = y - margin&#xa;    &#xa;    # Chose the shortest radius since the longest will go outside of im&#xa;    if np.cos(angle) == 0:&#xa;        r = dy&#xa;    elif np.sin(angle) == 0:&#xa;        r = dx&#xa;    else:&#xa;        r = min(np.abs(dx/np.cos(angle)), np.abs(dy/np.sin(angle)))&#xa;&#xa;    x1 = r * np.cos(angle) + x&#xa;    y1 = r * np.sin(angle) + y&#xa;    &#xa;    return x1, y1&#xa;&#xa;    &#xa;def get_gaze_line(angle, x, y, r, units='rad'):&#xa;    """"""&#xa;    Parameters&#xa;    ----------&#xa;    angle&#xa;    x&#xa;    y&#xa;    r&#xa;    units : Units of ""angle"", ""rad"" for radians or ""deg"" for degrees.&#xa;    &#xa;    Returns&#xa;    -------&#xa;    gzl_x&#xa;    gzl_y&#xa;    """"""  &#xa;    if units == 'rad':&#xa;        gzl_x = [x, x + r * np.cos(angle)]&#xa;        gzl_y = [y, y + r * np.sin(angle)]&#xa;    elif units == 'deg':&#xa;        gzl_x = [x, x + r * np.cos(np.deg2rad(angle))]&#xa;        gzl_y = [y, y + r * np.sin(np.deg2rad(angle))]&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')&#xa;    &#xa;    return gzl_x, gzl_y    &#xa;    &#xa;    &#xa;def anglediff(angles0, angles1, units='deg'):&#xa;    &#xa;    if units == 'deg':&#xa;        d = np.deg2rad(angles0) - np.deg2rad(angles1)&#xa;        adiff = np.rad2deg(np.arctan2(np.sin(d), np.cos(d)))&#xa;    elif units == 'rad':&#xa;        d = angles0 - angles1&#xa;        adiff = np.arctan2(np.sin(d), np.cos(d))&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')        &#xa;        &#xa;    return adiff&#xa;        &#xa;    &#xa;def angles2complex(angles, units='deg'):&#xa;    """""" &#xa;    Angles in degrees, array or scalar&#xa;    """""" &#xa;    if units == 'deg':&#xa;        z = np.cos(np.deg2rad(angles)) + np.sin(np.deg2rad(angles))*1j&#xa;    elif units == 'rad':&#xa;        z = np.cos(angles) + np.sin(angles)*1j&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""') &#xa;        &#xa;    return z&#xa;    &#xa;&#xa;def complex2angles(z, units='deg'):   &#xa;    """""" &#xa;    z complex array or scalar &#xa;    angles in degrees, array or scalar&#xa;    """""" &#xa;    if units == 'deg':    &#xa;        angles = np.rad2deg(np.arctan2(z.imag, z.real))&#xa;    elif units == 'rad':&#xa;        angles = np.arctan2(z.imag, z.real)&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')         &#xa;        &#xa;    return angles&#xa;    &#xa;    &#xa;def angle2class(angles, Nclass, angles_ok=None, units='deg'):&#xa;    """"""&#xa;    Arguments&#xa;    ---------&#xa;    angles      - A scalar or a vector of angles&#xa;    Nclass      - Number of classes to divide angles into.&#xa;    angles_ok   - A bool, scalar or vector. True if the corresponding angle&#xa;                  is ok, ie, the head orientation in the horizontal plane was&#xa;                  clearly visible, False otherwise.&#xa;                  If given, then class Nclass-1 will code angle not ok.&#xa;    units       - Unit of angles, ""deg"" or ""rad""&#xa;    &#xa;    Returns&#xa;    -------&#xa;    y           - A scalar or vector of same length as angles, with values that&#xa;                  run from 0 to Nclass-1. If angles_ok was given then class&#xa;                  Nclass-1 will code angle not ok.&#xa;    """"""&#xa;    &#xa;    if units == 'deg':&#xa;        a = np.deg2rad(angles)&#xa;    elif units == 'rad':&#xa;        a = angles&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')             &#xa;&#xa;    # angles range -pi:pi -> shift to pi:pi&#xa;    angles = (np.arctan2(np.sin(a), np.cos(a)) + np.pi) / (2 * np.pi)&#xa;    &#xa;    &#xa;    if angles_ok is None:&#xa;        y = np.int32(Nclass * angles)  # Bin 2pi rad into Nclass bins with values 0:Nclass-1.&#xa;        if np.isscalar(y):&#xa;            if y == Nclass:&#xa;                y = 0&#xa;        else:&#xa;            y[y == Nclass] = 0  # 0 & 2pi are same angle&#xa;    &#xa;    else:&#xa;        y = np.int32((Nclass-1) * angles)&#xa;        if np.isscalar(y):&#xa;            if y == (Nclass-1):&#xa;                y = 0&#xa;            if angles_ok is None:&#xa;                y = Nclass - 1&#xa;        else:&#xa;            y[y == Nclass] = 0&#xa;            # No clear head orientation in the horiz plane,&#xa;            # ie angle_ok = 0, is coded as the last class.&#xa;            y[np.logical_not(angles_ok)] = Nclass - 1&#xa;        &#xa;    return np.float32(y)&#xa;    &#xa;    &#xa;def circmean_weighted(angles, w, axis=0, units='deg'):&#xa;    &#xa;    if units == 'deg':&#xa;        angles = np.deg2rad(angles)&#xa;    elif units == 'rad':&#xa;        pass&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')&#xa;        &#xa;    s = np.nansum(np.sin(angles) * w, axis=axis)&#xa;    c = np.nansum(np.cos(angles) * w, axis=axis)&#xa;    &#xa;    wmean = np.arctan2(s, c)&#xa;    &#xa;    if units == 'deg':&#xa;        wmean = np.rad2deg(wmean)&#xa;    &#xa;    return wmean&#xa;    &#xa;    &#xa;def class2angle(y,  Nclass, units='deg'):&#xa;    """"""&#xa;    Angles are from -180 to 180&#xa;    y run from 0 to Nclass-1&#xa;    """"""&#xa;    if units == 'deg':&#xa;        angles = y * (360 / Nclass) + 180 / Nclass # angles run from -180:180&#xa;    elif units == 'rad':&#xa;        angles = y * (np.pi / Nclass) + np.pi / (2 * Nclass) # angles run from -180:180&#xa;    else:&#xa;        raise ValueError('""units"" has to be ""rad"" or ""deg""')         &#xa;        &#xa;    return angles - 180&#xa;    &#xa;   &#xa;def whiten(image):&#xa;    """"""&#xa;    """"""&#xa;    adjusted_std = max(image.std(), 1./ np.sqrt(image.size))&#xa;    image -= image.mean()&#xa;    &#xa;    return image/adjusted_std&#xa;"
8214932|"## This file is part of Invenio.&#xa;## Copyright (C) 2011, 2012, 2013 CERN.&#xa;##&#xa;## Invenio is free software; you can redistribute it and/or&#xa;## modify it under the terms of the GNU General Public License as&#xa;## published by the Free Software Foundation; either version 2 of the&#xa;## License, or (at your option) any later version.&#xa;##&#xa;## Invenio is distributed in the hope that it will be useful, but&#xa;## WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;## General Public License for more details.&#xa;##&#xa;## You should have received a copy of the GNU General Public License&#xa;## along with Invenio; if not, write to the Free Software Foundation, Inc.,&#xa;## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.&#xa;&#xa;"""""" Invenio Authorlist Data Conversion Engine. """"""&#xa;import time&#xa;try:&#xa;    import json&#xa;except ImportError:&#xa;    import simplejson as json&#xa;from xml.dom import minidom&#xa;&#xa;try:&#xa;    from xml.etree import ElementTree as ET&#xa;except ImportError:&#xa;    import elementtree.ElementTree as ET&#xa;&#xa;from invenio.webuser import page_not_authorized&#xa;from invenio.access_control_engine import acc_authorize_action&#xa;import invenio.authorlist_config as cfg&#xa;from invenio.search_engine import perform_request_search, record_exists&#xa;from invenio.search_engine_utils import get_fieldvalues&#xa;from invenio.bibedit_utils import get_record&#xa;# from lxml import etree&#xa;from invenio.authorlist_dblayer import get_owner&#xa;from invenio.textutils import escape_latex&#xa;&#xa;# default name that will be used, when affiliation name is missing&#xa;UNKNOWN_AFFILIATION = 'Unknown Affiliation'&#xa;# Namespaces used in the xml file&#xa;NAMESPACES = {'cal': 'http://www.slac.stanford.edu/spires/hepnames/authors_xml/',&#xa;              'foaf': 'http://xmlns.com/foaf/0.1/',&#xa;              }&#xa;&#xa;&#xa;def retrieve_data_from_record(recid):&#xa;    """"""&#xa;    Extract data from a record id in order to import it to the Author list&#xa;    interface&#xa;    """"""&#xa;    if not record_exists(recid):&#xa;        return&#xa;&#xa;    output = {}&#xa;&#xa;    DEFAULT_AFFILIATION_TYPE = cfg.OPTIONS.AUTHOR_AFFILIATION_TYPE[0]&#xa;    DEFAULT_IDENTIFIER = cfg.OPTIONS.IDENTIFIERS_LIST[0]&#xa;    IDENTIFIERS_MAPPING = cfg.OPTIONS.IDENTIFIERS_MAPPING&#xa;&#xa;    bibrecord = get_record(recid)&#xa;&#xa;    try:&#xa;        paper_title = get_fieldvalues(recid, '245__a')[0]&#xa;    except IndexError:&#xa;        paper_title = """"&#xa;    try:&#xa;        collaboration_name = get_fieldvalues(recid, '710__g')&#xa;    except IndexError:&#xa;        collaboration_name = """"&#xa;    try:&#xa;        experiment_number = get_fieldvalues(recid, '693__e')&#xa;    except IndexError:&#xa;        experiment_number = """"&#xa;&#xa;    record_authors = bibrecord.get('100', [])&#xa;    record_authors.extend(bibrecord.get('700', []))&#xa;&#xa;    author_list = []&#xa;    unique_affiliations = []&#xa;&#xa;    for i, field_instance in enumerate(record_authors, 1):&#xa;        family_name = """"&#xa;        given_name = """"&#xa;        name_on_paper = """"&#xa;        status = """"&#xa;        affiliations = []&#xa;        identifiers = []&#xa;        field = field_instance[0]&#xa;        for subfield_code, subfield_value in field:&#xa;            if subfield_code == ""a"":&#xa;                try:&#xa;                    family_name = subfield_value.split(',')[0]&#xa;                    given_name = subfield_value.split(',')[1].lstrip()&#xa;                except:&#xa;                    pass&#xa;                name_on_paper = subfield_value&#xa;            elif subfield_code == ""u"":&#xa;                affiliations.append([subfield_value, DEFAULT_AFFILIATION_TYPE])&#xa;                unique_affiliations.append(subfield_value)&#xa;            elif subfield_code == ""i"":&#xa;                # FIXME This will currently work only with INSPIRE IDs&#xa;                id_prefix = subfield_value.split(""-"")[0]&#xa;                if id_prefix in IDENTIFIERS_MAPPING:&#xa;                    identifiers.append([subfield_value, IDENTIFIERS_MAPPING[id_prefix]])&#xa;        if not identifiers:&#xa;            identifiers.append(['', DEFAULT_IDENTIFIER])&#xa;        if not affiliations:&#xa;            affiliations.append([UNKNOWN_AFFILIATION, DEFAULT_AFFILIATION_TYPE])&#xa;            unique_affiliations.append(UNKNOWN_AFFILIATION)&#xa;        author_list.append([&#xa;            i,              # Row number&#xa;            '',             # Place holder for the web interface&#xa;            family_name,&#xa;            given_name,&#xa;            name_on_paper,&#xa;            status,&#xa;            affiliations,&#xa;            identifiers&#xa;        ])&#xa;&#xa;    unique_affiliations = list(set(unique_affiliations))&#xa;&#xa;    output.update({'authors': author_list})&#xa;&#xa;    # Generate all the affiliation related information&#xa;    affiliation_list = []&#xa;    for i, affiliation in enumerate(unique_affiliations, 1):&#xa;        institution = perform_request_search(c=""Institutions"", p='110__u:""' + affiliation + '""')&#xa;        full_name = affiliation&#xa;        if len(institution) == 1:&#xa;            full_name_110_a = get_fieldvalues(institution[0], '110__a')&#xa;            if full_name_110_a:&#xa;                full_name = str(full_name_110_a[0])&#xa;            full_name_110_b = get_fieldvalues(institution[0], '110__b')&#xa;            if full_name_110_b:&#xa;                full_name += ', ' + str(full_name_110_b[0])&#xa;        affiliation = [i,&#xa;                       '',&#xa;                       affiliation,&#xa;                       '',&#xa;                       full_name,&#xa;                       '',&#xa;                       True,&#xa;                       '']&#xa;        affiliation_list.append(affiliation)&#xa;&#xa;    output.update({'affiliations': affiliation_list})&#xa;    output.update({'paper_title': paper_title,&#xa;                   'collaboration': collaboration_name,&#xa;                   'experiment_number': experiment_number,&#xa;                   'last_modified': int(time.time()),&#xa;                   'reference_ids': [],&#xa;                   'paper_id': '1'})&#xa;&#xa;    return output&#xa;&#xa;&#xa;def retrieve_data_from_xml(xml):&#xa;    """"""&#xa;    Extract data from an XML file to import it to the Author list&#xa;    interface&#xa;    """"""&#xa;&#xa;    def get_element_value_helper(element, tag):&#xa;        """"""&#xa;        Helper that takes an element and returns text from the first node&#xa;        of that element&#xa;        """"""&#xa;        text = ''&#xa;        elements_list = element.getElementsByTagName(tag)&#xa;        if elements_list:&#xa;            child = elements_list[0].firstChild&#xa;            if child:&#xa;                text = child.nodeValue&#xa;&#xa;        return text&#xa;&#xa;    output = {}&#xa;    # Save the affiliatons variable, the default value for ""Affiliation"" column&#xa;    # will be always first value from type_of_affiliation table&#xa;    type_of_affiliation = cfg.OPTIONS.AUTHOR_AFFILIATION_TYPE&#xa;    # Save the default identifier - first element from the list of identifiers&#xa;    default_identifier = cfg.OPTIONS.IDENTIFIERS_LIST[0]&#xa;    # Save identifiers mapping&#xa;    identifiers_mapping = cfg.OPTIONS.IDENTIFIERS_MAPPING&#xa;&#xa;    parsed_xml = minidom.parseString(xml)&#xa;&#xa;    # Extract collaboration name and experiment number&#xa;    collaboration_name = ''&#xa;    experiment_number = ''&#xa;    collaborations = parsed_xml.getElementsByTagName('cal:collaborations')&#xa;    if len(collaborations) == 1:&#xa;        collaboration_name = get_element_value_helper(collaborations[0], 'foaf:name')&#xa;        experiment_number = get_element_value_helper(collaborations[0], 'cal:experimentNumber')&#xa;&#xa;    # Extract affiliations&#xa;    affiliation_list = []&#xa;    affiliation_id_name = {}&#xa;&#xa;    affiliations = parsed_xml.getElementsByTagName('foaf:Organization')&#xa;    for i, affiliation in enumerate(affiliations):&#xa;        affiliation_id = affiliation.getAttribute('id') or ''&#xa;        affiliation_name = get_element_value_helper(affiliation, 'foaf:name')&#xa;        affiliation_acronym = get_element_value_helper(affiliation, 'cal:orgName')&#xa;        if not affiliation_acronym:&#xa;            # No acronym ? Use the name instead&#xa;            affiliation_acronym = affiliation_name&#xa;        affiliation_address = get_element_value_helper(affiliation, 'cal:orgAddress')&#xa;        if not affiliation_address:&#xa;            affiliation_address = affiliation_name&#xa;        affiliation_domain = get_element_value_helper(affiliation, 'cal:orgDomain')&#xa;        # saving {id:name}, it will be needed for authors affiliations&#xa;        if affiliation_id:&#xa;            # According to&#xa;            # http://stackoverflow.com/questions/8214932/how-to-check-if-a-value-exists-in-a-dictionary-python&#xa;            # itervalues is faster than values() and viewvalues()&#xa;            if affiliation_acronym in affiliation_id_name.itervalues():&#xa;                # in case we have a duplicate of acronym, make it unique by&#xa;                # appending the iteration number&#xa;                affiliation_acronym += str(i+1)&#xa;            affiliation_id_name[affiliation_id] = affiliation_acronym&#xa;&#xa;        affiliation_info = [long(i+1),&#xa;                            '',&#xa;                            affiliation_acronym,&#xa;                            '',&#xa;                            affiliation_address,&#xa;                            affiliation_domain,&#xa;                            True,&#xa;                            '']&#xa;        affiliation_list.append(affiliation_info)&#xa;&#xa;    # Extract authors&#xa;    author_list = []&#xa;    authors = parsed_xml.getElementsByTagName('foaf:Person')&#xa;    for i, author in enumerate(authors):&#xa;        first_name = get_element_value_helper(author, 'foaf:givenName')&#xa;        # In case there was no given name under previous field, we search for initials in cal:authorNamePaperGiven&#xa;        if not first_name:&#xa;            first_name = get_element_value_helper(author, 'cal:authorNamePaperGiven')&#xa;        last_name = get_element_value_helper(author, 'foaf:familyName')&#xa;        full_name = get_element_value_helper(author, 'cal:authorNamePaper')&#xa;        status = get_element_value_helper(author, 'cal:authorStatus')&#xa;&#xa;        # Extract author affiliations&#xa;        author_affiliations = []&#xa;        if author.getElementsByTagName('cal:authorAffiliations'):&#xa;            for afil in author.getElementsByTagName('cal:authorAffiliations')[0].getElementsByTagName('cal:authorAffiliation'):&#xa;                a_id = afil.getAttribute('organizationid')&#xa;                if afil.getAttribute('connection') in type_of_affiliation:&#xa;                    affiliation_type = afil.getAttribute('connection')&#xa;                else:&#xa;                    affiliation_type = type_of_affiliation[0]&#xa;                author_affiliations.append([affiliation_id_name.get(a_id, UNKNOWN_AFFILIATION), affiliation_type])&#xa;        else:&#xa;            author_affiliations = [UNKNOWN_AFFILIATION, type_of_affiliation[0]]&#xa;&#xa;        identifiers = []&#xa;        if author.getElementsByTagName('cal:authorids'):&#xa;            for author_id in author.getElementsByTagName('cal:authorids')[0].getElementsByTagName('cal:authorid'):&#xa;                if author_id.getAttribute('source') in identifiers_mapping and author_id.firstChild:&#xa;                    identifiers.append([&#xa;                        author_id.firstChild.nodeValue,&#xa;                        identifiers_mapping[author_id.getAttribute('source')]])&#xa;        if not identifiers:&#xa;            identifiers.append(['', default_identifier])&#xa;        author_info = [long(i+1),&#xa;                       '',&#xa;                       last_name,&#xa;                       first_name,&#xa;                       full_name,&#xa;                       status,&#xa;                       author_affiliations,&#xa;                       identifiers]&#xa;        author_list.append(author_info)&#xa;&#xa;    output.update({'authors': author_list})&#xa;    output.update({'affiliations': affiliation_list})&#xa;    # Add generic information about the paper&#xa;    output.update({'collaboration': collaboration_name,&#xa;                   'experiment_number': experiment_number,&#xa;                   'last_modified': int(time.time()),&#xa;                   'reference_ids': [],&#xa;                   'paper_id': '1',&#xa;                   'paper_title': ''})&#xa;&#xa;    return output&#xa;&#xa;&#xa;def user_authorization(req, ln):&#xa;    """""" Check user authorization to visit page """"""&#xa;    auth_code, auth_message = acc_authorize_action(req, 'runauthorlist')&#xa;    if auth_code != 0:&#xa;        referer = '/authorlist/'&#xa;        return page_not_authorized(req=req, referer=referer,&#xa;                                   text=auth_message, navmenuid=""authorlist"")&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;def check_user_rights(user_id, paper_id):&#xa;    """"""Check if user can modify this paper""""""&#xa;    # if the paper_id is empty - user is trying to create new record&#xa;    # we allow him, because everyone can do that&#xa;    if not paper_id or (user_id == get_owner(paper_id)):&#xa;        return True&#xa;    return False&#xa;&#xa;&#xa;class Converter(object):&#xa;    CONTENT_TYPE = 'text/plain'&#xa;    FILE_NAME = 'converted.txt'&#xa;&#xa;    def __init__(self):&#xa;        raise NotImplementedError&#xa;&#xa;    def dump(self, data):&#xa;        raise NotImplementedError&#xa;&#xa;    def dumps(self, data):&#xa;        raise NotImplementedError&#xa;&#xa;&#xa;class NA62Latex(Converter):&#xa;    FILE_NAME = 'la.tex'&#xa;&#xa;    def __init__(self):&#xa;        pass&#xa;&#xa;    def dump(self, data):&#xa;        pass&#xa;&#xa;    def dumps(self, data):&#xa;        pass&#xa;&#xa;&#xa;class ElsevierArticle(Converter):&#xa;    CONTENT_TYPE = 'text/plain'&#xa;    FILE_NAME = 'elsarticle.tex'&#xa;    cal = '{http://www.slac.stanford.edu/spires/hepnames/authors_xml/}'&#xa;    foaf = '{http://xmlns.com/foaf/0.1/}'&#xa;&#xa;    def __init__(self):&#xa;        pass&#xa;&#xa;    def dictionary_to_list(self, node):&#xa;        res = {}&#xa;        res[node.tag] = []&#xa;        self.xmltodict(node, res[node.tag])&#xa;        reply = {}&#xa;        reply[node.tag] = {'value': res[node.tag], 'attribs': node.attrib, 'tail': node.tail}&#xa;        return reply&#xa;&#xa;    def xmltodict(self, node, res):&#xa;        rep = {}&#xa;        if len(node):&#xa;            for n in list(node):&#xa;                rep[node.tag] = []&#xa;                value = self.xmltodict(n, rep[node.tag])&#xa;                if len(n):&#xa;                    value = {'value': rep[node.tag], 'attributes': n.attrib, 'tail': n.tail}&#xa;                    res.append({n.tag: value})&#xa;                else:&#xa;                    res.append(rep[node.tag][0])&#xa;        else:&#xa;            value = {}&#xa;            value = {'value': node.text, 'attributes': node.attrib, 'tail': node.tail}&#xa;            res.append({node.tag: value})&#xa;        return&#xa;&#xa;    def get_organizations(self, organizations):&#xa;        organization_dict = dict()&#xa;        for orgs_element in organizations:&#xa;            key = orgs_element.keys()[0]&#xa;            if key == self.foaf + 'Organization':&#xa;                for name_element in orgs_element[key]['value']:&#xa;                    value_key = name_element.keys()[0]&#xa;                    if value_key == self.cal + 'orgAddress':&#xa;                        if name_element[value_key]['value']:&#xa;                            organization_dict[orgs_element[key]['attributes']['id']] = name_element[value_key]['value'].encode('utf-8')&#xa;                        else:&#xa;                            organization_dict[orgs_element[key]['attributes']['id']] = ''&#xa;                        break&#xa;        return organization_dict&#xa;&#xa;    def get_authors(self, authors):&#xa;        author_list = []&#xa;        for auth_element in authors:&#xa;            key = auth_element.keys()[0]&#xa;            if key == self.foaf + 'Person':&#xa;                affiliation_list = []&#xa;            given_name = ''&#xa;            family_name = ''&#xa;            for name_element in auth_element[key]['value']:&#xa;                value_key = name_element.keys()[0]&#xa;                if value_key == self.foaf + 'familyName' and name_element[value_key]['value']:&#xa;                    family_name = name_element[value_key]['value'].encode('utf-8')&#xa;                elif value_key == self.foaf + 'givenName' and name_element[value_key]['value']:&#xa;                    given_name = name_element[value_key]['value'].encode('utf-8')&#xa;                elif value_key == self.cal + 'authorAffiliations':&#xa;                    for aff_element in name_element[value_key]['value']:&#xa;                        aff_key = aff_element.keys()[0]&#xa;                        if aff_key == self.cal + 'authorAffiliation':&#xa;                            if aff_element[aff_key]['attributes']['connection'] == 'Affiliated with':&#xa;                                affiliation_list.append(aff_element[aff_key]['attributes']['organizationid'])&#xa;            author_list.append([(given_name, family_name), tuple(affiliation_list)])&#xa;        return author_list&#xa;&#xa;    def dump(self, data):&#xa;&#xa;        AuthorsXMLConverter = Converters.get('authorsxml')&#xa;        AuthorsXML = dumps(data, AuthorsXMLConverter)&#xa;&#xa;        root = ET.fromstring(AuthorsXML)&#xa;        tree = ET.ElementTree(root)&#xa;&#xa;        res = self.dictionary_to_list(tree.getroot())&#xa;&#xa;        collaboration_author_list_values = res['collaborationauthorlist']['value']&#xa;        organization_dict = dict()&#xa;        author_list = []&#xa;&#xa;        for element in collaboration_author_list_values:&#xa;            key = element.keys()[0]&#xa;            # if the value of the key is empty, start next loop cycle&#xa;            if element[key]['value'] is None:&#xa;                continue&#xa;            if key == self.cal + 'organizations':&#xa;                organization_dict = self.get_organizations(element[key]['value'])&#xa;            elif key == self.cal + 'authors':&#xa;                author_list = self.get_authors(element[key]['value'])&#xa;&#xa;        clusters = []&#xa;        organization_codes = []&#xa;&#xa;        for element in author_list:&#xa;            if len(element[1]) >= 1:&#xa;                organization_code = element[1][0]&#xa;                other_affiliations = element[1][1:]&#xa;                author = [element[0]]&#xa;                if other_affiliations:&#xa;                    author.extend(other_affiliations)&#xa;                # if this organization already exists in the cluster&#xa;                if organization_code in organization_codes:&#xa;                    for cluster in clusters:&#xa;                        if cluster[0] == organization_code:&#xa;                            cluster.append(author)&#xa;                            break&#xa;                else:&#xa;                    organization_codes.append(organization_code)&#xa;                    clusters.append([organization_code, author])&#xa;&#xa;        myout = """"&#xa;&#xa;        myout += ""\\documentclass[a4paper,12pt]{article}\r\n""&#xa;        myout += ""\\usepackage[utf8]{inputenc}\r\n""&#xa;        myout += ""\\begin{document}\r\n""&#xa;        myout += ""\\begin{center}\r\n""&#xa;        myout += ""{\\Large Collaboration}\\\\\r\n""&#xa;        myout += ""\\vspace{2mm}\r\n%\r\n""&#xa;        primary_output_string = """"&#xa;        secondary_affiliation_count = 1&#xa;        secondary_affiliations = """"&#xa;        secondary_affiliations_pos = {}&#xa;        for data in clusters:&#xa;            primary_output = []&#xa;            organization_code = data[0]&#xa;            for author in data[1:]:&#xa;                name = "" "" + str(escape_latex(author[0][0])) + '~' + str(escape_latex(author[0][1]))&#xa;                if len(author) > 1:&#xa;                    for sec_affiliation in author[1:]:&#xa;                        if sec_affiliation in organization_dict.keys():&#xa;                            if organization_dict[sec_affiliation] in secondary_affiliations_pos.keys():&#xa;                                name += ""$\\,$\\footnotemark["" + str(secondary_affiliations_pos[organization_dict[sec_affiliation]]) + ""]""&#xa;                            else:&#xa;                                name += ""$\\,$\\footnotemark["" + str(secondary_affiliation_count) + ""]""&#xa;                                secondary_affiliations += ""%\r\n\\footnotetext["" + str(secondary_affiliation_count) + ""]{"" + str(escape_latex(organization_dict[sec_affiliation])) + ""}\r\n""&#xa;                                secondary_affiliations_pos[organization_dict[sec_affiliation]] = secondary_affiliation_count&#xa;                                secondary_affiliation_count += 1&#xa;                primary_output.append(name)&#xa;            if organization_dict.get(data[0]):&#xa;                organization = organization_dict.get(data[0])&#xa;            else:&#xa;                organization = UNKNOWN_AFFILIATION&#xa;            primary_output_string += ',\r\n'.join(primary_output) + "" \\\\\r\n{\\em \\small "" + str(escape_latex(organization)) + ""} \\\\[0.2cm]\r\n%\r\n""&#xa;&#xa;        myout += primary_output_string&#xa;        myout += ""\\end{center}\r\n""&#xa;        myout += ""\\setcounter{footnote}{0}\r\n""&#xa;        myout += secondary_affiliations&#xa;        myout += ""\\end{document}\r\n""&#xa;&#xa;        return myout&#xa;&#xa;    def dumps(self, data):&#xa;        return self.dump(data)&#xa;&#xa;&#xa;class APSpaper(Converter):&#xa;    CONTENT_TYPE = 'text/plain'&#xa;    FILE_NAME = 'APSpaper.tex'&#xa;&#xa;    def __init__(self):&#xa;        pass&#xa;&#xa;    def dump(self, data):&#xa;        AuthorsXMLConverter = Converters.get('authorsxml')&#xa;        AuthorsXML = dumps(data, AuthorsXMLConverter)&#xa;&#xa;        organizations_list = []&#xa;        authors_list = []&#xa;&#xa;        root = ET.fromstring(AuthorsXML)&#xa;        # save affiliations&#xa;        for organization in root.findall('{%s}organizations/{%s}Organization' % (NAMESPACES['cal'], NAMESPACES['foaf'])):&#xa;            org_id = organization.attrib['id']&#xa;            org_name = ''&#xa;            if organization.find('{%s}name' % NAMESPACES['foaf']) is not None:&#xa;                org_name = organization.find('{%s}name' % NAMESPACES['foaf']).text or ''&#xa;&#xa;            organizations_list.append([org_id, org_name.encode('utf-8')])&#xa;&#xa;        # save authors&#xa;        for author in root.findall('{%s}authors/{%s}Person' % (NAMESPACES['cal'], NAMESPACES['foaf'])):&#xa;            author_name = ''&#xa;            author_affiliations = []&#xa;            if author.find('{%s}authorNamePaper' % NAMESPACES['cal']) is not None:&#xa;                author_name = author.find('{%s}authorNamePaper' % NAMESPACES['cal']).text or ''&#xa;            for affil in author.findall('{%(cal)s}authorAffiliations/{%(cal)s}authorAffiliation' % {'cal': NAMESPACES['cal']}):&#xa;                author_affiliations.append(affil.attrib['organizationid'])&#xa;&#xa;            authors_list.append([author_name.encode('utf-8'), author_affiliations])&#xa;&#xa;        myout = ''&#xa;        for author in authors_list:&#xa;            myout += '\\author{' + str(escape_latex(author[0])) + '$^{' + ','.join(author[1]) + '}$}\r\n'&#xa;        for org in organizations_list:&#xa;            myout += '\\affiliation{$^{' + str(org[0]) + '}$ ' + str(escape_latex(org[1])) + '}\r\n'&#xa;&#xa;        return myout&#xa;&#xa;    def dumps(self, data):&#xa;        return self.dump(data)&#xa;&#xa;&#xa;class AuthorsXML(Converter):&#xa;    CONTENT_TYPE = 'text/xml'&#xa;    FILE_NAME = 'authors.xml'&#xa;&#xa;    def __init__(self):&#xa;        pass&#xa;&#xa;    def create_affiliation(self, document, parsed, organization_ids):&#xa;        affiliation = document.createElement('cal:authorAffiliation')&#xa;&#xa;        affiliation_acronym = parsed[cfg.JSON.AFFILIATION_ACRONYM]&#xa;        affiliation_status = parsed[cfg.JSON.AFFILIATION_STATUS]&#xa;&#xa;        if affiliation_acronym not in organization_ids:&#xa;            affiliation.setAttribute('organizationid',&#xa;                                     'Error - there is no organization called ' +&#xa;                                     affiliation_acronym)&#xa;        else:&#xa;            affiliation.setAttribute('organizationid',&#xa;                                     organization_ids[affiliation_acronym])&#xa;        affiliation.setAttribute('connection', affiliation_status)&#xa;&#xa;        return affiliation&#xa;&#xa;    def create_identifier(self, document, parsed):&#xa;        identifier = document.createElement('cal:authorid')&#xa;&#xa;        identifier_number = parsed[cfg.JSON.IDENTIFIER_NUMBER]&#xa;        identifier_name = parsed[cfg.JSON.IDENTIFIER_NAME]&#xa;&#xa;        identifier.setAttribute('source', identifier_name)&#xa;        identifier_text = document.createTextNode(identifier_number)&#xa;        identifier.appendChild(identifier_text)&#xa;&#xa;        return identifier&#xa;&#xa;    def create_authors(self, document, root, parsed, organization_ids):&#xa;        parsed_authors = parsed[cfg.JSON.AUTHORS_KEY]&#xa;&#xa;        authors = document.createElement('cal:authors')&#xa;        root.appendChild(authors)&#xa;&#xa;        for parsed_author in parsed_authors:&#xa;            author = self.create_author(document, parsed_author, organization_ids)&#xa;            authors.appendChild(author)&#xa;&#xa;    def create_author(self, document, parsed, organization_ids):&#xa;        author = document.createElement('foaf:Person')&#xa;&#xa;        # paper name&#xa;        paper_name = document.createElement('cal:authorNamePaper')&#xa;        paper_name_info = parsed[cfg.JSON.PAPER_NAME]&#xa;        paper_name_text = document.createTextNode(paper_name_info)&#xa;        paper_name.appendChild(paper_name_text)&#xa;        author.appendChild(paper_name)&#xa;&#xa;        # given name&#xa;        given_name_info = parsed[cfg.JSON.GIVEN_NAME]&#xa;        if (cfg.EMPTY.match(given_name_info) is None):&#xa;            given_name = document.createElement('foaf:givenName')&#xa;            given_name_text = document.createTextNode(given_name_info)&#xa;            given_name.appendChild(given_name_text)&#xa;            author.appendChild(given_name)&#xa;&#xa;        # family name&#xa;        family_name_info = parsed[cfg.JSON.FAMILY_NAME]&#xa;        if (cfg.EMPTY.match(family_name_info) is None):&#xa;            family_name = document.createElement('foaf:familyName')&#xa;            family_name_text = document.createTextNode(family_name_info)&#xa;            family_name.appendChild(family_name_text)&#xa;            author.appendChild(family_name)&#xa;&#xa;        # status&#xa;        author_status_info = parsed[cfg.JSON.STATUS]&#xa;        if (author_status_info):&#xa;            author_status = document.createElement('cal:authorStatus')&#xa;            author_status_text = document.createTextNode(author_status_info)&#xa;            author_status.appendChild(author_status_text)&#xa;            author.appendChild(author_status)&#xa;&#xa;        # collaboration&#xa;        collaboration = document.createElement('cal:authorCollaboration')&#xa;        collaboration.setAttribute('collaborationid', cfg.AuthorsXML.COLLABORATION_ID)&#xa;        author.appendChild(collaboration)&#xa;&#xa;        # affiliations&#xa;        affiliations = document.createElement('cal:authorAffiliations')&#xa;        author.appendChild(affiliations)&#xa;        for parsed_affiliation in parsed[cfg.JSON.AFFILIATIONS]:&#xa;            affiliation = self.create_affiliation(document, parsed_affiliation, organization_ids)&#xa;            affiliations.appendChild(affiliation)&#xa;&#xa;        # identifiers&#xa;        identifiers = document.createElement('cal:authorids')&#xa;        author.appendChild(identifiers)&#xa;        for parsed_identifier in parsed[cfg.JSON.IDENTIFIERS]:&#xa;            identifier = self.create_identifier(document, parsed_identifier)&#xa;            identifiers.appendChild(identifier)&#xa;&#xa;        return author&#xa;&#xa;    def create_collaboration(self, document, root, parsed):&#xa;        # collaborations&#xa;        collaborations = document.createElement('cal:collaborations')&#xa;        collaboration = document.createElement('cal:collaboration')&#xa;        collaboration.setAttribute('id', cfg.AuthorsXML.COLLABORATION_ID)&#xa;        collaborations.appendChild(collaboration)&#xa;&#xa;        # name&#xa;        name = document.createElement('foaf:name')&#xa;        name_info = parsed[cfg.JSON.COLLABORATION]&#xa;        name_text = document.createTextNode(name_info)&#xa;        name.appendChild(name_text)&#xa;        collaboration.appendChild(name)&#xa;&#xa;        # experiment number&#xa;        experiment_number_info = parsed[cfg.JSON.EXPERIMENT_NUMBER]&#xa;        if (cfg.EMPTY.match(experiment_number_info) is None):&#xa;            experiment_number = document.createElement('cal:experimentNumber')&#xa;            experiment_number_text = document.createTextNode(experiment_number_info)&#xa;            experiment_number.appendChild(experiment_number_text)&#xa;            collaboration.appendChild(experiment_number)&#xa;        root.appendChild(collaborations)&#xa;&#xa;    def create_document(self):&#xa;        dom = minidom.getDOMImplementation()&#xa;        document = dom.createDocument(None, 'collaborationauthorlist', None)&#xa;        root = document.documentElement&#xa;&#xa;        root.setAttribute('xmlns:foaf', 'http://xmlns.com/foaf/0.1/')&#xa;        root.setAttribute('xmlns:cal', 'http://www.slac.stanford.edu/spires/hepnames/authors_xml/')&#xa;&#xa;        return document, root&#xa;&#xa;    def create_header(self, document, root, parsed):&#xa;        # creation date&#xa;        creation_date = document.createElement('cal:creationDate')&#xa;        creation_date_info = time.strftime(cfg.AuthorsXML.TIME_FORMAT)&#xa;        creation_date_text = document.createTextNode(creation_date_info)&#xa;        creation_date.appendChild(creation_date_text)&#xa;        root.appendChild(creation_date)&#xa;&#xa;        # publication reference&#xa;        for reference_info in parsed[cfg.JSON.REFERENCE_IDS]:&#xa;            reference = document.createElement('cal:publicationReference')&#xa;            reference_text = document.createTextNode(reference_info)&#xa;            reference.appendChild(reference_text)&#xa;            root.appendChild(reference)&#xa;&#xa;    def create_organizations(self, document, root, parsed, ids):&#xa;        parsed_organizations = parsed[cfg.JSON.AFFILIATIONS_KEY]&#xa;&#xa;        # organizations container&#xa;        organizations = document.createElement('cal:organizations')&#xa;        root.appendChild(organizations)&#xa;&#xa;        # create individual organizations and append them&#xa;        for parsed_organization in parsed_organizations:&#xa;            organization = self.create_organization(document, parsed_organization, ids)&#xa;            organizations.appendChild(organization)&#xa;&#xa;    def create_organization(self, document, parsed, ids):&#xa;        acronym = parsed[cfg.JSON.ACRONYM]&#xa;        organization = document.createElement('foaf:Organization')&#xa;        organization.setAttribute('id', ids[acronym])&#xa;&#xa;        # create the domain node if field is set&#xa;        domain_info = parsed[cfg.JSON.DOMAIN]&#xa;        if (cfg.EMPTY.match(domain_info) is None):&#xa;            domain = document.createElement('cal:orgDomain')&#xa;            domain_text = document.createTextNode(domain_info)&#xa;            domain.appendChild(domain_text)&#xa;            organization.appendChild(domain)&#xa;&#xa;        # organization name, no presence check, already done on the client side&#xa;        name = document.createElement('foaf:name')&#xa;        name_info = parsed[cfg.JSON.NAME]&#xa;        name_text = document.createTextNode(name_info)&#xa;        name.appendChild(name_text)&#xa;        organization.appendChild(name)&#xa;&#xa;        # organization acronym&#xa;        org_acronym = document.createElement('cal:orgName')&#xa;        org_acronym_text = document.createTextNode(acronym)&#xa;        org_acronym.appendChild(org_acronym_text)&#xa;        organization.appendChild(org_acronym)&#xa;&#xa;        # organization identifier&#xa;        org_name_info = parsed[cfg.JSON.SPIRES_ID]&#xa;        if (cfg.EMPTY.match(org_name_info) is None):&#xa;            org_name = document.createElement('cal:orgName')&#xa;            org_name.setAttribute('source', cfg.AuthorsXML.SPIRES)&#xa;            org_name_text = document.createTextNode(org_name_info)&#xa;            org_name.appendChild(org_name_text)&#xa;            organization.appendChild(org_name)&#xa;        else:&#xa;            org_name_info = parsed[cfg.JSON.NAME]&#xa;            org_address = document.createElement('cal:orgAddress')&#xa;            org_address_text = document.createTextNode(org_name_info)&#xa;            org_address.appendChild(org_address_text)&#xa;            organization.appendChild(org_address)&#xa;&#xa;        # membership&#xa;        org_status_info = parsed[cfg.JSON.MEMBER]&#xa;        if (not org_status_info):&#xa;            org_status_info = cfg.AuthorsXML.NONMEMBER&#xa;        else:&#xa;            org_status_info = cfg.AuthorsXML.MEMBER&#xa;        org_status = document.createElement('cal:orgStatus')&#xa;        org_status_text = document.createTextNode(org_status_info)&#xa;        org_status.appendChild(org_status_text)&#xa;        organization.appendChild(org_status)&#xa;&#xa;        # umbrella organization/group&#xa;        group_info = parsed[cfg.JSON.UMBRELLA]&#xa;        if (cfg.EMPTY.match(group_info) is None):&#xa;            if group_info in ids.keys():&#xa;                group = document.createElement('cal:group')&#xa;                group.setAttribute('with', ids[group_info])&#xa;                organization.appendChild(group)&#xa;&#xa;        return organization&#xa;&#xa;    def dump(self, data):&#xa;        parsed = json.loads(data)&#xa;        document, root = self.create_document()&#xa;        affiliations = parsed[cfg.JSON.AFFILIATIONS_KEY]&#xa;&#xa;        organization_ids = self.generate_organization_ids(affiliations)&#xa;&#xa;        self.create_header(document, root, parsed)&#xa;        self.create_collaboration(document, root, parsed)&#xa;        self.create_organizations(document, root, parsed, organization_ids)&#xa;        self.create_authors(document, root, parsed, organization_ids)&#xa;&#xa;        return document&#xa;&#xa;    def dumps(self, data):&#xa;        # FIX for toprettyxml function from website:&#xa;        # http://ronrothman.com/public/leftbraned/xml-dom-minidom-toprettyxml-and-silly-whitespace/&#xa;        def fixed_writexml(self, writer, indent="""", addindent="""", newl=""""):&#xa;            # indent = current indentation&#xa;            # addindent = indentation to add to higher levels&#xa;            # newl = newline string&#xa;            writer.write(indent+""<"" + self.tagName)&#xa;&#xa;            attrs = self._get_attributes()&#xa;            a_names = attrs.keys()&#xa;            a_names.sort()&#xa;&#xa;            for a_name in a_names:&#xa;                writer.write("" %s=\"""" % a_name)&#xa;                minidom._write_data(writer, attrs[a_name].value)&#xa;                writer.write(""\"""")&#xa;            if self.childNodes:&#xa;                if len(self.childNodes) == 1 and self.childNodes[0].nodeType == minidom.Node.TEXT_NODE:&#xa;                    writer.write("">"")&#xa;                    self.childNodes[0].writexml(writer, """", """", """")&#xa;                    writer.write(""</%s>%s"" % (self.tagName, newl))&#xa;                    return&#xa;                writer.write("">%s"" % (newl))&#xa;                for node in self.childNodes:&#xa;                    node.writexml(writer, indent + addindent, addindent, newl)&#xa;                writer.write(""%s</%s>%s"" % (indent, self.tagName, newl))&#xa;            else:&#xa;                writer.write(""/>%s"" % (newl))&#xa;        # replace minidom's function with ours&#xa;        minidom.Element.writexml = fixed_writexml&#xa;        # End of FIX&#xa;        return self.dump(data).toprettyxml(indent='    ', newl='\r\n', encoding='utf-8')&#xa;&#xa;    def generate_organization_ids(self, organizations):&#xa;        ids = {}&#xa;        # Map each organization acronym to an id of the kind 'o[index]'&#xa;        for index, organization in enumerate(organizations):&#xa;            acronym = organization[cfg.JSON.ACRONYM]&#xa;            ids[acronym] = cfg.AuthorsXML.ORGANIZATION_ID + str(index)&#xa;&#xa;        return ids&#xa;&#xa;&#xa;class Converters:&#xa;    __converters__ = {'authorsxml': AuthorsXML, 'elsevier': ElsevierArticle, 'aps': APSpaper}&#xa;&#xa;    @classmethod&#xa;    def get(cls, format):&#xa;        return cls.__converters__.get(format)&#xa;&#xa;&#xa;def dump(data, converter):&#xa;    return converter().dump(data)&#xa;&#xa;&#xa;def dumps(data, converter):&#xa;    return converter().dumps(data)&#xa;"
764360|"# (c) 2014, Chris Church <chris@ninemoreminutes.com>&#xa;#&#xa;# This file is part of Ansible.&#xa;#&#xa;# Ansible is free software: you can redistribute it and/or modify&#xa;# it under the terms of the GNU General Public License as published by&#xa;# the Free Software Foundation, either version 3 of the License, or&#xa;# (at your option) any later version.&#xa;#&#xa;# Ansible is distributed in the hope that it will be useful,&#xa;# but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;# GNU General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License&#xa;# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.&#xa;from __future__ import (absolute_import, division, print_function)&#xa;__metaclass__ = type&#xa;&#xa;import base64&#xa;import os&#xa;import re&#xa;import shlex&#xa;&#xa;from ansible.errors import AnsibleError&#xa;from ansible.module_utils._text import to_bytes, to_text&#xa;&#xa;&#xa;_common_args = ['PowerShell', '-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted']&#xa;&#xa;# Primarily for testing, allow explicitly specifying PowerShell version via&#xa;# an environment variable.&#xa;_powershell_version = os.environ.get('POWERSHELL_VERSION', None)&#xa;if _powershell_version:&#xa;    _common_args = ['PowerShell', '-Version', _powershell_version] + _common_args[1:]&#xa;&#xa;exec_wrapper = br'''&#xa;#Requires -Version 3.0&#xa;begin {&#xa;    $DebugPreference = ""Continue""&#xa;    $ErrorActionPreference = ""Stop""&#xa;    Set-StrictMode -Version 2&#xa;&#xa;    function ConvertTo-HashtableFromPsCustomObject ($myPsObject){&#xa;        $output = @{};&#xa;        $myPsObject | Get-Member -MemberType *Property | % {&#xa;            $val = $myPsObject.($_.name);&#xa;            If ($val -is [psobject]) {&#xa;                $val = ConvertTo-HashtableFromPsCustomObject $val&#xa;            }&#xa;            $output.($_.name) = $val&#xa;        }&#xa;        return $output;&#xa;    }&#xa;    # stream JSON including become_pw, ps_module_payload, bin_module_payload, become_payload, write_payload_path, preserve directives&#xa;    # exec runspace, capture output, cleanup, return module output&#xa;&#xa;    # NB: do not adjust the following line- it is replaced when doing non-streamed module output&#xa;    $json_raw = ''&#xa;}&#xa;process {&#xa;    $input_as_string = [string]$input&#xa;&#xa;    $json_raw += $input_as_string&#xa;}&#xa;end {&#xa;    If (-not $json_raw) {&#xa;        Write-Error ""no input given"" -Category InvalidArgument&#xa;    }&#xa;    $payload = ConvertTo-HashtableFromPsCustomObject (ConvertFrom-Json $json_raw)&#xa;&#xa;    # TODO: handle binary modules&#xa;    # TODO: handle persistence&#xa;&#xa;    $actions = $payload.actions&#xa;&#xa;    # pop 0th action as entrypoint&#xa;    $entrypoint = $payload.($actions[0])&#xa;    $payload.actions = $payload.actions[1..99]&#xa;&#xa;    $entrypoint = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($entrypoint))&#xa;&#xa;    # load the current action entrypoint as a module custom object with a Run method&#xa;    $entrypoint = New-Module -ScriptBlock ([scriptblock]::Create($entrypoint)) -AsCustomObject&#xa;&#xa;    Set-Variable -Scope global -Name complex_args -Value $payload[""module_args""] | Out-Null&#xa;&#xa;    # dynamically create/load modules&#xa;    ForEach ($mod in $payload.powershell_modules.GetEnumerator()) {&#xa;        $decoded_module = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($mod.Value))&#xa;        New-Module -ScriptBlock ([scriptblock]::Create($decoded_module)) -Name $mod.Key | Import-Module -WarningAction SilentlyContinue | Out-Null&#xa;    }&#xa;&#xa;    $output = $entrypoint.Run($payload)&#xa;&#xa;    Write-Output $output&#xa;}&#xa;&#xa;'''  # end exec_wrapper&#xa;&#xa;leaf_exec = br'''&#xa;Function Run($payload) {&#xa;    $entrypoint = $payload.module_entry&#xa;&#xa;    $entrypoint = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($entrypoint))&#xa;&#xa;    $ps = [powershell]::Create()&#xa;&#xa;    $ps.AddStatement().AddCommand(""Set-Variable"").AddParameters(@{Scope=""global"";Name=""complex_args"";Value=$payload.module_args}) | Out-Null&#xa;    $ps.AddCommand(""Out-Null"") | Out-Null&#xa;&#xa;    # redefine Write-Host to dump to output instead of failing- lots of scripts use it&#xa;    $ps.AddStatement().AddScript(""Function Write-Host(`$msg){ Write-Output `$msg }"") | Out-Null&#xa;&#xa;    ForEach ($env_kv in $payload.environment.GetEnumerator()) {&#xa;        $escaped_env_set = ""`$env:{0} = '{1}'"" -f $env_kv.Key,$env_kv.Value.Replace(""'"",""''"")&#xa;        $ps.AddStatement().AddScript($escaped_env_set) | Out-Null&#xa;    }&#xa;&#xa;    # dynamically create/load modules&#xa;    ForEach ($mod in $payload.powershell_modules.GetEnumerator()) {&#xa;        $decoded_module = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($mod.Value))&#xa;        $ps.AddStatement().AddCommand(""New-Module"").AddParameters(@{ScriptBlock=([scriptblock]::Create($decoded_module));Name=$mod.Key}) | Out-Null&#xa;        $ps.AddCommand(""Import-Module"").AddParameters(@{WarningAction=""SilentlyContinue""}) | Out-Null&#xa;        $ps.AddCommand(""Out-Null"") | Out-Null&#xa;    }&#xa;&#xa;    # force input encoding to preamble-free UTF8 so PS sub-processes (eg, Start-Job) don't blow up&#xa;    $ps.AddStatement().AddScript(""[Console]::InputEncoding = New-Object Text.UTF8Encoding `$false"") | Out-Null&#xa;&#xa;    $ps.AddStatement().AddScript($entrypoint) | Out-Null&#xa;&#xa;    $output = $ps.Invoke()&#xa;&#xa;    $output&#xa;&#xa;    # PS3 doesn't properly set HadErrors in many cases, inspect the error stream as a fallback&#xa;    If ($ps.HadErrors -or ($PSVersionTable.PSVersion.Major -lt 4 -and $ps.Streams.Error.Count -gt 0)) {&#xa;        [System.Console]::Error.WriteLine($($ps.Streams.Error | Out-String))&#xa;        $exit_code = $ps.Runspace.SessionStateProxy.GetVariable(""LASTEXITCODE"")&#xa;        If(-not $exit_code) {&#xa;            $exit_code = 1&#xa;        }&#xa;        # need to use this instead of Exit keyword to prevent runspace from crashing with dynamic modules&#xa;        $host.SetShouldExit($exit_code)&#xa;    }&#xa;}&#xa;'''  # end leaf_exec&#xa;&#xa;&#xa;become_wrapper = br'''&#xa;Set-StrictMode -Version 2&#xa;$ErrorActionPreference = ""Stop""&#xa;&#xa;$helper_def = @""&#xa;using System;&#xa;using System.Diagnostics;&#xa;using System.IO;&#xa;using System.Text;&#xa;using System.Threading;&#xa;using System.Security;&#xa;using System.Security.AccessControl;&#xa;using System.Security.Principal;&#xa;using System.Runtime.InteropServices;&#xa;&#xa;namespace Ansible.Shell&#xa;{&#xa;    public class NativeProcessUtil&#xa;    {&#xa;        public static void GetProcessOutput(StreamReader stdoutStream, StreamReader stderrStream, out string stdout, out string stderr)&#xa;        {&#xa;            var sowait = new EventWaitHandle(false, EventResetMode.ManualReset);&#xa;            var sewait = new EventWaitHandle(false, EventResetMode.ManualReset);&#xa;&#xa;            string so = null, se = null;&#xa;&#xa;            ThreadPool.QueueUserWorkItem((s)=>&#xa;            {&#xa;                so = stdoutStream.ReadToEnd();&#xa;                sowait.Set();&#xa;            });&#xa;&#xa;            ThreadPool.QueueUserWorkItem((s) =>&#xa;            {&#xa;                se = stderrStream.ReadToEnd();&#xa;                sewait.Set();&#xa;            });&#xa;&#xa;            foreach(var wh in new WaitHandle[] { sowait, sewait })&#xa;                wh.WaitOne();&#xa;&#xa;            stdout = so;&#xa;            stderr = se;&#xa;        }&#xa;&#xa;        // http://stackoverflow.com/a/30687230/139652&#xa;        public static void GrantAccessToWindowStationAndDesktop(string username)&#xa;        {&#xa;            const int WindowStationAllAccess = 0x000f037f;&#xa;            GrantAccess(username, GetProcessWindowStation(), WindowStationAllAccess);&#xa;            const int DesktopRightsAllAccess = 0x000f01ff;&#xa;            GrantAccess(username, GetThreadDesktop(GetCurrentThreadId()), DesktopRightsAllAccess);&#xa;        }&#xa;&#xa;        public static string SearchPath(string findThis)&#xa;        {&#xa;            StringBuilder sbOut = new StringBuilder(1024);&#xa;            IntPtr filePartOut;&#xa;&#xa;            if(SearchPath(null, findThis, null, sbOut.Capacity, sbOut, out filePartOut) == 0)&#xa;                throw new FileNotFoundException(""Couldn't locate "" + findThis + "" on path"");&#xa;&#xa;            return sbOut.ToString();&#xa;        }&#xa;&#xa;        public static uint GetProcessExitCode(IntPtr processHandle) {&#xa;            new NativeWaitHandle(processHandle).WaitOne();&#xa;            uint exitCode;&#xa;            if(!GetExitCodeProcess(processHandle, out exitCode)) {&#xa;                throw new Exception(""Error getting process exit code: "" + Marshal.GetLastWin32Error());&#xa;            }&#xa;            return exitCode;&#xa;        }&#xa;&#xa;        private static void GrantAccess(string username, IntPtr handle, int accessMask)&#xa;        {&#xa;            SafeHandle safeHandle = new NoopSafeHandle(handle);&#xa;            GenericSecurity security =&#xa;                new GenericSecurity(false, ResourceType.WindowObject, safeHandle, AccessControlSections.Access);&#xa;&#xa;            security.AddAccessRule(&#xa;                new GenericAccessRule(new NTAccount(username), accessMask, AccessControlType.Allow));&#xa;            security.Persist(safeHandle, AccessControlSections.Access);&#xa;        }&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true, CharSet=CharSet.Unicode)]&#xa;        public static extern uint SearchPath (&#xa;            string lpPath,&#xa;            string lpFileName,&#xa;            string lpExtension,&#xa;            int nBufferLength,&#xa;            [MarshalAs (UnmanagedType.LPTStr)]&#xa;            StringBuilder lpBuffer,&#xa;            out IntPtr lpFilePart);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        private static extern bool GetExitCodeProcess(IntPtr hProcess, out uint lpExitCode);&#xa;&#xa;        [DllImport(""kernel32.dll"")]&#xa;        public static extern bool CreatePipe(out IntPtr hReadPipe, out IntPtr hWritePipe, SECURITY_ATTRIBUTES lpPipeAttributes, uint nSize);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        public static extern IntPtr GetStdHandle(StandardHandleValues nStdHandle);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        public static extern bool SetHandleInformation(IntPtr hObject, HandleFlags dwMask, int dwFlags);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        public static extern bool CloseHandle(IntPtr hObject);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        public static extern bool InitializeProcThreadAttributeList(IntPtr lpAttributeList, int dwAttributeCount, int dwFlags, ref int lpSize);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;        public static extern bool UpdateProcThreadAttribute(&#xa;                IntPtr lpAttributeList,&#xa;                uint dwFlags,&#xa;                IntPtr Attribute,&#xa;                IntPtr lpValue,&#xa;                IntPtr cbSize,&#xa;                IntPtr lpPreviousValue,&#xa;                IntPtr lpReturnSize);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError=true, CharSet=CharSet.Unicode, BestFitMapping=false)]&#xa;        public static extern bool CreateProcess(&#xa;            [MarshalAs(UnmanagedType.LPTStr)]&#xa;            string lpApplicationName,&#xa;            StringBuilder lpCommandLine,&#xa;            IntPtr lpProcessAttributes,&#xa;            IntPtr lpThreadAttributes,&#xa;            bool bInheritHandles,&#xa;            uint dwCreationFlags,&#xa;            IntPtr lpEnvironment,&#xa;            [MarshalAs(UnmanagedType.LPTStr)]&#xa;            string lpCurrentDirectory,&#xa;            STARTUPINFO lpStartupInfo,&#xa;            out PROCESS_INFORMATION lpProcessInformation);&#xa;&#xa;&#xa;        [DllImport(""advapi32.dll"", SetLastError=true, CharSet=CharSet.Unicode)]&#xa;        public static extern bool CreateProcessWithLogonW(&#xa;            string             userName,&#xa;            string             domain,&#xa;            string             password,&#xa;            LOGON_FLAGS         logonFlags,&#xa;            string             applicationName,&#xa;            string             commandLine,&#xa;            uint          creationFlags,&#xa;            IntPtr             environment,&#xa;            string             currentDirectory,&#xa;            STARTUPINFOEX  startupInfo,&#xa;            out PROCESS_INFORMATION     processInformation);&#xa;&#xa;        [DllImport(""user32.dll"", SetLastError = true)]&#xa;        private static extern IntPtr GetProcessWindowStation();&#xa;&#xa;        [DllImport(""user32.dll"", SetLastError = true)]&#xa;        private static extern IntPtr GetThreadDesktop(int dwThreadId);&#xa;&#xa;        [DllImport(""kernel32.dll"", SetLastError = true)]&#xa;        private static extern int GetCurrentThreadId();&#xa;&#xa;        private class GenericAccessRule : AccessRule&#xa;        {&#xa;            public GenericAccessRule(IdentityReference identity, int accessMask, AccessControlType type) :&#xa;                base(identity, accessMask, false, InheritanceFlags.None, PropagationFlags.None, type) { }&#xa;        }&#xa;&#xa;        private class GenericSecurity : NativeObjectSecurity&#xa;        {&#xa;            public GenericSecurity(bool isContainer, ResourceType resType, SafeHandle objectHandle, AccessControlSections sectionsRequested)&#xa;                : base(isContainer, resType, objectHandle, sectionsRequested) { }&#xa;&#xa;            public new void Persist(SafeHandle handle, AccessControlSections includeSections) { base.Persist(handle, includeSections); }&#xa;&#xa;            public new void AddAccessRule(AccessRule rule) { base.AddAccessRule(rule); }&#xa;&#xa;            public override Type AccessRightType { get { throw new NotImplementedException(); } }&#xa;&#xa;            public override AccessRule AccessRuleFactory(System.Security.Principal.IdentityReference identityReference, int accessMask, bool isInherited,&#xa;                InheritanceFlags inheritanceFlags, PropagationFlags propagationFlags, AccessControlType type) { throw new NotImplementedException(); }&#xa;&#xa;            public override Type AccessRuleType { get { return typeof(AccessRule); } }&#xa;&#xa;            public override AuditRule AuditRuleFactory(System.Security.Principal.IdentityReference identityReference, int accessMask, bool isInherited,&#xa;                InheritanceFlags inheritanceFlags, PropagationFlags propagationFlags, AuditFlags flags) { throw new NotImplementedException(); }&#xa;&#xa;            public override Type AuditRuleType { get { return typeof(AuditRule); } }&#xa;        }&#xa;&#xa;        private class NoopSafeHandle : SafeHandle&#xa;        {&#xa;            public NoopSafeHandle(IntPtr handle) : base(handle, false) { }&#xa;            public override bool IsInvalid { get { return false; } }&#xa;            protected override bool ReleaseHandle() { return true; }&#xa;        }&#xa;&#xa;&#xa;    }&#xa;&#xa;    class NativeWaitHandle : WaitHandle {&#xa;        public NativeWaitHandle(IntPtr handle) {&#xa;            this.Handle = handle;&#xa;        }&#xa;    }&#xa;&#xa;    [Flags]&#xa;    public enum LOGON_FLAGS&#xa;    {&#xa;        LOGON_WITH_PROFILE     = 0x00000001,&#xa;        LOGON_NETCREDENTIALS_ONLY  = 0x00000002&#xa;    }&#xa;&#xa;    [Flags]&#xa;    public enum CREATION_FLAGS&#xa;    {&#xa;        CREATE_SUSPENDED = 0x00000004,&#xa;        CREATE_NEW_CONSOLE = 0x00000010,&#xa;        CREATE_UNICODE_ENVIRONMENT = 0x000000400,&#xa;        CREATE_BREAKAWAY_FROM_JOB = 0x01000000,&#xa;        EXTENDED_STARTUPINFO_PRESENT = 0x00080000,&#xa;    }&#xa;&#xa;&#xa;&#xa;    [Flags]&#xa;    public enum StartupInfoFlags : uint&#xa;    {&#xa;        USESTDHANDLES = 0x00000100&#xa;    }&#xa;&#xa;    public enum StandardHandleValues : int&#xa;    {&#xa;        STD_INPUT_HANDLE = -10,&#xa;        STD_OUTPUT_HANDLE = -11,&#xa;        STD_ERROR_HANDLE = -12&#xa;    }&#xa;&#xa;    [Flags]&#xa;    public enum HandleFlags : uint&#xa;    {&#xa;        None = 0,&#xa;        INHERIT = 1&#xa;    }&#xa;&#xa;&#xa;    [StructLayout(LayoutKind.Sequential)]&#xa;    public class SECURITY_ATTRIBUTES&#xa;    {&#xa;        public int nLength;&#xa;        public IntPtr lpSecurityDescriptor;&#xa;        public bool bInheritHandle = false;&#xa;&#xa;        public SECURITY_ATTRIBUTES() {&#xa;            nLength = Marshal.SizeOf(this);&#xa;        }&#xa;    }&#xa;&#xa;&#xa;    [StructLayout(LayoutKind.Sequential)]&#xa;    public class STARTUPINFO&#xa;    {&#xa;        public Int32 cb;&#xa;        public IntPtr lpReserved;&#xa;        public IntPtr lpDesktop;&#xa;        public IntPtr lpTitle;&#xa;        public Int32 dwX;&#xa;        public Int32 dwY;&#xa;        public Int32 dwXSize;&#xa;        public Int32 dwYSize;&#xa;        public Int32 dwXCountChars;&#xa;        public Int32 dwYCountChars;&#xa;        public Int32 dwFillAttribute;&#xa;        public Int32 dwFlags;&#xa;        public Int16 wShowWindow;&#xa;        public Int16 cbReserved2;&#xa;        public IntPtr lpReserved2;&#xa;        public IntPtr hStdInput;&#xa;        public IntPtr hStdOutput;&#xa;        public IntPtr hStdError;&#xa;&#xa;        public STARTUPINFO() {&#xa;            cb = Marshal.SizeOf(this);&#xa;        }&#xa;    }&#xa;&#xa;    [StructLayout(LayoutKind.Sequential)]&#xa;    public class STARTUPINFOEX {&#xa;        public STARTUPINFO startupInfo;&#xa;        public IntPtr lpAttributeList;&#xa;&#xa;        public STARTUPINFOEX() {&#xa;            startupInfo = new STARTUPINFO();&#xa;            startupInfo.cb = Marshal.SizeOf(this);&#xa;        }&#xa;    }&#xa;&#xa;    [StructLayout(LayoutKind.Sequential)]&#xa;    public struct PROCESS_INFORMATION&#xa;    {&#xa;        public IntPtr hProcess;&#xa;        public IntPtr hThread;&#xa;        public int dwProcessId;&#xa;        public int dwThreadId;&#xa;    }&#xa;&#xa;&#xa;&#xa;}&#xa;""@&#xa;&#xa;$exec_wrapper = {&#xa;#Requires -Version 3.0&#xa;$DebugPreference = ""Continue""&#xa;$ErrorActionPreference = ""Stop""&#xa;Set-StrictMode -Version 2&#xa;&#xa;function ConvertTo-HashtableFromPsCustomObject ($myPsObject){&#xa;    $output = @{};&#xa;    $myPsObject | Get-Member -MemberType *Property | % {&#xa;        $val = $myPsObject.($_.name);&#xa;        If ($val -is [psobject]) {&#xa;            $val = ConvertTo-HashtableFromPsCustomObject $val&#xa;        }&#xa;        $output.($_.name) = $val&#xa;    }&#xa;    return $output;&#xa;}&#xa;# stream JSON including become_pw, ps_module_payload, bin_module_payload, become_payload, write_payload_path, preserve directives&#xa;# exec runspace, capture output, cleanup, return module output&#xa;&#xa;$json_raw = [System.Console]::In.ReadToEnd()&#xa;&#xa;If (-not $json_raw) {&#xa;    Write-Error ""no input given"" -Category InvalidArgument&#xa;}&#xa;&#xa;$payload = ConvertTo-HashtableFromPsCustomObject (ConvertFrom-Json $json_raw)&#xa;&#xa;# TODO: handle binary modules&#xa;# TODO: handle persistence&#xa;&#xa;$actions = $payload.actions&#xa;&#xa;# pop 0th action as entrypoint&#xa;$entrypoint = $payload.($actions[0])&#xa;$payload.actions = $payload.actions[1..99]&#xa;&#xa;$entrypoint = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($entrypoint))&#xa;&#xa;# load the current action entrypoint as a module custom object with a Run method&#xa;$entrypoint = New-Module -ScriptBlock ([scriptblock]::Create($entrypoint)) -AsCustomObject&#xa;&#xa;Set-Variable -Scope global -Name complex_args -Value $payload[""module_args""] | Out-Null&#xa;&#xa;# dynamically create/load modules&#xa;ForEach ($mod in $payload.powershell_modules.GetEnumerator()) {&#xa;    $decoded_module = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($mod.Value))&#xa;    New-Module -ScriptBlock ([scriptblock]::Create($decoded_module)) -Name $mod.Key | Import-Module -WarningAction SilentlyContinue | Out-Null&#xa;}&#xa;&#xa;$output = $entrypoint.Run($payload)&#xa;&#xa;Write-Output $output&#xa;&#xa;} # end exec_wrapper&#xa;&#xa;Function Dump-Error ($excep) {&#xa;    $eo = @{failed=$true}&#xa;&#xa;    $eo.msg = $excep.Exception.Message&#xa;    $eo.exception = $excep | Out-String&#xa;    $host.SetShouldExit(1)&#xa;&#xa;    $eo | ConvertTo-Json -Depth 10&#xa;}&#xa;&#xa;Function Run($payload) {&#xa;    # NB: action popping handled inside subprocess wrapper&#xa;&#xa;    $username = $payload.become_user&#xa;    $password = $payload.become_password&#xa;&#xa;    # FUTURE: convert to SafeHandle so we can stop ignoring warnings?&#xa;    Add-Type -TypeDefinition $helper_def -Debug:$false -IgnoreWarnings&#xa;&#xa;    $exec_args = $null&#xa;&#xa;    $exec_application = ""powershell""&#xa;&#xa;    # NB: CreateProcessWithLogonW commandline maxes out at 1024 chars, must bootstrap via filesystem&#xa;    $temp = [System.IO.Path]::Combine([System.IO.Path]::GetTempPath(), [System.IO.Path]::GetRandomFileName() + "".ps1"")&#xa;    $exec_wrapper.ToString() | Set-Content -Path $temp&#xa;    # allow (potentially unprivileged) target user access to the tempfile (NB: this likely won't work if traverse checking is enabled)&#xa;    $acl = Get-Acl $temp&#xa;    $acl.AddAccessRule($(New-Object System.Security.AccessControl.FileSystemAccessRule($username, ""FullControl"", ""Allow"")))&#xa;    Set-Acl $temp $acl | Out-Null&#xa;&#xa;    # TODO: grant target user permissions on tempfile/tempdir&#xa;&#xa;    Try {&#xa;        $exec_args = @(""-noninteractive"", $temp)&#xa;&#xa;        # FUTURE: move these flags into C# enum?&#xa;        # start process suspended + breakaway so we can record the watchdog pid without worrying about a completion race&#xa;        Set-Variable CREATE_BREAKAWAY_FROM_JOB -Value ([uint32]0x01000000) -Option Constant&#xa;        Set-Variable CREATE_SUSPENDED -Value ([uint32]0x00000004) -Option Constant&#xa;        Set-Variable CREATE_UNICODE_ENVIRONMENT -Value ([uint32]0x000000400) -Option Constant&#xa;        Set-Variable CREATE_NEW_CONSOLE -Value ([uint32]0x00000010) -Option Constant&#xa;        Set-Variable EXTENDED_STARTUPINFO_PRESENT -Value ([uint32]0x00080000) -Option Constant&#xa;&#xa;        $pstartup_flags = $CREATE_BREAKAWAY_FROM_JOB -bor $CREATE_UNICODE_ENVIRONMENT -bor $CREATE_NEW_CONSOLE # -bor $EXTENDED_STARTUPINFO_PRESENT&#xa;&#xa;        $si = New-Object Ansible.Shell.STARTUPINFOEX&#xa;&#xa;        $pipesec = New-Object Ansible.Shell.SECURITY_ATTRIBUTES&#xa;        $pipesec.bInheritHandle = $true&#xa;        $stdout_read = $stdout_write = $stderr_read = $stderr_write = 0&#xa;&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::CreatePipe([ref]$stdout_read, [ref]$stdout_write, $pipesec, 0)) {&#xa;            throw ""Stdout pipe setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::SetHandleInformation($stdout_read, [Ansible.Shell.HandleFlags]::INHERIT, 0)) {&#xa;            throw ""Stdout handle setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::CreatePipe([ref]$stderr_read, [ref]$stderr_write, $pipesec, 0)) {&#xa;            throw ""Stderr pipe setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::SetHandleInformation($stderr_read, [Ansible.Shell.HandleFlags]::INHERIT, 0)) {&#xa;            throw ""Stderr handle setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;&#xa;        # setup stdin redirection, we'll leave stdout/stderr as normal&#xa;        $si.startupInfo.dwFlags = [Ansible.Shell.StartupInfoFlags]::USESTDHANDLES&#xa;        $si.startupInfo.hStdOutput = $stdout_write #[Ansible.Shell.NativeProcessUtil]::GetStdHandle([Ansible.Shell.StandardHandleValues]::STD_OUTPUT_HANDLE)&#xa;        $si.startupInfo.hStdError = $stderr_write #[Ansible.Shell.NativeProcessUtil]::GetStdHandle([Ansible.Shell.StandardHandleValues]::STD_ERROR_HANDLE)&#xa;&#xa;        $stdin_read = $stdin_write = 0&#xa;&#xa;        $pipesec = New-Object Ansible.Shell.SECURITY_ATTRIBUTES&#xa;        $pipesec.bInheritHandle = $true&#xa;&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::CreatePipe([ref]$stdin_read, [ref]$stdin_write, $pipesec, 0)) {&#xa;            throw ""Stdin pipe setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::SetHandleInformation($stdin_write, [Ansible.Shell.HandleFlags]::INHERIT, 0)) {&#xa;            throw ""Stdin handle setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;        $si.startupInfo.hStdInput = $stdin_read&#xa;&#xa;&#xa;        # create an attribute list with our explicit handle inheritance list to pass to CreateProcess&#xa;        [int]$buf_sz = 0&#xa;&#xa;        # determine the buffer size necessary for our attribute list&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::InitializeProcThreadAttributeList([IntPtr]::Zero, 1, 0, [ref]$buf_sz)) {&#xa;            $last_err = [System.Runtime.InteropServices.Marshal]::GetLastWin32Error()&#xa;            If($last_err -ne 122) { # ERROR_INSUFFICIENT_BUFFER&#xa;                throw ""Attribute list size query failed, Win32Error: $last_err""&#xa;            }&#xa;        }&#xa;&#xa;        $si.lpAttributeList = [System.Runtime.InteropServices.Marshal]::AllocHGlobal($buf_sz)&#xa;&#xa;        # initialize the attribute list&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::InitializeProcThreadAttributeList($si.lpAttributeList, 1, 0, [ref]$buf_sz)) {&#xa;            throw ""Attribute list init failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;&#xa;        $handles_to_inherit = [IntPtr[]]@($stdin_read,$stdout_write,$stderr_write)&#xa;        $pinned_handles = [System.Runtime.InteropServices.GCHandle]::Alloc($handles_to_inherit, [System.Runtime.InteropServices.GCHandleType]::Pinned)&#xa;&#xa;        # update the attribute list with the handles we want to inherit&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::UpdateProcThreadAttribute($si.lpAttributeList, 0, 0x20002, `&#xa;            $pinned_handles.AddrOfPinnedObject(), [System.Runtime.InteropServices.Marshal]::SizeOf([type][IntPtr]) * $handles_to_inherit.Length, `&#xa;            [System.IntPtr]::Zero, [System.IntPtr]::Zero)) {&#xa;            throw ""Attribute list update failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;&#xa;        # need to use a preamble-free version of UTF8Encoding&#xa;        $utf8_encoding = New-Object System.Text.UTF8Encoding @($false)&#xa;        $stdin_fs = New-Object System.IO.FileStream @($stdin_write, [System.IO.FileAccess]::Write, $true, 32768)&#xa;        $stdin = New-Object System.IO.StreamWriter @($stdin_fs, $utf8_encoding, 32768)&#xa;&#xa;        $pi = New-Object Ansible.Shell.PROCESS_INFORMATION&#xa;&#xa;        # FUTURE: direct cmdline CreateProcess path lookup fails- this works but is sub-optimal&#xa;        $exec_cmd = [Ansible.Shell.NativeProcessUtil]::SearchPath(""powershell.exe"")&#xa;        $exec_args = New-Object System.Text.StringBuilder @(""-NonInteractive -NoProfile -ExecutionPolicy Bypass $temp"")&#xa;&#xa;        [Ansible.Shell.NativeProcessUtil]::GrantAccessToWindowStationAndDesktop($username)&#xa;&#xa;        If($username.Contains(""\"")) {&#xa;            $sp = $username.Split(@([char]""\""), 2)&#xa;            $domain = $sp[0]&#xa;            $username = $sp[1]&#xa;        }&#xa;        ElseIf ($username.Contains(""@"")) {&#xa;            $domain = $null&#xa;        }&#xa;        Else {&#xa;            $domain = "".""&#xa;        }&#xa;&#xa;        # TODO: use proper Win32Exception + error&#xa;        If(-not [Ansible.Shell.NativeProcessUtil]::CreateProcessWithLogonW($username, $domain, $password, [Ansible.Shell.LOGON_FLAGS]::LOGON_WITH_PROFILE,&#xa;         $exec_cmd, $exec_args,&#xa;            $pstartup_flags, [IntPtr]::Zero, $env:windir, $si, [ref]$pi)) {&#xa;            #throw New-Object System.ComponentModel.Win32Exception&#xa;            throw ""Worker creation failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;        }&#xa;&#xa;        $stdout_fs = New-Object System.IO.FileStream @($stdout_read, [System.IO.FileAccess]::Read, $true, 4096)&#xa;        $stdout = New-Object System.IO.StreamReader @($stdout_fs, $utf8_encoding, $true, 4096)&#xa;        $stderr_fs = New-Object System.IO.FileStream @($stderr_read, [System.IO.FileAccess]::Read, $true, 4096)&#xa;        $stderr = New-Object System.IO.StreamReader @($stderr_fs, $utf8_encoding, $true, 4096)&#xa;&#xa;        # close local write ends of stdout/stderr pipes so the open handles won't prevent EOF&#xa;        [Ansible.Shell.NativeProcessUtil]::CloseHandle($stdout_write)&#xa;        [Ansible.Shell.NativeProcessUtil]::CloseHandle($stderr_write)&#xa;&#xa;        $payload_string = $payload | ConvertTo-Json -Depth 99 -Compress&#xa;&#xa;        # push the execution payload over stdin&#xa;        $stdin.WriteLine($payload_string)&#xa;        $stdin.Close()&#xa;&#xa;        $str_stdout = $str_stderr = """"&#xa;&#xa;        [Ansible.Shell.NativeProcessUtil]::GetProcessOutput($stdout, $stderr, [ref] $str_stdout, [ref] $str_stderr)&#xa;&#xa;        # FUTURE: decode CLIXML stderr output (and other streams?)&#xa;&#xa;        #$proc.WaitForExit() | Out-Null&#xa;&#xa;&#xa;        # TODO: wait on process handle for exit, get process exit code&#xa;        $rc = [Ansible.Shell.NativeProcessUtil]::GetProcessExitCode($pi.hProcess)&#xa;&#xa;        If ($rc -eq 0) {&#xa;            $str_stdout&#xa;            $str_stderr&#xa;        }&#xa;        Else {&#xa;            Throw ""failed, rc was $rc, stderr was $stderr, stdout was $stdout""&#xa;        }&#xa;&#xa;    }&#xa;    Catch {&#xa;        $excep = $_&#xa;        Dump-Error $excep&#xa;    }&#xa;    Finally {&#xa;        Remove-Item $temp -ErrorAction SilentlyContinue&#xa;    }&#xa;&#xa;}&#xa;&#xa;'''  # end become_wrapper&#xa;&#xa;&#xa;async_wrapper = br'''&#xa;Set-StrictMode -Version 2&#xa;$ErrorActionPreference = ""Stop""&#xa;&#xa;# build exec_wrapper encoded command&#xa;# start powershell with breakaway running exec_wrapper encodedcommand&#xa;# stream payload to powershell with normal exec, but normal exec writes results to resultfile instead of stdout/stderr&#xa;# return asyncresult to controller&#xa;&#xa;$exec_wrapper = {&#xa;#Requires -Version 3.0&#xa;$DebugPreference = ""Continue""&#xa;$ErrorActionPreference = ""Stop""&#xa;Set-StrictMode -Version 2&#xa;&#xa;function ConvertTo-HashtableFromPsCustomObject ($myPsObject){&#xa;    $output = @{};&#xa;    $myPsObject | Get-Member -MemberType *Property | % {&#xa;        $val = $myPsObject.($_.name);&#xa;        If ($val -is [psobject]) {&#xa;            $val = ConvertTo-HashtableFromPsCustomObject $val&#xa;        }&#xa;        $output.($_.name) = $val&#xa;    }&#xa;    return $output;&#xa;}&#xa;# stream JSON including become_pw, ps_module_payload, bin_module_payload, become_payload, write_payload_path, preserve directives&#xa;# exec runspace, capture output, cleanup, return module output&#xa;&#xa;$json_raw = [System.Console]::In.ReadToEnd()&#xa;&#xa;If (-not $json_raw) {&#xa;    Write-Error ""no input given"" -Category InvalidArgument&#xa;}&#xa;&#xa;$payload = ConvertTo-HashtableFromPsCustomObject (ConvertFrom-Json $json_raw)&#xa;&#xa;# TODO: handle binary modules&#xa;# TODO: handle persistence&#xa;&#xa;$actions = $payload.actions&#xa;&#xa;# pop 0th action as entrypoint&#xa;$entrypoint = $payload.($actions[0])&#xa;$payload.actions = $payload.actions[1..99]&#xa;&#xa;$entrypoint = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($entrypoint))&#xa;&#xa;# load the current action entrypoint as a module custom object with a Run method&#xa;$entrypoint = New-Module -ScriptBlock ([scriptblock]::Create($entrypoint)) -AsCustomObject&#xa;&#xa;Set-Variable -Scope global -Name complex_args -Value $payload[""module_args""] | Out-Null&#xa;&#xa;# dynamically create/load modules&#xa;ForEach ($mod in $payload.powershell_modules.GetEnumerator()) {&#xa;    $decoded_module = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($mod.Value))&#xa;    New-Module -ScriptBlock ([scriptblock]::Create($decoded_module)) -Name $mod.Key | Import-Module -WarningAction SilentlyContinue | Out-Null&#xa;}&#xa;&#xa;$output = $entrypoint.Run($payload)&#xa;&#xa;Write-Output $output&#xa;&#xa;} # end exec_wrapper&#xa;&#xa;&#xa;Function Run($payload) {&#xa;# BEGIN Ansible.Async native type definition&#xa;    $native_process_util = @""&#xa;        using Microsoft.Win32.SafeHandles;&#xa;        using System;&#xa;        using System.ComponentModel;&#xa;        using System.Diagnostics;&#xa;        using System.IO;&#xa;        using System.Linq;&#xa;        using System.Runtime.InteropServices;&#xa;        using System.Text;&#xa;        using System.Threading;&#xa;&#xa;        namespace Ansible.Async {&#xa;&#xa;            public static class NativeProcessUtil&#xa;            {&#xa;                [DllImport(""kernel32.dll"", SetLastError=true, CharSet=CharSet.Unicode, BestFitMapping=false)]&#xa;                public static extern bool CreateProcess(&#xa;                    [MarshalAs(UnmanagedType.LPTStr)]&#xa;                    string lpApplicationName,&#xa;                    StringBuilder lpCommandLine,&#xa;                    IntPtr lpProcessAttributes,&#xa;                    IntPtr lpThreadAttributes,&#xa;                    bool bInheritHandles,&#xa;                    uint dwCreationFlags,&#xa;                    IntPtr lpEnvironment,&#xa;                    [MarshalAs(UnmanagedType.LPTStr)]&#xa;                    string lpCurrentDirectory,&#xa;                    STARTUPINFOEX lpStartupInfo,&#xa;                    out PROCESS_INFORMATION lpProcessInformation);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true, CharSet=CharSet.Unicode)]&#xa;                public static extern uint SearchPath (&#xa;                    string lpPath,&#xa;                    string lpFileName,&#xa;                    string lpExtension,&#xa;                    int nBufferLength,&#xa;                    [MarshalAs (UnmanagedType.LPTStr)]&#xa;                    StringBuilder lpBuffer,&#xa;                    out IntPtr lpFilePart);&#xa;&#xa;                [DllImport(""kernel32.dll"")]&#xa;                public static extern bool CreatePipe(out IntPtr hReadPipe, out IntPtr hWritePipe, SECURITY_ATTRIBUTES lpPipeAttributes, uint nSize);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                public static extern IntPtr GetStdHandle(StandardHandleValues nStdHandle);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                public static extern bool SetHandleInformation(IntPtr hObject, HandleFlags dwMask, int dwFlags);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                public static extern bool InitializeProcThreadAttributeList(IntPtr lpAttributeList, int dwAttributeCount, int dwFlags, ref int lpSize);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                public static extern bool UpdateProcThreadAttribute(&#xa;                     IntPtr lpAttributeList,&#xa;                     uint dwFlags,&#xa;                     IntPtr Attribute,&#xa;                     IntPtr lpValue,&#xa;                     IntPtr cbSize,&#xa;                     IntPtr lpPreviousValue,&#xa;                     IntPtr lpReturnSize);&#xa;&#xa;                public static string SearchPath(string findThis)&#xa;                {&#xa;                    StringBuilder sbOut = new StringBuilder(1024);&#xa;                    IntPtr filePartOut;&#xa;&#xa;                    if(SearchPath(null, findThis, null, sbOut.Capacity, sbOut, out filePartOut) == 0)&#xa;                        throw new FileNotFoundException(""Couldn't locate "" + findThis + "" on path"");&#xa;&#xa;                    return sbOut.ToString();&#xa;                }&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                static extern SafeFileHandle OpenThread(&#xa;                    ThreadAccessRights dwDesiredAccess,&#xa;                    bool bInheritHandle,&#xa;                    int dwThreadId);&#xa;&#xa;                [DllImport(""kernel32.dll"", SetLastError=true)]&#xa;                static extern int ResumeThread(SafeHandle hThread);&#xa;&#xa;                public static void ResumeThreadById(int threadId)&#xa;                {&#xa;                    var threadHandle = OpenThread(ThreadAccessRights.SUSPEND_RESUME, false, threadId);&#xa;                    if(threadHandle.IsInvalid)&#xa;                        throw new Exception(String.Format(""Thread ID {0} is invalid ({1})"", threadId,&#xa;                            new Win32Exception(Marshal.GetLastWin32Error()).Message));&#xa;&#xa;                    try&#xa;                    {&#xa;                        if(ResumeThread(threadHandle) == -1)&#xa;                            throw new Exception(String.Format(""Thread ID {0} cannot be resumed ({1})"", threadId,&#xa;                                new Win32Exception(Marshal.GetLastWin32Error()).Message));&#xa;                    }&#xa;                    finally&#xa;                    {&#xa;                        threadHandle.Dispose();&#xa;                    }&#xa;                }&#xa;&#xa;                public static void ResumeProcessById(int pid)&#xa;                {&#xa;                    var proc = Process.GetProcessById(pid);&#xa;&#xa;                    // wait for at least one suspended thread in the process (this handles possible slow startup race where&#xa;                    // primary thread of created-suspended process has not yet become runnable)&#xa;                    var retryCount = 0;&#xa;                    while(!proc.Threads.OfType<ProcessThread>().Any(t=>t.ThreadState == System.Diagnostics.ThreadState.Wait &&&#xa;                        t.WaitReason == ThreadWaitReason.Suspended))&#xa;                    {&#xa;                        proc.Refresh();&#xa;                        Thread.Sleep(50);&#xa;                        if (retryCount > 100)&#xa;                            throw new InvalidOperationException(String.Format(""No threads were suspended in target PID {0} after 5s"", pid));&#xa;                    }&#xa;&#xa;                    foreach(var thread in proc.Threads.OfType<ProcessThread>().Where(t => t.ThreadState == System.Diagnostics.ThreadState.Wait &&&#xa;                        t.WaitReason == ThreadWaitReason.Suspended))&#xa;                        ResumeThreadById(thread.Id);&#xa;                }&#xa;            }&#xa;&#xa;            [StructLayout(LayoutKind.Sequential)]&#xa;            public class SECURITY_ATTRIBUTES&#xa;            {&#xa;                public int nLength;&#xa;                public IntPtr lpSecurityDescriptor;&#xa;                public bool bInheritHandle = false;&#xa;&#xa;                public SECURITY_ATTRIBUTES() {&#xa;                    nLength = Marshal.SizeOf(this);&#xa;                }&#xa;            }&#xa;&#xa;            [StructLayout(LayoutKind.Sequential)]&#xa;            public class STARTUPINFO&#xa;            {&#xa;                public Int32 cb;&#xa;                public IntPtr lpReserved;&#xa;                public IntPtr lpDesktop;&#xa;                public IntPtr lpTitle;&#xa;                public Int32 dwX;&#xa;                public Int32 dwY;&#xa;                public Int32 dwXSize;&#xa;                public Int32 dwYSize;&#xa;                public Int32 dwXCountChars;&#xa;                public Int32 dwYCountChars;&#xa;                public Int32 dwFillAttribute;&#xa;                public Int32 dwFlags;&#xa;                public Int16 wShowWindow;&#xa;                public Int16 cbReserved2;&#xa;                public IntPtr lpReserved2;&#xa;                public IntPtr hStdInput;&#xa;                public IntPtr hStdOutput;&#xa;                public IntPtr hStdError;&#xa;&#xa;                public STARTUPINFO() {&#xa;                    cb = Marshal.SizeOf(this);&#xa;                }&#xa;            }&#xa;&#xa;            [StructLayout(LayoutKind.Sequential)]&#xa;            public class STARTUPINFOEX {&#xa;                public STARTUPINFO startupInfo;&#xa;                public IntPtr lpAttributeList;&#xa;&#xa;                public STARTUPINFOEX() {&#xa;                    startupInfo = new STARTUPINFO();&#xa;                    startupInfo.cb = Marshal.SizeOf(this);&#xa;                }&#xa;            }&#xa;&#xa;            [StructLayout(LayoutKind.Sequential)]&#xa;            public struct PROCESS_INFORMATION&#xa;            {&#xa;                public IntPtr hProcess;&#xa;                public IntPtr hThread;&#xa;                public int dwProcessId;&#xa;                public int dwThreadId;&#xa;            }&#xa;&#xa;            [Flags]&#xa;            enum ThreadAccessRights : uint&#xa;            {&#xa;                SUSPEND_RESUME = 0x0002&#xa;            }&#xa;&#xa;            [Flags]&#xa;            public enum StartupInfoFlags : uint&#xa;            {&#xa;                USESTDHANDLES = 0x00000100&#xa;            }&#xa;&#xa;            public enum StandardHandleValues : int&#xa;            {&#xa;                STD_INPUT_HANDLE = -10,&#xa;                STD_OUTPUT_HANDLE = -11,&#xa;                STD_ERROR_HANDLE = -12&#xa;            }&#xa;&#xa;            [Flags]&#xa;            public enum HandleFlags : uint&#xa;            {&#xa;                None = 0,&#xa;                INHERIT = 1&#xa;            }&#xa;        }&#xa;""@ # END Ansible.Async native type definition&#xa;&#xa;    # calculate the result path so we can include it in the worker payload&#xa;    $jid = $payload.async_jid&#xa;    $local_jid = $jid + ""."" + $pid&#xa;&#xa;    $results_path = [System.IO.Path]::Combine($env:LOCALAPPDATA, "".ansible_async"", $local_jid)&#xa;&#xa;    $payload.async_results_path = $results_path&#xa;&#xa;    [System.IO.Directory]::CreateDirectory([System.IO.Path]::GetDirectoryName($results_path)) | Out-Null&#xa;&#xa;    Add-Type -TypeDefinition $native_process_util -Debug:$false&#xa;&#xa;    # FUTURE: create under new job to ensure all children die on exit?&#xa;&#xa;    # FUTURE: move these flags into C# enum?&#xa;    # start process suspended + breakaway so we can record the watchdog pid without worrying about a completion race&#xa;    Set-Variable CREATE_BREAKAWAY_FROM_JOB -Value ([uint32]0x01000000) -Option Constant&#xa;    Set-Variable CREATE_SUSPENDED -Value ([uint32]0x00000004) -Option Constant&#xa;    Set-Variable CREATE_UNICODE_ENVIRONMENT -Value ([uint32]0x000000400) -Option Constant&#xa;    Set-Variable CREATE_NEW_CONSOLE -Value ([uint32]0x00000010) -Option Constant&#xa;    Set-Variable EXTENDED_STARTUPINFO_PRESENT -Value ([uint32]0x00080000) -Option Constant&#xa;&#xa;    $pstartup_flags = $CREATE_BREAKAWAY_FROM_JOB -bor $CREATE_UNICODE_ENVIRONMENT -bor $CREATE_NEW_CONSOLE `&#xa;        -bor $CREATE_SUSPENDED -bor $EXTENDED_STARTUPINFO_PRESENT&#xa;&#xa;    # execute the dynamic watchdog as a breakway process to free us from the WinRM job, which will in turn exec the module&#xa;    $si = New-Object Ansible.Async.STARTUPINFOEX&#xa;&#xa;    # setup stdin redirection, we'll leave stdout/stderr as normal&#xa;    $si.startupInfo.dwFlags = [Ansible.Async.StartupInfoFlags]::USESTDHANDLES&#xa;    $si.startupInfo.hStdOutput = [Ansible.Async.NativeProcessUtil]::GetStdHandle([Ansible.Async.StandardHandleValues]::STD_OUTPUT_HANDLE)&#xa;    $si.startupInfo.hStdError = [Ansible.Async.NativeProcessUtil]::GetStdHandle([Ansible.Async.StandardHandleValues]::STD_ERROR_HANDLE)&#xa;&#xa;    $stdin_read = $stdin_write = 0&#xa;&#xa;    $pipesec = New-Object Ansible.Async.SECURITY_ATTRIBUTES&#xa;    $pipesec.bInheritHandle = $true&#xa;&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::CreatePipe([ref]$stdin_read, [ref]$stdin_write, $pipesec, 0)) {&#xa;        throw ""Stdin pipe setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;    }&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::SetHandleInformation($stdin_write, [Ansible.Async.HandleFlags]::INHERIT, 0)) {&#xa;        throw ""Stdin handle setup failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;    }&#xa;    $si.startupInfo.hStdInput = $stdin_read&#xa;&#xa;    # create an attribute list with our explicit handle inheritance list to pass to CreateProcess&#xa;    [int]$buf_sz = 0&#xa;&#xa;    # determine the buffer size necessary for our attribute list&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::InitializeProcThreadAttributeList([IntPtr]::Zero, 1, 0, [ref]$buf_sz)) {&#xa;        $last_err = [System.Runtime.InteropServices.Marshal]::GetLastWin32Error()&#xa;        If($last_err -ne 122) { # ERROR_INSUFFICIENT_BUFFER&#xa;            throw ""Attribute list size query failed, Win32Error: $last_err""&#xa;        }&#xa;    }&#xa;&#xa;    $si.lpAttributeList = [System.Runtime.InteropServices.Marshal]::AllocHGlobal($buf_sz)&#xa;&#xa;    # initialize the attribute list&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::InitializeProcThreadAttributeList($si.lpAttributeList, 1, 0, [ref]$buf_sz)) {&#xa;        throw ""Attribute list init failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;    }&#xa;&#xa;    $handles_to_inherit = [IntPtr[]]@($stdin_read)&#xa;    $pinned_handles = [System.Runtime.InteropServices.GCHandle]::Alloc($handles_to_inherit, [System.Runtime.InteropServices.GCHandleType]::Pinned)&#xa;&#xa;    # update the attribute list with the handles we want to inherit&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::UpdateProcThreadAttribute($si.lpAttributeList, 0, 0x20002 <# PROC_THREAD_ATTRIBUTE_HANDLE_LIST #>, `&#xa;        $pinned_handles.AddrOfPinnedObject(), [System.Runtime.InteropServices.Marshal]::SizeOf([type][IntPtr]) * $handles_to_inherit.Length, `&#xa;        [System.IntPtr]::Zero, [System.IntPtr]::Zero)) {&#xa;        throw ""Attribute list update failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;    }&#xa;&#xa;    # need to use a preamble-free version of UTF8Encoding&#xa;    $utf8_encoding = New-Object System.Text.UTF8Encoding @($false)&#xa;    $stdin_fs = New-Object System.IO.FileStream @($stdin_write, [System.IO.FileAccess]::Write, $true, 32768)&#xa;    $stdin = New-Object System.IO.StreamWriter @($stdin_fs, $utf8_encoding, 32768)&#xa;&#xa;    $pi = New-Object Ansible.Async.PROCESS_INFORMATION&#xa;&#xa;    $encoded_command = [Convert]::ToBase64String([System.Text.Encoding]::Unicode.GetBytes($exec_wrapper.ToString()))&#xa;&#xa;    # FUTURE: direct cmdline CreateProcess path lookup fails- this works but is sub-optimal&#xa;    $exec_cmd = [Ansible.Async.NativeProcessUtil]::SearchPath(""powershell.exe"")&#xa;    $exec_args = New-Object System.Text.StringBuilder @(""`""$exec_cmd`"" -NonInteractive -NoProfile -ExecutionPolicy Bypass -EncodedCommand $encoded_command"")&#xa;&#xa;    # TODO: use proper Win32Exception + error&#xa;    If(-not [Ansible.Async.NativeProcessUtil]::CreateProcess($exec_cmd, $exec_args,&#xa;        [IntPtr]::Zero, [IntPtr]::Zero, $true, $pstartup_flags, [IntPtr]::Zero, $env:windir, $si, [ref]$pi)) {&#xa;        #throw New-Object System.ComponentModel.Win32Exception&#xa;        throw ""Worker creation failed, Win32Error: $([System.Runtime.InteropServices.Marshal]::GetLastWin32Error())""&#xa;    }&#xa;&#xa;    # FUTURE: watch process for quick exit, capture stdout/stderr and return failure&#xa;&#xa;    $watchdog_pid = $pi.dwProcessId&#xa;&#xa;    [Ansible.Async.NativeProcessUtil]::ResumeProcessById($watchdog_pid)&#xa;&#xa;    # once process is resumed, we can send payload over stdin&#xa;    $payload_string = $payload | ConvertTo-Json -Depth 99 -Compress&#xa;    $stdin.WriteLine($payload_string)&#xa;    $stdin.Close()&#xa;&#xa;    # populate initial results before we resume the process to avoid result race&#xa;    $result = @{&#xa;        started=1;&#xa;        finished=0;&#xa;        results_file=$results_path;&#xa;        ansible_job_id=$local_jid;&#xa;        _ansible_suppress_tmpdir_delete=$true;&#xa;        ansible_async_watchdog_pid=$watchdog_pid&#xa;    }&#xa;&#xa;    $result_json = ConvertTo-Json $result&#xa;    Set-Content $results_path -Value $result_json&#xa;&#xa;    return $result_json&#xa;}&#xa;&#xa;'''  # end async_wrapper&#xa;&#xa;async_watchdog = br'''&#xa;Set-StrictMode -Version 2&#xa;$ErrorActionPreference = ""Stop""&#xa;&#xa;Add-Type -AssemblyName System.Web.Extensions&#xa;&#xa;Function Log {&#xa;    Param(&#xa;        [string]$msg&#xa;    )&#xa;&#xa;    If(Get-Variable -Name log_path -ErrorAction SilentlyContinue) {&#xa;        Add-Content $log_path $msg&#xa;    }&#xa;}&#xa;&#xa;Function Deserialize-Json {&#xa;    Param(&#xa;        [Parameter(ValueFromPipeline=$true)]&#xa;        [string]$json&#xa;    )&#xa;&#xa;    # FUTURE: move this into module_utils/powershell.ps1 and use for everything (sidestep PSCustomObject issues)&#xa;    # FUTURE: won't work w/ Nano Server/.NET Core- fallback to DataContractJsonSerializer (which can't handle dicts on .NET 4.0)&#xa;&#xa;    Log ""Deserializing:`n$json""&#xa;&#xa;    $jss = New-Object System.Web.Script.Serialization.JavaScriptSerializer&#xa;    return $jss.DeserializeObject($json)&#xa;}&#xa;&#xa;Function Write-Result {&#xa;    Param(&#xa;        [hashtable]$result,&#xa;        [string]$resultfile_path&#xa;    )&#xa;&#xa;    $result | ConvertTo-Json | Set-Content -Path $resultfile_path&#xa;}&#xa;&#xa;Function Run($payload) {&#xa;    $actions = $payload.actions&#xa;&#xa;    # pop 0th action as entrypoint&#xa;    $entrypoint = $payload.($actions[0])&#xa;    $entrypoint = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($entrypoint))&#xa;&#xa;    $payload.actions = $payload.actions[1..99]&#xa;&#xa;    $resultfile_path = $payload.async_results_path&#xa;    $max_exec_time_sec = $payload.async_timeout_sec&#xa;&#xa;    Log ""deserializing existing resultfile args""&#xa;    # read in existing resultsfile to merge w/ module output (it should be written by the time we're unsuspended and running)&#xa;    $result = Get-Content $resultfile_path -Raw | Deserialize-Json&#xa;&#xa;    Log ""deserialized result is $($result | Out-String)""&#xa;&#xa;    Log ""creating runspace""&#xa;&#xa;    $rs = [runspacefactory]::CreateRunspace()&#xa;    $rs.Open()&#xa;&#xa;    Log ""creating Powershell object""&#xa;&#xa;    $job = [powershell]::Create()&#xa;    $job.Runspace = $rs&#xa;&#xa;    $job.AddScript($entrypoint) | Out-Null&#xa;    $job.AddStatement().AddCommand(""Run"").AddArgument($payload) | Out-Null&#xa;&#xa;    Log ""job BeginInvoke()""&#xa;&#xa;    $job_asyncresult = $job.BeginInvoke()&#xa;&#xa;    Log ""waiting $max_exec_time_sec seconds for job to complete""&#xa;&#xa;    $signaled = $job_asyncresult.AsyncWaitHandle.WaitOne($max_exec_time_sec * 1000)&#xa;&#xa;    $result[""finished""] = 1&#xa;&#xa;    If($job_asyncresult.IsCompleted) {&#xa;        Log ""job completed, calling EndInvoke()""&#xa;&#xa;        $job_output = $job.EndInvoke($job_asyncresult)&#xa;        $job_error = $job.Streams.Error&#xa;&#xa;        Log ""raw module stdout: \r\n$job_output""&#xa;        If($job_error) {&#xa;            Log ""raw module stderr: \r\n$job_error""&#xa;        }&#xa;&#xa;        # write success/output/error to result object&#xa;&#xa;        # TODO: cleanse leading/trailing junk&#xa;        Try {&#xa;            $module_result = Deserialize-Json $job_output&#xa;            # TODO: check for conflicting keys&#xa;            $result = $result + $module_result&#xa;        }&#xa;        Catch {&#xa;            $excep = $_&#xa;&#xa;            $result.failed = $true&#xa;            $result.msg = ""failed to parse module output: $excep""&#xa;        }&#xa;&#xa;        # TODO: determine success/fail, or always include stderr if nonempty?&#xa;        Write-Result $result $resultfile_path&#xa;&#xa;        Log ""wrote output to $resultfile_path""&#xa;    }&#xa;    Else {&#xa;        $job.BeginStop($null, $null) | Out-Null # best effort stop&#xa;        # write timeout to result object&#xa;        $result.failed = $true&#xa;        $result.msg = ""timed out waiting for module completion""&#xa;        Write-Result $result $resultfile_path&#xa;&#xa;        Log ""wrote timeout to $resultfile_path""&#xa;    }&#xa;&#xa;    # in the case of a hung pipeline, this will cause the process to stay alive until it's un-hung...&#xa;    #$rs.Close() | Out-Null&#xa;}&#xa;&#xa;'''  # end async_watchdog&#xa;&#xa;&#xa;class ShellModule(object):&#xa;&#xa;    # Common shell filenames that this plugin handles&#xa;    # Powershell is handled differently.  It's selected when winrm is the&#xa;    # connection&#xa;    COMPATIBLE_SHELLS = frozenset()&#xa;    # Family of shells this has.  Must match the filename without extension&#xa;    SHELL_FAMILY = 'powershell'&#xa;&#xa;    env = dict()&#xa;&#xa;    # We're being overly cautious about which keys to accept (more so than&#xa;    # the Windows environment is capable of doing), since the powershell&#xa;    # env provider's limitations don't appear to be documented.&#xa;    safe_envkey = re.compile(r'^[\d\w_]{1,255}$')&#xa;&#xa;    # TODO: implement module transfer&#xa;    # TODO: implement #Requires -Modules parser/locator&#xa;    # TODO: add KEEP_REMOTE_FILES support + debug wrapper dump&#xa;    # TODO: add binary module support&#xa;&#xa;    def assert_safe_env_key(self, key):&#xa;        if not self.safe_envkey.match(key):&#xa;            raise AnsibleError(""Invalid PowerShell environment key: %s"" % key)&#xa;        return key&#xa;&#xa;    def safe_env_value(self, key, value):&#xa;        if len(value) > 32767:&#xa;            raise AnsibleError(""PowerShell environment value for key '%s' exceeds 32767 characters in length"" % key)&#xa;        # powershell single quoted literals need single-quote doubling as their only escaping&#xa;        value = value.replace(""'"", ""''"")&#xa;        return to_text(value, errors='surrogate_or_strict')&#xa;&#xa;    def env_prefix(self, **kwargs):&#xa;        # powershell/winrm env handling is handled in the exec wrapper&#xa;        return """"&#xa;&#xa;    def join_path(self, *args):&#xa;        parts = []&#xa;        for arg in args:&#xa;            arg = self._unquote(arg).replace('/', '\\')&#xa;            parts.extend([a for a in arg.split('\\') if a])&#xa;        path = '\\'.join(parts)&#xa;        if path.startswith('~'):&#xa;            return path&#xa;        return '\'%s\'' % path&#xa;&#xa;    def get_remote_filename(self, pathname):&#xa;        # powershell requires that script files end with .ps1&#xa;        base_name = os.path.basename(pathname.strip())&#xa;        name, ext = os.path.splitext(base_name.strip())&#xa;        if ext.lower() not in ['.ps1', '.exe']:&#xa;            return name + '.ps1'&#xa;&#xa;        return base_name.strip()&#xa;&#xa;    def path_has_trailing_slash(self, path):&#xa;        # Allow Windows paths to be specified using either slash.&#xa;        path = self._unquote(path)&#xa;        return path.endswith('/') or path.endswith('\\')&#xa;&#xa;    def chmod(self, paths, mode):&#xa;        raise NotImplementedError('chmod is not implemented for Powershell')&#xa;&#xa;    def chown(self, paths, user):&#xa;        raise NotImplementedError('chown is not implemented for Powershell')&#xa;&#xa;    def set_user_facl(self, paths, user, mode):&#xa;        raise NotImplementedError('set_user_facl is not implemented for Powershell')&#xa;&#xa;    def remove(self, path, recurse=False):&#xa;        path = self._escape(self._unquote(path))&#xa;        if recurse:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force -Recurse;''' % path)&#xa;        else:&#xa;            return self._encode_script('''Remove-Item ""%s"" -Force;''' % path)&#xa;&#xa;    def mkdtemp(self, basefile, system=False, mode=None, tmpdir=None):&#xa;        basefile = self._escape(self._unquote(basefile))&#xa;        # FIXME: Support system temp path and passed in tmpdir!&#xa;        return self._encode_script('''(New-Item -Type Directory -Path $env:temp -Name ""%s"").FullName | Write-Host -Separator '';''' % basefile)&#xa;&#xa;    def expand_user(self, user_home_path):&#xa;        # PowerShell only supports ""~"" (not ""~username"").  Resolve-Path ~ does&#xa;        # not seem to work remotely, though by default we are always starting&#xa;        # in the user's home directory.&#xa;        user_home_path = self._unquote(user_home_path)&#xa;        if user_home_path == '~':&#xa;            script = 'Write-Host (Get-Location).Path'&#xa;        elif user_home_path.startswith('~\\'):&#xa;            script = 'Write-Host ((Get-Location).Path + ""%s"")' % self._escape(user_home_path[1:])&#xa;        else:&#xa;            script = 'Write-Host ""%s""' % self._escape(user_home_path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def exists(self, path):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path ""%s"")&#xa;            {&#xa;                $res = 0;&#xa;            }&#xa;            Else&#xa;            {&#xa;                $res = 1;&#xa;            }&#xa;            Write-Host ""$res"";&#xa;            Exit $res;&#xa;         ''' % path&#xa;        return self._encode_script(script)&#xa;&#xa;    def checksum(self, path, *args, **kwargs):&#xa;        path = self._escape(self._unquote(path))&#xa;        script = '''&#xa;            If (Test-Path -PathType Leaf ""%(path)s"")&#xa;            {&#xa;                $sp = new-object -TypeName System.Security.Cryptography.SHA1CryptoServiceProvider;&#xa;                $fp = [System.IO.File]::Open(""%(path)s"", [System.IO.Filemode]::Open, [System.IO.FileAccess]::Read);&#xa;                [System.BitConverter]::ToString($sp.ComputeHash($fp)).Replace(""-"", """").ToLower();&#xa;                $fp.Dispose();&#xa;            }&#xa;            ElseIf (Test-Path -PathType Container ""%(path)s"")&#xa;            {&#xa;                Write-Host ""3"";&#xa;            }&#xa;            Else&#xa;            {&#xa;                Write-Host ""1"";&#xa;            }&#xa;        ''' % dict(path=path)&#xa;        return self._encode_script(script)&#xa;&#xa;    def build_module_command(self, env_string, shebang, cmd, arg_path=None, rm_tmp=None):&#xa;        # pipelining bypass&#xa;        if cmd == '':&#xa;            return '-'&#xa;&#xa;        # non-pipelining&#xa;&#xa;        cmd_parts = shlex.split(cmd, posix=False)&#xa;        cmd_parts = list(map(to_text, cmd_parts))&#xa;        if shebang and shebang.lower() == '#!powershell':&#xa;            if not self._unquote(cmd_parts[0]).lower().endswith('.ps1'):&#xa;                cmd_parts[0] = '""%s.ps1""' % self._unquote(cmd_parts[0])&#xa;            cmd_parts.insert(0, '&')&#xa;        elif shebang and shebang.startswith('#!'):&#xa;            cmd_parts.insert(0, shebang[2:])&#xa;        elif not shebang:&#xa;            # The module is assumed to be a binary&#xa;            cmd_parts[0] = self._unquote(cmd_parts[0])&#xa;            cmd_parts.append(arg_path)&#xa;        script = '''&#xa;            Try&#xa;            {&#xa;                %s&#xa;                %s&#xa;            }&#xa;            Catch&#xa;            {&#xa;                $_obj = @{ failed = $true }&#xa;                If ($_.Exception.GetType)&#xa;                {&#xa;                    $_obj.Add('msg', $_.Exception.Message)&#xa;                }&#xa;                Else&#xa;                {&#xa;                    $_obj.Add('msg', $_.ToString())&#xa;                }&#xa;                If ($_.InvocationInfo.PositionMessage)&#xa;                {&#xa;                    $_obj.Add('exception', $_.InvocationInfo.PositionMessage)&#xa;                }&#xa;                ElseIf ($_.ScriptStackTrace)&#xa;                {&#xa;                    $_obj.Add('exception', $_.ScriptStackTrace)&#xa;                }&#xa;                Try&#xa;                {&#xa;                    $_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))&#xa;                }&#xa;                Catch&#xa;                {&#xa;                }&#xa;                Echo $_obj | ConvertTo-Json -Compress -Depth 99&#xa;                Exit 1&#xa;            }&#xa;        ''' % (env_string, ' '.join(cmd_parts))&#xa;        if rm_tmp:&#xa;            rm_tmp = self._escape(self._unquote(rm_tmp))&#xa;            rm_cmd = 'Remove-Item ""%s"" -Force -Recurse -ErrorAction SilentlyContinue' % rm_tmp&#xa;            script = '%s\nFinally { %s }' % (script, rm_cmd)&#xa;        return self._encode_script(script, preserve_rc=False)&#xa;&#xa;    def wrap_for_exec(self, cmd):&#xa;        return '& %s' % cmd&#xa;&#xa;    def _unquote(self, value):&#xa;        '''Remove any matching quotes that wrap the given value.'''&#xa;        value = to_text(value or '')&#xa;        m = re.match(r'^\s*?\'(.*?)\'\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        m = re.match(r'^\s*?""(.*?)""\s*?$', value)&#xa;        if m:&#xa;            return m.group(1)&#xa;        return value&#xa;&#xa;    def _escape(self, value, include_vars=False):&#xa;        '''Return value escaped for use in PowerShell command.'''&#xa;        # http://www.techotopia.com/index.php/Windows_PowerShell_1.0_String_Quoting_and_Escape_Sequences&#xa;        # http://stackoverflow.com/questions/764360/a-list-of-string-replacements-in-python&#xa;        subs = [('\n', '`n'), ('\r', '`r'), ('\t', '`t'), ('\a', '`a'),&#xa;                ('\b', '`b'), ('\f', '`f'), ('\v', '`v'), ('""', '`""'),&#xa;                ('\'', '`\''), ('`', '``'), ('\x00', '`0')]&#xa;        if include_vars:&#xa;            subs.append(('$', '`$'))&#xa;        pattern = '|'.join('(%s)' % re.escape(p) for p, s in subs)&#xa;        substs = [s for p, s in subs]&#xa;&#xa;        def replace(m):&#xa;            return substs[m.lastindex - 1]&#xa;&#xa;        return re.sub(pattern, replace, value)&#xa;&#xa;    def _encode_script(self, script, as_list=False, strict_mode=True, preserve_rc=True):&#xa;        '''Convert a PowerShell script to a single base64-encoded command.'''&#xa;        script = to_text(script)&#xa;&#xa;        if script == u'-':&#xa;            cmd_parts = _common_args + ['-']&#xa;&#xa;        else:&#xa;            if strict_mode:&#xa;                script = u'Set-StrictMode -Version Latest\r\n%s' % script&#xa;            # try to propagate exit code if present- won't work with begin/process/end-style scripts (ala put_file)&#xa;            # NB: the exit code returned may be incorrect in the case of a successful command followed by an invalid command&#xa;            if preserve_rc:&#xa;                script = u'%s\r\nIf (-not $?) { If (Get-Variable LASTEXITCODE -ErrorAction SilentlyContinue) { exit $LASTEXITCODE } Else { exit 1 } }\r\n'\&#xa;                    % script&#xa;            script = '\n'.join([x.strip() for x in script.splitlines() if x.strip()])&#xa;            encoded_script = to_text(base64.b64encode(script.encode('utf-16-le')), 'utf-8')&#xa;            cmd_parts = _common_args + ['-EncodedCommand', encoded_script]&#xa;&#xa;        if as_list:&#xa;            return cmd_parts&#xa;        return ' '.join(cmd_parts)&#xa;"
29082268|#!/usr/bin/env python3&#xa;&#xa;#&#xa;# Copyright 2014 Paul Donohue <Tray_Apps@PaulSD.com>&#xa;#&#xa;# This program is free software: you can redistribute it and/or modify it under the terms of the GNU&#xa;# General Public License as published by the Free Software Foundation, either version 3 of the&#xa;# License, or (at your option) any later version.&#xa;#&#xa;# This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without&#xa;# even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU&#xa;# General Public License for more details.&#xa;#&#xa;# You should have received a copy of the GNU General Public License along with this program.  If&#xa;# not, see <http://www.gnu.org/licenses/>.&#xa;#&#xa;&#xa;&#xa;&#xa;#&#xa;# Prerequisites:&#xa;# Install GtkTrayIcon (from the gtktrayicon/ subdirectory)&#xa;# sudo apt-get install --no-install-recommends libgirepository1.0-dev gobject-introspection \&#xa;#  gir1.2-gtk-3.0&#xa;#&#xa;&#xa;&#xa;&#xa;import gi&#xa;gi.require_version('Gtkti', '3.0')&#xa;from gi.repository import Gtkti, Gtk, Gdk, GLib&#xa;import signal, sys, os&#xa;import threading&#xa;import datetime&#xa;&#xa;# In Python 2, threading.Event().wait() wakes up frequently and burns a lot of CPU.&#xa;# This does not happen in Python 3, so I'm simply using Python 3 instead of Python 2 for this app.&#xa;# See: http://stackoverflow.com/questions/29082268/python-time-sleep-vs-event-wait&#xa;# I don't know if there are any work-arounds for this issue in Python 2.&#xa;&#xa;# WARNING: Variable scope for Python inline functions and lambdas does not work like other&#xa;# languages!  To ensure that definition-scope variables are passed into the function/lambda's scope&#xa;# as expected, explicitly add 'var=var' (optional/defaulted) parameters to the end of the function/&#xa;# lambda's parameter list.&#xa;&#xa;class TimeApp:&#xa;&#xa;  def __init__(self):&#xa;    self.show_date = True&#xa;    self.prefix = ''&#xa;    #self.date_format = '%x '  # Locale-appropriate date format&#xa;    self.date_format = '%Y.%m.%d '&#xa;    self.time_format = '%H:%M'&#xa;    self.show_seconds = False&#xa;    self.seconds_format = ':%S'&#xa;    self.suffix = ' '&#xa;&#xa;    self.time_fudge = datetime.timedelta(seconds=.25)&#xa;&#xa;    self.build_ui()&#xa;    self.gtk_update_ui()&#xa;    self.start_update_thread()&#xa;&#xa;  def build_ui(self):&#xa;    self.tray = tray = Gtkti.TrayIcon()&#xa;    eventbox = Gtk.EventBox()&#xa;    tray.add(eventbox)&#xa;    self.tray_label = tray_label = Gtk.Label(self.prefix+self.suffix)&#xa;    eventbox.add(tray_label)&#xa;    tray.show_all()&#xa;&#xa;    menu = Gtk.Menu()&#xa;    item_show_date = Gtk.CheckMenuItem('Show Date')&#xa;    item_show_date.set_active(self.show_date)&#xa;    def toggle_date(item_show_date, self=self):&#xa;      self.show_date = item_show_date.get_active()&#xa;      self.gtk_update_ui()&#xa;    item_show_date.connect('toggled', toggle_date)&#xa;    menu.append(item_show_date)&#xa;    item_show_seconds = Gtk.CheckMenuItem('Show Seconds')&#xa;    item_show_seconds.set_active(self.show_seconds)&#xa;    self.toggle_seconds_event = threading.Event()&#xa;    def toggle_seconds(item_show_seconds, self=self):&#xa;      self.show_seconds = item_show_seconds.get_active()&#xa;      # Wake the update thread, which will update the UI, then sleep again&#xa;      self.toggle_seconds_event.set()&#xa;    item_show_seconds.connect('toggled', toggle_seconds)&#xa;    menu.append(item_show_seconds)&#xa;    item_quit = Gtk.MenuItem('Quit')&#xa;    def quit(menu_item):&#xa;      if sys.version_info < (3, 0):&#xa;        os.kill(os.getpid(), signal.SIGINT)&#xa;      else:&#xa;        Gtk.main_quit()&#xa;    item_quit.connect('activate', quit)&#xa;    menu.append(item_quit)&#xa;    menu.show_all()&#xa;    def button_pressed(eventbox, event, menu=menu):&#xa;      if event.type == Gdk.EventType.BUTTON_PRESS and event.button == 3:&#xa;        menu.popup(None, None, None, None, event.button, event.time)&#xa;    eventbox.connect('button-press-event', button_pressed)&#xa;&#xa;  # Update the UI (thread-safe)&#xa;  def update_ui(self):&#xa;    GLib.idle_add(self.gtk_update_ui)&#xa;&#xa;  # Update the UI (within the GTK main thread ; not thread-safe)&#xa;  def gtk_update_ui(self):&#xa;    fmt = self.prefix&#xa;    if self.show_date: fmt += self.date_format&#xa;    fmt += self.time_format&#xa;    if self.show_seconds: fmt += self.seconds_format&#xa;    fmt += self.suffix&#xa;&#xa;    # Update events should fire as close as possible to the second or minute boundary, but if they&#xa;    # fire early, the previous time will incorrectly be displayed until the next update.  Fudge the&#xa;    # time to make sure the display is incremented even if the event fires slightly early.&#xa;    now = datetime.datetime.now() + self.time_fudge&#xa;&#xa;    self.tray_label.set_text(now.strftime(fmt))&#xa;&#xa;    # Return false to unregister this method as a GLib idle handler&#xa;    return False&#xa;&#xa;  def start_update_thread(self):&#xa;    def run_in_thread(self=self):&#xa;      while True:&#xa;        fired_update = datetime.datetime.utcnow()&#xa;        self.update_ui()&#xa;        now = datetime.datetime.utcnow()&#xa;        if self.show_seconds:&#xa;          time_to_next_update = 1 - now.microsecond/1000000.0&#xa;          if (1000000.0 - fired_update.microsecond) < self.time_fudge.microseconds:&#xa;            time_to_next_update += 1&#xa;        else:&#xa;          time_to_next_update = 60 - now.second - now.microsecond/1000000.0&#xa;          if time_to_next_update < 1 and (1000000.0 - fired_update.microsecond) < self.time_fudge.microseconds:&#xa;            time_to_next_update += 60&#xa;        self.toggle_seconds_event.wait(time_to_next_update) ; self.toggle_seconds_event.clear()&#xa;    thread = threading.Thread(target=run_in_thread)&#xa;    thread.daemon = True&#xa;    thread.start()&#xa;&#xa;if __name__ == '__main__':&#xa;  TimeApp()&#xa;&#xa;  def on_sigint(_signum, _frame):&#xa;    Gtk.main_quit()&#xa;  signal.signal(signal.SIGINT, on_sigint)&#xa;&#xa;  # If the main thread is running C code (such as Gtk.main()), then Python signal handlers will not&#xa;  # run until that C code returns.  To work around this, run the C code in a separate thread, then&#xa;  # sleep the main thread.  Unfortunately, threading.Thread().join() and threading.Event().wait() in&#xa;  # Python 2.X (but not 3.X) also block signal handlers (see http://bugs.python.org/issue1167930).&#xa;  # To work around this, sleep the main thread using signal.pause(), and wake it from the 'Quit'&#xa;  # menu item above using `os.kill(os.getpid(), signal.SIGINT)`.&#xa;  thread = threading.Thread(target=Gtk.main)&#xa;  thread.start()&#xa;  if sys.version_info < (3, 0):&#xa;    signal.pause()&#xa;  thread.join()&#xa;
35817|"# Copyright (c) 2012 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.system_test&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;&#xa;def ensure_directory_exists(path):&#xa;  dir = os.path.dirname(path)&#xa;  if dir and not os.path.exists(dir):&#xa;    os.makedirs(dir)&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) $(ARFLAGS.$(TOOLSET)) $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;# TODO(thakis): Find out and document the difference between shared_library and&#xa;# loadable_module on mac.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;# TODO(thakis): The solink_module rule is likely wrong. Xcode seems to pass&#xa;# -bundle -single_module here (for osmesa.so).&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) $(ARFLAGS.$(TOOLSET)) $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;#&#xa;# Note: flock is used to seralize linking. Linking is a memory-intensive&#xa;# process so running parallel links can often lead to thrashing.  To disable&#xa;# the serialization, override LINK via an envrionment variable as follows:&#xa;#&#xa;#   export LINK=g++&#xa;#&#xa;# This will allow make to invoke N linker processes as specified in -jN.&#xa;LINK ?= %(flock)s $(builddir)/linker.lock $(CXX)&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;ARFLAGS.target ?= %(ARFLAGS.target)s&#xa;&#xa;# N.B.: the logic of which commands to run should match the computation done&#xa;# in gyp's make.py where ARFLAGS.host etc. is computed.&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;ARFLAGS.host := %(ARFLAGS.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = ln -f ""$<"" ""$@"" 2>/dev/null || (rm -rf ""$@"" && cp -af ""$<"" ""$@"")&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds, and deletes the output file when done&#xa;# if any of the postbuilds failed.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    F=$$?;\\&#xa;    if [ $$F -ne 0 ]; then\\&#xa;      E=$$F;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;SHARED_HEADER_SUN_COMMANDS = """"""&#xa;# gyp-sun-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_sun_tool = SUNTOOL $(4) $<&#xa;cmd_sun_tool = ./gyp-sun-tool $(4) $< ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;def InvertRelativePath(path):&#xa;  """"""Given a relative path like foo/bar, return the inverse relative path:&#xa;  the path from the relative path back to the origin dir.&#xa;&#xa;  E.g. os.path.normpath(os.path.join(path, InvertRelativePath(path)))&#xa;  should always produce the empty string.""""""&#xa;&#xa;  if not path:&#xa;    return path&#xa;  # Only need to handle relative paths into subdirectories for now.&#xa;  assert '..' not in path, path&#xa;  depth = len(path.split(os.path.sep))&#xa;  return os.path.sep.join(['..'] * depth)&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter:&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    ensure_directory_exists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if self.type in self._INSTALLABLE_TARGETS:&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    ensure_directory_exists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      command = gyp.common.EncodePOSIXShellList(action['action'])&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      env = self.GetSortedXcodeEnv()&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs + ['FORCE_DO_CMD'], actions)&#xa;        for output in outputs:&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() calls normpath, stripping trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                      part_of_all=True)&#xa;      bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.GetTargetPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if self.type in self._INSTALLABLE_TARGETS:&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    # TODO(evanm): just make order_only a list of deps instead of these hacks.&#xa;    if order_only:&#xa;      order_insert = '| '&#xa;      pick_output = ' '.join(outputs)&#xa;    else:&#xa;      order_insert = ''&#xa;      pick_output = outputs[0]&#xa;    if force:&#xa;      force_append = ' FORCE_DO_CMD'&#xa;    else:&#xa;      force_append = ''&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    self.WriteLn('%s: %s%s%s' % (pick_output, order_insert, ' '.join(inputs),&#xa;                                 force_append))&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    if not order_only and len(outputs) > 1:&#xa;      # If we have more than one output, a rule like&#xa;      #   foo bar: baz&#xa;      # that for *each* output we must run the action, potentially&#xa;      # in parallel.  That is not what we're trying to write -- what&#xa;      # we want is that we run the action once and it generates all&#xa;      # the files.&#xa;      # http://www.gnu.org/software/hello/manual/automake/Multiple-Outputs.html&#xa;      # discusses this problem and has this solution:&#xa;      # 1) Write the naive rule that would produce parallel runs of&#xa;      # the action.&#xa;      # 2) Make the outputs seralized on each other, so we won't start&#xa;      # a parallel run until the first run finishes, at which point&#xa;      # we'll have generated all the outputs and we're done.&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs[1:]), outputs[0]))&#xa;      # Add a dummy command to the ""extra outputs"" rule, otherwise make seems to&#xa;      # think these outputs haven't (couldn't have?) changed, and thus doesn't&#xa;      # flag them as changed (i.e. include in '$?') when evaluating dependent&#xa;      # rules, which in turn causes do_cmd() to skip running dependent commands.&#xa;      self.WriteLn('%s: ;' % (' '.join(outputs[1:])))&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # path is no existing file in this case, but calling normpath is still&#xa;      # important for trimming trailing slashes.&#xa;      return os.path.normpath(path)&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;    if (self.type == 'shared_library' and&#xa;        (self.flavor != 'mac' or self.toolset != 'target')):&#xa;      # Install all shared libs into a common directory (per toolset) for&#xa;      # convenient access with LD_LIBRARY_PATH.&#xa;      return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def RunSystemTests(flavor):&#xa;  """"""Run tests against the system to compute default settings for commands.&#xa;&#xa;  Returns:&#xa;    dictionary of settings matching the block of command-lines used in&#xa;    SHARED_HEADER.  E.g. the dictionary will contain a ARFLAGS.target&#xa;    key for the default ARFLAGS for the target ar command.&#xa;  """"""&#xa;  # Compute flags used for building static archives.&#xa;  # N.B.: this fallback logic should match the logic in SHARED_HEADER.&#xa;  # See comment there for more details.&#xa;  ar_target = GetEnvironFallback(('AR_target', 'AR'), 'ar')&#xa;  cc_target = GetEnvironFallback(('CC_target', 'CC'), 'cc')&#xa;  arflags_target = 'crs'&#xa;  # ar -T enables thin archives on Linux. OS X's ar supports a -T flag, but it&#xa;  # does something useless (it limits filenames in the archive to 15 chars).&#xa;  if flavor != 'mac' and gyp.system_test.TestArSupportsT(ar_command=ar_target,&#xa;                                                         cc_command=cc_target):&#xa;    arflags_target = 'crsT'&#xa;&#xa;  ar_host = os.environ.get('AR_host', 'ar')&#xa;  cc_host = os.environ.get('CC_host', 'gcc')&#xa;  arflags_host = 'crs'&#xa;  # It feels redundant to compute this again given that most builds aren't&#xa;  # cross-compiles, but due to quirks of history CC_host defaults to 'gcc'&#xa;  # while CC_target defaults to 'cc', so the commands really are different&#xa;  # even though they're nearly guaranteed to run the same code underneath.&#xa;  if flavor != 'mac' and gyp.system_test.TestArSupportsT(ar_command=ar_host,&#xa;                                                         cc_command=cc_host):&#xa;    arflags_host = 'crsT'&#xa;&#xa;  return { 'ARFLAGS.target': arflags_target,&#xa;           'ARFLAGS.host': arflags_host }&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(options.generator_output, output_file)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(options.generator_output, makefile_path)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-sun-tool flock',&#xa;        'flock_index': 2,&#xa;        'extra_commands': SHARED_HEADER_SUN_COMMANDS,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;&#xa;  header_params.update(RunSystemTests(flavor))&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LD_target', 'LD'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LD_host',), 'g++'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    if key == 'LINK':&#xa;      make_global_settings += ('%s ?= %s $(builddir)/linker.lock %s\n' %&#xa;                               (key, flock_command, value))&#xa;    elif key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      if key in os.environ:&#xa;        value = os.environ[key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  ensure_directory_exists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets."")&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
284115|"# Licensed under a 3-clause BSD style license - see LICENSE.rst&#xa;from __future__ import absolute_import, unicode_literals&#xa;&#xa;import contextlib&#xa;import functools&#xa;import imp&#xa;import inspect&#xa;import os&#xa;import sys&#xa;import textwrap&#xa;import types&#xa;import warnings&#xa;&#xa;try:&#xa;    from importlib import machinery as import_machinery&#xa;    # Python 3.2 does not have SourceLoader&#xa;    if not hasattr(import_machinery, 'SourceLoader'):&#xa;        import_machinery = None&#xa;except ImportError:&#xa;    import_machinery = None&#xa;&#xa;&#xa;# Python 3.3's importlib caches filesystem reads for faster imports in the&#xa;# general case. But sometimes it's necessary to manually invalidate those&#xa;# caches so that the import system can pick up new generated files.  See&#xa;# https://github.com/astropy/astropy/issues/820&#xa;if sys.version_info[:2] >= (3, 3):&#xa;    from importlib import invalidate_caches&#xa;else:&#xa;    invalidate_caches = lambda: None&#xa;&#xa;&#xa;# Note: The following Warning subclasses are simply copies of the Warnings in&#xa;# Astropy of the same names.&#xa;class AstropyWarning(Warning):&#xa;    """"""&#xa;    The base warning class from which all Astropy warnings should inherit.&#xa;&#xa;    Any warning inheriting from this class is handled by the Astropy logger.&#xa;    """"""&#xa;&#xa;&#xa;class AstropyDeprecationWarning(AstropyWarning):&#xa;    """"""&#xa;    A warning class to indicate a deprecated feature.&#xa;    """"""&#xa;&#xa;&#xa;class AstropyPendingDeprecationWarning(PendingDeprecationWarning,&#xa;                                       AstropyWarning):&#xa;    """"""&#xa;    A warning class to indicate a soon-to-be deprecated feature.&#xa;    """"""&#xa;&#xa;&#xa;def _get_platlib_dir(cmd):&#xa;    """"""&#xa;    Given a build command, return the name of the appropriate platform-specific&#xa;    build subdirectory directory (e.g. build/lib.linux-x86_64-2.7)&#xa;    """"""&#xa;&#xa;    plat_specifier = '.{0}-{1}'.format(cmd.plat_name, sys.version[0:3])&#xa;    return os.path.join(cmd.build_base, 'lib' + plat_specifier)&#xa;&#xa;&#xa;def get_numpy_include_path():&#xa;    """"""&#xa;    Gets the path to the numpy headers.&#xa;    """"""&#xa;    # We need to go through this nonsense in case setuptools&#xa;    # downloaded and installed Numpy for us as part of the build or&#xa;    # install, since Numpy may still think it's in ""setup mode"", when&#xa;    # in fact we're ready to use it to build astropy now.&#xa;&#xa;    if sys.version_info[0] >= 3:&#xa;        import builtins&#xa;        if hasattr(builtins, '__NUMPY_SETUP__'):&#xa;            del builtins.__NUMPY_SETUP__&#xa;        import imp&#xa;        import numpy&#xa;        imp.reload(numpy)&#xa;    else:&#xa;        import __builtin__&#xa;        if hasattr(__builtin__, '__NUMPY_SETUP__'):&#xa;            del __builtin__.__NUMPY_SETUP__&#xa;        import numpy&#xa;        reload(numpy)&#xa;&#xa;    try:&#xa;        numpy_include = numpy.get_include()&#xa;    except AttributeError:&#xa;        numpy_include = numpy.get_numpy_include()&#xa;    return numpy_include&#xa;&#xa;&#xa;class _DummyFile(object):&#xa;    """"""A noop writeable object.""""""&#xa;&#xa;    errors = ''  # Required for Python 3.x&#xa;&#xa;    def write(self, s):&#xa;        pass&#xa;&#xa;    def flush(self):&#xa;        pass&#xa;&#xa;&#xa;@contextlib.contextmanager&#xa;def silence():&#xa;    """"""A context manager that silences sys.stdout and sys.stderr.""""""&#xa;&#xa;    old_stdout = sys.stdout&#xa;    old_stderr = sys.stderr&#xa;    sys.stdout = _DummyFile()&#xa;    sys.stderr = _DummyFile()&#xa;    exception_occurred = False&#xa;    try:&#xa;        yield&#xa;    except:&#xa;        exception_occurred = True&#xa;        # Go ahead and clean up so that exception handling can work normally&#xa;        sys.stdout = old_stdout&#xa;        sys.stderr = old_stderr&#xa;        raise&#xa;&#xa;    if not exception_occurred:&#xa;        sys.stdout = old_stdout&#xa;        sys.stderr = old_stderr&#xa;&#xa;&#xa;if sys.platform == 'win32':&#xa;    import ctypes&#xa;&#xa;    def _has_hidden_attribute(filepath):&#xa;        """"""&#xa;        Returns True if the given filepath has the hidden attribute on&#xa;        MS-Windows.  Based on a post here:&#xa;        http://stackoverflow.com/questions/284115/cross-platform-hidden-file-detection&#xa;        """"""&#xa;        if isinstance(filepath, bytes):&#xa;            filepath = filepath.decode(sys.getfilesystemencoding())&#xa;        try:&#xa;            attrs = ctypes.windll.kernel32.GetFileAttributesW(filepath)&#xa;            assert attrs != -1&#xa;            result = bool(attrs & 2)&#xa;        except (AttributeError, AssertionError):&#xa;            result = False&#xa;        return result&#xa;else:&#xa;    def _has_hidden_attribute(filepath):&#xa;        return False&#xa;&#xa;&#xa;def is_path_hidden(filepath):&#xa;    """"""&#xa;    Determines if a given file or directory is hidden.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    filepath : str&#xa;        The path to a file or directory&#xa;&#xa;    Returns&#xa;    -------&#xa;    hidden : bool&#xa;        Returns `True` if the file is hidden&#xa;    """"""&#xa;&#xa;    name = os.path.basename(os.path.abspath(filepath))&#xa;    if isinstance(name, bytes):&#xa;        is_dotted = name.startswith(b'.')&#xa;    else:&#xa;        is_dotted = name.startswith('.')&#xa;    return is_dotted or _has_hidden_attribute(filepath)&#xa;&#xa;&#xa;def walk_skip_hidden(top, onerror=None, followlinks=False):&#xa;    """"""&#xa;    A wrapper for `os.walk` that skips hidden files and directories.&#xa;&#xa;    This function does not have the parameter `topdown` from&#xa;    `os.walk`: the directories must always be recursed top-down when&#xa;    using this function.&#xa;&#xa;    See also&#xa;    --------&#xa;    os.walk : For a description of the parameters&#xa;    """"""&#xa;&#xa;    for root, dirs, files in os.walk(&#xa;            top, topdown=True, onerror=onerror,&#xa;            followlinks=followlinks):&#xa;        # These lists must be updated in-place so os.walk will skip&#xa;        # hidden directories&#xa;        dirs[:] = [d for d in dirs if not is_path_hidden(d)]&#xa;        files[:] = [f for f in files if not is_path_hidden(f)]&#xa;        yield root, dirs, files&#xa;&#xa;&#xa;def write_if_different(filename, data):&#xa;    """"""Write `data` to `filename`, if the content of the file is different.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    filename : str&#xa;        The file name to be written to.&#xa;    data : bytes&#xa;        The data to be written to `filename`.&#xa;    """"""&#xa;&#xa;    assert isinstance(data, bytes)&#xa;&#xa;    if os.path.exists(filename):&#xa;        with open(filename, 'rb') as fd:&#xa;            original_data = fd.read()&#xa;    else:&#xa;        original_data = None&#xa;&#xa;    if original_data != data:&#xa;        with open(filename, 'wb') as fd:&#xa;            fd.write(data)&#xa;&#xa;&#xa;def import_file(filename, name=None):&#xa;    """"""&#xa;    Imports a module from a single file as if it doesn't belong to a&#xa;    particular package.&#xa;&#xa;    The returned module will have the optional ``name`` if given, or else&#xa;    a name generated from the filename.&#xa;    """"""&#xa;    # Specifying a traditional dot-separated fully qualified name here&#xa;    # results in a number of ""Parent module 'astropy' not found while&#xa;    # handling absolute import"" warnings.  Using the same name, the&#xa;    # namespaces of the modules get merged together.  So, this&#xa;    # generates an underscore-separated name which is more likely to&#xa;    # be unique, and it doesn't really matter because the name isn't&#xa;    # used directly here anyway.&#xa;    mode = 'U' if sys.version_info[0] < 3 else 'r'&#xa;&#xa;    if name is None:&#xa;        basename = os.path.splitext(filename)[0]&#xa;        name = '_'.join(os.path.relpath(basename).split(os.sep)[1:])&#xa;&#xa;    if import_machinery:&#xa;        loader = import_machinery.SourceFileLoader(name, filename)&#xa;        mod = loader.load_module()&#xa;    else:&#xa;        with open(filename, mode) as fd:&#xa;            mod = imp.load_module(name, fd, filename, ('.py', mode, 1))&#xa;&#xa;    return mod&#xa;&#xa;&#xa;def resolve_name(name):&#xa;    """"""Resolve a name like ``module.object`` to an object and return it.&#xa;&#xa;    Raise `ImportError` if the module or name is not found.&#xa;    """"""&#xa;&#xa;    parts = name.split('.')&#xa;    cursor = len(parts) - 1&#xa;    module_name = parts[:cursor]&#xa;    attr_name = parts[-1]&#xa;&#xa;    while cursor > 0:&#xa;        try:&#xa;            ret = __import__('.'.join(module_name), fromlist=[attr_name])&#xa;            break&#xa;        except ImportError:&#xa;            if cursor == 0:&#xa;                raise&#xa;            cursor -= 1&#xa;            module_name = parts[:cursor]&#xa;            attr_name = parts[cursor]&#xa;            ret = ''&#xa;&#xa;    for part in parts[cursor:]:&#xa;        try:&#xa;            ret = getattr(ret, part)&#xa;        except AttributeError:&#xa;            raise ImportError(name)&#xa;&#xa;    return ret&#xa;&#xa;&#xa;if sys.version_info[0] >= 3:&#xa;    def iteritems(dictionary):&#xa;        return dictionary.items()&#xa;else:&#xa;    def iteritems(dictionary):&#xa;        return dictionary.iteritems()&#xa;&#xa;&#xa;def extends_doc(extended_func):&#xa;    """"""&#xa;    A function decorator for use when wrapping an existing function but adding&#xa;    additional functionality.  This copies the docstring from the original&#xa;    function, and appends to it (along with a newline) the docstring of the&#xa;    wrapper function.&#xa;&#xa;    Example&#xa;    -------&#xa;&#xa;        >>> def foo():&#xa;        ...     '''Hello.'''&#xa;        ...&#xa;        >>> @extends_doc(foo)&#xa;        ... def bar():&#xa;        ...     '''Goodbye.'''&#xa;        ...&#xa;        >>> print(bar.__doc__)&#xa;        Hello.&#xa;&#xa;        Goodbye.&#xa;&#xa;    """"""&#xa;&#xa;    def decorator(func):&#xa;        if not (extended_func.__doc__ is None or func.__doc__ is None):&#xa;            func.__doc__ = '\n\n'.join([extended_func.__doc__.rstrip('\n'),&#xa;                                        func.__doc__.lstrip('\n')])&#xa;        return func&#xa;&#xa;    return decorator&#xa;&#xa;&#xa;# Duplicated from astropy.utils.decorators.deprecated&#xa;# When fixing issues in this function fix them in astropy first, then&#xa;# port the fixes over to astropy-helpers&#xa;def deprecated(since, message='', name='', alternative='', pending=False,&#xa;               obj_type=None):&#xa;    """"""&#xa;    Used to mark a function or class as deprecated.&#xa;&#xa;    To mark an attribute as deprecated, use `deprecated_attribute`.&#xa;&#xa;    Parameters&#xa;    ------------&#xa;    since : str&#xa;        The release at which this API became deprecated.  This is&#xa;        required.&#xa;&#xa;    message : str, optional&#xa;        Override the default deprecation message.  The format&#xa;        specifier ``func`` may be used for the name of the function,&#xa;        and ``alternative`` may be used in the deprecation message&#xa;        to insert the name of an alternative to the deprecated&#xa;        function. ``obj_type`` may be used to insert a friendly name&#xa;        for the type of object being deprecated.&#xa;&#xa;    name : str, optional&#xa;        The name of the deprecated function or class; if not provided&#xa;        the name is automatically determined from the passed in&#xa;        function or class, though this is useful in the case of&#xa;        renamed functions, where the new function is just assigned to&#xa;        the name of the deprecated function.  For example::&#xa;&#xa;            def new_function():&#xa;                ...&#xa;            oldFunction = new_function&#xa;&#xa;    alternative : str, optional&#xa;        An alternative function or class name that the user may use in&#xa;        place of the deprecated object.  The deprecation warning will&#xa;        tell the user about this alternative if provided.&#xa;&#xa;    pending : bool, optional&#xa;        If True, uses a AstropyPendingDeprecationWarning instead of a&#xa;        AstropyDeprecationWarning.&#xa;&#xa;    obj_type : str, optional&#xa;        The type of this object, if the automatically determined one&#xa;        needs to be overridden.&#xa;    """"""&#xa;&#xa;    method_types = (classmethod, staticmethod, types.MethodType)&#xa;&#xa;    def deprecate_doc(old_doc, message):&#xa;        """"""&#xa;        Returns a given docstring with a deprecation message prepended&#xa;        to it.&#xa;        """"""&#xa;        if not old_doc:&#xa;            old_doc = ''&#xa;        old_doc = textwrap.dedent(old_doc).strip('\n')&#xa;        new_doc = (('\n.. deprecated:: %(since)s'&#xa;                    '\n    %(message)s\n\n' %&#xa;                    {'since': since, 'message': message.strip()}) + old_doc)&#xa;        if not old_doc:&#xa;            # This is to prevent a spurious 'unexpected unindent' warning from&#xa;            # docutils when the original docstring was blank.&#xa;            new_doc += r'\ '&#xa;        return new_doc&#xa;&#xa;    def get_function(func):&#xa;        """"""&#xa;        Given a function or classmethod (or other function wrapper type), get&#xa;        the function object.&#xa;        """"""&#xa;        if isinstance(func, method_types):&#xa;            try:&#xa;                func = func.__func__&#xa;            except AttributeError:&#xa;                # classmethods in Python2.6 and below lack the __func__&#xa;                # attribute so we need to hack around to get it&#xa;                method = func.__get__(None, object)&#xa;                if isinstance(method, types.FunctionType):&#xa;                    # For staticmethods anyways the wrapped object is just a&#xa;                    # plain function (not a bound method or anything like that)&#xa;                    func = method&#xa;                elif hasattr(method, '__func__'):&#xa;                    func = method.__func__&#xa;                elif hasattr(method, 'im_func'):&#xa;                    func = method.im_func&#xa;                else:&#xa;                    # Nothing we can do really...  just return the original&#xa;                    # classmethod, etc.&#xa;                    return func&#xa;        return func&#xa;&#xa;    def deprecate_function(func, message):&#xa;        """"""&#xa;        Returns a wrapped function that displays an&#xa;        ``AstropyDeprecationWarning`` when it is called.&#xa;        """"""&#xa;&#xa;        if isinstance(func, method_types):&#xa;            func_wrapper = type(func)&#xa;        else:&#xa;            func_wrapper = lambda f: f&#xa;&#xa;        func = get_function(func)&#xa;&#xa;        def deprecated_func(*args, **kwargs):&#xa;            if pending:&#xa;                category = AstropyPendingDeprecationWarning&#xa;            else:&#xa;                category = AstropyDeprecationWarning&#xa;&#xa;            warnings.warn(message, category, stacklevel=2)&#xa;&#xa;            return func(*args, **kwargs)&#xa;&#xa;        # If this is an extension function, we can't call&#xa;        # functools.wraps on it, but we normally don't care.&#xa;        # This crazy way to get the type of a wrapper descriptor is&#xa;        # straight out of the Python 3.3 inspect module docs.&#xa;        if type(func) != type(str.__dict__['__add__']):&#xa;            deprecated_func = functools.wraps(func)(deprecated_func)&#xa;&#xa;        deprecated_func.__doc__ = deprecate_doc(&#xa;            deprecated_func.__doc__, message)&#xa;&#xa;        return func_wrapper(deprecated_func)&#xa;&#xa;    def deprecate_class(cls, message):&#xa;        """"""&#xa;        Returns a wrapper class with the docstrings updated and an&#xa;        __init__ function that will raise an&#xa;        ``AstropyDeprectationWarning`` warning when called.&#xa;        """"""&#xa;        # Creates a new class with the same name and bases as the&#xa;        # original class, but updates the dictionary with a new&#xa;        # docstring and a wrapped __init__ method.  __module__ needs&#xa;        # to be manually copied over, since otherwise it will be set&#xa;        # to *this* module (astropy.utils.misc).&#xa;&#xa;        # This approach seems to make Sphinx happy (the new class&#xa;        # looks enough like the original class), and works with&#xa;        # extension classes (which functools.wraps does not, since&#xa;        # it tries to modify the original class).&#xa;&#xa;        # We need to add a custom pickler or you'll get&#xa;        #     Can't pickle <class ..>: it's not found as ...&#xa;        # errors. Picklability is required for any class that is&#xa;        # documented by Sphinx.&#xa;&#xa;        members = cls.__dict__.copy()&#xa;&#xa;        members.update({&#xa;            '__doc__': deprecate_doc(cls.__doc__, message),&#xa;            '__init__': deprecate_function(get_function(cls.__init__),&#xa;                                           message),&#xa;        })&#xa;&#xa;        return type(cls.__name__, cls.__bases__, members)&#xa;&#xa;    def deprecate(obj, message=message, name=name, alternative=alternative,&#xa;                  pending=pending):&#xa;        if obj_type is None:&#xa;            if isinstance(obj, type):&#xa;                obj_type_name = 'class'&#xa;            elif inspect.isfunction(obj):&#xa;                obj_type_name = 'function'&#xa;            elif inspect.ismethod(obj) or isinstance(obj, method_types):&#xa;                obj_type_name = 'method'&#xa;            else:&#xa;                obj_type_name = 'object'&#xa;        else:&#xa;            obj_type_name = obj_type&#xa;&#xa;        if not name:&#xa;            name = get_function(obj).__name__&#xa;&#xa;        altmessage = ''&#xa;        if not message or type(message) == type(deprecate):&#xa;            if pending:&#xa;                message = ('The %(func)s %(obj_type)s will be deprecated in a '&#xa;                           'future version.')&#xa;            else:&#xa;                message = ('The %(func)s %(obj_type)s is deprecated and may '&#xa;                           'be removed in a future version.')&#xa;            if alternative:&#xa;                altmessage = '\n        Use %s instead.' % alternative&#xa;&#xa;        message = ((message % {&#xa;            'func': name,&#xa;            'name': name,&#xa;            'alternative': alternative,&#xa;            'obj_type': obj_type_name}) +&#xa;            altmessage)&#xa;&#xa;        if isinstance(obj, type):&#xa;            return deprecate_class(obj, message)&#xa;        else:&#xa;            return deprecate_function(obj, message)&#xa;&#xa;    if type(message) == type(deprecate):&#xa;        return deprecate(message)&#xa;&#xa;    return deprecate&#xa;&#xa;&#xa;def deprecated_attribute(name, since, message=None, alternative=None,&#xa;                         pending=False):&#xa;    """"""&#xa;    Used to mark a public attribute as deprecated.  This creates a&#xa;    property that will warn when the given attribute name is accessed.&#xa;    To prevent the warning (i.e. for internal code), use the private&#xa;    name for the attribute by prepending an underscore&#xa;    (i.e. ``self._name``).&#xa;&#xa;    Parameters&#xa;    ----------&#xa;    name : str&#xa;        The name of the deprecated attribute.&#xa;&#xa;    since : str&#xa;        The release at which this API became deprecated.  This is&#xa;        required.&#xa;&#xa;    message : str, optional&#xa;        Override the default deprecation message.  The format&#xa;        specifier ``name`` may be used for the name of the attribute,&#xa;        and ``alternative`` may be used in the deprecation message&#xa;        to insert the name of an alternative to the deprecated&#xa;        function.&#xa;&#xa;    alternative : str, optional&#xa;        An alternative attribute that the user may use in place of the&#xa;        deprecated attribute.  The deprecation warning will tell the&#xa;        user about this alternative if provided.&#xa;&#xa;    pending : bool, optional&#xa;        If True, uses a AstropyPendingDeprecationWarning instead of a&#xa;        AstropyDeprecationWarning.&#xa;&#xa;    Examples&#xa;    --------&#xa;&#xa;    ::&#xa;&#xa;        class MyClass:&#xa;            # Mark the old_name as deprecated&#xa;            old_name = misc.deprecated_attribute('old_name', '0.1')&#xa;&#xa;            def method(self):&#xa;                self._old_name = 42&#xa;    """"""&#xa;    private_name = '_' + name&#xa;&#xa;    @deprecated(since, name=name, obj_type='attribute')&#xa;    def get(self):&#xa;        return getattr(self, private_name)&#xa;&#xa;    @deprecated(since, name=name, obj_type='attribute')&#xa;    def set(self, val):&#xa;        setattr(self, private_name, val)&#xa;&#xa;    @deprecated(since, name=name, obj_type='attribute')&#xa;    def delete(self):&#xa;        delattr(self, private_name)&#xa;&#xa;    return property(get, set, delete)&#xa;&#xa;&#xa;def minversion(module, version, inclusive=True, version_path='__version__'):&#xa;    """"""&#xa;    Returns `True` if the specified Python module satisfies a minimum version&#xa;    requirement, and `False` if not.&#xa;&#xa;    By default this uses `pkg_resources.parse_version` to do the version&#xa;    comparison if available.  Otherwise it falls back on&#xa;    `distutils.version.LooseVersion`.&#xa;&#xa;    Parameters&#xa;    ----------&#xa;&#xa;    module : module or `str`&#xa;        An imported module of which to check the version, or the name of&#xa;        that module (in which case an import of that module is attempted--&#xa;        if this fails `False` is returned).&#xa;&#xa;    version : `str`&#xa;        The version as a string that this module must have at a minimum (e.g.&#xa;        ``'0.12'``).&#xa;&#xa;    inclusive : `bool`&#xa;        The specified version meets the requirement inclusively (i.e. ``>=``)&#xa;        as opposed to strictly greater than (default: `True`).&#xa;&#xa;    version_path : `str`&#xa;        A dotted attribute path to follow in the module for the version.&#xa;        Defaults to just ``'__version__'``, which should work for most Python&#xa;        modules.&#xa;&#xa;    Examples&#xa;    --------&#xa;&#xa;    >>> import astropy&#xa;    >>> minversion(astropy, '0.4.4')&#xa;    True&#xa;    """"""&#xa;&#xa;    if isinstance(module, types.ModuleType):&#xa;        module_name = module.__name__&#xa;    elif isinstance(module, six.string_types):&#xa;        module_name = module&#xa;        try:&#xa;            module = resolve_name(module_name)&#xa;        except ImportError:&#xa;            return False&#xa;    else:&#xa;        raise ValueError('module argument must be an actual imported '&#xa;                         'module, or the import name of the module; '&#xa;                         'got {0!r}'.format(module))&#xa;&#xa;    if '.' not in version_path:&#xa;        have_version = getattr(module, version_path)&#xa;    else:&#xa;        have_version = resolve_name('.'.join([module.__name__, version_path]))&#xa;&#xa;    try:&#xa;        from pkg_resources import parse_version&#xa;    except ImportError:&#xa;        from distutils.version import LooseVersion as parse_version&#xa;&#xa;    if inclusive:&#xa;        return parse_version(have_version) >= parse_version(version)&#xa;    else:&#xa;        return parse_version(have_version) > parse_version(version)&#xa;"
15299878|"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.&#xa;#&#xa;# Licensed under the Apache License, Version 2.0 (the ""License"");&#xa;# you may not use this file except in compliance with the License.&#xa;# You may obtain a copy of the License at&#xa;#&#xa;#     http://www.apache.org/licenses/LICENSE-2.0&#xa;#&#xa;# Unless required by applicable law or agreed to in writing, software&#xa;# distributed under the License is distributed on an ""AS IS"" BASIS,&#xa;# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#xa;# See the License for the specific language governing permissions and&#xa;# limitations under the License.&#xa;# ==============================================================================&#xa;&#xa;""""""Minimal runtime type checking library.&#xa;&#xa;This module should not be considered public API.&#xa;""""""&#xa;# TODO(ericmc,shoyer): Delete this in favor of using pytype or mypy&#xa;from __future__ import absolute_import&#xa;from __future__ import division&#xa;from __future__ import print_function&#xa;&#xa;import collections&#xa;import functools&#xa;import re&#xa;&#xa;from tensorflow.python.util import tf_inspect&#xa;&#xa;# used for register_type_abbreviation and _type_repr below.&#xa;_TYPE_ABBREVIATIONS = {}&#xa;&#xa;&#xa;class Type(object):&#xa;  """"""Base class for type checker types.&#xa;&#xa;  The custom types defined in this module are based on types in the standard&#xa;  library's typing module (in Python 3.5):&#xa;  https://docs.python.org/3/library/typing.html&#xa;&#xa;  The only difference should be that we use actual instances of Type classes to&#xa;  represent custom types rather than the metaclass magic typing uses to create&#xa;  new class objects. In practice, all this should mean is that we use&#xa;  `List(int)` rather than `List[int]`.&#xa;&#xa;  Custom types should implement __instancecheck__ and inherit from Type. Every&#xa;  argument in the constructor must be a type or Type instance, and these&#xa;  arguments must be stored as a tuple on the `_types` attribute.&#xa;  """"""&#xa;&#xa;  def __init__(self, *types):&#xa;    self._types = types&#xa;&#xa;  def __repr__(self):&#xa;    args_repr = "", "".join(repr(t) for t in self._types)&#xa;    return  ""typecheck.%s(%s)"" % (type(self).__name__, args_repr)&#xa;&#xa;&#xa;class _SingleArgumentType(Type):&#xa;  """"""Use this subclass for parametric types that accept only one argument.""""""&#xa;&#xa;  def __init__(self, tpe):&#xa;    super(_SingleArgumentType, self).__init__(tpe)&#xa;&#xa;  @property&#xa;  def _type(self):&#xa;    tpe, = self._types  # pylint: disable=unbalanced-tuple-unpacking&#xa;    return tpe&#xa;&#xa;&#xa;class _TwoArgumentType(Type):&#xa;  """"""Use this subclass for parametric types that accept two arguments.""""""&#xa;&#xa;  def __init__(self, first_type, second_type):&#xa;    super(_TwoArgumentType, self).__init__(first_type, second_type)&#xa;&#xa;&#xa;class Union(Type):&#xa;  """"""A sum type.&#xa;&#xa;  A correct type is any of the types provided.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return isinstance(instance, self._types)&#xa;&#xa;&#xa;class Optional(_SingleArgumentType):&#xa;  """"""An optional type.&#xa;&#xa;  A correct type is either the provided type or NoneType.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    # types.NoneType does not exist in Python 3&#xa;    return isinstance(instance, (self._type, type(None)))&#xa;&#xa;&#xa;class List(_SingleArgumentType):&#xa;  """"""A typed list.&#xa;&#xa;  A correct type is a list where each element has the single provided type.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return (isinstance(instance, list)&#xa;            and all(isinstance(x, self._type) for x in instance))&#xa;&#xa;&#xa;class Sequence(_SingleArgumentType):&#xa;  """"""A typed sequence.&#xa;&#xa;  A correct type is a sequence where each element has the single provided type.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return (isinstance(instance, collections.Sequence)&#xa;            and all(isinstance(x, self._type) for x in instance))&#xa;&#xa;&#xa;class Collection(_SingleArgumentType):&#xa;  """"""A sized, iterable container.&#xa;&#xa;  A correct type is an iterable and container with known size where each element&#xa;  has the single provided type.&#xa;&#xa;  We use this in preference to Iterable because we check each instance of the&#xa;  iterable at runtime, and hence need to avoid iterables that could be&#xa;  exhausted.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return (isinstance(instance, collections.Iterable)&#xa;            and isinstance(instance, collections.Sized)&#xa;            and isinstance(instance, collections.Container)&#xa;            and all(isinstance(x, self._type) for x in instance))&#xa;&#xa;&#xa;class Tuple(Type):&#xa;  """"""A typed tuple.&#xa;&#xa;  A correct type is a tuple with the correct length where each element has&#xa;  the correct type.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return (isinstance(instance, tuple)&#xa;            and len(instance) == len(self._types)&#xa;            and all(isinstance(x, t) for x, t in zip(instance, self._types)))&#xa;&#xa;&#xa;class Mapping(_TwoArgumentType):&#xa;  """"""A typed mapping.&#xa;&#xa;  A correct type has the correct parametric types for keys and values.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    key_type, value_type = self._types  # pylint: disable=unbalanced-tuple-unpacking&#xa;    return (isinstance(instance, collections.Mapping)&#xa;            and all(isinstance(k, key_type) for k in instance.keys())&#xa;            and all(isinstance(k, value_type) for k in instance.values()))&#xa;&#xa;&#xa;class Dict(Mapping):&#xa;  """"""A typed dict.&#xa;&#xa;  A correct type has the correct parametric types for keys and values.&#xa;  """"""&#xa;&#xa;  def __instancecheck__(self, instance):&#xa;    return (isinstance(instance, dict)&#xa;            and super(Dict, self).__instancecheck__(instance))&#xa;&#xa;&#xa;def _replace_forward_references(t, context):&#xa;  """"""Replace forward references in the given type.""""""&#xa;  if isinstance(t, str):&#xa;    return context[t]&#xa;  elif isinstance(t, Type):&#xa;    return type(t)(*[_replace_forward_references(t, context) for t in t._types])  # pylint: disable=protected-access&#xa;  else:&#xa;    return t&#xa;&#xa;&#xa;def register_type_abbreviation(name, alias):&#xa;  """"""Register an abbreviation for a type in typecheck tracebacks.&#xa;&#xa;  This makes otherwise very long typecheck errors much more readable.&#xa;&#xa;  Example:&#xa;    typecheck.register_type_abbreviation(tf.Dimension, 'tf.Dimension')&#xa;&#xa;  Args:&#xa;    name: type or class to abbreviate.&#xa;    alias: string alias to substitute.&#xa;  """"""&#xa;  _TYPE_ABBREVIATIONS[name] = alias&#xa;&#xa;&#xa;def _type_repr(t):&#xa;  """"""A more succinct repr for typecheck tracebacks.""""""&#xa;  string = repr(t)&#xa;  for type_, alias in _TYPE_ABBREVIATIONS.items():&#xa;    string = string.replace(repr(type_), alias)&#xa;  string = re.sub(r""<(class|type) '([\w.]+)'>"", r""\2"", string)&#xa;  string = re.sub(r""typecheck\.(\w+)"", r""\1"", string)&#xa;  return string&#xa;&#xa;&#xa;class Error(TypeError):&#xa;  """"""Exception for typecheck failures.""""""&#xa;&#xa;&#xa;def accepts(*types):&#xa;  """"""A decorator which checks the input types of a function.&#xa;&#xa;  Based on:&#xa;  http://stackoverflow.com/questions/15299878/how-to-use-python-decorators-to-check-function-arguments&#xa;  The above draws from:&#xa;  https://www.python.org/dev/peps/pep-0318/&#xa;&#xa;  Args:&#xa;    *types: A list of Python types.&#xa;&#xa;  Returns:&#xa;    A function to use as a decorator.&#xa;  """"""&#xa;&#xa;  def check_accepts(f):&#xa;    """"""Check the types.""""""&#xa;    spec = tf_inspect.getargspec(f)&#xa;&#xa;    num_function_arguments = len(spec.args)&#xa;    if len(types) != num_function_arguments:&#xa;      raise Error(&#xa;          ""Function %r has %d arguments but only %d types were provided in the ""&#xa;          ""annotation."" % (f, num_function_arguments, len(types)))&#xa;&#xa;    if spec.defaults:&#xa;      num_defaults = len(spec.defaults)&#xa;      for (name, a, t) in zip(spec.args[-num_defaults:],&#xa;                              spec.defaults,&#xa;                              types[-num_defaults:]):&#xa;        allowed_type = _replace_forward_references(t, f.__globals__)&#xa;        if not isinstance(a, allowed_type):&#xa;          raise Error(""default argument value %r of type %r is not an instance ""&#xa;                      ""of the allowed type %s for the %s argument to %r""&#xa;                      % (a, type(a), _type_repr(allowed_type), name, f))&#xa;&#xa;    @functools.wraps(f)&#xa;    def new_f(*args, **kwds):&#xa;      """"""A helper function.""""""&#xa;      for (a, t) in zip(args, types):&#xa;        allowed_type = _replace_forward_references(t, f.__globals__)&#xa;        if not isinstance(a, allowed_type):&#xa;          raise Error(""%r of type %r is not an instance of the allowed type %s ""&#xa;                      ""for %r"" % (a, type(a), _type_repr(allowed_type), f))&#xa;      return f(*args, **kwds)&#xa;&#xa;    return new_f&#xa;&#xa;  return check_accepts&#xa;&#xa;&#xa;def returns(*types):&#xa;  """"""A decorator which checks the return types of a function.&#xa;&#xa;  Based on:&#xa;  http://stackoverflow.com/questions/15299878/how-to-use-python-decorators-to-check-function-arguments&#xa;  The above draws from:&#xa;  https://www.python.org/dev/peps/pep-0318/&#xa;&#xa;  Args:&#xa;    *types: A list of Python types.&#xa;      A list of one element corresponds to a single return value.&#xa;      A list of several elements corresponds to several return values.&#xa;      Note that a function with no explicit return value has an implicit&#xa;      NoneType return and should be annotated correspondingly.&#xa;&#xa;  Returns:&#xa;    A function to use as a decorator.&#xa;  """"""&#xa;&#xa;  def check_returns(f):&#xa;    """"""Check the types.""""""&#xa;    if not types:&#xa;      raise TypeError(""A return type annotation must contain at least one type"")&#xa;&#xa;    @functools.wraps(f)&#xa;    def new_f(*args, **kwds):&#xa;      """"""A helper function.""""""&#xa;      return_value = f(*args, **kwds)&#xa;&#xa;      if len(types) == 1:&#xa;        # The function has a single return value.&#xa;        allowed_type = _replace_forward_references(types[0], f.__globals__)&#xa;        if not isinstance(return_value, allowed_type):&#xa;          raise Error(""%r of type %r is not an instance of the allowed type %s ""&#xa;                      ""for %r""&#xa;                      % (return_value, type(return_value),&#xa;                         _type_repr(allowed_type), f))&#xa;&#xa;      else:&#xa;        if len(return_value) != len(types):&#xa;          raise Error(&#xa;              ""Function %r has %d return values but only %d types were ""&#xa;              ""provided in the annotation."" %&#xa;              (f, len(return_value), len(types)))&#xa;&#xa;        for (r, t) in zip(return_value, types):&#xa;          allowed_type = _replace_forward_references(t, f.__globals__)&#xa;          if not isinstance(r, allowed_type):&#xa;            raise Error(""%r of type %r is not an instance of allowed type %s ""&#xa;                        ""for %r"" % (r, type(r), _type_repr(allowed_type), f))&#xa;&#xa;      return return_value&#xa;&#xa;    return new_f&#xa;&#xa;  return check_returns&#xa;"
1189781|"# Copyright (c) 2012 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;&#xa;def ensure_directory_exists(path):&#xa;  dir = os.path.dirname(path)&#xa;  if dir and not os.path.exists(dir):&#xa;    os.makedirs(dir)&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;# TODO(thakis): Find out and document the difference between shared_library and&#xa;# loadable_module on mac.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;# TODO(thakis): The solink_module rule is likely wrong. Xcode seems to pass&#xa;# -bundle -single_module here (for osmesa.so).&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;#&#xa;# Note: flock is used to seralize linking. Linking is a memory-intensive&#xa;# process so running parallel links can often lead to thrashing.  To disable&#xa;# the serialization, override LINK via an envrionment variable as follows:&#xa;#&#xa;#   export LINK=g++&#xa;#&#xa;# This will allow make to invoke N linker processes as specified in -jN.&#xa;LINK ?= %(flock)s $(builddir)/linker.lock $(CXX.target)&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?=&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?=&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = ln -f ""$<"" ""$@"" 2>/dev/null || (rm -rf ""$@"" && cp -af ""$<"" ""$@"")&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds, and deletes the output file when done&#xa;# if any of the postbuilds failed.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    F=$$?;\\&#xa;    if [ $$F -ne 0 ]; then\\&#xa;      E=$$F;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;SHARED_HEADER_SUN_COMMANDS = """"""&#xa;# gyp-sun-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_sun_tool = SUNTOOL $(4) $<&#xa;cmd_sun_tool = ./gyp-sun-tool $(4) $< ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter:&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    ensure_directory_exists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    ensure_directory_exists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs + ['FORCE_DO_CMD'], actions)&#xa;        for output in outputs:&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                      part_of_all=True)&#xa;      bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.GetTargetPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    # TODO(evanm): just make order_only a list of deps instead of these hacks.&#xa;    if order_only:&#xa;      order_insert = '| '&#xa;      pick_output = ' '.join(outputs)&#xa;    else:&#xa;      order_insert = ''&#xa;      pick_output = outputs[0]&#xa;    if force:&#xa;      force_append = ' FORCE_DO_CMD'&#xa;    else:&#xa;      force_append = ''&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    self.WriteLn('%s: %s%s%s' % (pick_output, order_insert, ' '.join(inputs),&#xa;                                 force_append))&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    if not order_only and len(outputs) > 1:&#xa;      # If we have more than one output, a rule like&#xa;      #   foo bar: baz&#xa;      # that for *each* output we must run the action, potentially&#xa;      # in parallel.  That is not what we're trying to write -- what&#xa;      # we want is that we run the action once and it generates all&#xa;      # the files.&#xa;      # http://www.gnu.org/software/hello/manual/automake/Multiple-Outputs.html&#xa;      # discusses this problem and has this solution:&#xa;      # 1) Write the naive rule that would produce parallel runs of&#xa;      # the action.&#xa;      # 2) Make the outputs seralized on each other, so we won't start&#xa;      # a parallel run until the first run finishes, at which point&#xa;      # we'll have generated all the outputs and we're done.&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs[1:]), outputs[0]))&#xa;      # Add a dummy command to the ""extra outputs"" rule, otherwise make seems to&#xa;      # think these outputs haven't (couldn't have?) changed, and thus doesn't&#xa;      # flag them as changed (i.e. include in '$?') when evaluating dependent&#xa;      # rules, which in turn causes do_cmd() to skip running dependent commands.&#xa;      self.WriteLn('%s: ;' % (' '.join(outputs[1:])))&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;    if (self.type == 'shared_library' and&#xa;        (self.flavor != 'mac' or self.toolset != 'target')):&#xa;      # Install all shared libs into a common directory (per toolset) for&#xa;      # convenient access with LD_LIBRARY_PATH.&#xa;      return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(options.generator_output, output_file)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(options.generator_output, makefile_path)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-sun-tool flock',&#xa;        'flock_index': 2,&#xa;        'extra_commands': SHARED_HEADER_SUN_COMMANDS,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LD_target', 'LD'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host',), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host',), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host',), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LD_host',), 'g++'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    if key == 'LINK':&#xa;      make_global_settings += ('%s ?= %s $(builddir)/linker.lock %s\n' %&#xa;                               (key, flock_command, value))&#xa;    elif key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      if key in os.environ:&#xa;        value = os.environ[key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  ensure_directory_exists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets."")&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
2572172|"# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).&#xa;# Licensed under the Apache License, Version 2.0 (see LICENSE).&#xa;&#xa;from __future__ import absolute_import, print_function&#xa;&#xa;import os&#xa;import subprocess&#xa;import sys&#xa;from contextlib import contextmanager&#xa;from distutils import sysconfig&#xa;from site import USER_SITE&#xa;&#xa;import pkg_resources&#xa;from pkg_resources import EntryPoint, WorkingSet, find_distributions&#xa;&#xa;from .common import die&#xa;from .compatibility import exec_function&#xa;from .environment import PEXEnvironment&#xa;from .finders import get_entry_point_from_console_script, get_script_from_distributions&#xa;from .interpreter import PythonInterpreter&#xa;from .orderedset import OrderedSet&#xa;from .pex_info import PexInfo&#xa;from .tracer import TRACER&#xa;from .variables import ENV&#xa;&#xa;&#xa;class DevNull(object):&#xa;  def __init__(self):&#xa;    pass&#xa;&#xa;  def write(self, *args, **kw):&#xa;    pass&#xa;&#xa;  def flush(self):&#xa;    pass&#xa;&#xa;&#xa;class PEX(object):  # noqa: T000&#xa;  """"""PEX, n. A self-contained python environment.""""""&#xa;&#xa;  class Error(Exception): pass&#xa;  class NotFound(Error): pass&#xa;&#xa;  @classmethod&#xa;  def clean_environment(cls):&#xa;    try:&#xa;      del os.environ['MACOSX_DEPLOYMENT_TARGET']&#xa;    except KeyError:&#xa;      pass&#xa;    # Cannot change dictionary size during __iter__&#xa;    filter_keys = [key for key in os.environ if key.startswith('PEX_')]&#xa;    for key in filter_keys:&#xa;      del os.environ[key]&#xa;&#xa;  def __init__(self, pex=sys.argv[0], interpreter=None, env=ENV):&#xa;    self._pex = pex&#xa;    self._interpreter = interpreter or PythonInterpreter.get()&#xa;    self._pex_info = PexInfo.from_pex(self._pex)&#xa;    self._pex_info_overrides = PexInfo.from_env(env=env)&#xa;    self._vars = env&#xa;    self._envs = []&#xa;    self._working_set = None&#xa;&#xa;  def _activate(self):&#xa;    if not self._working_set:&#xa;      working_set = WorkingSet([])&#xa;&#xa;      # set up the local .pex environment&#xa;      pex_info = self._pex_info.copy()&#xa;      pex_info.update(self._pex_info_overrides)&#xa;      self._envs.append(PEXEnvironment(self._pex, pex_info))&#xa;&#xa;      # set up other environments as specified in PEX_PATH&#xa;      for pex_path in filter(None, self._vars.PEX_PATH.split(os.pathsep)):&#xa;        pex_info = PexInfo.from_pex(pex_path)&#xa;        pex_info.update(self._pex_info_overrides)&#xa;        self._envs.append(PEXEnvironment(pex_path, pex_info))&#xa;&#xa;      # activate all of them&#xa;      for env in self._envs:&#xa;        for dist in env.activate():&#xa;          working_set.add(dist)&#xa;&#xa;      self._working_set = working_set&#xa;&#xa;    return self._working_set&#xa;&#xa;  @classmethod&#xa;  def _extras_paths(cls):&#xa;    standard_lib = sysconfig.get_python_lib(standard_lib=True)&#xa;    try:&#xa;      makefile = sysconfig.parse_makefile(sysconfig.get_makefile_filename())&#xa;    except (AttributeError, IOError):&#xa;      # This is not available by default in PyPy's distutils.sysconfig or it simply is&#xa;      # no longer available on the system (IOError ENOENT)&#xa;      makefile = {}&#xa;    extras_paths = filter(None, makefile.get('EXTRASPATH', '').split(':'))&#xa;    for path in extras_paths:&#xa;      yield os.path.join(standard_lib, path)&#xa;&#xa;  @classmethod&#xa;  def _site_libs(cls):&#xa;    try:&#xa;      from site import getsitepackages&#xa;      site_libs = set(getsitepackages())&#xa;    except ImportError:&#xa;      site_libs = set()&#xa;    site_libs.update([sysconfig.get_python_lib(plat_specific=False),&#xa;                      sysconfig.get_python_lib(plat_specific=True)])&#xa;    # On windows getsitepackages() returns the python stdlib too.&#xa;    if sys.prefix in site_libs:&#xa;      site_libs.remove(sys.prefix)&#xa;    real_site_libs = set(os.path.realpath(path) for path in site_libs)&#xa;    return site_libs | real_site_libs&#xa;&#xa;  @classmethod&#xa;  def _tainted_path(cls, path, site_libs):&#xa;    paths = frozenset([path, os.path.realpath(path)])&#xa;    return any(path.startswith(site_lib) for site_lib in site_libs for path in paths)&#xa;&#xa;  @classmethod&#xa;  def minimum_sys_modules(cls, site_libs, modules=None):&#xa;    """"""Given a set of site-packages paths, return a ""clean"" sys.modules.&#xa;&#xa;    When importing site, modules within sys.modules have their __path__'s populated with&#xa;    additional paths as defined by *-nspkg.pth in site-packages, or alternately by distribution&#xa;    metadata such as *.dist-info/namespace_packages.txt.  This can possibly cause namespace&#xa;    packages to leak into imports despite being scrubbed from sys.path.&#xa;&#xa;    NOTE: This method mutates modules' __path__ attributes in sys.module, so this is currently an&#xa;    irreversible operation.&#xa;    """"""&#xa;&#xa;    modules = modules or sys.modules&#xa;    new_modules = {}&#xa;&#xa;    for module_name, module in modules.items():&#xa;      # builtins can stay&#xa;      if not hasattr(module, '__path__'):&#xa;        new_modules[module_name] = module&#xa;        continue&#xa;&#xa;      # Unexpected objects, e.g. namespace packages, should just be dropped:&#xa;      if not isinstance(module.__path__, list):&#xa;        TRACER.log('Dropping %s' % (module_name,), V=3)&#xa;        continue&#xa;&#xa;      # Pop off site-impacting __path__ elements in-place.&#xa;      for k in reversed(range(len(module.__path__))):&#xa;        if cls._tainted_path(module.__path__[k], site_libs):&#xa;          TRACER.log('Scrubbing %s.__path__: %s' % (module_name, module.__path__[k]), V=3)&#xa;          module.__path__.pop(k)&#xa;&#xa;      # It still contains path elements not in site packages, so it can stay in sys.modules&#xa;      if module.__path__:&#xa;        new_modules[module_name] = module&#xa;&#xa;    return new_modules&#xa;&#xa;  @classmethod&#xa;  def minimum_sys_path(cls, site_libs):&#xa;    site_distributions = OrderedSet()&#xa;    user_site_distributions = OrderedSet()&#xa;&#xa;    def all_distribution_paths(path):&#xa;      locations = set(dist.location for dist in find_distributions(path))&#xa;      return set([path]) | locations | set(os.path.realpath(path) for path in locations)&#xa;&#xa;    for path_element in sys.path:&#xa;      if cls._tainted_path(path_element, site_libs):&#xa;        TRACER.log('Tainted path element: %s' % path_element)&#xa;        site_distributions.update(all_distribution_paths(path_element))&#xa;      else:&#xa;        TRACER.log('Not a tainted path element: %s' % path_element, V=2)&#xa;&#xa;    user_site_distributions.update(all_distribution_paths(USER_SITE))&#xa;&#xa;    for path in site_distributions:&#xa;      TRACER.log('Scrubbing from site-packages: %s' % path)&#xa;&#xa;    for path in user_site_distributions:&#xa;      TRACER.log('Scrubbing from user site: %s' % path)&#xa;&#xa;    scrub_paths = site_distributions | user_site_distributions&#xa;    scrubbed_sys_path = list(OrderedSet(sys.path) - scrub_paths)&#xa;    scrub_from_importer_cache = filter(&#xa;      lambda key: any(key.startswith(path) for path in scrub_paths),&#xa;      sys.path_importer_cache.keys())&#xa;    scrubbed_importer_cache = dict((key, value) for (key, value) in sys.path_importer_cache.items()&#xa;      if key not in scrub_from_importer_cache)&#xa;&#xa;    for importer_cache_entry in scrub_from_importer_cache:&#xa;      TRACER.log('Scrubbing from path_importer_cache: %s' % importer_cache_entry, V=2)&#xa;&#xa;    return scrubbed_sys_path, scrubbed_importer_cache&#xa;&#xa;  @classmethod&#xa;  def minimum_sys(cls):&#xa;    """"""Return the minimum sys necessary to run this interpreter, a la python -S.&#xa;&#xa;    :returns: (sys.path, sys.path_importer_cache, sys.modules) tuple of a&#xa;      bare python installation.&#xa;    """"""&#xa;    site_libs = set(cls._site_libs())&#xa;    for site_lib in site_libs:&#xa;      TRACER.log('Found site-library: %s' % site_lib)&#xa;    for extras_path in cls._extras_paths():&#xa;      TRACER.log('Found site extra: %s' % extras_path)&#xa;      site_libs.add(extras_path)&#xa;    site_libs = set(os.path.normpath(path) for path in site_libs)&#xa;&#xa;    sys_path, sys_path_importer_cache = cls.minimum_sys_path(site_libs)&#xa;    sys_modules = cls.minimum_sys_modules(site_libs)&#xa;&#xa;    return sys_path, sys_path_importer_cache, sys_modules&#xa;&#xa;  @classmethod&#xa;  @contextmanager&#xa;  def patch_pkg_resources(cls, working_set):&#xa;    """"""Patch pkg_resources given a new working set.""""""&#xa;    def patch(working_set):&#xa;      pkg_resources.working_set = working_set&#xa;      pkg_resources.require = working_set.require&#xa;      pkg_resources.iter_entry_points = working_set.iter_entry_points&#xa;      pkg_resources.run_script = pkg_resources.run_main = working_set.run_script&#xa;      pkg_resources.add_activation_listener = working_set.subscribe&#xa;&#xa;    old_working_set = pkg_resources.working_set&#xa;    patch(working_set)&#xa;    try:&#xa;      yield&#xa;    finally:&#xa;      patch(old_working_set)&#xa;&#xa;  # Thar be dragons -- when this contextmanager exits, the interpreter is&#xa;  # potentially in a wonky state since the patches here (minimum_sys_modules&#xa;  # for example) actually mutate global state.  This should not be&#xa;  # considered a reversible operation despite being a contextmanager.&#xa;  @classmethod&#xa;  @contextmanager&#xa;  def patch_sys(cls):&#xa;    """"""Patch sys with all site scrubbed.""""""&#xa;    def patch_dict(old_value, new_value):&#xa;      old_value.clear()&#xa;      old_value.update(new_value)&#xa;&#xa;    def patch_all(path, path_importer_cache, modules):&#xa;      sys.path[:] = path&#xa;      patch_dict(sys.path_importer_cache, path_importer_cache)&#xa;      patch_dict(sys.modules, modules)&#xa;&#xa;    old_sys_path, old_sys_path_importer_cache, old_sys_modules = (&#xa;        sys.path[:], sys.path_importer_cache.copy(), sys.modules.copy())&#xa;    new_sys_path, new_sys_path_importer_cache, new_sys_modules = cls.minimum_sys()&#xa;&#xa;    patch_all(new_sys_path, new_sys_path_importer_cache, new_sys_modules)&#xa;    yield&#xa;&#xa;  def _wrap_coverage(self, runner, *args):&#xa;    if not self._vars.PEX_COVERAGE and self._vars.PEX_COVERAGE_FILENAME is None:&#xa;      runner(*args)&#xa;      return&#xa;&#xa;    try:&#xa;      import coverage&#xa;    except ImportError:&#xa;      die('Could not bootstrap coverage module, aborting.')&#xa;&#xa;    pex_coverage_filename = self._vars.PEX_COVERAGE_FILENAME&#xa;    if pex_coverage_filename is not None:&#xa;      cov = coverage.coverage(data_file=pex_coverage_filename)&#xa;    else:&#xa;      cov = coverage.coverage(data_suffix=True)&#xa;&#xa;    TRACER.log('Starting coverage.')&#xa;    cov.start()&#xa;&#xa;    try:&#xa;      runner(*args)&#xa;    finally:&#xa;      TRACER.log('Stopping coverage')&#xa;      cov.stop()&#xa;&#xa;      # TODO(wickman) Post-process coverage to elide $PEX_ROOT and make&#xa;      # the report more useful/less noisy.  #89&#xa;      if pex_coverage_filename:&#xa;        cov.save()&#xa;      else:&#xa;        cov.report(show_missing=False, ignore_errors=True, file=sys.stdout)&#xa;&#xa;  def _wrap_profiling(self, runner, *args):&#xa;    if not self._vars.PEX_PROFILE and self._vars.PEX_PROFILE_FILENAME is None:&#xa;      runner(*args)&#xa;      return&#xa;&#xa;    pex_profile_filename = self._vars.PEX_PROFILE_FILENAME&#xa;    pex_profile_sort = self._vars.PEX_PROFILE_SORT&#xa;    try:&#xa;      import cProfile as profile&#xa;    except ImportError:&#xa;      import profile&#xa;&#xa;    profiler = profile.Profile()&#xa;&#xa;    try:&#xa;      return profiler.runcall(runner, *args)&#xa;    finally:&#xa;      if pex_profile_filename is not None:&#xa;        profiler.dump_stats(pex_profile_filename)&#xa;      else:&#xa;        profiler.print_stats(sort=pex_profile_sort)&#xa;&#xa;  def execute(self):&#xa;    """"""Execute the PEX.&#xa;&#xa;    This function makes assumptions that it is the last function called by&#xa;    the interpreter.&#xa;    """"""&#xa;    teardown_verbosity = self._vars.PEX_TEARDOWN_VERBOSE&#xa;    try:&#xa;      with self.patch_sys():&#xa;        working_set = self._activate()&#xa;        TRACER.log('PYTHONPATH contains:')&#xa;        for element in sys.path:&#xa;          TRACER.log('  %c %s' % (' ' if os.path.exists(element) else '*', element))&#xa;        TRACER.log('  * - paths that do not exist or will be imported via zipimport')&#xa;        with self.patch_pkg_resources(working_set):&#xa;          self._wrap_coverage(self._wrap_profiling, self._execute)&#xa;    except Exception:&#xa;      # Allow the current sys.excepthook to handle this app exception before we tear things down in&#xa;      # finally, then reraise so that the exit status is reflected correctly.&#xa;      sys.excepthook(*sys.exc_info())&#xa;      raise&#xa;    except SystemExit as se:&#xa;      # Print a SystemExit error message, avoiding a traceback in python3.&#xa;      # This must happen here, as sys.stderr is about to be torn down&#xa;      if not isinstance(se.code, int) and se.code is not None:&#xa;        print(se.code, file=sys.stderr)&#xa;      raise&#xa;    finally:&#xa;      # squash all exceptions on interpreter teardown -- the primary type here are&#xa;      # atexit handlers failing to run because of things such as:&#xa;      #   http://stackoverflow.com/questions/2572172/referencing-other-modules-in-atexit&#xa;      if not teardown_verbosity:&#xa;        sys.stderr.flush()&#xa;        sys.stderr = DevNull()&#xa;        sys.excepthook = lambda *a, **kw: None&#xa;&#xa;  def _execute(self):&#xa;    force_interpreter = self._vars.PEX_INTERPRETER&#xa;&#xa;    self.clean_environment()&#xa;&#xa;    if force_interpreter:&#xa;      TRACER.log('PEX_INTERPRETER specified, dropping into interpreter')&#xa;      return self.execute_interpreter()&#xa;&#xa;    if self._pex_info_overrides.script and self._pex_info_overrides.entry_point:&#xa;      die('Cannot specify both script and entry_point for a PEX!')&#xa;&#xa;    if self._pex_info.script and self._pex_info.entry_point:&#xa;      die('Cannot specify both script and entry_point for a PEX!')&#xa;&#xa;    if self._pex_info_overrides.script:&#xa;      return self.execute_script(self._pex_info_overrides.script)&#xa;    elif self._pex_info_overrides.entry_point:&#xa;      return self.execute_entry(self._pex_info_overrides.entry_point)&#xa;    elif self._pex_info.script:&#xa;      return self.execute_script(self._pex_info.script)&#xa;    elif self._pex_info.entry_point:&#xa;      return self.execute_entry(self._pex_info.entry_point)&#xa;    else:&#xa;      TRACER.log('No entry point specified, dropping into interpreter')&#xa;      return self.execute_interpreter()&#xa;&#xa;  def execute_interpreter(self):&#xa;    if sys.argv[1:]:&#xa;      try:&#xa;        with open(sys.argv[1]) as fp:&#xa;          name, content = sys.argv[1], fp.read()&#xa;      except IOError as e:&#xa;        die(""Could not open %s in the environment [%s]: %s"" % (sys.argv[1], sys.argv[0], e))&#xa;      sys.argv = sys.argv[1:]&#xa;      self.execute_content(name, content)&#xa;    else:&#xa;      import code&#xa;      code.interact()&#xa;&#xa;  def execute_script(self, script_name):&#xa;    dists = list(self._activate())&#xa;&#xa;    entry_point = get_entry_point_from_console_script(script_name, dists)&#xa;    if entry_point:&#xa;      return self.execute_entry(entry_point)&#xa;&#xa;    dist, script_path, script_content = get_script_from_distributions(script_name, dists)&#xa;    if not dist:&#xa;      raise self.NotFound('Could not find script %s in pex!' % script_name)&#xa;    TRACER.log('Found script %s in %s' % (script_name, dist))&#xa;    return self.execute_content(script_path, script_content, argv0=script_name)&#xa;&#xa;  @classmethod&#xa;  def execute_content(cls, name, content, argv0=None):&#xa;    argv0 = argv0 or name&#xa;    try:&#xa;      ast = compile(content, name, 'exec', flags=0, dont_inherit=1)&#xa;    except SyntaxError:&#xa;      die('Unable to parse %s.  PEX script support only supports Python scripts.' % name)&#xa;    old_name, old_file = globals().get('__name__'), globals().get('__file__')&#xa;    try:&#xa;      old_argv0, sys.argv[0] = sys.argv[0], argv0&#xa;      globals()['__name__'] = '__main__'&#xa;      globals()['__file__'] = name&#xa;      exec_function(ast, globals())&#xa;    finally:&#xa;      if old_name:&#xa;        globals()['__name__'] = old_name&#xa;      else:&#xa;        globals().pop('__name__')&#xa;      if old_file:&#xa;        globals()['__file__'] = old_file&#xa;      else:&#xa;        globals().pop('__file__')&#xa;      sys.argv[0] = old_argv0&#xa;&#xa;  @classmethod&#xa;  def execute_entry(cls, entry_point):&#xa;    runner = cls.execute_pkg_resources if ':' in entry_point else cls.execute_module&#xa;    runner(entry_point)&#xa;&#xa;  @staticmethod&#xa;  def execute_module(module_name):&#xa;    import runpy&#xa;    runpy.run_module(module_name, run_name='__main__')&#xa;&#xa;  @staticmethod&#xa;  def execute_pkg_resources(spec):&#xa;    entry = EntryPoint.parse(""run = {0}"".format(spec))&#xa;&#xa;    # See https://pythonhosted.org/setuptools/history.html#id25 for rationale here.&#xa;    if hasattr(entry, 'resolve'):&#xa;      # setuptools >= 11.3&#xa;      runner = entry.resolve()&#xa;    else:&#xa;      # setuptools < 11.3&#xa;      runner = entry.load(require=False)&#xa;    runner()&#xa;&#xa;  def cmdline(self, args=()):&#xa;    """"""The commandline to run this environment.&#xa;&#xa;    :keyword args: Additional arguments to be passed to the application being invoked by the&#xa;      environment.&#xa;    """"""&#xa;    cmds = [self._interpreter.binary]&#xa;    cmds.append(self._pex)&#xa;    cmds.extend(args)&#xa;    return cmds&#xa;&#xa;  def run(self, args=(), with_chroot=False, blocking=True, setsid=False, **kw):&#xa;    """"""Run the PythonEnvironment in an interpreter in a subprocess.&#xa;&#xa;    :keyword args: Additional arguments to be passed to the application being invoked by the&#xa;      environment.&#xa;    :keyword with_chroot: Run with cwd set to the environment's working directory.&#xa;    :keyword blocking: If true, return the return code of the subprocess.&#xa;      If false, return the Popen object of the invoked subprocess.&#xa;    :keyword setsid: If true, run the PEX in a separate operating system session.&#xa;&#xa;    Remaining keyword arguments are passed directly to subprocess.Popen.&#xa;    """"""&#xa;    self.clean_environment()&#xa;&#xa;    cmdline = self.cmdline(args)&#xa;    TRACER.log('PEX.run invoking %s' % ' '.join(cmdline))&#xa;    process = subprocess.Popen(&#xa;        cmdline,&#xa;        cwd=self._pex if with_chroot else os.getcwd(),&#xa;        preexec_fn=os.setsid if setsid else None,&#xa;        **kw)&#xa;    return process.wait() if blocking else process&#xa;"
12772927|"# -*- coding: utf-8 -*-&#xa;#&#xa;# Read the Docs Template documentation build configuration file, created by&#xa;# sphinx-quickstart on Tue Aug 26 14:19:49 2014.&#xa;#&#xa;# This file is execfile()d with the current directory set to its&#xa;# containing dir.&#xa;#&#xa;# Note that not all possible configuration values are present in this&#xa;# autogenerated file.&#xa;#&#xa;# All configuration values have a default; values that are commented out&#xa;# serve to show the default.&#xa;&#xa;import sys, os&#xa;import re&#xa;from subprocess import call, Popen, PIPE&#xa;import shlex&#xa;&#xa;# If extensions (or modules to document with autodoc) are in another directory,&#xa;# add these directories to sys.path here. If the directory is relative to the&#xa;# documentation root, use os.path.abspath to make it absolute, like shown here.&#xa;sys.path.insert(0, os.path.abspath('.'))&#xa;&#xa;from repo_util import run_cmd_get_output&#xa;&#xa;# Call Doxygen to get XML files from the header files&#xa;print ""Calling Doxygen to generate latest XML files""&#xa;call('doxygen')&#xa;# Generate 'api_name.inc' files using the XML files by Doxygen&#xa;os.system(""python gen-dxd.py"")&#xa;&#xa;# http://stackoverflow.com/questions/12772927/specifying-an-online-image-in-sphinx-restructuredtext-format&#xa;# &#xa;suppress_warnings = ['image.nonlocal_uri']&#xa;&#xa;# -- General configuration ------------------------------------------------&#xa;&#xa;# If your documentation needs a minimal Sphinx version, state it here.&#xa;#needs_sphinx = '1.0'&#xa;&#xa;# Add any Sphinx extension module names here, as strings. They can be&#xa;# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom&#xa;# ones.&#xa;extensions = ['breathe', 'link-roles' ]&#xa;&#xa;# Breathe extension variables&#xa;breathe_projects = { ""epaper-29-ws"": ""xml/"" }&#xa;breathe_default_project = ""epaper-29-ws""&#xa;&#xa;# Add any paths that contain templates here, relative to this directory.&#xa;templates_path = ['_templates']&#xa;&#xa;# The suffix of source filenames.&#xa;source_suffix = '.rst'&#xa;&#xa;# The encoding of source files.&#xa;#source_encoding = 'utf-8-sig'&#xa;&#xa;# The master toctree document.&#xa;master_doc = 'index'&#xa;&#xa;# General information about the project.&#xa;project = u'epaper-29-ws'&#xa;copyright = u'2016 - 2017, Espressif'&#xa;&#xa;# The version info for the project you're documenting, acts as replacement for&#xa;# |version| and |release|, also used in various other places throughout the&#xa;# built documents.&#xa;#&#xa;&#xa;# Readthedocs largely ignores 'version' and 'release', and displays one of&#xa;# 'latest', tag name, or branch name, depending on the build type.&#xa;# Still, this is useful for non-RTD builds.&#xa;# This is supposed to be ""the short X.Y version"", but it's the only version&#xa;# visible when you open index.html.&#xa;# Display full version to make things less confusing.&#xa;version = run_cmd_get_output('git describe')&#xa;# The full version, including alpha/beta/rc tags.&#xa;# If needed, nearest tag is returned by 'git describe --abbrev=0'.&#xa;release = version&#xa;print 'Version: {0}  Release: {1}'.format(version, release)&#xa;&#xa;# The language for content autogenerated by Sphinx. Refer to documentation&#xa;# for a list of supported languages.&#xa;#language = None&#xa;&#xa;# There are two options for replacing |today|: either, you set today to some&#xa;# non-false value, then it is used:&#xa;#today = ''&#xa;# Else, today_fmt is used as the format for a strftime call.&#xa;#today_fmt = '%B %d, %Y'&#xa;&#xa;# List of patterns, relative to source directory, that match files and&#xa;# directories to ignore when looking for source files.&#xa;exclude_patterns = ['_build']&#xa;&#xa;# The reST default role (used for this markup: `text`) to use for all&#xa;# documents.&#xa;#default_role = None&#xa;&#xa;# If true, '()' will be appended to :func: etc. cross-reference text.&#xa;#add_function_parentheses = True&#xa;&#xa;# If true, the current module name will be prepended to all description&#xa;# unit titles (such as .. function::).&#xa;#add_module_names = True&#xa;&#xa;# If true, sectionauthor and moduleauthor directives will be shown in the&#xa;# output. They are ignored by default.&#xa;#show_authors = False&#xa;&#xa;# The name of the Pygments (syntax highlighting) style to use.&#xa;pygments_style = 'sphinx'&#xa;&#xa;# A list of ignored prefixes for module index sorting.&#xa;#modindex_common_prefix = []&#xa;&#xa;# If true, keep warnings as ""system message"" paragraphs in the built documents.&#xa;#keep_warnings = False&#xa;&#xa;&#xa;# -- Options for HTML output ----------------------------------------------&#xa;&#xa;# The theme to use for HTML and HTML Help pages.  See the documentation for&#xa;# a list of builtin themes.&#xa;html_theme = 'default'&#xa;&#xa;# Theme options are theme-specific and customize the look and feel of a theme&#xa;# further.  For a list of options available for each theme, see the&#xa;# documentation.&#xa;#html_theme_options = {}&#xa;&#xa;# Add any paths that contain custom themes here, relative to this directory.&#xa;#html_theme_path = []&#xa;&#xa;# The name for this set of Sphinx documents.  If None, it defaults to&#xa;# ""<project> v<release> documentation"".&#xa;#html_title = None&#xa;&#xa;# A shorter title for the navigation bar.  Default is the same as html_title.&#xa;#html_short_title = None&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top&#xa;# of the sidebar.&#xa;#html_logo = None&#xa;&#xa;# The name of an image file (within the static path) to use as favicon of the&#xa;# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32&#xa;# pixels large.&#xa;#html_favicon = None&#xa;&#xa;# Add any paths that contain custom static files (such as style sheets) here,&#xa;# relative to this directory. They are copied after the builtin static files,&#xa;# so a file named ""default.css"" will overwrite the builtin ""default.css"".&#xa;html_static_path = []&#xa;&#xa;# Add any extra paths that contain custom files (such as robots.txt or&#xa;# .htaccess) here, relative to this directory. These files are copied&#xa;# directly to the root of the documentation.&#xa;#html_extra_path = []&#xa;&#xa;# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,&#xa;# using the given strftime format.&#xa;#html_last_updated_fmt = '%b %d, %Y'&#xa;&#xa;# If true, SmartyPants will be used to convert quotes and dashes to&#xa;# typographically correct entities.&#xa;#html_use_smartypants = True&#xa;&#xa;# Custom sidebar templates, maps document names to template names.&#xa;#html_sidebars = {}&#xa;&#xa;# Additional templates that should be rendered to pages, maps page names to&#xa;# template names.&#xa;#html_additional_pages = {}&#xa;&#xa;# If false, no module index is generated.&#xa;#html_domain_indices = True&#xa;&#xa;# If false, no index is generated.&#xa;#html_use_index = True&#xa;&#xa;# If true, the index is split into individual pages for each letter.&#xa;#html_split_index = False&#xa;&#xa;# If true, links to the reST sources are added to the pages.&#xa;#html_show_sourcelink = True&#xa;&#xa;# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.&#xa;#html_show_sphinx = True&#xa;&#xa;# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.&#xa;#html_show_copyright = True&#xa;&#xa;# If true, an OpenSearch description file will be output, and all pages will&#xa;# contain a <link> tag referring to it.  The value of this option must be the&#xa;# base URL from which the finished HTML is served.&#xa;#html_use_opensearch = ''&#xa;&#xa;# This is the file name suffix for HTML files (e.g. "".xhtml"").&#xa;#html_file_suffix = None&#xa;&#xa;# Output file base name for HTML help builder.&#xa;htmlhelp_basename = 'ReadtheDocsTemplatedoc'&#xa;&#xa;&#xa;# -- Options for LaTeX output ---------------------------------------------&#xa;&#xa;latex_elements = {&#xa;# The paper size ('letterpaper' or 'a4paper').&#xa;#'papersize': 'letterpaper',&#xa;&#xa;# The font size ('10pt', '11pt' or '12pt').&#xa;#'pointsize': '10pt',&#xa;&#xa;# Additional stuff for the LaTeX preamble.&#xa;#'preamble': '',&#xa;}&#xa;&#xa;# Grouping the document tree into LaTeX files. List of tuples&#xa;# (source start file, target name, title,&#xa;#  author, documentclass [howto, manual, or own class]).&#xa;latex_documents = [&#xa;  ('index', 'ReadtheDocsTemplate.tex', u'Read the Docs Template Documentation',&#xa;   u'Read the Docs', 'manual'),&#xa;]&#xa;&#xa;# The name of an image file (relative to this directory) to place at the top of&#xa;# the title page.&#xa;#latex_logo = None&#xa;&#xa;# For ""manual"" documents, if this is true, then toplevel headings are parts,&#xa;# not chapters.&#xa;#latex_use_parts = False&#xa;&#xa;# If true, show page references after internal links.&#xa;#latex_show_pagerefs = False&#xa;&#xa;# If true, show URL addresses after external links.&#xa;#latex_show_urls = False&#xa;&#xa;# Documents to append as an appendix to all manuals.&#xa;#latex_appendices = []&#xa;&#xa;# If false, no module index is generated.&#xa;#latex_domain_indices = True&#xa;&#xa;&#xa;# -- Options for manual page output ---------------------------------------&#xa;&#xa;# One entry per manual page. List of tuples&#xa;# (source start file, name, description, authors, manual section).&#xa;man_pages = [&#xa;    ('index', 'readthedocstemplate', u'Read the Docs Template Documentation',&#xa;     [u'Read the Docs'], 1)&#xa;]&#xa;&#xa;# If true, show URL addresses after external links.&#xa;#man_show_urls = False&#xa;&#xa;&#xa;# -- Options for Texinfo output -------------------------------------------&#xa;&#xa;# Grouping the document tree into Texinfo files. List of tuples&#xa;# (source start file, target name, title, author,&#xa;#  dir menu entry, description, category)&#xa;texinfo_documents = [&#xa;  ('index', 'ReadtheDocsTemplate', u'Read the Docs Template Documentation',&#xa;   u'Read the Docs', 'ReadtheDocsTemplate', 'One line description of project.',&#xa;   'Miscellaneous'),&#xa;]&#xa;&#xa;# Documents to append as an appendix to all manuals.&#xa;#texinfo_appendices = []&#xa;&#xa;# If false, no module index is generated.&#xa;#texinfo_domain_indices = True&#xa;&#xa;# How to display URL addresses: 'footnote', 'no', or 'inline'.&#xa;#texinfo_show_urls = 'footnote'&#xa;&#xa;# If true, do not generate a @detailmenu in the ""Top"" node's menu.&#xa;#texinfo_no_detailmenu = False&#xa;&#xa;# -- Use sphinx_rtd_theme for local builds --------------------------------&#xa;# ref. https://github.com/snide/sphinx_rtd_theme#using-this-theme-locally-then-building-on-read-the-docs&#xa;#&#xa;# on_rtd is whether we are on readthedocs.org&#xa;on_rtd = os.environ.get('READTHEDOCS', None) == 'True'&#xa;&#xa;if not on_rtd:  # only import and set the theme if we're building docs locally&#xa;    import sphinx_rtd_theme&#xa;    html_theme = 'sphinx_rtd_theme'&#xa;    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]&#xa;&#xa;# otherwise, readthedocs.org uses their theme by default, so no need to specify it&#xa;&#xa;"
16837946|""""""" pydevd_vars deals with variables:&#xa;    resolution/conversion to XML.&#xa;""""""&#xa;import pickle&#xa;from _pydevd_bundle.pydevd_constants import dict_contains, get_frame, get_thread_id&#xa;&#xa;from _pydevd_bundle.pydevd_custom_frames import get_custom_frame&#xa;from _pydevd_bundle.pydevd_xml import ExceptionOnEvaluate, get_type, var_to_xml&#xa;from _pydev_imps._pydev_saved_modules import thread&#xa;&#xa;try:&#xa;    from StringIO import StringIO&#xa;except ImportError:&#xa;    from io import StringIO&#xa;import sys  # @Reimport&#xa;&#xa;from _pydev_imps._pydev_saved_modules import threading&#xa;import traceback&#xa;from _pydevd_bundle import pydevd_save_locals&#xa;from _pydev_bundle.pydev_imports import Exec, quote, execfile&#xa;from _pydevd_bundle.pydevd_utils import to_string&#xa;&#xa;&#xa;# -------------------------------------------------------------------------- defining true and false for earlier versions&#xa;&#xa;try:&#xa;    __setFalse = False&#xa;except:&#xa;    import __builtin__&#xa;&#xa;    setattr(__builtin__, 'True', 1)&#xa;    setattr(__builtin__, 'False', 0)&#xa;&#xa;&#xa;# ------------------------------------------------------------------------------------------------------ class for errors&#xa;&#xa;class VariableError(RuntimeError): pass&#xa;&#xa;&#xa;class FrameNotFoundError(RuntimeError): pass&#xa;&#xa;&#xa;def _iter_frames(initialFrame):&#xa;    '''NO-YIELD VERSION: Iterates through all the frames starting at the specified frame (which will be the first returned item)'''&#xa;    # cannot use yield&#xa;    frames = []&#xa;&#xa;    while initialFrame is not None:&#xa;        frames.append(initialFrame)&#xa;        initialFrame = initialFrame.f_back&#xa;&#xa;    return frames&#xa;&#xa;&#xa;def dump_frames(thread_id):&#xa;    sys.stdout.write('dumping frames\n')&#xa;    if thread_id != get_thread_id(threading.currentThread()):&#xa;        raise VariableError(""find_frame: must execute on same thread"")&#xa;&#xa;    curFrame = get_frame()&#xa;    for frame in _iter_frames(curFrame):&#xa;        sys.stdout.write('%s\n' % pickle.dumps(frame))&#xa;&#xa;&#xa;# ===============================================================================&#xa;# AdditionalFramesContainer&#xa;# ===============================================================================&#xa;class AdditionalFramesContainer:&#xa;    lock = thread.allocate_lock()&#xa;    additional_frames = {}  # dict of dicts&#xa;&#xa;&#xa;def add_additional_frame_by_id(thread_id, frames_by_id):&#xa;    AdditionalFramesContainer.additional_frames[thread_id] = frames_by_id&#xa;&#xa;&#xa;addAdditionalFrameById = add_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def remove_additional_frame_by_id(thread_id):&#xa;    del AdditionalFramesContainer.additional_frames[thread_id]&#xa;&#xa;&#xa;removeAdditionalFrameById = remove_additional_frame_by_id  # Backward compatibility&#xa;&#xa;&#xa;def has_additional_frames_by_id(thread_id):&#xa;    return dict_contains(AdditionalFramesContainer.additional_frames, thread_id)&#xa;&#xa;&#xa;def get_additional_frames_by_id(thread_id):&#xa;    return AdditionalFramesContainer.additional_frames.get(thread_id)&#xa;&#xa;&#xa;def find_frame(thread_id, frame_id):&#xa;    """""" returns a frame on the thread that has a given frame_id """"""&#xa;    try:&#xa;        curr_thread_id = get_thread_id(threading.currentThread())&#xa;        if thread_id != curr_thread_id:&#xa;            try:&#xa;                return get_custom_frame(thread_id, frame_id)  # I.e.: thread_id could be a stackless frame id + thread_id.&#xa;            except:&#xa;                pass&#xa;&#xa;            raise VariableError(""find_frame: must execute on same thread (%s != %s)"" % (thread_id, curr_thread_id))&#xa;&#xa;        lookingFor = int(frame_id)&#xa;&#xa;        if AdditionalFramesContainer.additional_frames:&#xa;            if dict_contains(AdditionalFramesContainer.additional_frames, thread_id):&#xa;                frame = AdditionalFramesContainer.additional_frames[thread_id].get(lookingFor)&#xa;&#xa;                if frame is not None:&#xa;                    return frame&#xa;&#xa;        curFrame = get_frame()&#xa;        if frame_id == ""*"":&#xa;            return curFrame  # any frame is specified with ""*""&#xa;&#xa;        frameFound = None&#xa;&#xa;        for frame in _iter_frames(curFrame):&#xa;            if lookingFor == id(frame):&#xa;                frameFound = frame&#xa;                del frame&#xa;                break&#xa;&#xa;            del frame&#xa;&#xa;        # Important: python can hold a reference to the frame from the current context&#xa;        # if an exception is raised, so, if we don't explicitly add those deletes&#xa;        # we might have those variables living much more than we'd want to.&#xa;&#xa;        # I.e.: sys.exc_info holding reference to frame that raises exception (so, other places&#xa;        # need to call sys.exc_clear())&#xa;        del curFrame&#xa;&#xa;        if frameFound is None:&#xa;            msgFrames = ''&#xa;            i = 0&#xa;&#xa;            for frame in _iter_frames(get_frame()):&#xa;                i += 1&#xa;                msgFrames += str(id(frame))&#xa;                if i % 5 == 0:&#xa;                    msgFrames += '\n'&#xa;                else:&#xa;                    msgFrames += '  -  '&#xa;&#xa;            errMsg = '''find_frame: frame not found.&#xa;    Looking for thread_id:%s, frame_id:%s&#xa;    Current     thread_id:%s, available frames:&#xa;    %s\n&#xa;    ''' % (thread_id, lookingFor, curr_thread_id, msgFrames)&#xa;&#xa;            sys.stderr.write(errMsg)&#xa;            return None&#xa;&#xa;        return frameFound&#xa;    except:&#xa;        import traceback&#xa;        traceback.print_exc()&#xa;        return None&#xa;&#xa;&#xa;def getVariable(thread_id, frame_id, scope, attrs):&#xa;    """"""&#xa;    returns the value of a variable&#xa;&#xa;    :scope: can be BY_ID, EXPRESSION, GLOBAL, LOCAL, FRAME&#xa;&#xa;    BY_ID means we'll traverse the list of all objects alive to get the object.&#xa;&#xa;    :attrs: after reaching the proper scope, we have to get the attributes until we find&#xa;            the proper location (i.e.: obj\tattr1\tattr2)&#xa;&#xa;    :note: when BY_ID is used, the frame_id is considered the id of the object to find and&#xa;           not the frame (as we don't care about the frame in this case).&#xa;    """"""&#xa;    if scope == 'BY_ID':&#xa;        if thread_id != get_thread_id(threading.currentThread()):&#xa;            raise VariableError(""getVariable: must execute on same thread"")&#xa;&#xa;        try:&#xa;            import gc&#xa;            objects = gc.get_objects()&#xa;        except:&#xa;            pass  # Not all python variants have it.&#xa;        else:&#xa;            frame_id = int(frame_id)&#xa;            for var in objects:&#xa;                if id(var) == frame_id:&#xa;                    if attrs is not None:&#xa;                        attrList = attrs.split('\t')&#xa;                        for k in attrList:&#xa;                            _type, _typeName, resolver = get_type(var)&#xa;                            var = resolver.resolve(var, k)&#xa;&#xa;                    return var&#xa;&#xa;        # If it didn't return previously, we coudn't find it by id (i.e.: alrceady garbage collected).&#xa;        sys.stderr.write('Unable to find object with id: %s\n' % (frame_id,))&#xa;        return None&#xa;&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return {}&#xa;&#xa;    if attrs is not None:&#xa;        attrList = attrs.split('\t')&#xa;    else:&#xa;        attrList = []&#xa;&#xa;    for attr in attrList:&#xa;        attr.replace(""@_@TAB_CHAR@_@"", '\t')&#xa;&#xa;    if scope == 'EXPRESSION':&#xa;        for count in xrange(len(attrList)):&#xa;            if count == 0:&#xa;                # An Expression can be in any scope (globals/locals), therefore it needs to evaluated as an expression&#xa;                var = evaluate_expression(thread_id, frame_id, attrList[count], False)&#xa;            else:&#xa;                _type, _typeName, resolver = get_type(var)&#xa;                var = resolver.resolve(var, attrList[count])&#xa;    else:&#xa;        if scope == ""GLOBAL"":&#xa;            var = frame.f_globals&#xa;            del attrList[0]  # globals are special, and they get a single dummy unused attribute&#xa;        else:&#xa;            # in a frame access both locals and globals as Python does&#xa;            var = {}&#xa;            var.update(frame.f_globals)&#xa;            var.update(frame.f_locals)&#xa;&#xa;        for k in attrList:&#xa;            _type, _typeName, resolver = get_type(var)&#xa;            var = resolver.resolve(var, k)&#xa;&#xa;    return var&#xa;&#xa;&#xa;def resolve_compound_variable(thread_id, frame_id, scope, attrs):&#xa;    """""" returns the value of the compound variable as a dictionary""""""&#xa;&#xa;    var = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        _type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        sys.stderr.write('Error evaluating: thread_id: %s\nframe_id: %s\nscope: %s\nattrs: %s\n' % (&#xa;            thread_id, frame_id, scope, attrs,))&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def resolve_var(var, attrs):&#xa;    attrList = attrs.split('\t')&#xa;&#xa;    for k in attrList:&#xa;        type, _typeName, resolver = get_type(var)&#xa;&#xa;        var = resolver.resolve(var, k)&#xa;&#xa;    try:&#xa;        type, _typeName, resolver = get_type(var)&#xa;        return resolver.get_dictionary(var)&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def custom_operation(thread_id, frame_id, scope, attrs, style, code_or_file, operation_fn_name):&#xa;    """"""&#xa;    We'll execute the code_or_file and then search in the namespace the operation_fn_name to execute with the given var.&#xa;&#xa;    code_or_file: either some code (i.e.: from pprint import pprint) or a file to be executed.&#xa;    operation_fn_name: the name of the operation to execute after the exec (i.e.: pprint)&#xa;    """"""&#xa;    expressionValue = getVariable(thread_id, frame_id, scope, attrs)&#xa;&#xa;    try:&#xa;        namespace = {'__name__': '<custom_operation>'}&#xa;        if style == ""EXECFILE"":&#xa;            namespace['__file__'] = code_or_file&#xa;            execfile(code_or_file, namespace, namespace)&#xa;        else:  # style == EXEC&#xa;            namespace['__file__'] = '<customOperationCode>'&#xa;            Exec(code_or_file, namespace, namespace)&#xa;&#xa;        return str(namespace[operation_fn_name](expressionValue))&#xa;    except:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;def eval_in_context(expression, globals, locals):&#xa;    result = None&#xa;    try:&#xa;        result = eval(expression, globals, locals)&#xa;    except Exception:&#xa;        s = StringIO()&#xa;        traceback.print_exc(file=s)&#xa;        result = s.getvalue()&#xa;&#xa;        try:&#xa;            try:&#xa;                etype, value, tb = sys.exc_info()&#xa;                result = value&#xa;            finally:&#xa;                etype = value = tb = None&#xa;        except:&#xa;            pass&#xa;&#xa;        result = ExceptionOnEvaluate(result)&#xa;&#xa;        # Ok, we have the initial error message, but let's see if we're dealing with a name mangling error...&#xa;        try:&#xa;            if '__' in expression:&#xa;                # Try to handle '__' name mangling...&#xa;                split = expression.split('.')&#xa;                curr = locals.get(split[0])&#xa;                for entry in split[1:]:&#xa;                    if entry.startswith('__') and not hasattr(curr, entry):&#xa;                        entry = '_%s%s' % (curr.__class__.__name__, entry)&#xa;                    curr = getattr(curr, entry)&#xa;&#xa;                result = curr&#xa;        except:&#xa;            pass&#xa;    return result&#xa;&#xa;&#xa;def evaluate_expression(thread_id, frame_id, expression, doExec):&#xa;    '''returns the result of the evaluated expression&#xa;    @param doExec: determines if we should do an exec or an eval&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    # Not using frame.f_globals because of https://sourceforge.net/tracker2/?func=detail&aid=2541355&group_id=85796&atid=577329&#xa;    # (Names not resolved in generator expression in method)&#xa;    # See message: http://mail.python.org/pipermail/python-list/2009-January/526522.html&#xa;    updated_globals = {}&#xa;    updated_globals.update(frame.f_globals)&#xa;    updated_globals.update(frame.f_locals)  # locals later because it has precedence over the actual globals&#xa;&#xa;    try:&#xa;        expression = str(expression.replace('@LINE@', '\n'))&#xa;&#xa;        if doExec:&#xa;            try:&#xa;                # try to make it an eval (if it is an eval we can print it, otherwise we'll exec it and&#xa;                # it will have whatever the user actually did)&#xa;                compiled = compile(expression, '<string>', 'eval')&#xa;            except:&#xa;                Exec(expression, updated_globals, frame.f_locals)&#xa;                pydevd_save_locals.save_locals(frame)&#xa;            else:&#xa;                result = eval(compiled, updated_globals, frame.f_locals)&#xa;                if result is not None:  # Only print if it's not None (as python does)&#xa;                    sys.stdout.write('%s\n' % (result,))&#xa;            return&#xa;&#xa;        else:&#xa;            return eval_in_context(expression, updated_globals, frame.f_locals)&#xa;    finally:&#xa;        # Should not be kept alive if an exception happens and this frame is kept in the stack.&#xa;        del updated_globals&#xa;        del frame&#xa;&#xa;&#xa;def change_attr_expression(thread_id, frame_id, attr, expression, dbg, value=None):&#xa;    '''Changes some attribute in a given frame.&#xa;    '''&#xa;    frame = find_frame(thread_id, frame_id)&#xa;    if frame is None:&#xa;        return&#xa;&#xa;    try:&#xa;        expression = expression.replace('@LINE@', '\n')&#xa;&#xa;        if dbg.plugin and not value:&#xa;            result = dbg.plugin.change_variable(frame, attr, expression)&#xa;            if result:&#xa;                return result&#xa;&#xa;        if attr[:7] == ""Globals"":&#xa;            attr = attr[8:]&#xa;            if attr in frame.f_globals:&#xa;                if value is None:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_globals[attr] = value&#xa;                return frame.f_globals[attr]&#xa;        else:&#xa;            if pydevd_save_locals.is_save_locals_available():&#xa;                if value is None:&#xa;                    value = eval(expression, frame.f_globals, frame.f_locals)&#xa;                frame.f_locals[attr] = value&#xa;                pydevd_save_locals.save_locals(frame)&#xa;                return frame.f_locals[attr]&#xa;&#xa;            # default way (only works for changing it in the topmost frame)&#xa;            if value is None:&#xa;                value = eval(expression, frame.f_globals, frame.f_locals)&#xa;            result = value&#xa;            Exec('%s=%s' % (attr, expression), frame.f_globals, frame.f_locals)&#xa;            return result&#xa;&#xa;&#xa;    except Exception:&#xa;        traceback.print_exc()&#xa;&#xa;&#xa;MAXIMUM_ARRAY_SIZE = 100&#xa;MAX_SLICE_SIZE = 1000&#xa;&#xa;&#xa;def table_like_struct_to_xml(array, name, roffset, coffset, rows, cols, format):&#xa;    _, type_name, _ = get_type(array)&#xa;    if type_name == 'ndarray':&#xa;        array, metaxml, r, c, f = array_to_meta_xml(array, name, format)&#xa;        xml = metaxml&#xa;        format = '%' + f&#xa;        if rows == -1 and cols == -1:&#xa;            rows = r&#xa;            cols = c&#xa;        xml += array_to_xml(array, roffset, coffset, rows, cols, format)&#xa;    elif type_name == 'DataFrame':&#xa;        xml = dataframe_to_xml(array, name, roffset, coffset, rows, cols, format)&#xa;    else:&#xa;        raise VariableError(""Do not know how to convert type %s to table"" % (type_name))&#xa;&#xa;    return ""<xml>%s</xml>"" % xml&#xa;&#xa;&#xa;def array_to_xml(array, roffset, coffset, rows, cols, format):&#xa;    xml = """"&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(cols, MAXIMUM_ARRAY_SIZE)&#xa;&#xa;    # there is no obvious rule for slicing (at least 5 choices)&#xa;    if len(array) == 1 and (rows > 1 or cols > 1):&#xa;        array = array[0]&#xa;    if array.size > len(array):&#xa;        array = array[roffset:, coffset:]&#xa;        rows = min(rows, len(array))&#xa;        cols = min(cols, len(array[0]))&#xa;        if len(array) == 1:&#xa;            array = array[0]&#xa;    elif array.size == len(array):&#xa;        if roffset == 0 and rows == 1:&#xa;            array = array[coffset:]&#xa;            cols = min(cols, len(array))&#xa;        elif coffset == 0 and cols == 1:&#xa;            array = array[roffset:]&#xa;            rows = min(rows, len(array))&#xa;&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>"" % to_string(row)&#xa;        for col in range(cols):&#xa;            value = array&#xa;            if rows == 1 or cols == 1:&#xa;                if rows == 1 and cols == 1:&#xa;                    value = array[0]&#xa;                else:&#xa;                    if rows == 1:&#xa;                        dim = col&#xa;                    else:&#xa;                        dim = row&#xa;                    value = array[dim]&#xa;                    if ""ndarray"" in str(type(value)):&#xa;                        value = value[0]&#xa;            else:&#xa;                value = array[row][col]&#xa;            value = format % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;&#xa;&#xa;def array_to_meta_xml(array, name, format):&#xa;    type = array.dtype.kind&#xa;    slice = name&#xa;    l = len(array.shape)&#xa;&#xa;    # initial load, compute slice&#xa;    if format == '%':&#xa;        if l > 2:&#xa;            slice += '[0]' * (l - 2)&#xa;            for r in range(l - 2):&#xa;                array = array[0]&#xa;        if type == 'f':&#xa;            format = '.5f'&#xa;        elif type == 'i' or type == 'u':&#xa;            format = 'd'&#xa;        else:&#xa;            format = 's'&#xa;    else:&#xa;        format = format.replace('%', '')&#xa;&#xa;    l = len(array.shape)&#xa;    reslice = """"&#xa;    if l > 2:&#xa;        raise Exception(""%s has more than 2 dimensions."" % slice)&#xa;    elif l == 1:&#xa;        # special case with 1D arrays arr[i, :] - row, but arr[:, i] - column with equal shape and ndim&#xa;        # http://stackoverflow.com/questions/16837946/numpy-a-2-rows-1-column-file-loadtxt-returns-1row-2-columns&#xa;        # explanation: http://stackoverflow.com/questions/15165170/how-do-i-maintain-row-column-orientation-of-vectors-in-numpy?rq=1&#xa;        # we use kind of a hack - get information about memory from C_CONTIGUOUS&#xa;        is_row = array.flags['C_CONTIGUOUS']&#xa;&#xa;        if is_row:&#xa;            rows = 1&#xa;            cols = min(len(array), MAX_SLICE_SIZE)&#xa;            if cols < len(array):&#xa;                reslice = '[0:%s]' % (cols)&#xa;            array = array[0:cols]&#xa;        else:&#xa;            cols = 1&#xa;            rows = min(len(array), MAX_SLICE_SIZE)&#xa;            if rows < len(array):&#xa;                reslice = '[0:%s]' % (rows)&#xa;            array = array[0:rows]&#xa;    elif l == 2:&#xa;        rows = min(array.shape[-2], MAX_SLICE_SIZE)&#xa;        cols = min(array.shape[-1], MAX_SLICE_SIZE)&#xa;        if cols < array.shape[-1] or rows < array.shape[-2]:&#xa;            reslice = '[0:%s, 0:%s]' % (rows, cols)&#xa;        array = array[0:rows, 0:cols]&#xa;&#xa;    # avoid slice duplication&#xa;    if not slice.endswith(reslice):&#xa;        slice += reslice&#xa;&#xa;    bounds = (0, 0)&#xa;    if type in ""biufc"":&#xa;        bounds = (array.min(), array.max())&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""%s\"" type=\""%s\"" max=\""%s\"" min=\""%s\""/>' % \&#xa;          (slice, rows, cols, format, type, bounds[1], bounds[0])&#xa;    return array, xml, rows, cols, format&#xa;&#xa;&#xa;def dataframe_to_xml(df, name, roffset, coffset, rows, cols, format):&#xa;    """"""&#xa;    :type df: pandas.core.frame.DataFrame&#xa;    :type name: str&#xa;    :type coffset: int&#xa;    :type roffset: int&#xa;    :type rows: int&#xa;    :type cols: int&#xa;    :type format: str&#xa;&#xa;&#xa;    """"""&#xa;    num_rows = min(df.shape[0], MAX_SLICE_SIZE)&#xa;    num_cols = min(df.shape[1], MAX_SLICE_SIZE)&#xa;    if (num_rows, num_cols) != df.shape:&#xa;        df = df.iloc[0:num_rows, 0: num_cols]&#xa;        slice = '.iloc[0:%s, 0:%s]' % (num_rows, num_cols)&#xa;    else:&#xa;        slice = ''&#xa;    slice = name + slice&#xa;    xml = '<array slice=\""%s\"" rows=\""%s\"" cols=\""%s\"" format=\""\"" type=\""\"" max=\""0\"" min=\""0\""/>\n' % \&#xa;          (slice, num_rows, num_cols)&#xa;&#xa;    if (rows, cols) == (-1, -1):&#xa;        rows, cols = num_rows, num_cols&#xa;&#xa;    rows = min(rows, MAXIMUM_ARRAY_SIZE)&#xa;    cols = min(min(cols, MAXIMUM_ARRAY_SIZE), num_cols)&#xa;    # need to precompute column bounds here before slicing!&#xa;    col_bounds = [None] * cols&#xa;    for col in range(cols):&#xa;        dtype = df.dtypes.iloc[col].kind&#xa;        if dtype in ""biufc"":&#xa;            cvalues = df.iloc[:, col]&#xa;            bounds = (cvalues.min(), cvalues.max())&#xa;        else:&#xa;            bounds = (0, 0)&#xa;        col_bounds[col] = bounds&#xa;&#xa;    df = df.iloc[roffset: roffset + rows, coffset: coffset + cols]&#xa;    rows, cols = df.shape&#xa;&#xa;    def default_format(type):&#xa;        if type == 'f':&#xa;            return '.5f'&#xa;        elif type == 'i' or type == 'u':&#xa;            return 'd'&#xa;        else:&#xa;            return 's'&#xa;&#xa;    xml += ""<headerdata rows=\""%s\"" cols=\""%s\"">\n"" % (rows, cols)&#xa;    format = format.replace('%', '')&#xa;    col_formats = []&#xa;    for col in range(cols):&#xa;        label = df.axes[1].values[col]&#xa;        if isinstance(label, tuple):&#xa;            label = '/'.join(label)&#xa;        label = str(label)&#xa;        dtype = df.dtypes.iloc[col].kind&#xa;        fmt = format if (dtype == 'f' and format) else default_format(dtype)&#xa;        col_formats.append('%' + fmt)&#xa;        bounds = col_bounds[col]&#xa;&#xa;        xml += '<colheader index=\""%s\"" label=\""%s\"" type=\""%s\"" format=\""%s\"" max=\""%s\"" min=\""%s\"" />\n' % \&#xa;               (str(col), label, dtype, fmt, bounds[1], bounds[0])&#xa;    for row, label in enumerate(iter(df.axes[0])):&#xa;        if isinstance(label, tuple):&#xa;            label = '/'.join(label)&#xa;        xml += ""<rowheader index=\""%s\"" label = \""%s\""/>\n"" % \&#xa;               (str(row), label)&#xa;    xml += ""</headerdata>\n""&#xa;    xml += ""<arraydata rows=\""%s\"" cols=\""%s\""/>\n"" % (rows, cols)&#xa;    for row in range(rows):&#xa;        xml += ""<row index=\""%s\""/>\n"" % str(row)&#xa;        for col in range(cols):&#xa;            value = df.iat[row, col]&#xa;            value = col_formats[col] % value&#xa;            xml += var_to_xml(value, '')&#xa;    return xml&#xa;"
132058|"# -*- coding: utf-8 -*-&#xa;##############################################################################&#xa;#&#xa;#    OpenERP, Open Source Management Solution&#xa;#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).&#xa;#    Copyright (C) 2010-2014 OpenERP s.a. (<http://openerp.com>).&#xa;#&#xa;#    This program is free software: you can redistribute it and/or modify&#xa;#    it under the terms of the GNU Affero General Public License as&#xa;#    published by the Free Software Foundation, either version 3 of the&#xa;#    License, or (at your option) any later version.&#xa;#&#xa;#    This program is distributed in the hope that it will be useful,&#xa;#    but WITHOUT ANY WARRANTY; without even the implied warranty of&#xa;#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the&#xa;#    GNU Affero General Public License for more details.&#xa;#&#xa;#    You should have received a copy of the GNU Affero General Public License&#xa;#    along with this program.  If not, see <http://www.gnu.org/licenses/>.&#xa;#&#xa;##############################################################################&#xa;&#xa;&#xa;""""""&#xa;Miscellaneous tools used by OpenERP.&#xa;""""""&#xa;&#xa;from functools import wraps&#xa;import cProfile&#xa;from contextlib import contextmanager&#xa;import subprocess&#xa;import logging&#xa;import os&#xa;import socket&#xa;import sys&#xa;import threading&#xa;import time&#xa;import werkzeug.utils&#xa;import zipfile&#xa;from collections import defaultdict, Mapping&#xa;from datetime import datetime&#xa;from itertools import islice, izip, groupby&#xa;from lxml import etree&#xa;from which import which&#xa;from threading import local&#xa;import traceback&#xa;&#xa;try:&#xa;    from html2text import html2text&#xa;except ImportError:&#xa;    html2text = None&#xa;&#xa;from config import config&#xa;from cache import *&#xa;from .parse_version import parse_version &#xa;&#xa;import openerp&#xa;# get_encodings, ustr and exception_to_unicode were originally from tools.misc.&#xa;# There are moved to loglevels until we refactor tools.&#xa;from openerp.loglevels import get_encodings, ustr, exception_to_unicode     # noqa&#xa;&#xa;_logger = logging.getLogger(__name__)&#xa;&#xa;# List of etree._Element subclasses that we choose to ignore when parsing XML.&#xa;# We include the *Base ones just in case, currently they seem to be subclasses of the _* ones.&#xa;SKIPPED_ELEMENT_TYPES = (etree._Comment, etree._ProcessingInstruction, etree.CommentBase, etree.PIBase)&#xa;&#xa;def find_in_path(name):&#xa;    try:&#xa;        return which(name)&#xa;    except IOError:&#xa;        return None&#xa;&#xa;def find_pg_tool(name):&#xa;    path = None&#xa;    if config['pg_path'] and config['pg_path'] != 'None':&#xa;        path = config['pg_path']&#xa;    try:&#xa;        return which(name, path=path)&#xa;    except IOError:&#xa;        return None&#xa;&#xa;def exec_pg_command(name, *args):&#xa;    prog = find_pg_tool(name)&#xa;    if not prog:&#xa;        raise Exception('Couldn\'t find %s' % name)&#xa;    args2 = (prog,) + args&#xa;&#xa;    with open(os.devnull) as dn:&#xa;        return subprocess.call(args2, stdout=dn, stderr=subprocess.STDOUT)&#xa;&#xa;def exec_pg_command_pipe(name, *args):&#xa;    prog = find_pg_tool(name)&#xa;    if not prog:&#xa;        raise Exception('Couldn\'t find %s' % name)&#xa;    # on win32, passing close_fds=True is not compatible&#xa;    # with redirecting std[in/err/out]&#xa;    pop = subprocess.Popen((prog,) + args, bufsize= -1,&#xa;          stdin=subprocess.PIPE, stdout=subprocess.PIPE,&#xa;          close_fds=(os.name==""posix""))&#xa;    return pop.stdin, pop.stdout&#xa;&#xa;def exec_command_pipe(name, *args):&#xa;    prog = find_in_path(name)&#xa;    if not prog:&#xa;        raise Exception('Couldn\'t find %s' % name)&#xa;    # on win32, passing close_fds=True is not compatible&#xa;    # with redirecting std[in/err/out]&#xa;    pop = subprocess.Popen((prog,) + args, bufsize= -1,&#xa;          stdin=subprocess.PIPE, stdout=subprocess.PIPE,&#xa;          close_fds=(os.name==""posix""))&#xa;    return pop.stdin, pop.stdout&#xa;&#xa;#----------------------------------------------------------&#xa;# File paths&#xa;#----------------------------------------------------------&#xa;#file_path_root = os.getcwd()&#xa;#file_path_addons = os.path.join(file_path_root, 'addons')&#xa;&#xa;def file_open(name, mode=""r"", subdir='addons', pathinfo=False):&#xa;    """"""Open a file from the OpenERP root, using a subdir folder.&#xa;&#xa;    Example::&#xa;    &#xa;    >>> file_open('hr/report/timesheer.xsl')&#xa;    >>> file_open('addons/hr/report/timesheet.xsl')&#xa;    >>> file_open('../../base/report/rml_template.xsl', subdir='addons/hr/report', pathinfo=True)&#xa;&#xa;    @param name name of the file&#xa;    @param mode file open mode&#xa;    @param subdir subdirectory&#xa;    @param pathinfo if True returns tuple (fileobject, filepath)&#xa;&#xa;    @return fileobject if pathinfo is False else (fileobject, filepath)&#xa;    """"""&#xa;    import openerp.modules as addons&#xa;    adps = addons.module.ad_paths&#xa;    rtp = os.path.normcase(os.path.abspath(config['root_path']))&#xa;&#xa;    basename = name&#xa;&#xa;    if os.path.isabs(name):&#xa;        # It is an absolute path&#xa;        # Is it below 'addons_path' or 'root_path'?&#xa;        name = os.path.normcase(os.path.normpath(name))&#xa;        for root in adps + [rtp]:&#xa;            root = os.path.normcase(os.path.normpath(root)) + os.sep&#xa;            if name.startswith(root):&#xa;                base = root.rstrip(os.sep)&#xa;                name = name[len(base) + 1:]&#xa;                break&#xa;        else:&#xa;            # It is outside the OpenERP root: skip zipfile lookup.&#xa;            base, name = os.path.split(name)&#xa;        return _fileopen(name, mode=mode, basedir=base, pathinfo=pathinfo, basename=basename)&#xa;&#xa;    if name.replace(os.sep, '/').startswith('addons/'):&#xa;        subdir = 'addons'&#xa;        name2 = name[7:]&#xa;    elif subdir:&#xa;        name = os.path.join(subdir, name)&#xa;        if name.replace(os.sep, '/').startswith('addons/'):&#xa;            subdir = 'addons'&#xa;            name2 = name[7:]&#xa;        else:&#xa;            name2 = name&#xa;&#xa;    # First, try to locate in addons_path&#xa;    if subdir:&#xa;        for adp in adps:&#xa;            try:&#xa;                return _fileopen(name2, mode=mode, basedir=adp,&#xa;                                 pathinfo=pathinfo, basename=basename)&#xa;            except IOError:&#xa;                pass&#xa;&#xa;    # Second, try to locate in root_path&#xa;    return _fileopen(name, mode=mode, basedir=rtp, pathinfo=pathinfo, basename=basename)&#xa;&#xa;&#xa;def _fileopen(path, mode, basedir, pathinfo, basename=None):&#xa;    name = os.path.normpath(os.path.join(basedir, path))&#xa;&#xa;    if basename is None:&#xa;        basename = name&#xa;    # Give higher priority to module directories, which is&#xa;    # a more common case than zipped modules.&#xa;    if os.path.isfile(name):&#xa;        fo = open(name, mode)&#xa;        if pathinfo:&#xa;            return fo, name&#xa;        return fo&#xa;&#xa;    # Support for loading modules in zipped form.&#xa;    # This will not work for zipped modules that are sitting&#xa;    # outside of known addons paths.&#xa;    head = os.path.normpath(path)&#xa;    zipname = False&#xa;    while os.sep in head:&#xa;        head, tail = os.path.split(head)&#xa;        if not tail:&#xa;            break&#xa;        if zipname:&#xa;            zipname = os.path.join(tail, zipname)&#xa;        else:&#xa;            zipname = tail&#xa;        zpath = os.path.join(basedir, head + '.zip')&#xa;        if zipfile.is_zipfile(zpath):&#xa;            from cStringIO import StringIO&#xa;            zfile = zipfile.ZipFile(zpath)&#xa;            try:&#xa;                fo = StringIO()&#xa;                fo.write(zfile.read(os.path.join(&#xa;                    os.path.basename(head), zipname).replace(&#xa;                        os.sep, '/')))&#xa;                fo.seek(0)&#xa;                if pathinfo:&#xa;                    return fo, name&#xa;                return fo&#xa;            except Exception:&#xa;                pass&#xa;    # Not found&#xa;    if name.endswith('.rml'):&#xa;        raise IOError('Report %r doesn\'t exist or deleted' % basename)&#xa;    raise IOError('File not found: %s' % basename)&#xa;&#xa;&#xa;#----------------------------------------------------------&#xa;# iterables&#xa;#----------------------------------------------------------&#xa;def flatten(list):&#xa;    """"""Flatten a list of elements into a uniqu list&#xa;    Author: Christophe Simonis (christophe@tinyerp.com)&#xa;&#xa;    Examples::&#xa;    >>> flatten(['a'])&#xa;    ['a']&#xa;    >>> flatten('b')&#xa;    ['b']&#xa;    >>> flatten( [] )&#xa;    []&#xa;    >>> flatten( [[], [[]]] )&#xa;    []&#xa;    >>> flatten( [[['a','b'], 'c'], 'd', ['e', [], 'f']] )&#xa;    ['a', 'b', 'c', 'd', 'e', 'f']&#xa;    >>> t = (1,2,(3,), [4, 5, [6, [7], (8, 9), ([10, 11, (12, 13)]), [14, [], (15,)], []]])&#xa;    >>> flatten(t)&#xa;    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]&#xa;    """"""&#xa;&#xa;    def isiterable(x):&#xa;        return hasattr(x, ""__iter__"")&#xa;&#xa;    r = []&#xa;    for e in list:&#xa;        if isiterable(e):&#xa;            map(r.append, flatten(e))&#xa;        else:&#xa;            r.append(e)&#xa;    return r&#xa;&#xa;def reverse_enumerate(l):&#xa;    """"""Like enumerate but in the other sens&#xa;    &#xa;    Usage::&#xa;    >>> a = ['a', 'b', 'c']&#xa;    >>> it = reverse_enumerate(a)&#xa;    >>> it.next()&#xa;    (2, 'c')&#xa;    >>> it.next()&#xa;    (1, 'b')&#xa;    >>> it.next()&#xa;    (0, 'a')&#xa;    >>> it.next()&#xa;    Traceback (most recent call last):&#xa;      File ""<stdin>"", line 1, in <module>&#xa;    StopIteration&#xa;    """"""&#xa;    return izip(xrange(len(l)-1, -1, -1), reversed(l))&#xa;&#xa;def topological_sort(elems):&#xa;    """""" Return a list of elements sorted so that their dependencies are listed&#xa;    before them in the result.&#xa;&#xa;    :param elems: specifies the elements to sort with their dependencies; it is&#xa;        a dictionary like `{element: dependencies}` where `dependencies` is a&#xa;        collection of elements that must appear before `element`. The elements&#xa;        of `dependencies` are not required to appear in `elems`; they will&#xa;        simply not appear in the result.&#xa;&#xa;    :returns: a list with the keys of `elems` sorted according to their&#xa;        specification.&#xa;    """"""&#xa;    # the algorithm is inspired by [Tarjan 1976],&#xa;    # http://en.wikipedia.org/wiki/Topological_sorting#Algorithms&#xa;    result = []&#xa;    visited = set()&#xa;&#xa;    def visit(n):&#xa;        if n not in visited:&#xa;            visited.add(n)&#xa;            if n in elems:&#xa;                # first visit all dependencies of n, then append n to result&#xa;                map(visit, elems[n])&#xa;                result.append(n)&#xa;&#xa;    map(visit, elems)&#xa;&#xa;    return result&#xa;&#xa;&#xa;class UpdateableStr(local):&#xa;    """""" Class that stores an updateable string (used in wizards)&#xa;    """"""&#xa;&#xa;    def __init__(self, string=''):&#xa;        self.string = string&#xa;&#xa;    def __str__(self):&#xa;        return str(self.string)&#xa;&#xa;    def __repr__(self):&#xa;        return str(self.string)&#xa;&#xa;    def __nonzero__(self):&#xa;        return bool(self.string)&#xa;&#xa;&#xa;class UpdateableDict(local):&#xa;    """"""Stores an updateable dict to use in wizards&#xa;    """"""&#xa;&#xa;    def __init__(self, dict=None):&#xa;        if dict is None:&#xa;            dict = {}&#xa;        self.dict = dict&#xa;&#xa;    def __str__(self):&#xa;        return str(self.dict)&#xa;&#xa;    def __repr__(self):&#xa;        return str(self.dict)&#xa;&#xa;    def clear(self):&#xa;        return self.dict.clear()&#xa;&#xa;    def keys(self):&#xa;        return self.dict.keys()&#xa;&#xa;    def __setitem__(self, i, y):&#xa;        self.dict.__setitem__(i, y)&#xa;&#xa;    def __getitem__(self, i):&#xa;        return self.dict.__getitem__(i)&#xa;&#xa;    def copy(self):&#xa;        return self.dict.copy()&#xa;&#xa;    def iteritems(self):&#xa;        return self.dict.iteritems()&#xa;&#xa;    def iterkeys(self):&#xa;        return self.dict.iterkeys()&#xa;&#xa;    def itervalues(self):&#xa;        return self.dict.itervalues()&#xa;&#xa;    def pop(self, k, d=None):&#xa;        return self.dict.pop(k, d)&#xa;&#xa;    def popitem(self):&#xa;        return self.dict.popitem()&#xa;&#xa;    def setdefault(self, k, d=None):&#xa;        return self.dict.setdefault(k, d)&#xa;&#xa;    def update(self, E, **F):&#xa;        return self.dict.update(E, F)&#xa;&#xa;    def values(self):&#xa;        return self.dict.values()&#xa;&#xa;    def get(self, k, d=None):&#xa;        return self.dict.get(k, d)&#xa;&#xa;    def has_key(self, k):&#xa;        return self.dict.has_key(k)&#xa;&#xa;    def items(self):&#xa;        return self.dict.items()&#xa;&#xa;    def __cmp__(self, y):&#xa;        return self.dict.__cmp__(y)&#xa;&#xa;    def __contains__(self, k):&#xa;        return self.dict.__contains__(k)&#xa;&#xa;    def __delitem__(self, y):&#xa;        return self.dict.__delitem__(y)&#xa;&#xa;    def __eq__(self, y):&#xa;        return self.dict.__eq__(y)&#xa;&#xa;    def __ge__(self, y):&#xa;        return self.dict.__ge__(y)&#xa;&#xa;    def __gt__(self, y):&#xa;        return self.dict.__gt__(y)&#xa;&#xa;    def __hash__(self):&#xa;        return self.dict.__hash__()&#xa;&#xa;    def __iter__(self):&#xa;        return self.dict.__iter__()&#xa;&#xa;    def __le__(self, y):&#xa;        return self.dict.__le__(y)&#xa;&#xa;    def __len__(self):&#xa;        return self.dict.__len__()&#xa;&#xa;    def __lt__(self, y):&#xa;        return self.dict.__lt__(y)&#xa;&#xa;    def __ne__(self, y):&#xa;        return self.dict.__ne__(y)&#xa;&#xa;class currency(float):&#xa;    """""" Deprecate&#xa;    &#xa;    .. warning::&#xa;    &#xa;    Don't use ! Use res.currency.round()&#xa;    """"""&#xa;&#xa;    def __init__(self, value, accuracy=2, rounding=None):&#xa;        if rounding is None:&#xa;            rounding=10**-accuracy&#xa;        self.rounding=rounding&#xa;        self.accuracy=accuracy&#xa;&#xa;    def __new__(cls, value, accuracy=2, rounding=None):&#xa;        return float.__new__(cls, round(value, accuracy))&#xa;&#xa;    #def __str__(self):&#xa;    #   display_value = int(self*(10**(-self.accuracy))/self.rounding)*self.rounding/(10**(-self.accuracy))&#xa;    #   return str(display_value)&#xa;&#xa;def to_xml(s):&#xa;    return s.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;')&#xa;&#xa;def get_iso_codes(lang):&#xa;    if lang.find('_') != -1:&#xa;        if lang.split('_')[0] == lang.split('_')[1].lower():&#xa;            lang = lang.split('_')[0]&#xa;    return lang&#xa;&#xa;ALL_LANGUAGES = {&#xa;        'ab_RU': u'Abkhazian / ',&#xa;        'am_ET': u'Amharic / ',&#xa;        'ar_SY': u'Arabic / ',&#xa;        'bg_BG': u'Bulgarian /  ',&#xa;        'bs_BA': u'Bosnian / bosanski jezik',&#xa;        'ca_ES': u'Catalan / Catal',&#xa;        'cs_CZ': u'Czech / etina',&#xa;        'da_DK': u'Danish / Dansk',&#xa;        'de_DE': u'German / Deutsch',&#xa;        'el_GR': u'Greek / ',&#xa;        'en_CA': u'English (CA)',&#xa;        'en_GB': u'English (UK)',&#xa;        'en_US': u'English (US)',&#xa;        'es_AR': u'Spanish (AR) / Espaol (AR)',&#xa;        'es_BO': u'Spanish (BO) / Espaol (BO)',&#xa;        'es_CL': u'Spanish (CL) / Espaol (CL)',&#xa;        'es_CO': u'Spanish (CO) / Espaol (CO)',&#xa;        'es_CR': u'Spanish (CR) / Espaol (CR)',&#xa;        'es_DO': u'Spanish (DO) / Espaol (DO)',&#xa;        'es_EC': u'Spanish (EC) / Espaol (EC)',&#xa;        'es_ES': u'Spanish / Espaol',&#xa;        'es_GT': u'Spanish (GT) / Espaol (GT)',&#xa;        'es_HN': u'Spanish (HN) / Espaol (HN)',&#xa;        'es_MX': u'Spanish (MX) / Espaol (MX)',&#xa;        'es_NI': u'Spanish (NI) / Espaol (NI)',&#xa;        'es_PA': u'Spanish (PA) / Espaol (PA)',&#xa;        'es_PE': u'Spanish (PE) / Espaol (PE)',&#xa;        'es_PR': u'Spanish (PR) / Espaol (PR)',&#xa;        'es_PY': u'Spanish (PY) / Espaol (PY)',&#xa;        'es_SV': u'Spanish (SV) / Espaol (SV)',&#xa;        'es_UY': u'Spanish (UY) / Espaol (UY)',&#xa;        'es_VE': u'Spanish (VE) / Espaol (VE)',&#xa;        'et_EE': u'Estonian / Eesti keel',&#xa;        'fa_IR': u'Persian / ',&#xa;        'fi_FI': u'Finnish / Suomi',&#xa;        'fr_BE': u'French (BE) / Franais (BE)',&#xa;        'fr_CA': u'French (CA) / Franais (CA)',&#xa;        'fr_CH': u'French (CH) / Franais (CH)',&#xa;        'fr_FR': u'French / Franais',&#xa;        'gl_ES': u'Galician / Galego',&#xa;        'gu_IN': u'Gujarati / ',&#xa;        'he_IL': u'Hebrew / ',&#xa;        'hi_IN': u'Hindi / ',&#xa;        'hr_HR': u'Croatian / hrvatski jezik',&#xa;        'hu_HU': u'Hungarian / Magyar',&#xa;        'id_ID': u'Indonesian / Bahasa Indonesia',&#xa;        'it_IT': u'Italian / Italiano',&#xa;        'iu_CA': u'Inuktitut / ',&#xa;        'ja_JP': u'Japanese / ',&#xa;        'ko_KP': u'Korean (KP) /  (KP)',&#xa;        'ko_KR': u'Korean (KR) /  (KR)',&#xa;        'lo_LA': u'Lao / ',&#xa;        'lt_LT': u'Lithuanian / Lietuvi kalba',&#xa;        'lv_LV': u'Latvian / latvieu valoda',&#xa;        'mk_MK': u'Macedonian /  ',&#xa;        'ml_IN': u'Malayalam / ',&#xa;        'mn_MN': u'Mongolian / ',&#xa;        'nb_NO': u'Norwegian Bokml / Norsk bokml',&#xa;        'nl_NL': u'Dutch / Nederlands',&#xa;        'nl_BE': u'Flemish (BE) / Vlaams (BE)',&#xa;        'oc_FR': u'Occitan (FR, post 1500) / Occitan',&#xa;        'pl_PL': u'Polish / Jzyk polski',&#xa;        'pt_BR': u'Portuguese (BR) / Portugus (BR)',&#xa;        'pt_PT': u'Portuguese / Portugus',&#xa;        'ro_RO': u'Romanian / romn',&#xa;        'ru_RU': u'Russian /  ',&#xa;        'si_LK': u'Sinhalese / ',&#xa;        'sl_SI': u'Slovenian / slovenina',&#xa;        'sk_SK': u'Slovak / Slovensk jazyk',&#xa;        'sq_AL': u'Albanian / Shqip',&#xa;        'sr_RS': u'Serbian (Cyrillic) / ',&#xa;        'sr@latin': u'Serbian (Latin) / srpski',&#xa;        'sv_SE': u'Swedish / svenska',&#xa;        'te_IN': u'Telugu / ',&#xa;        'tr_TR': u'Turkish / Trke',&#xa;        'vi_VN': u'Vietnamese / Ting Vit',&#xa;        'uk_UA': u'Ukrainian / ',&#xa;        'ur_PK': u'Urdu / ',&#xa;        'zh_CN': u'Chinese (CN) / ',&#xa;        'zh_HK': u'Chinese (HK)',&#xa;        'zh_TW': u'Chinese (TW) / ',&#xa;        'th_TH': u'Thai / ',&#xa;        'tlh_TLH': u'Klingon',&#xa;    }&#xa;&#xa;def scan_languages():&#xa;    """""" Returns all languages supported by OpenERP for translation&#xa;&#xa;    :returns: a list of (lang_code, lang_name) pairs&#xa;    :rtype: [(str, unicode)]&#xa;    """"""&#xa;    return sorted(ALL_LANGUAGES.iteritems(), key=lambda k: k[1])&#xa;&#xa;def get_user_companies(cr, user):&#xa;    def _get_company_children(cr, ids):&#xa;        if not ids:&#xa;            return []&#xa;        cr.execute('SELECT id FROM res_company WHERE parent_id IN %s', (tuple(ids),))&#xa;        res = [x[0] for x in cr.fetchall()]&#xa;        res.extend(_get_company_children(cr, res))&#xa;        return res&#xa;    cr.execute('SELECT company_id FROM res_users WHERE id=%s', (user,))&#xa;    user_comp = cr.fetchone()[0]&#xa;    if not user_comp:&#xa;        return []&#xa;    return [user_comp] + _get_company_children(cr, [user_comp])&#xa;&#xa;def mod10r(number):&#xa;    """"""&#xa;    Input number : account or invoice number&#xa;    Output return: the same number completed with the recursive mod10&#xa;    key&#xa;    """"""&#xa;    codec=[0,9,4,6,8,2,7,1,3,5]&#xa;    report = 0&#xa;    result=""""&#xa;    for digit in number:&#xa;        result += digit&#xa;        if digit.isdigit():&#xa;            report = codec[ (int(digit) + report) % 10 ]&#xa;    return result + str((10 - report) % 10)&#xa;&#xa;&#xa;def human_size(sz):&#xa;    """"""&#xa;    Return the size in a human readable format&#xa;    """"""&#xa;    if not sz:&#xa;        return False&#xa;    units = ('bytes', 'Kb', 'Mb', 'Gb')&#xa;    if isinstance(sz,basestring):&#xa;        sz=len(sz)&#xa;    s, i = float(sz), 0&#xa;    while s >= 1024 and i < len(units)-1:&#xa;        s /= 1024&#xa;        i += 1&#xa;    return ""%0.2f %s"" % (s, units[i])&#xa;&#xa;def logged(f):&#xa;    @wraps(f)&#xa;    def wrapper(*args, **kwargs):&#xa;        from pprint import pformat&#xa;&#xa;        vector = ['Call -> function: %r' % f]&#xa;        for i, arg in enumerate(args):&#xa;            vector.append('  arg %02d: %s' % (i, pformat(arg)))&#xa;        for key, value in kwargs.items():&#xa;            vector.append('  kwarg %10s: %s' % (key, pformat(value)))&#xa;&#xa;        timeb4 = time.time()&#xa;        res = f(*args, **kwargs)&#xa;&#xa;        vector.append('  result: %s' % pformat(res))&#xa;        vector.append('  time delta: %s' % (time.time() - timeb4))&#xa;        _logger.debug('\n'.join(vector))&#xa;        return res&#xa;&#xa;    return wrapper&#xa;&#xa;class profile(object):&#xa;    def __init__(self, fname=None):&#xa;        self.fname = fname&#xa;&#xa;    def __call__(self, f):&#xa;        @wraps(f)&#xa;        def wrapper(*args, **kwargs):&#xa;            profile = cProfile.Profile()&#xa;            result = profile.runcall(f, *args, **kwargs)&#xa;            profile.dump_stats(self.fname or (""%s.cprof"" % (f.func_name,)))&#xa;            return result&#xa;&#xa;        return wrapper&#xa;&#xa;__icons_list = ['STOCK_ABOUT', 'STOCK_ADD', 'STOCK_APPLY', 'STOCK_BOLD',&#xa;'STOCK_CANCEL', 'STOCK_CDROM', 'STOCK_CLEAR', 'STOCK_CLOSE', 'STOCK_COLOR_PICKER',&#xa;'STOCK_CONNECT', 'STOCK_CONVERT', 'STOCK_COPY', 'STOCK_CUT', 'STOCK_DELETE',&#xa;'STOCK_DIALOG_AUTHENTICATION', 'STOCK_DIALOG_ERROR', 'STOCK_DIALOG_INFO',&#xa;'STOCK_DIALOG_QUESTION', 'STOCK_DIALOG_WARNING', 'STOCK_DIRECTORY', 'STOCK_DISCONNECT',&#xa;'STOCK_DND', 'STOCK_DND_MULTIPLE', 'STOCK_EDIT', 'STOCK_EXECUTE', 'STOCK_FILE',&#xa;'STOCK_FIND', 'STOCK_FIND_AND_REPLACE', 'STOCK_FLOPPY', 'STOCK_GOTO_BOTTOM',&#xa;'STOCK_GOTO_FIRST', 'STOCK_GOTO_LAST', 'STOCK_GOTO_TOP', 'STOCK_GO_BACK',&#xa;'STOCK_GO_DOWN', 'STOCK_GO_FORWARD', 'STOCK_GO_UP', 'STOCK_HARDDISK',&#xa;'STOCK_HELP', 'STOCK_HOME', 'STOCK_INDENT', 'STOCK_INDEX', 'STOCK_ITALIC',&#xa;'STOCK_JUMP_TO', 'STOCK_JUSTIFY_CENTER', 'STOCK_JUSTIFY_FILL',&#xa;'STOCK_JUSTIFY_LEFT', 'STOCK_JUSTIFY_RIGHT', 'STOCK_MEDIA_FORWARD',&#xa;'STOCK_MEDIA_NEXT', 'STOCK_MEDIA_PAUSE', 'STOCK_MEDIA_PLAY',&#xa;'STOCK_MEDIA_PREVIOUS', 'STOCK_MEDIA_RECORD', 'STOCK_MEDIA_REWIND',&#xa;'STOCK_MEDIA_STOP', 'STOCK_MISSING_IMAGE', 'STOCK_NETWORK', 'STOCK_NEW',&#xa;'STOCK_NO', 'STOCK_OK', 'STOCK_OPEN', 'STOCK_PASTE', 'STOCK_PREFERENCES',&#xa;'STOCK_PRINT', 'STOCK_PRINT_PREVIEW', 'STOCK_PROPERTIES', 'STOCK_QUIT',&#xa;'STOCK_REDO', 'STOCK_REFRESH', 'STOCK_REMOVE', 'STOCK_REVERT_TO_SAVED',&#xa;'STOCK_SAVE', 'STOCK_SAVE_AS', 'STOCK_SELECT_COLOR', 'STOCK_SELECT_FONT',&#xa;'STOCK_SORT_ASCENDING', 'STOCK_SORT_DESCENDING', 'STOCK_SPELL_CHECK',&#xa;'STOCK_STOP', 'STOCK_STRIKETHROUGH', 'STOCK_UNDELETE', 'STOCK_UNDERLINE',&#xa;'STOCK_UNDO', 'STOCK_UNINDENT', 'STOCK_YES', 'STOCK_ZOOM_100',&#xa;'STOCK_ZOOM_FIT', 'STOCK_ZOOM_IN', 'STOCK_ZOOM_OUT',&#xa;'terp-account', 'terp-crm', 'terp-mrp', 'terp-product', 'terp-purchase',&#xa;'terp-sale', 'terp-tools', 'terp-administration', 'terp-hr', 'terp-partner',&#xa;'terp-project', 'terp-report', 'terp-stock', 'terp-calendar', 'terp-graph',&#xa;'terp-check','terp-go-month','terp-go-year','terp-go-today','terp-document-new','terp-camera_test',&#xa;'terp-emblem-important','terp-gtk-media-pause','terp-gtk-stop','terp-gnome-cpu-frequency-applet+',&#xa;'terp-dialog-close','terp-gtk-jump-to-rtl','terp-gtk-jump-to-ltr','terp-accessories-archiver',&#xa;'terp-stock_align_left_24','terp-stock_effects-object-colorize','terp-go-home','terp-gtk-go-back-rtl',&#xa;'terp-gtk-go-back-ltr','terp-personal','terp-personal-','terp-personal+','terp-accessories-archiver-minus',&#xa;'terp-accessories-archiver+','terp-stock_symbol-selection','terp-call-start','terp-dolar',&#xa;'terp-face-plain','terp-folder-blue','terp-folder-green','terp-folder-orange','terp-folder-yellow',&#xa;'terp-gdu-smart-failing','terp-go-week','terp-gtk-select-all','terp-locked','terp-mail-forward',&#xa;'terp-mail-message-new','terp-mail-replied','terp-rating-rated','terp-stage','terp-stock_format-scientific',&#xa;'terp-dolar_ok!','terp-idea','terp-stock_format-default','terp-mail-','terp-mail_delete'&#xa;]&#xa;&#xa;def icons(*a, **kw):&#xa;    global __icons_list&#xa;    return [(x, x) for x in __icons_list ]&#xa;&#xa;def detect_ip_addr():&#xa;    """"""Try a very crude method to figure out a valid external&#xa;       IP or hostname for the current machine. Don't rely on this&#xa;       for binding to an interface, but it could be used as basis&#xa;       for constructing a remote URL to the server.&#xa;    """"""&#xa;    def _detect_ip_addr():&#xa;        from array import array&#xa;        from struct import pack, unpack&#xa;&#xa;        try:&#xa;            import fcntl&#xa;        except ImportError:&#xa;            fcntl = None&#xa;&#xa;        ip_addr = None&#xa;&#xa;        if not fcntl: # not UNIX:&#xa;            host = socket.gethostname()&#xa;            ip_addr = socket.gethostbyname(host)&#xa;        else: # UNIX:&#xa;            # get all interfaces:&#xa;            nbytes = 128 * 32&#xa;            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)&#xa;            names = array('B', '\0' * nbytes)&#xa;            #print 'names: ', names&#xa;            outbytes = unpack('iL', fcntl.ioctl( s.fileno(), 0x8912, pack('iL', nbytes, names.buffer_info()[0])))[0]&#xa;            namestr = names.tostring()&#xa;&#xa;            # try 64 bit kernel:&#xa;            for i in range(0, outbytes, 40):&#xa;                name = namestr[i:i+16].split('\0', 1)[0]&#xa;                if name != 'lo':&#xa;                    ip_addr = socket.inet_ntoa(namestr[i+20:i+24])&#xa;                    break&#xa;&#xa;            # try 32 bit kernel:&#xa;            if ip_addr is None:&#xa;                ifaces = filter(None, [namestr[i:i+32].split('\0', 1)[0] for i in range(0, outbytes, 32)])&#xa;&#xa;                for ifname in [iface for iface in ifaces if iface != 'lo']:&#xa;                    ip_addr = socket.inet_ntoa(fcntl.ioctl(s.fileno(), 0x8915, pack('256s', ifname[:15]))[20:24])&#xa;                    break&#xa;&#xa;        return ip_addr or 'localhost'&#xa;&#xa;    try:&#xa;        ip_addr = _detect_ip_addr()&#xa;    except Exception:&#xa;        ip_addr = 'localhost'&#xa;    return ip_addr&#xa;&#xa;# RATIONALE BEHIND TIMESTAMP CALCULATIONS AND TIMEZONE MANAGEMENT:&#xa;#  The server side never does any timestamp calculation, always&#xa;#  sends them in a naive (timezone agnostic) format supposed to be&#xa;#  expressed within the server timezone, and expects the clients to&#xa;#  provide timestamps in the server timezone as well.&#xa;#  It stores all timestamps in the database in naive format as well,&#xa;#  which also expresses the time in the server timezone.&#xa;#  For this reason the server makes its timezone name available via the&#xa;#  common/timezone_get() rpc method, which clients need to read&#xa;#  to know the appropriate time offset to use when reading/writing&#xa;#  times.&#xa;def get_win32_timezone():&#xa;    """"""Attempt to return the ""standard name"" of the current timezone on a win32 system.&#xa;       @return the standard name of the current win32 timezone, or False if it cannot be found.&#xa;    """"""&#xa;    res = False&#xa;    if sys.platform == ""win32"":&#xa;        try:&#xa;            import _winreg&#xa;            hklm = _winreg.ConnectRegistry(None,_winreg.HKEY_LOCAL_MACHINE)&#xa;            current_tz_key = _winreg.OpenKey(hklm, r""SYSTEM\CurrentControlSet\Control\TimeZoneInformation"", 0,_winreg.KEY_ALL_ACCESS)&#xa;            res = str(_winreg.QueryValueEx(current_tz_key,""StandardName"")[0])  # [0] is value, [1] is type code&#xa;            _winreg.CloseKey(current_tz_key)&#xa;            _winreg.CloseKey(hklm)&#xa;        except Exception:&#xa;            pass&#xa;    return res&#xa;&#xa;def detect_server_timezone():&#xa;    """"""Attempt to detect the timezone to use on the server side.&#xa;       Defaults to UTC if no working timezone can be found.&#xa;       @return the timezone identifier as expected by pytz.timezone.&#xa;    """"""&#xa;    try:&#xa;        import pytz&#xa;    except Exception:&#xa;        _logger.warning(""Python pytz module is not available. ""&#xa;            ""Timezone will be set to UTC by default."")&#xa;        return 'UTC'&#xa;&#xa;    # Option 1: the configuration option (did not exist before, so no backwards compatibility issue)&#xa;    # Option 2: to be backwards compatible with 5.0 or earlier, the value from time.tzname[0], but only if it is known to pytz&#xa;    # Option 3: the environment variable TZ&#xa;    sources = [ (config['timezone'], 'OpenERP configuration'),&#xa;                (time.tzname[0], 'time.tzname'),&#xa;                (os.environ.get('TZ',False),'TZ environment variable'), ]&#xa;    # Option 4: OS-specific: /etc/timezone on Unix&#xa;    if os.path.exists(""/etc/timezone""):&#xa;        tz_value = False&#xa;        try:&#xa;            f = open(""/etc/timezone"")&#xa;            tz_value = f.read(128).strip()&#xa;        except Exception:&#xa;            pass&#xa;        finally:&#xa;            f.close()&#xa;        sources.append((tz_value,""/etc/timezone file""))&#xa;    # Option 5: timezone info from registry on Win32&#xa;    if sys.platform == ""win32"":&#xa;        # Timezone info is stored in windows registry.&#xa;        # However this is not likely to work very well as the standard name&#xa;        # of timezones in windows is rarely something that is known to pytz.&#xa;        # But that's ok, it is always possible to use a config option to set&#xa;        # it explicitly.&#xa;        sources.append((get_win32_timezone(),""Windows Registry""))&#xa;&#xa;    for (value,source) in sources:&#xa;        if value:&#xa;            try:&#xa;                tz = pytz.timezone(value)&#xa;                _logger.info(""Using timezone %s obtained from %s."", tz.zone, source)&#xa;                return value&#xa;            except pytz.UnknownTimeZoneError:&#xa;                _logger.warning(""The timezone specified in %s (%s) is invalid, ignoring it."", source, value)&#xa;&#xa;    _logger.warning(""No valid timezone could be detected, using default UTC ""&#xa;        ""timezone. You can specify it explicitly with option 'timezone' in ""&#xa;        ""the server configuration."")&#xa;    return 'UTC'&#xa;&#xa;def get_server_timezone():&#xa;    return ""UTC""&#xa;&#xa;&#xa;DEFAULT_SERVER_DATE_FORMAT = ""%Y-%m-%d""&#xa;DEFAULT_SERVER_TIME_FORMAT = ""%H:%M:%S""&#xa;DEFAULT_SERVER_DATETIME_FORMAT = ""%s %s"" % (&#xa;    DEFAULT_SERVER_DATE_FORMAT,&#xa;    DEFAULT_SERVER_TIME_FORMAT)&#xa;&#xa;# Python's strftime supports only the format directives&#xa;# that are available on the platform's libc, so in order to&#xa;# be cross-platform we map to the directives required by&#xa;# the C standard (1989 version), always available on platforms&#xa;# with a C standard implementation.&#xa;DATETIME_FORMATS_MAP = {&#xa;        '%C': '', # century&#xa;        '%D': '%m/%d/%Y', # modified %y->%Y&#xa;        '%e': '%d',&#xa;        '%E': '', # special modifier&#xa;        '%F': '%Y-%m-%d',&#xa;        '%g': '%Y', # modified %y->%Y&#xa;        '%G': '%Y',&#xa;        '%h': '%b',&#xa;        '%k': '%H',&#xa;        '%l': '%I',&#xa;        '%n': '\n',&#xa;        '%O': '', # special modifier&#xa;        '%P': '%p',&#xa;        '%R': '%H:%M',&#xa;        '%r': '%I:%M:%S %p',&#xa;        '%s': '', #num of seconds since epoch&#xa;        '%T': '%H:%M:%S',&#xa;        '%t': ' ', # tab&#xa;        '%u': ' %w',&#xa;        '%V': '%W',&#xa;        '%y': '%Y', # Even if %y works, it's ambiguous, so we should use %Y&#xa;        '%+': '%Y-%m-%d %H:%M:%S',&#xa;&#xa;        # %Z is a special case that causes 2 problems at least:&#xa;        #  - the timezone names we use (in res_user.context_tz) come&#xa;        #    from pytz, but not all these names are recognized by&#xa;        #    strptime(), so we cannot convert in both directions&#xa;        #    when such a timezone is selected and %Z is in the format&#xa;        #  - %Z is replaced by an empty string in strftime() when&#xa;        #    there is not tzinfo in a datetime value (e.g when the user&#xa;        #    did not pick a context_tz). The resulting string does not&#xa;        #    parse back if the format requires %Z.&#xa;        # As a consequence, we strip it completely from format strings.&#xa;        # The user can always have a look at the context_tz in&#xa;        # preferences to check the timezone.&#xa;        '%z': '',&#xa;        '%Z': '',&#xa;}&#xa;&#xa;POSIX_TO_LDML = {&#xa;    'a': 'E',&#xa;    'A': 'EEEE',&#xa;    'b': 'MMM',&#xa;    'B': 'MMMM',&#xa;    #'c': '',&#xa;    'd': 'dd',&#xa;    'H': 'HH',&#xa;    'I': 'hh',&#xa;    'j': 'DDD',&#xa;    'm': 'MM',&#xa;    'M': 'mm',&#xa;    'p': 'a',&#xa;    'S': 'ss',&#xa;    'U': 'w',&#xa;    'w': 'e',&#xa;    'W': 'w',&#xa;    'y': 'yy',&#xa;    'Y': 'yyyy',&#xa;    # see comments above, and babel's format_datetime assumes an UTC timezone&#xa;    # for naive datetime objects&#xa;    #'z': 'Z',&#xa;    #'Z': 'z',&#xa;}&#xa;&#xa;def posix_to_ldml(fmt, locale):&#xa;    """""" Converts a posix/strftime pattern into an LDML date format pattern.&#xa;&#xa;    :param fmt: non-extended C89/C90 strftime pattern&#xa;    :param locale: babel locale used for locale-specific conversions (e.g. %x and %X)&#xa;    :return: unicode&#xa;    """"""&#xa;    buf = []&#xa;    pc = False&#xa;    quoted = []&#xa;&#xa;    for c in fmt:&#xa;        # LDML date format patterns uses letters, so letters must be quoted&#xa;        if not pc and c.isalpha():&#xa;            quoted.append(c if c != ""'"" else ""''"")&#xa;            continue&#xa;        if quoted:&#xa;            buf.append(""'"")&#xa;            buf.append(''.join(quoted))&#xa;            buf.append(""'"")&#xa;            quoted = []&#xa;&#xa;        if pc:&#xa;            if c == '%': # escaped percent&#xa;                buf.append('%')&#xa;            elif c == 'x': # date format, short seems to match&#xa;                buf.append(locale.date_formats['short'].pattern)&#xa;            elif c == 'X': # time format, seems to include seconds. short does not&#xa;                buf.append(locale.time_formats['medium'].pattern)&#xa;            else: # look up format char in static mapping&#xa;                buf.append(POSIX_TO_LDML[c])&#xa;            pc = False&#xa;        elif c == '%':&#xa;            pc = True&#xa;        else:&#xa;            buf.append(c)&#xa;&#xa;    # flush anything remaining in quoted buffer&#xa;    if quoted:&#xa;        buf.append(""'"")&#xa;        buf.append(''.join(quoted))&#xa;        buf.append(""'"")&#xa;&#xa;    return ''.join(buf)&#xa;&#xa;def server_to_local_timestamp(src_tstamp_str, src_format, dst_format, dst_tz_name,&#xa;        tz_offset=True, ignore_unparsable_time=True):&#xa;    """"""&#xa;    Convert a source timestamp string into a destination timestamp string, attempting to apply the&#xa;    correct offset if both the server and local timezone are recognized, or no&#xa;    offset at all if they aren't or if tz_offset is false (i.e. assuming they are both in the same TZ).&#xa;&#xa;    WARNING: This method is here to allow formatting dates correctly for inclusion in strings where&#xa;             the client would not be able to format/offset it correctly. DO NOT use it for returning&#xa;             date fields directly, these are supposed to be handled by the client!!&#xa;&#xa;    @param src_tstamp_str: the str value containing the timestamp in the server timezone.&#xa;    @param src_format: the format to use when parsing the server timestamp.&#xa;    @param dst_format: the format to use when formatting the resulting timestamp for the local/client timezone.&#xa;    @param dst_tz_name: name of the destination timezone (such as the 'tz' value of the client context)&#xa;    @param ignore_unparsable_time: if True, return False if src_tstamp_str cannot be parsed&#xa;                                   using src_format or formatted using dst_format.&#xa;&#xa;    @return local/client formatted timestamp, expressed in the local/client timezone if possible&#xa;            and if tz_offset is true, or src_tstamp_str if timezone offset could not be determined.&#xa;    """"""&#xa;    if not src_tstamp_str:&#xa;        return False&#xa;&#xa;    res = src_tstamp_str&#xa;    if src_format and dst_format:&#xa;        # find out server timezone&#xa;        server_tz = get_server_timezone()&#xa;        try:&#xa;            # dt_value needs to be a datetime.datetime object (so no time.struct_time or mx.DateTime.DateTime here!)&#xa;            dt_value = datetime.strptime(src_tstamp_str, src_format)&#xa;            if tz_offset and dst_tz_name:&#xa;                try:&#xa;                    import pytz&#xa;                    src_tz = pytz.timezone(server_tz)&#xa;                    dst_tz = pytz.timezone(dst_tz_name)&#xa;                    src_dt = src_tz.localize(dt_value, is_dst=True)&#xa;                    dt_value = src_dt.astimezone(dst_tz)&#xa;                except Exception:&#xa;                    pass&#xa;            res = dt_value.strftime(dst_format)&#xa;        except Exception:&#xa;            # Normal ways to end up here are if strptime or strftime failed&#xa;            if not ignore_unparsable_time:&#xa;                return False&#xa;    return res&#xa;&#xa;&#xa;def split_every(n, iterable, piece_maker=tuple):&#xa;    """"""Splits an iterable into length-n pieces. The last piece will be shorter&#xa;       if ``n`` does not evenly divide the iterable length.&#xa;       @param ``piece_maker``: function to build the pieces&#xa;       from the slices (tuple,list,...)&#xa;    """"""&#xa;    iterator = iter(iterable)&#xa;    piece = piece_maker(islice(iterator, n))&#xa;    while piece:&#xa;        yield piece&#xa;        piece = piece_maker(islice(iterator, n))&#xa;&#xa;if __name__ == '__main__':&#xa;    import doctest&#xa;    doctest.testmod()&#xa;&#xa;class upload_data_thread(threading.Thread):&#xa;    def __init__(self, email, data, type):&#xa;        self.args = [('email',email),('type',type),('data',data)]&#xa;        super(upload_data_thread,self).__init__()&#xa;    def run(self):&#xa;        try:&#xa;            import urllib&#xa;            args = urllib.urlencode(self.args)&#xa;            fp = urllib.urlopen('http://www.openerp.com/scripts/survey.php', args)&#xa;            fp.read()&#xa;            fp.close()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;def upload_data(email, data, type='SURVEY'):&#xa;    a = upload_data_thread(email, data, type)&#xa;    a.start()&#xa;    return True&#xa;&#xa;def get_and_group_by_field(cr, uid, obj, ids, field, context=None):&#xa;    """""" Read the values of ``field for the given ``ids and group ids by value.&#xa;&#xa;       :param string field: name of the field we want to read and group by&#xa;       :return: mapping of field values to the list of ids that have it&#xa;       :rtype: dict&#xa;    """"""&#xa;    res = {}&#xa;    for record in obj.read(cr, uid, ids, [field], context=context):&#xa;        key = record[field]&#xa;        res.setdefault(key[0] if isinstance(key, tuple) else key, []).append(record['id'])&#xa;    return res&#xa;&#xa;def get_and_group_by_company(cr, uid, obj, ids, context=None):&#xa;    return get_and_group_by_field(cr, uid, obj, ids, field='company_id', context=context)&#xa;&#xa;# port of python 2.6's attrgetter with support for dotted notation&#xa;def resolve_attr(obj, attr):&#xa;    for name in attr.split("".""):&#xa;        obj = getattr(obj, name)&#xa;    return obj&#xa;&#xa;def attrgetter(*items):&#xa;    if len(items) == 1:&#xa;        attr = items[0]&#xa;        def g(obj):&#xa;            return resolve_attr(obj, attr)&#xa;    else:&#xa;        def g(obj):&#xa;            return tuple(resolve_attr(obj, attr) for attr in items)&#xa;    return g&#xa;&#xa;class unquote(str):&#xa;    """"""A subclass of str that implements repr() without enclosing quotation marks&#xa;       or escaping, keeping the original string untouched. The name come from Lisp's unquote.&#xa;       One of the uses for this is to preserve or insert bare variable names within dicts during eval()&#xa;       of a dict's repr(). Use with care.&#xa;&#xa;       Some examples (notice that there are never quotes surrounding&#xa;       the ``active_id`` name:&#xa;&#xa;       >>> unquote('active_id')&#xa;       active_id&#xa;       >>> d = {'test': unquote('active_id')}&#xa;       >>> d&#xa;       {'test': active_id}&#xa;       >>> print d&#xa;       {'test': active_id}&#xa;    """"""&#xa;    def __repr__(self):&#xa;        return self&#xa;&#xa;class UnquoteEvalContext(defaultdict):&#xa;    """"""Defaultdict-based evaluation context that returns &#xa;       an ``unquote`` string for any missing name used during&#xa;       the evaluation.&#xa;       Mostly useful for evaluating OpenERP domains/contexts that&#xa;       may refer to names that are unknown at the time of eval,&#xa;       so that when the context/domain is converted back to a string,&#xa;       the original names are preserved.&#xa;&#xa;       **Warning**: using an ``UnquoteEvalContext`` as context for ``eval()`` or&#xa;       ``safe_eval()`` will shadow the builtins, which may cause other&#xa;       failures, depending on what is evaluated.&#xa;&#xa;       Example (notice that ``section_id`` is preserved in the final&#xa;       result) :&#xa;&#xa;       >>> context_str = ""{'default_user_id': uid, 'default_section_id': section_id}""&#xa;       >>> eval(context_str, UnquoteEvalContext(uid=1))&#xa;       {'default_user_id': 1, 'default_section_id': section_id}&#xa;&#xa;       """"""&#xa;    def __init__(self, *args, **kwargs):&#xa;        super(UnquoteEvalContext, self).__init__(None, *args, **kwargs)&#xa;&#xa;    def __missing__(self, key):&#xa;        return unquote(key)&#xa;&#xa;&#xa;class mute_logger(object):&#xa;    """"""Temporary suppress the logging.&#xa;    Can be used as context manager or decorator.&#xa;&#xa;        @mute_logger('openerp.plic.ploc')&#xa;        def do_stuff():&#xa;            blahblah()&#xa;&#xa;        with mute_logger('openerp.foo.bar'):&#xa;            do_suff()&#xa;&#xa;    """"""&#xa;    def __init__(self, *loggers):&#xa;        self.loggers = loggers&#xa;&#xa;    def filter(self, record):&#xa;        return 0&#xa;&#xa;    def __enter__(self):&#xa;        for logger in self.loggers:&#xa;            assert isinstance(logger, basestring),\&#xa;                ""A logger name must be a string, got %s"" % type(logger)&#xa;            logging.getLogger(logger).addFilter(self)&#xa;&#xa;    def __exit__(self, exc_type=None, exc_val=None, exc_tb=None):&#xa;        for logger in self.loggers:&#xa;            logging.getLogger(logger).removeFilter(self)&#xa;&#xa;    def __call__(self, func):&#xa;        @wraps(func)&#xa;        def deco(*args, **kwargs):&#xa;            with self:&#xa;                return func(*args, **kwargs)&#xa;        return deco&#xa;&#xa;_ph = object()&#xa;class CountingStream(object):&#xa;    """""" Stream wrapper counting the number of element it has yielded. Similar&#xa;    role to ``enumerate``, but for use when the iteration process of the stream&#xa;    isn't fully under caller control (the stream can be iterated from multiple&#xa;    points including within a library)&#xa;&#xa;    ``start`` allows overriding the starting index (the index before the first&#xa;    item is returned).&#xa;&#xa;    On each iteration (call to :meth:`~.next`), increases its :attr:`~.index`&#xa;    by one.&#xa;&#xa;    .. attribute:: index&#xa;&#xa;        ``int``, index of the last yielded element in the stream. If the stream&#xa;        has ended, will give an index 1-past the stream&#xa;    """"""&#xa;    def __init__(self, stream, start=-1):&#xa;        self.stream = iter(stream)&#xa;        self.index = start&#xa;        self.stopped = False&#xa;    def __iter__(self):&#xa;        return self&#xa;    def next(self):&#xa;        if self.stopped: raise StopIteration()&#xa;        self.index += 1&#xa;        val = next(self.stream, _ph)&#xa;        if val is _ph:&#xa;            self.stopped = True&#xa;            raise StopIteration()&#xa;        return val&#xa;&#xa;def stripped_sys_argv(*strip_args):&#xa;    """"""Return sys.argv with some arguments stripped, suitable for reexecution or subprocesses""""""&#xa;    strip_args = sorted(set(strip_args) | set(['-s', '--save', '-d', '--database', '-u', '--update', '-i', '--init']))&#xa;    assert all(config.parser.has_option(s) for s in strip_args)&#xa;    takes_value = dict((s, config.parser.get_option(s).takes_value()) for s in strip_args)&#xa;&#xa;    longs, shorts = list(tuple(y) for _, y in groupby(strip_args, lambda x: x.startswith('--')))&#xa;    longs_eq = tuple(l + '=' for l in longs if takes_value[l])&#xa;&#xa;    args = sys.argv[:]&#xa;&#xa;    def strip(args, i):&#xa;        return args[i].startswith(shorts) \&#xa;            or args[i].startswith(longs_eq) or (args[i] in longs) \&#xa;            or (i >= 1 and (args[i - 1] in strip_args) and takes_value[args[i - 1]])&#xa;&#xa;    return [x for i, x in enumerate(args) if not strip(args, i)]&#xa;&#xa;&#xa;class ConstantMapping(Mapping):&#xa;    """"""&#xa;    An immutable mapping returning the provided value for every single key.&#xa;&#xa;    Useful for default value to methods&#xa;    """"""&#xa;    __slots__ = ['_value']&#xa;    def __init__(self, val):&#xa;        self._value = val&#xa;&#xa;    def __len__(self):&#xa;        """"""&#xa;        defaultdict updates its length for each individually requested key, is&#xa;        that really useful?&#xa;        """"""&#xa;        return 0&#xa;&#xa;    def __iter__(self):&#xa;        """"""&#xa;        same as len, defaultdict udpates its iterable keyset with each key&#xa;        requested, is there a point for this?&#xa;        """"""&#xa;        return iter([])&#xa;&#xa;    def __getitem__(self, item):&#xa;        return self._value&#xa;&#xa;&#xa;def dumpstacks(sig=None, frame=None):&#xa;    """""" Signal handler: dump a stack trace for each existing thread.""""""&#xa;    code = []&#xa;&#xa;    def extract_stack(stack):&#xa;        for filename, lineno, name, line in traceback.extract_stack(stack):&#xa;            yield 'File: ""%s"", line %d, in %s' % (filename, lineno, name)&#xa;            if line:&#xa;                yield ""  %s"" % (line.strip(),)&#xa;&#xa;    # code from http://stackoverflow.com/questions/132058/getting-stack-trace-from-a-running-python-application#answer-2569696&#xa;    # modified for python 2.5 compatibility&#xa;    threads_info = dict([(th.ident, {'name': th.name, 'uid': getattr(th, 'uid', 'n/a')})&#xa;                        for th in threading.enumerate()])&#xa;    for threadId, stack in sys._current_frames().items():&#xa;        thread_info = threads_info.get(threadId)&#xa;        code.append(""\n# Thread: %s (id:%s) (uid:%s)"" %&#xa;                    (thread_info and thread_info['name'] or 'n/a',&#xa;                     threadId,&#xa;                     thread_info and thread_info['uid'] or 'n/a'))&#xa;        for line in extract_stack(stack):&#xa;            code.append(line)&#xa;&#xa;    if openerp.evented:&#xa;        # code from http://stackoverflow.com/questions/12510648/in-gevent-how-can-i-dump-stack-traces-of-all-running-greenlets&#xa;        import gc&#xa;        from greenlet import greenlet&#xa;        for ob in gc.get_objects():&#xa;            if not isinstance(ob, greenlet) or not ob:&#xa;                continue&#xa;            code.append(""\n# Greenlet: %r"" % (ob,))&#xa;            for line in extract_stack(ob.gr_frame):&#xa;                code.append(line)&#xa;&#xa;    _logger.info(""\n"".join(code))&#xa;&#xa;class frozendict(dict):&#xa;    """""" An implementation of an immutable dictionary. """"""&#xa;    def __delitem__(self, key):&#xa;        raise NotImplementedError(""'__delitem__' not supported on frozendict"")&#xa;    def __setitem__(self, key, val):&#xa;        raise NotImplementedError(""'__setitem__' not supported on frozendict"")&#xa;    def clear(self):&#xa;        raise NotImplementedError(""'clear' not supported on frozendict"")&#xa;    def pop(self, key, default=None):&#xa;        raise NotImplementedError(""'pop' not supported on frozendict"")&#xa;    def popitem(self):&#xa;        raise NotImplementedError(""'popitem' not supported on frozendict"")&#xa;    def setdefault(self, key, default=None):&#xa;        raise NotImplementedError(""'setdefault' not supported on frozendict"")&#xa;    def update(self, *args, **kwargs):&#xa;        raise NotImplementedError(""'update' not supported on frozendict"")&#xa;&#xa;@contextmanager&#xa;def ignore(*exc):&#xa;    try:&#xa;        yield&#xa;    except exc:&#xa;        pass&#xa;&#xa;# Avoid DeprecationWarning while still remaining compatible with werkzeug pre-0.9&#xa;if parse_version(getattr(werkzeug, '__version__', '0.0')) < parse_version('0.9.0'):&#xa;    def html_escape(text):&#xa;        return werkzeug.utils.escape(text, quote=True)&#xa;else:&#xa;    def html_escape(text):&#xa;        return werkzeug.utils.escape(text)&#xa;&#xa;# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:&#xa;"
1602934|"#Duplicate of Scenery cog except for scenery.&#xa;&#xa;&#xa;import discord&#xa;from discord.ext import commands&#xa;from __main__ import send_cmd_help&#xa;from cogs.utils.dataIO import dataIO&#xa;import os #Used to create folder at first load.&#xa;import random #Used for selecting random scenery&#xa;&#xa;#Global variables&#xa;JSON_mainKey = ""scenery"" #Key for JSON files.&#xa;JSON_imageURLKey = ""url"" #Key for URL&#xa;JSON_isPixiv = ""is_pixiv"" #Key that specifies if image is from pixiv. If true, pixivID should be set.&#xa;JSON_pixivID = ""id"" #Key for Pixiv ID, used to create URL to pixiv image page, if applicable.&#xa;saveFolder = ""data/lui-cogs/scenery/"" #Path to save folder.&#xa;&#xa;def checkFolder():&#xa;    """"""Used to create the data folder at first startup""""""&#xa;    if not os.path.exists(saveFolder):&#xa;        print(""Creating "" + saveFolder + "" folder..."")&#xa;        os.makedirs(saveFolder)&#xa;&#xa;def checkFiles():&#xa;    """"""Used to initialize an empty database at first startup""""""&#xa;    base = { JSON_mainKey : [{ JSON_imageURLKey :""https://cdn.awwni.me/utpd.jpg"" , ""id"" : ""null"", ""is_pixiv"" : False}]}&#xa;    empty = { JSON_mainKey : []}&#xa;    &#xa;    f = saveFolder + ""links-web.json""&#xa;    if not dataIO.is_valid_json(f):&#xa;        print(""Creating default Scenery links-web.json..."")&#xa;        dataIO.save_json(f, base)&#xa;        &#xa;    f = saveFolder + ""links-localx10.json""&#xa;    if not dataIO.is_valid_json(f):&#xa;        print(""Creating default Scenery links-localx10.json..."")&#xa;        dataIO.save_json(f, empty)&#xa;        &#xa;    f = saveFolder + ""links-local.json""&#xa;    if not dataIO.is_valid_json(f):&#xa;        print(""Creating default Scenery links-local.json..."")&#xa;        dataIO.save_json(f, empty)&#xa;        &#xa;    f = saveFolder + ""links-pending.json""&#xa;    if not dataIO.is_valid_json(f):&#xa;        print(""Creating default Scenery links-pending.json..."")&#xa;        dataIO.save_json(f, empty)&#xa;            &#xa;class Scenery_beta:&#xa;    """"""Display cute nyaas~""""""&#xa;&#xa;&#xa;    def refreshDatabase(self):&#xa;        """"""Refreshes the JSON files""""""&#xa;        #Local scenery allow for prepending predefined domain, if you have a place where you're hosting your own scenery.&#xa;        self.filepath_local = saveFolder + ""links-local.json""&#xa;        self.filepath_localx10 = saveFolder + ""links-localx10.json""&#xa;        &#xa;        #Web scenery will take on full URLs.&#xa;        self.filepath_web = saveFolder + ""links-web.json""&#xa;&#xa;        #List of pending scenery waiting to be added.&#xa;        self.filepath_pending = saveFolder + ""links-pending.json""&#xa;        &#xa;        #scenery&#xa;        self.pictures_local = dataIO.load_json(self.filepath_local)&#xa;        self.pictures_localx10 = dataIO.load_json(self.filepath_localx10)&#xa;        self.pictures_web = dataIO.load_json(self.filepath_web)&#xa;        self.pictures_pending = dataIO.load_json(self.filepath_pending)&#xa;        &#xa;&#xa;        #Custom key which holds an array of Scenery filenames/paths&#xa;        self.JSON_mainKey = ""scenery""&#xa;        &#xa;        #Prepend local listings with domain name.&#xa;        for x in range(0,len(self.pictures_local[JSON_mainKey])):&#xa;            self.pictures_local[JSON_mainKey][x][JSON_imageURLKey] = ""https://nekomimi.injabie3.moe/scenery/"" + self.pictures_local[JSON_mainKey][x][JSON_imageURLKey]&#xa;&#xa;        #Prepend hosted listings with domain name.&#xa;        for x in range(0,len(self.pictures_localx10[JSON_mainKey])):&#xa;            self.pictures_localx10[JSON_mainKey][x][JSON_imageURLKey] = ""http://injabie3.x10.mx/scenery/"" + self.pictures_localx10[JSON_mainKey][x][JSON_imageURLKey]&#xa;        &#xa;        self.scenery = self.pictures_local[JSON_mainKey] + self.pictures_web[JSON_mainKey] + self.pictures_localx10[JSON_mainKey]&#xa;        self.pending = self.pictures_pending[JSON_mainKey]&#xa;        &#xa;    def __init__(self, bot):&#xa;        self.bot = bot&#xa;        checkFolder()&#xa;        checkFiles()&#xa;        self.refreshDatabase()&#xa;        &#xa;    #[p]Scenery&#xa;    @commands.command(name=""scenery"")&#xa;    async def _scenerymain(self):&#xa;        """"""Display some beautiful scenery :3""""""&#xa;        randScenery = random.choice(self.scenery)&#xa;        embed = discord.Embed()&#xa;        embed.colour = discord.Colour.red()&#xa;        embed.title = ""Scenery""&#xa;        embed.url = randScenery[JSON_imageURLKey]&#xa;        if randScenery[JSON_isPixiv]:&#xa;            source = ""[{}]({})"".format(""Original Source"",""http://www.pixiv.net/member_illust.php?mode=medium&illust_id=""+randScenery[JSON_pixivID])&#xa;            embed.add_field(name=""Pixiv"",value=source)&#xa;            customFooter = ""ID: "" + randScenery[JSON_pixivID]&#xa;            embed.set_footer(text=customFooter)&#xa;        #Implemented the following with the help of http://stackoverflow.com/questions/1602934/check-if-a-given-key-already-exists-in-a-dictionary&#xa;        if ""character"" in randScenery:&#xa;            embed.add_field(name=""Info"",value=randScenery[""character""], inline=False)&#xa;        embed.set_image(url=randScenery[JSON_imageURLKey])&#xa;        try:&#xa;            await self.bot.say("""",embed=embed)&#xa;        except Exception as e:&#xa;            await self.bot.say(""Please try again."")&#xa;            print(""Scenery exception:"")&#xa;            print(e)&#xa;            print(""=========="")&#xa;&#xa;&#xa;    @commands.group(name=""scenery+"", pass_context=True, no_pm=False)&#xa;    async def _scenery(self, ctx):&#xa;        """"""Scenery! \o/""""""&#xa;        if ctx.invoked_subcommand is None:&#xa;            await send_cmd_help(ctx)&#xa;&#xa;    #[p]scenery+ about&#xa;    @_scenery.command(pass_context=True, no_pm=False)&#xa;    async def about(self, ctx):&#xa;        """"""Displays information about this module""""""&#xa;        customAuthor = ""[{}]({})"".format(""@Injabie3#1660"",""https://injabie3.moe/"")&#xa;        embed = discord.Embed()&#xa;        embed.title = ""About this module""&#xa;        embed.add_field(name=""Name"", value=""Scenery Module"")&#xa;        embed.add_field(name=""Author"", value=customAuthor)&#xa;        embed.add_field(name=""Initial Version Date"", value=""2017-02-11"")&#xa;        embed.add_field(name=""Description"", value=""A module to display pseudo-random scenery images.  Image links are stored in the local database, separated into different lists (depending on if they are hosted locally or on another domain).  See https://github.com/Injabie3/lui-cogs for more info."")&#xa;        embed.set_footer(text=""lui-cogs/scenery"")&#xa;        await self.bot.say(content="""",embed=embed)&#xa;        &#xa;    #[p]scenery+ numbers&#xa;    @_scenery.command(pass_context=True, no_pm=False)&#xa;    async def numbers(self, ctx):&#xa;        """"""Displays the number of images in the database.""""""&#xa;        await self.bot.say(""There are:\n - **"" + str(len(self.scenery)) + ""** images available.\n - **"" + str(len(self.pictures_pending[JSON_mainKey])) + ""** pending images."")&#xa;&#xa;    #[p]scenery+ refresh - Also allow for refresh in a DM to the bot.&#xa;    @_scenery.command(pass_context=True, no_pm=False)&#xa;    async def refresh(self, ctx):&#xa;        """"""Refreshes the internal database of nekomimi images.""""""&#xa;        self.refreshDatabase()&#xa;        await self.bot.say(""List reloaded.  There are:\n - **"" + str(len(self.scenery)) + ""** images available.\n - **"" + str(len(self.pictures_pending[JSON_mainKey])) + ""** pending images."")&#xa;&#xa;    #[p] nyaa debug&#xa;    @_scenery.command(pass_context=True, no_pm=False)&#xa;    async def debug(self, ctx):&#xa;        """"""Sends entire list via DM for debugging.""""""&#xa;        msg = ""Debug Mode\nscenery:\n```""&#xa;        for x in range(0,len(self.scenery)):&#xa;            msg += self.scenery[x][JSON_imageURLKey] + ""\n""&#xa;            if len(msg) > 1900:&#xa;               msg += ""```""&#xa;               await self.bot.send_message(ctx.message.author, msg)&#xa;               msg = ""```""&#xa;        msg += ""```""&#xa;        await self.bot.send_message(ctx.message.author, msg)&#xa;        &#xa;        msg = ""Catboys:\n```""&#xa;        for x in range(0,len(self.catboys)):&#xa;            msg += self.catboys[x][JSON_imageURLKey] + ""\n""&#xa;            if len(msg) > 1900:&#xa;               msg += ""```""&#xa;               await self.bot.send_message(ctx.message.author, msg)&#xa;               msg = ""```""&#xa;        msg += ""```""&#xa;        await self.bot.send_message(ctx.message.author, msg)&#xa;    &#xa;    #[p]scenery+ add&#xa;    @_scenery.command(pass_context=True, no_pm=True)&#xa;    async def add(self, ctx, link: str, description: str=""""):&#xa;        """"""&#xa;        Add a Scenery image to the pending database.&#xa;        Will be screened before it is added to the global list. WIP&#xa;        &#xa;        link          The full URL to an image, use \"" \"" around the link.&#xa;        description   Description of character (optional)&#xa;        """"""&#xa;    &#xa;        temp = {}&#xa;        temp[""url""] = link&#xa;        temp[""character""] = description&#xa;        temp[""submitter""] = ctx.message.author.name&#xa;        temp[""id""] = None&#xa;        temp[""is_pixiv""] = False&#xa;        &#xa;    &#xa;        self.pictures_pending[JSON_mainKey].append(temp)&#xa;        dataIO.save_json(self.filepath_pending, self.pictures_pending)&#xa;&#xa;        #Get owner ID.&#xa;        owner = discord.utils.get(self.bot.get_all_members(),id=self.bot.settings.owner)&#xa;                              &#xa;        try:&#xa;            await self.bot.send_message(owner, ""New Scenery image is pending approval. Please check the list!"")&#xa;        except discord.errors.InvalidArgument:&#xa;            await self.bot.say(""Added, but could not notify owner."")&#xa;        else:&#xa;            await self.bot.say(""Added, notified and pending approval. :ok_hand:"")&#xa;                &#xa;        &#xa;            &#xa;        &#xa;&#xa;def setup(bot):&#xa;    checkFolder()   #Make sure the data folder exists!&#xa;    checkFiles()    #Make sure we have a local database!&#xa;    bot.add_cog(Scenery_beta(bot))&#xa;"
267399|"# module pyparsing.py&#xa;#&#xa;# Copyright (c) 2003-2016  Paul T. McGuire&#xa;#&#xa;# Permission is hereby granted, free of charge, to any person obtaining&#xa;# a copy of this software and associated documentation files (the&#xa;# ""Software""), to deal in the Software without restriction, including&#xa;# without limitation the rights to use, copy, modify, merge, publish,&#xa;# distribute, sublicense, and/or sell copies of the Software, and to&#xa;# permit persons to whom the Software is furnished to do so, subject to&#xa;# the following conditions:&#xa;#&#xa;# The above copyright notice and this permission notice shall be&#xa;# included in all copies or substantial portions of the Software.&#xa;#&#xa;# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,&#xa;# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF&#xa;# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.&#xa;# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY&#xa;# CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,&#xa;# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE&#xa;# SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&#xa;#&#xa;&#xa;__doc__ = \&#xa;""""""&#xa;pyparsing module - Classes and methods to define and execute parsing grammars&#xa;&#xa;The pyparsing module is an alternative approach to creating and executing simple grammars,&#xa;vs. the traditional lex/yacc approach, or the use of regular expressions.  With pyparsing, you&#xa;don't need to learn a new syntax for defining grammars or matching expressions - the parsing module&#xa;provides a library of classes that you use to construct the grammar directly in Python.&#xa;&#xa;Here is a program to parse ""Hello, World!"" (or any greeting of the form &#xa;C{""<salutation>, <addressee>!""}), built up using L{Word}, L{Literal}, and L{And} elements &#xa;(L{'+'<ParserElement.__add__>} operator gives L{And} expressions, strings are auto-converted to&#xa;L{Literal} expressions)::&#xa;&#xa;    from pyparsing import Word, alphas&#xa;&#xa;    # define grammar of a greeting&#xa;    greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;&#xa;    hello = ""Hello, World!""&#xa;    print (hello, ""->"", greet.parseString(hello))&#xa;&#xa;The program outputs the following::&#xa;&#xa;    Hello, World! -> ['Hello', ',', 'World', '!']&#xa;&#xa;The Python representation of the grammar is quite readable, owing to the self-explanatory&#xa;class names, and the use of '+', '|' and '^' operators.&#xa;&#xa;The L{ParseResults} object returned from L{ParserElement.parseString<ParserElement.parseString>} can be accessed as a nested list, a dictionary, or an&#xa;object with named attributes.&#xa;&#xa;The pyparsing module handles some of the problems that are typically vexing when writing text parsers:&#xa; - extra or missing whitespace (the above program will also handle ""Hello,World!"", ""Hello  ,  World  !"", etc.)&#xa; - quoted strings&#xa; - embedded comments&#xa;""""""&#xa;&#xa;__version__ = ""2.1.10""&#xa;__versionTime__ = ""07 Oct 2016 01:31 UTC""&#xa;__author__ = ""Paul McGuire <ptmcg@users.sourceforge.net>""&#xa;&#xa;import string&#xa;from weakref import ref as wkref&#xa;import copy&#xa;import sys&#xa;import warnings&#xa;import re&#xa;import sre_constants&#xa;import collections&#xa;import pprint&#xa;import traceback&#xa;import types&#xa;from datetime import datetime&#xa;&#xa;try:&#xa;    from _thread import RLock&#xa;except ImportError:&#xa;    from threading import RLock&#xa;&#xa;try:&#xa;    from collections import OrderedDict as _OrderedDict&#xa;except ImportError:&#xa;    try:&#xa;        from ordereddict import OrderedDict as _OrderedDict&#xa;    except ImportError:&#xa;        _OrderedDict = None&#xa;&#xa;#~ sys.stderr.write( ""testing pyparsing module, version %s, %s\n"" % (__version__,__versionTime__ ) )&#xa;&#xa;__all__ = [&#xa;'And', 'CaselessKeyword', 'CaselessLiteral', 'CharsNotIn', 'Combine', 'Dict', 'Each', 'Empty',&#xa;'FollowedBy', 'Forward', 'GoToColumn', 'Group', 'Keyword', 'LineEnd', 'LineStart', 'Literal',&#xa;'MatchFirst', 'NoMatch', 'NotAny', 'OneOrMore', 'OnlyOnce', 'Optional', 'Or',&#xa;'ParseBaseException', 'ParseElementEnhance', 'ParseException', 'ParseExpression', 'ParseFatalException',&#xa;'ParseResults', 'ParseSyntaxException', 'ParserElement', 'QuotedString', 'RecursiveGrammarException',&#xa;'Regex', 'SkipTo', 'StringEnd', 'StringStart', 'Suppress', 'Token', 'TokenConverter', &#xa;'White', 'Word', 'WordEnd', 'WordStart', 'ZeroOrMore',&#xa;'alphanums', 'alphas', 'alphas8bit', 'anyCloseTag', 'anyOpenTag', 'cStyleComment', 'col',&#xa;'commaSeparatedList', 'commonHTMLEntity', 'countedArray', 'cppStyleComment', 'dblQuotedString',&#xa;'dblSlashComment', 'delimitedList', 'dictOf', 'downcaseTokens', 'empty', 'hexnums',&#xa;'htmlComment', 'javaStyleComment', 'line', 'lineEnd', 'lineStart', 'lineno',&#xa;'makeHTMLTags', 'makeXMLTags', 'matchOnlyAtCol', 'matchPreviousExpr', 'matchPreviousLiteral',&#xa;'nestedExpr', 'nullDebugAction', 'nums', 'oneOf', 'opAssoc', 'operatorPrecedence', 'printables',&#xa;'punc8bit', 'pythonStyleComment', 'quotedString', 'removeQuotes', 'replaceHTMLEntity', &#xa;'replaceWith', 'restOfLine', 'sglQuotedString', 'srange', 'stringEnd',&#xa;'stringStart', 'traceParseAction', 'unicodeString', 'upcaseTokens', 'withAttribute',&#xa;'indentedBlock', 'originalTextFor', 'ungroup', 'infixNotation','locatedExpr', 'withClass',&#xa;'CloseMatch', 'tokenMap', 'pyparsing_common',&#xa;]&#xa;&#xa;system_version = tuple(sys.version_info)[:3]&#xa;PY_3 = system_version[0] == 3&#xa;if PY_3:&#xa;    _MAX_INT = sys.maxsize&#xa;    basestring = str&#xa;    unichr = chr&#xa;    _ustr = str&#xa;&#xa;    # build list of single arg builtins, that can be used as parse actions&#xa;    singleArgBuiltins = [sum, len, sorted, reversed, list, tuple, set, any, all, min, max]&#xa;&#xa;else:&#xa;    _MAX_INT = sys.maxint&#xa;    range = xrange&#xa;&#xa;    def _ustr(obj):&#xa;        """"""Drop-in replacement for str(obj) that tries to be Unicode friendly. It first tries&#xa;           str(obj). If that fails with a UnicodeEncodeError, then it tries unicode(obj). It&#xa;           then < returns the unicode object | encodes it with the default encoding | ... >.&#xa;        """"""&#xa;        if isinstance(obj,unicode):&#xa;            return obj&#xa;&#xa;        try:&#xa;            # If this works, then _ustr(obj) has the same behaviour as str(obj), so&#xa;            # it won't break any existing code.&#xa;            return str(obj)&#xa;&#xa;        except UnicodeEncodeError:&#xa;            # Else encode it&#xa;            ret = unicode(obj).encode(sys.getdefaultencoding(), 'xmlcharrefreplace')&#xa;            xmlcharref = Regex('&#\d+;')&#xa;            xmlcharref.setParseAction(lambda t: '\\u' + hex(int(t[0][2:-1]))[2:])&#xa;            return xmlcharref.transformString(ret)&#xa;&#xa;    # build list of single arg builtins, tolerant of Python version, that can be used as parse actions&#xa;    singleArgBuiltins = []&#xa;    import __builtin__&#xa;    for fname in ""sum len sorted reversed list tuple set any all min max"".split():&#xa;        try:&#xa;            singleArgBuiltins.append(getattr(__builtin__,fname))&#xa;        except AttributeError:&#xa;            continue&#xa;            &#xa;_generatorType = type((y for y in range(1)))&#xa; &#xa;def _xml_escape(data):&#xa;    """"""Escape &, <, >, "", ', etc. in a string of data.""""""&#xa;&#xa;    # ampersand must be replaced first&#xa;    from_symbols = '&><""\''&#xa;    to_symbols = ('&'+s+';' for s in ""amp gt lt quot apos"".split())&#xa;    for from_,to_ in zip(from_symbols, to_symbols):&#xa;        data = data.replace(from_, to_)&#xa;    return data&#xa;&#xa;class _Constants(object):&#xa;    pass&#xa;&#xa;alphas     = string.ascii_uppercase + string.ascii_lowercase&#xa;nums       = ""0123456789""&#xa;hexnums    = nums + ""ABCDEFabcdef""&#xa;alphanums  = alphas + nums&#xa;_bslash    = chr(92)&#xa;printables = """".join(c for c in string.printable if c not in string.whitespace)&#xa;&#xa;class ParseBaseException(Exception):&#xa;    """"""base exception class for all parsing runtime exceptions""""""&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, pstr, loc=0, msg=None, elem=None ):&#xa;        self.loc = loc&#xa;        if msg is None:&#xa;            self.msg = pstr&#xa;            self.pstr = """"&#xa;        else:&#xa;            self.msg = msg&#xa;            self.pstr = pstr&#xa;        self.parserElement = elem&#xa;        self.args = (pstr, loc, msg)&#xa;&#xa;    @classmethod&#xa;    def _from_exception(cls, pe):&#xa;        """"""&#xa;        internal factory method to simplify creating one type of ParseException &#xa;        from another - avoids having __init__ signature conflicts among subclasses&#xa;        """"""&#xa;        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)&#xa;&#xa;    def __getattr__( self, aname ):&#xa;        """"""supported attributes by name are:&#xa;            - lineno - returns the line number of the exception text&#xa;            - col - returns the column number of the exception text&#xa;            - line - returns the line containing the exception text&#xa;        """"""&#xa;        if( aname == ""lineno"" ):&#xa;            return lineno( self.loc, self.pstr )&#xa;        elif( aname in (""col"", ""column"") ):&#xa;            return col( self.loc, self.pstr )&#xa;        elif( aname == ""line"" ):&#xa;            return line( self.loc, self.pstr )&#xa;        else:&#xa;            raise AttributeError(aname)&#xa;&#xa;    def __str__( self ):&#xa;        return ""%s (at char %d), (line:%d, col:%d)"" % \&#xa;                ( self.msg, self.loc, self.lineno, self.column )&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;    def markInputline( self, markerString = "">!<"" ):&#xa;        """"""Extracts the exception line from the input string, and marks&#xa;           the location of the exception with a special symbol.&#xa;        """"""&#xa;        line_str = self.line&#xa;        line_column = self.column - 1&#xa;        if markerString:&#xa;            line_str = """".join((line_str[:line_column],&#xa;                                markerString, line_str[line_column:]))&#xa;        return line_str.strip()&#xa;    def __dir__(self):&#xa;        return ""lineno col line"".split() + dir(type(self))&#xa;&#xa;class ParseException(ParseBaseException):&#xa;    """"""&#xa;    Exception thrown when parse expressions don't match class;&#xa;    supported attributes by name are:&#xa;     - lineno - returns the line number of the exception text&#xa;     - col - returns the column number of the exception text&#xa;     - line - returns the line containing the exception text&#xa;        &#xa;    Example::&#xa;        try:&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")&#xa;        except ParseException as pe:&#xa;            print(pe)&#xa;            print(""column: {}"".format(pe.col))&#xa;            &#xa;    prints::&#xa;       Expected integer (at char 0), (line:1, col:1)&#xa;        column: 1&#xa;    """"""&#xa;    pass&#xa;&#xa;class ParseFatalException(ParseBaseException):&#xa;    """"""user-throwable exception thrown when inconsistent parse content&#xa;       is found; stops all parsing immediately""""""&#xa;    pass&#xa;&#xa;class ParseSyntaxException(ParseFatalException):&#xa;    """"""just like L{ParseFatalException}, but thrown internally when an&#xa;       L{ErrorStop<And._ErrorStop>} ('-' operator) indicates that parsing is to stop &#xa;       immediately because an unbacktrackable syntax error has been found""""""&#xa;    pass&#xa;&#xa;#~ class ReparseException(ParseBaseException):&#xa;    #~ """"""Experimental class - parse actions can raise this exception to cause&#xa;       #~ pyparsing to reparse the input string:&#xa;        #~ - with a modified input string, and/or&#xa;        #~ - with a modified start location&#xa;       #~ Set the values of the ReparseException in the constructor, and raise the&#xa;       #~ exception in a parse action to cause pyparsing to use the new string/location.&#xa;       #~ Setting the values as None causes no change to be made.&#xa;       #~ """"""&#xa;    #~ def __init_( self, newstring, restartLoc ):&#xa;        #~ self.newParseText = newstring&#xa;        #~ self.reparseLoc = restartLoc&#xa;&#xa;class RecursiveGrammarException(Exception):&#xa;    """"""exception thrown by L{ParserElement.validate} if the grammar could be improperly recursive""""""&#xa;    def __init__( self, parseElementList ):&#xa;        self.parseElementTrace = parseElementList&#xa;&#xa;    def __str__( self ):&#xa;        return ""RecursiveGrammarException: %s"" % self.parseElementTrace&#xa;&#xa;class _ParseResultsWithOffset(object):&#xa;    def __init__(self,p1,p2):&#xa;        self.tup = (p1,p2)&#xa;    def __getitem__(self,i):&#xa;        return self.tup[i]&#xa;    def __repr__(self):&#xa;        return repr(self.tup[0])&#xa;    def setOffset(self,i):&#xa;        self.tup = (self.tup[0],i)&#xa;&#xa;class ParseResults(object):&#xa;    """"""&#xa;    Structured parse results, to provide multiple means of access to the parsed data:&#xa;       - as a list (C{len(results)})&#xa;       - by list index (C{results[0], results[1]}, etc.)&#xa;       - by attribute (C{results.<resultsName>} - see L{ParserElement.setResultsName})&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;        # equivalent form:&#xa;        # date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;&#xa;        # parseString returns a ParseResults object&#xa;        result = date_str.parseString(""1999/12/31"")&#xa;&#xa;        def test(s, fn=repr):&#xa;            print(""%s -> %s"" % (s, fn(eval(s))))&#xa;        test(""list(result)"")&#xa;        test(""result[0]"")&#xa;        test(""result['month']"")&#xa;        test(""result.day"")&#xa;        test(""'month' in result"")&#xa;        test(""'minutes' in result"")&#xa;        test(""result.dump()"", str)&#xa;    prints::&#xa;        list(result) -> ['1999', '/', '12', '/', '31']&#xa;        result[0] -> '1999'&#xa;        result['month'] -> '12'&#xa;        result.day -> '31'&#xa;        'month' in result -> True&#xa;        'minutes' in result -> False&#xa;        result.dump() -> ['1999', '/', '12', '/', '31']&#xa;        - day: 31&#xa;        - month: 12&#xa;        - year: 1999&#xa;    """"""&#xa;    def __new__(cls, toklist=None, name=None, asList=True, modal=True ):&#xa;        if isinstance(toklist, cls):&#xa;            return toklist&#xa;        retobj = object.__new__(cls)&#xa;        retobj.__doinit = True&#xa;        return retobj&#xa;&#xa;    # Performance tuning: we construct a *lot* of these, so keep this&#xa;    # constructor as small and fast as possible&#xa;    def __init__( self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance ):&#xa;        if self.__doinit:&#xa;            self.__doinit = False&#xa;            self.__name = None&#xa;            self.__parent = None&#xa;            self.__accumNames = {}&#xa;            self.__asList = asList&#xa;            self.__modal = modal&#xa;            if toklist is None:&#xa;                toklist = []&#xa;            if isinstance(toklist, list):&#xa;                self.__toklist = toklist[:]&#xa;            elif isinstance(toklist, _generatorType):&#xa;                self.__toklist = list(toklist)&#xa;            else:&#xa;                self.__toklist = [toklist]&#xa;            self.__tokdict = dict()&#xa;&#xa;        if name is not None and name:&#xa;            if not modal:&#xa;                self.__accumNames[name] = 0&#xa;            if isinstance(name,int):&#xa;                name = _ustr(name) # will always return a str, but use _ustr for consistency&#xa;            self.__name = name&#xa;            if not (isinstance(toklist, (type(None), basestring, list)) and toklist in (None,'',[])):&#xa;                if isinstance(toklist,basestring):&#xa;                    toklist = [ toklist ]&#xa;                if asList:&#xa;                    if isinstance(toklist,ParseResults):&#xa;                        self[name] = _ParseResultsWithOffset(toklist.copy(),0)&#xa;                    else:&#xa;                        self[name] = _ParseResultsWithOffset(ParseResults(toklist[0]),0)&#xa;                    self[name].__name = name&#xa;                else:&#xa;                    try:&#xa;                        self[name] = toklist[0]&#xa;                    except (KeyError,TypeError,IndexError):&#xa;                        self[name] = toklist&#xa;&#xa;    def __getitem__( self, i ):&#xa;        if isinstance( i, (int,slice) ):&#xa;            return self.__toklist[i]&#xa;        else:&#xa;            if i not in self.__accumNames:&#xa;                return self.__tokdict[i][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[i] ])&#xa;&#xa;    def __setitem__( self, k, v, isinstance=isinstance ):&#xa;        if isinstance(v,_ParseResultsWithOffset):&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [v]&#xa;            sub = v[0]&#xa;        elif isinstance(k,(int,slice)):&#xa;            self.__toklist[k] = v&#xa;            sub = v&#xa;        else:&#xa;            self.__tokdict[k] = self.__tokdict.get(k,list()) + [_ParseResultsWithOffset(v,0)]&#xa;            sub = v&#xa;        if isinstance(sub,ParseResults):&#xa;            sub.__parent = wkref(self)&#xa;&#xa;    def __delitem__( self, i ):&#xa;        if isinstance(i,(int,slice)):&#xa;            mylen = len( self.__toklist )&#xa;            del self.__toklist[i]&#xa;&#xa;            # convert int to slice&#xa;            if isinstance(i, int):&#xa;                if i < 0:&#xa;                    i += mylen&#xa;                i = slice(i, i+1)&#xa;            # get removed indices&#xa;            removed = list(range(*i.indices(mylen)))&#xa;            removed.reverse()&#xa;            # fixup indices in token dictionary&#xa;            for name,occurrences in self.__tokdict.items():&#xa;                for j in removed:&#xa;                    for k, (value, position) in enumerate(occurrences):&#xa;                        occurrences[k] = _ParseResultsWithOffset(value, position - (position > j))&#xa;        else:&#xa;            del self.__tokdict[i]&#xa;&#xa;    def __contains__( self, k ):&#xa;        return k in self.__tokdict&#xa;&#xa;    def __len__( self ): return len( self.__toklist )&#xa;    def __bool__(self): return ( not not self.__toklist )&#xa;    __nonzero__ = __bool__&#xa;    def __iter__( self ): return iter( self.__toklist )&#xa;    def __reversed__( self ): return iter( self.__toklist[::-1] )&#xa;    def _iterkeys( self ):&#xa;        if hasattr(self.__tokdict, ""iterkeys""):&#xa;            return self.__tokdict.iterkeys()&#xa;        else:&#xa;            return iter(self.__tokdict)&#xa;&#xa;    def _itervalues( self ):&#xa;        return (self[k] for k in self._iterkeys())&#xa;            &#xa;    def _iteritems( self ):&#xa;        return ((k, self[k]) for k in self._iterkeys())&#xa;&#xa;    if PY_3:&#xa;        keys = _iterkeys       &#xa;        """"""Returns an iterator of all named result keys (Python 3.x only).""""""&#xa;&#xa;        values = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 3.x only).""""""&#xa;&#xa;        items = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 3.x only).""""""&#xa;&#xa;    else:&#xa;        iterkeys = _iterkeys&#xa;        """"""Returns an iterator of all named result keys (Python 2.x only).""""""&#xa;&#xa;        itervalues = _itervalues&#xa;        """"""Returns an iterator of all named result values (Python 2.x only).""""""&#xa;&#xa;        iteritems = _iteritems&#xa;        """"""Returns an iterator of all named result key-value tuples (Python 2.x only).""""""&#xa;&#xa;        def keys( self ):&#xa;            """"""Returns all named result keys (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iterkeys())&#xa;&#xa;        def values( self ):&#xa;            """"""Returns all named result values (as a list in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.itervalues())&#xa;                &#xa;        def items( self ):&#xa;            """"""Returns all named result key-values (as a list of tuples in Python 2.x, as an iterator in Python 3.x).""""""&#xa;            return list(self.iteritems())&#xa;&#xa;    def haskeys( self ):&#xa;        """"""Since keys() returns an iterator, this method is helpful in bypassing&#xa;           code that looks for the existence of any defined results names.""""""&#xa;        return bool(self.__tokdict)&#xa;        &#xa;    def pop( self, *args, **kwargs):&#xa;        """"""&#xa;        Removes and returns item at specified index (default=C{last}).&#xa;        Supports both C{list} and C{dict} semantics for C{pop()}. If passed no&#xa;        argument or an integer argument, it will use C{list} semantics&#xa;        and pop tokens from the list of parsed tokens. If passed a &#xa;        non-integer argument (most likely a string), it will use C{dict}&#xa;        semantics and pop the corresponding value from any defined &#xa;        results names. A second default return value argument is &#xa;        supported, just as in C{dict.pop()}.&#xa;&#xa;        Example::&#xa;            def remove_first(tokens):&#xa;                tokens.pop(0)&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            print(OneOrMore(Word(nums)).addParseAction(remove_first).parseString(""0 123 321"")) # -> ['123', '321']&#xa;&#xa;            label = Word(alphas)&#xa;            patt = label(""LABEL"") + OneOrMore(Word(nums))&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;&#xa;            # Use pop() in a parse action to remove named result (note that corresponding value is not&#xa;            # removed from list form of results)&#xa;            def remove_LABEL(tokens):&#xa;                tokens.pop(""LABEL"")&#xa;                return tokens&#xa;            patt.addParseAction(remove_LABEL)&#xa;            print(patt.parseString(""AAB 123 321"").dump())&#xa;        prints::&#xa;            ['AAB', '123', '321']&#xa;            - LABEL: AAB&#xa;&#xa;            ['AAB', '123', '321']&#xa;        """"""&#xa;        if not args:&#xa;            args = [-1]&#xa;        for k,v in kwargs.items():&#xa;            if k == 'default':&#xa;                args = (args[0], v)&#xa;            else:&#xa;                raise TypeError(""pop() got an unexpected keyword argument '%s'"" % k)&#xa;        if (isinstance(args[0], int) or &#xa;                        len(args) == 1 or &#xa;                        args[0] in self):&#xa;            index = args[0]&#xa;            ret = self[index]&#xa;            del self[index]&#xa;            return ret&#xa;        else:&#xa;            defaultvalue = args[1]&#xa;            return defaultvalue&#xa;&#xa;    def get(self, key, defaultValue=None):&#xa;        """"""&#xa;        Returns named result matching the given key, or if there is no&#xa;        such name, then returns the given C{defaultValue} or C{None} if no&#xa;        C{defaultValue} is specified.&#xa;&#xa;        Similar to C{dict.get()}.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            result = date_str.parseString(""1999/12/31"")&#xa;            print(result.get(""year"")) # -> '1999'&#xa;            print(result.get(""hour"", ""not specified"")) # -> 'not specified'&#xa;            print(result.get(""hour"")) # -> None&#xa;        """"""&#xa;        if key in self:&#xa;            return self[key]&#xa;        else:&#xa;            return defaultValue&#xa;&#xa;    def insert( self, index, insStr ):&#xa;        """"""&#xa;        Inserts new element at location index in the list of parsed tokens.&#xa;        &#xa;        Similar to C{list.insert()}.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;&#xa;            # use a parse action to insert the parse location in the front of the parsed results&#xa;            def insert_locn(locn, tokens):&#xa;                tokens.insert(0, locn)&#xa;            print(OneOrMore(Word(nums)).addParseAction(insert_locn).parseString(""0 123 321"")) # -> [0, '0', '123', '321']&#xa;        """"""&#xa;        self.__toklist.insert(index, insStr)&#xa;        # fixup indices in token dictionary&#xa;        for name,occurrences in self.__tokdict.items():&#xa;            for k, (value, position) in enumerate(occurrences):&#xa;                occurrences[k] = _ParseResultsWithOffset(value, position + (position > index))&#xa;&#xa;    def append( self, item ):&#xa;        """"""&#xa;        Add single element to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            print(OneOrMore(Word(nums)).parseString(""0 123 321"")) # -> ['0', '123', '321']&#xa;            &#xa;            # use a parse action to compute the sum of the parsed integers, and add it to the end&#xa;            def append_sum(tokens):&#xa;                tokens.append(sum(map(int, tokens)))&#xa;            print(OneOrMore(Word(nums)).addParseAction(append_sum).parseString(""0 123 321"")) # -> ['0', '123', '321', 444]&#xa;        """"""&#xa;        self.__toklist.append(item)&#xa;&#xa;    def extend( self, itemseq ):&#xa;        """"""&#xa;        Add sequence of elements to end of ParseResults list of elements.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            &#xa;            # use a parse action to append the reverse of the matched strings, to make a palindrome&#xa;            def make_palindrome(tokens):&#xa;                tokens.extend(reversed([t[::-1] for t in tokens]))&#xa;                return ''.join(tokens)&#xa;            print(patt.addParseAction(make_palindrome).parseString(""lskdj sdlkjf lksd"")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'&#xa;        """"""&#xa;        if isinstance(itemseq, ParseResults):&#xa;            self += itemseq&#xa;        else:&#xa;            self.__toklist.extend(itemseq)&#xa;&#xa;    def clear( self ):&#xa;        """"""&#xa;        Clear all elements and results names.&#xa;        """"""&#xa;        del self.__toklist[:]&#xa;        self.__tokdict.clear()&#xa;&#xa;    def __getattr__( self, name ):&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            return """"&#xa;            &#xa;        if name in self.__tokdict:&#xa;            if name not in self.__accumNames:&#xa;                return self.__tokdict[name][-1][0]&#xa;            else:&#xa;                return ParseResults([ v[0] for v in self.__tokdict[name] ])&#xa;        else:&#xa;            return """"&#xa;&#xa;    def __add__( self, other ):&#xa;        ret = self.copy()&#xa;        ret += other&#xa;        return ret&#xa;&#xa;    def __iadd__( self, other ):&#xa;        if other.__tokdict:&#xa;            offset = len(self.__toklist)&#xa;            addoffset = lambda a: offset if a<0 else a+offset&#xa;            otheritems = other.__tokdict.items()&#xa;            otherdictitems = [(k, _ParseResultsWithOffset(v[0],addoffset(v[1])) )&#xa;                                for (k,vlist) in otheritems for v in vlist]&#xa;            for k,v in otherdictitems:&#xa;                self[k] = v&#xa;                if isinstance(v[0],ParseResults):&#xa;                    v[0].__parent = wkref(self)&#xa;            &#xa;        self.__toklist += other.__toklist&#xa;        self.__accumNames.update( other.__accumNames )&#xa;        return self&#xa;&#xa;    def __radd__(self, other):&#xa;        if isinstance(other,int) and other == 0:&#xa;            # useful for merging many ParseResults using sum() builtin&#xa;            return self.copy()&#xa;        else:&#xa;            # this may raise a TypeError - so be it&#xa;            return other + self&#xa;        &#xa;    def __repr__( self ):&#xa;        return ""(%s, %s)"" % ( repr( self.__toklist ), repr( self.__tokdict ) )&#xa;&#xa;    def __str__( self ):&#xa;        return '[' + ', '.join(_ustr(i) if isinstance(i, ParseResults) else repr(i) for i in self.__toklist) + ']'&#xa;&#xa;    def _asStringList( self, sep='' ):&#xa;        out = []&#xa;        for item in self.__toklist:&#xa;            if out and sep:&#xa;                out.append(sep)&#xa;            if isinstance( item, ParseResults ):&#xa;                out += item._asStringList()&#xa;            else:&#xa;                out.append( _ustr(item) )&#xa;        return out&#xa;&#xa;    def asList( self ):&#xa;        """"""&#xa;        Returns the parse results as a nested list of matching tokens, all converted to strings.&#xa;&#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            result = patt.parseString(""sldkj lsdkj sldkj"")&#xa;            # even though the result prints in string-like form, it is actually a pyparsing ParseResults&#xa;            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']&#xa;            &#xa;            # Use asList() to create an actual list&#xa;            result_list = result.asList()&#xa;            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']&#xa;        """"""&#xa;        return [res.asList() if isinstance(res,ParseResults) else res for res in self.__toklist]&#xa;&#xa;    def asDict( self ):&#xa;        """"""&#xa;        Returns the named parse results as a nested dictionary.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})&#xa;            &#xa;            result_dict = result.asDict()&#xa;            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}&#xa;&#xa;            # even though a ParseResults supports dict-like access, sometime you just need to have a dict&#xa;            import json&#xa;            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable&#xa;            print(json.dumps(result.asDict())) # -> {""month"": ""31"", ""day"": ""1999"", ""year"": ""12""}&#xa;        """"""&#xa;        if PY_3:&#xa;            item_fn = self.items&#xa;        else:&#xa;            item_fn = self.iteritems&#xa;            &#xa;        def toItem(obj):&#xa;            if isinstance(obj, ParseResults):&#xa;                if obj.haskeys():&#xa;                    return obj.asDict()&#xa;                else:&#xa;                    return [toItem(v) for v in obj]&#xa;            else:&#xa;                return obj&#xa;                &#xa;        return dict((k,toItem(v)) for k,v in item_fn())&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Returns a new copy of a C{ParseResults} object.&#xa;        """"""&#xa;        ret = ParseResults( self.__toklist )&#xa;        ret.__tokdict = self.__tokdict.copy()&#xa;        ret.__parent = self.__parent&#xa;        ret.__accumNames.update( self.__accumNames )&#xa;        ret.__name = self.__name&#xa;        return ret&#xa;&#xa;    def asXML( self, doctag=None, namedItemsOnly=False, indent="""", formatted=True ):&#xa;        """"""&#xa;        (Deprecated) Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.&#xa;        """"""&#xa;        nl = ""\n""&#xa;        out = []&#xa;        namedItems = dict((v[1],k) for (k,vlist) in self.__tokdict.items()&#xa;                                                            for v in vlist)&#xa;        nextLevelIndent = indent + ""  ""&#xa;&#xa;        # collapse out indents if formatting is not desired&#xa;        if not formatted:&#xa;            indent = """"&#xa;            nextLevelIndent = """"&#xa;            nl = """"&#xa;&#xa;        selfTag = None&#xa;        if doctag is not None:&#xa;            selfTag = doctag&#xa;        else:&#xa;            if self.__name:&#xa;                selfTag = self.__name&#xa;&#xa;        if not selfTag:&#xa;            if namedItemsOnly:&#xa;                return """"&#xa;            else:&#xa;                selfTag = ""ITEM""&#xa;&#xa;        out += [ nl, indent, ""<"", selfTag, "">"" ]&#xa;&#xa;        for i,res in enumerate(self.__toklist):&#xa;            if isinstance(res,ParseResults):&#xa;                if i in namedItems:&#xa;                    out += [ res.asXML(namedItems[i],&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;                else:&#xa;                    out += [ res.asXML(None,&#xa;                                        namedItemsOnly and doctag is None,&#xa;                                        nextLevelIndent,&#xa;                                        formatted)]&#xa;            else:&#xa;                # individual token, see if there is a name for it&#xa;                resTag = None&#xa;                if i in namedItems:&#xa;                    resTag = namedItems[i]&#xa;                if not resTag:&#xa;                    if namedItemsOnly:&#xa;                        continue&#xa;                    else:&#xa;                        resTag = ""ITEM""&#xa;                xmlBodyText = _xml_escape(_ustr(res))&#xa;                out += [ nl, nextLevelIndent, ""<"", resTag, "">"",&#xa;                                                xmlBodyText,&#xa;                                                ""</"", resTag, "">"" ]&#xa;&#xa;        out += [ nl, indent, ""</"", selfTag, "">"" ]&#xa;        return """".join(out)&#xa;&#xa;    def __lookup(self,sub):&#xa;        for k,vlist in self.__tokdict.items():&#xa;            for v,loc in vlist:&#xa;                if sub is v:&#xa;                    return k&#xa;        return None&#xa;&#xa;    def getName(self):&#xa;        """"""&#xa;        Returns the results name for this token expression. Useful when several &#xa;        different expressions might match at a particular location.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            ssn_expr = Regex(r""\d\d\d-\d\d-\d\d\d\d"")&#xa;            house_number_expr = Suppress('#') + Word(nums, alphanums)&#xa;            user_data = (Group(house_number_expr)(""house_number"") &#xa;                        | Group(ssn_expr)(""ssn"")&#xa;                        | Group(integer)(""age""))&#xa;            user_info = OneOrMore(user_data)&#xa;            &#xa;            result = user_info.parseString(""22 111-22-3333 #221B"")&#xa;            for item in result:&#xa;                print(item.getName(), ':', item[0])&#xa;        prints::&#xa;            age : 22&#xa;            ssn : 111-22-3333&#xa;            house_number : 221B&#xa;        """"""&#xa;        if self.__name:&#xa;            return self.__name&#xa;        elif self.__parent:&#xa;            par = self.__parent()&#xa;            if par:&#xa;                return par.__lookup(self)&#xa;            else:&#xa;                return None&#xa;        elif (len(self) == 1 and&#xa;               len(self.__tokdict) == 1 and&#xa;               next(iter(self.__tokdict.values()))[0][1] in (0,-1)):&#xa;            return next(iter(self.__tokdict.keys()))&#xa;        else:&#xa;            return None&#xa;&#xa;    def dump(self, indent='', depth=0, full=True):&#xa;        """"""&#xa;        Diagnostic method for listing out the contents of a C{ParseResults}.&#xa;        Accepts an optional C{indent} argument so that this string can be embedded&#xa;        in a nested display of other data.&#xa;&#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;            &#xa;            result = date_str.parseString('12/31/1999')&#xa;            print(result.dump())&#xa;        prints::&#xa;            ['12', '/', '31', '/', '1999']&#xa;            - day: 1999&#xa;            - month: 31&#xa;            - year: 12&#xa;        """"""&#xa;        out = []&#xa;        NL = '\n'&#xa;        out.append( indent+_ustr(self.asList()) )&#xa;        if full:&#xa;            if self.haskeys():&#xa;                items = sorted((str(k), v) for k,v in self.items())&#xa;                for k,v in items:&#xa;                    if out:&#xa;                        out.append(NL)&#xa;                    out.append( ""%s%s- %s: "" % (indent,('  '*depth), k) )&#xa;                    if isinstance(v,ParseResults):&#xa;                        if v:&#xa;                            out.append( v.dump(indent,depth+1) )&#xa;                        else:&#xa;                            out.append(_ustr(v))&#xa;                    else:&#xa;                        out.append(repr(v))&#xa;            elif any(isinstance(vv,ParseResults) for vv in self):&#xa;                v = self&#xa;                for i,vv in enumerate(v):&#xa;                    if isinstance(vv,ParseResults):&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),vv.dump(indent,depth+1) ))&#xa;                    else:&#xa;                        out.append(""\n%s%s[%d]:\n%s%s%s"" % (indent,('  '*(depth)),i,indent,('  '*(depth+1)),_ustr(vv)))&#xa;            &#xa;        return """".join(out)&#xa;&#xa;    def pprint(self, *args, **kwargs):&#xa;        """"""&#xa;        Pretty-printer for parsed results as a list, using the C{pprint} module.&#xa;        Accepts additional positional or keyword args as defined for the &#xa;        C{pprint.pprint} method. (U{http://docs.python.org/3/library/pprint.html#pprint.pprint})&#xa;&#xa;        Example::&#xa;            ident = Word(alphas, alphanums)&#xa;            num = Word(nums)&#xa;            func = Forward()&#xa;            term = ident | num | Group('(' + func + ')')&#xa;            func <<= ident + Group(Optional(delimitedList(term)))&#xa;            result = func.parseString(""fna a,b,(fnb c,d,200),100"")&#xa;            result.pprint(width=40)&#xa;        prints::&#xa;            ['fna',&#xa;             ['a',&#xa;              'b',&#xa;              ['(', 'fnb', ['c', 'd', '200'], ')'],&#xa;              '100']]&#xa;        """"""&#xa;        pprint.pprint(self.asList(), *args, **kwargs)&#xa;&#xa;    # add support for pickle protocol&#xa;    def __getstate__(self):&#xa;        return ( self.__toklist,&#xa;                 ( self.__tokdict.copy(),&#xa;                   self.__parent is not None and self.__parent() or None,&#xa;                   self.__accumNames,&#xa;                   self.__name ) )&#xa;&#xa;    def __setstate__(self,state):&#xa;        self.__toklist = state[0]&#xa;        (self.__tokdict,&#xa;         par,&#xa;         inAccumNames,&#xa;         self.__name) = state[1]&#xa;        self.__accumNames = {}&#xa;        self.__accumNames.update(inAccumNames)&#xa;        if par is not None:&#xa;            self.__parent = wkref(par)&#xa;        else:&#xa;            self.__parent = None&#xa;&#xa;    def __getnewargs__(self):&#xa;        return self.__toklist, self.__name, self.__asList, self.__modal&#xa;&#xa;    def __dir__(self):&#xa;        return (dir(type(self)) + list(self.keys()))&#xa;&#xa;collections.MutableMapping.register(ParseResults)&#xa;&#xa;def col (loc,strg):&#xa;    """"""Returns current column within a string, counting newlines as line separators.&#xa;   The first column is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    s = strg&#xa;    return 1 if 0<loc<len(s) and s[loc-1] == '\n' else loc - s.rfind(""\n"", 0, loc)&#xa;&#xa;def lineno(loc,strg):&#xa;    """"""Returns current line number within a string, counting newlines as line separators.&#xa;   The first line is number 1.&#xa;&#xa;   Note: the default parsing behavior is to expand tabs in the input string&#xa;   before starting the parsing process.  See L{I{ParserElement.parseString}<ParserElement.parseString>} for more information&#xa;   on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;   consistent view of the parsed string, the parse location, and line and column&#xa;   positions within the parsed string.&#xa;   """"""&#xa;    return strg.count(""\n"",0,loc) + 1&#xa;&#xa;def line( loc, strg ):&#xa;    """"""Returns the line of text containing loc within a string, counting newlines as line separators.&#xa;       """"""&#xa;    lastCR = strg.rfind(""\n"", 0, loc)&#xa;    nextCR = strg.find(""\n"", loc)&#xa;    if nextCR >= 0:&#xa;        return strg[lastCR+1:nextCR]&#xa;    else:&#xa;        return strg[lastCR+1:]&#xa;&#xa;def _defaultStartDebugAction( instring, loc, expr ):&#xa;    print ((""Match "" + _ustr(expr) + "" at loc "" + _ustr(loc) + ""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) )))&#xa;&#xa;def _defaultSuccessDebugAction( instring, startloc, endloc, expr, toks ):&#xa;    print (""Matched "" + _ustr(expr) + "" -> "" + str(toks.asList()))&#xa;&#xa;def _defaultExceptionDebugAction( instring, loc, expr, exc ):&#xa;    print (""Exception raised:"" + _ustr(exc))&#xa;&#xa;def nullDebugAction(*args):&#xa;    """"""'Do-nothing' debug action, to suppress debugging output during parsing.""""""&#xa;    pass&#xa;&#xa;# Only works on Python 3.x - nonlocal is toxic to Python 2 installs&#xa;#~ 'decorator to trim function calls to match the arity of the target'&#xa;#~ def _trim_arity(func, maxargs=3):&#xa;    #~ if func in singleArgBuiltins:&#xa;        #~ return lambda s,l,t: func(t)&#xa;    #~ limit = 0&#xa;    #~ foundArity = False&#xa;    #~ def wrapper(*args):&#xa;        #~ nonlocal limit,foundArity&#xa;        #~ while 1:&#xa;            #~ try:&#xa;                #~ ret = func(*args[limit:])&#xa;                #~ foundArity = True&#xa;                #~ return ret&#xa;            #~ except TypeError:&#xa;                #~ if limit == maxargs or foundArity:&#xa;                    #~ raise&#xa;                #~ limit += 1&#xa;                #~ continue&#xa;    #~ return wrapper&#xa;&#xa;# this version is Python 2.x-3.x cross-compatible&#xa;'decorator to trim function calls to match the arity of the target'&#xa;def _trim_arity(func, maxargs=2):&#xa;    if func in singleArgBuiltins:&#xa;        return lambda s,l,t: func(t)&#xa;    limit = [0]&#xa;    foundArity = [False]&#xa;    &#xa;    # traceback return data structure changed in Py3.5 - normalize back to plain tuples&#xa;    if system_version[:2] >= (3,5):&#xa;        def extract_stack(limit=0):&#xa;            # special handling for Python 3.5.0 - extra deep call stack by 1&#xa;            offset = -3 if system_version == (3,5,0) else -2&#xa;            frame_summary = traceback.extract_stack(limit=-offset+limit-1)[offset]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;        def extract_tb(tb, limit=0):&#xa;            frames = traceback.extract_tb(tb, limit=limit)&#xa;            frame_summary = frames[-1]&#xa;            return [(frame_summary.filename, frame_summary.lineno)]&#xa;    else:&#xa;        extract_stack = traceback.extract_stack&#xa;        extract_tb = traceback.extract_tb&#xa;    &#xa;    # synthesize what would be returned by traceback.extract_stack at the call to &#xa;    # user's parse action 'func', so that we don't incur call penalty at parse time&#xa;    &#xa;    LINE_DIFF = 6&#xa;    # IF ANY CODE CHANGES, EVEN JUST COMMENTS OR BLANK LINES, BETWEEN THE NEXT LINE AND &#xa;    # THE CALL TO FUNC INSIDE WRAPPER, LINE_DIFF MUST BE MODIFIED!!!!&#xa;    this_line = extract_stack(limit=2)[-1]&#xa;    pa_call_line_synth = (this_line[0], this_line[1]+LINE_DIFF)&#xa;&#xa;    def wrapper(*args):&#xa;        while 1:&#xa;            try:&#xa;                ret = func(*args[limit[0]:])&#xa;                foundArity[0] = True&#xa;                return ret&#xa;            except TypeError:&#xa;                # re-raise TypeErrors if they did not come from our arity testing&#xa;                if foundArity[0]:&#xa;                    raise&#xa;                else:&#xa;                    try:&#xa;                        tb = sys.exc_info()[-1]&#xa;                        if not extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth:&#xa;                            raise&#xa;                    finally:&#xa;                        del tb&#xa;&#xa;                if limit[0] <= maxargs:&#xa;                    limit[0] += 1&#xa;                    continue&#xa;                raise&#xa;&#xa;    # copy func name to wrapper for sensible debug output&#xa;    func_name = ""<parse action>""&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    wrapper.__name__ = func_name&#xa;&#xa;    return wrapper&#xa;&#xa;class ParserElement(object):&#xa;    """"""Abstract base level parser element class.""""""&#xa;    DEFAULT_WHITE_CHARS = "" \n\t\r""&#xa;    verbose_stacktrace = False&#xa;&#xa;    @staticmethod&#xa;    def setDefaultWhitespaceChars( chars ):&#xa;        r""""""&#xa;        Overrides the default whitespace chars&#xa;&#xa;        Example::&#xa;            # default whitespace chars are space, <TAB> and newline&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def', 'ghi', 'jkl']&#xa;            &#xa;            # change to just treat newline as significant&#xa;            ParserElement.setDefaultWhitespaceChars("" \t"")&#xa;            OneOrMore(Word(alphas)).parseString(""abc def\nghi jkl"")  # -> ['abc', 'def']&#xa;        """"""&#xa;        ParserElement.DEFAULT_WHITE_CHARS = chars&#xa;&#xa;    @staticmethod&#xa;    def inlineLiteralsUsing(cls):&#xa;        """"""&#xa;        Set class to be used for inclusion of string literals into a parser.&#xa;        &#xa;        Example::&#xa;            # default literal class used is Literal&#xa;            integer = Word(nums)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;&#xa;            # change to Suppress&#xa;            ParserElement.inlineLiteralsUsing(Suppress)&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")           &#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '12', '31']&#xa;        """"""&#xa;        ParserElement._literalStringClass = cls&#xa;&#xa;    def __init__( self, savelist=False ):&#xa;        self.parseAction = list()&#xa;        self.failAction = None&#xa;        #~ self.name = ""<unknown>""  # don't define self.name, let subclasses try/except upcall&#xa;        self.strRepr = None&#xa;        self.resultsName = None&#xa;        self.saveAsList = savelist&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        self.copyDefaultWhiteChars = True&#xa;        self.mayReturnEmpty = False # used when checking for left-recursion&#xa;        self.keepTabs = False&#xa;        self.ignoreExprs = list()&#xa;        self.debug = False&#xa;        self.streamlined = False&#xa;        self.mayIndexError = True # used to optimize exception handling for subclasses that don't advance parse index&#xa;        self.errmsg = """"&#xa;        self.modalResults = True # used to mark results names as modal (report only last) or cumulative (list all)&#xa;        self.debugActions = ( None, None, None ) #custom debug actions&#xa;        self.re = None&#xa;        self.callPreparse = True # used to avoid redundant calls to preParse&#xa;        self.callDuringTry = False&#xa;&#xa;    def copy( self ):&#xa;        """"""&#xa;        Make a copy of this C{ParserElement}.  Useful for defining different parse actions&#xa;        for the same parsing pattern, using copies of the original parse element.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            integerK = integer.copy().addParseAction(lambda toks: toks[0]*1024) + Suppress(""K"")&#xa;            integerM = integer.copy().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;            &#xa;            print(OneOrMore(integerK | integerM | integer).parseString(""5K 100 640K 256M""))&#xa;        prints::&#xa;            [5120, 100, 655360, 268435456]&#xa;        Equivalent form of C{expr.copy()} is just C{expr()}::&#xa;            integerM = integer().addParseAction(lambda toks: toks[0]*1024*1024) + Suppress(""M"")&#xa;        """"""&#xa;        cpy = copy.copy( self )&#xa;        cpy.parseAction = self.parseAction[:]&#xa;        cpy.ignoreExprs = self.ignoreExprs[:]&#xa;        if self.copyDefaultWhiteChars:&#xa;            cpy.whiteChars = ParserElement.DEFAULT_WHITE_CHARS&#xa;        return cpy&#xa;&#xa;    def setName( self, name ):&#xa;        """"""&#xa;        Define name for this expression, makes debugging and exception messages clearer.&#xa;        &#xa;        Example::&#xa;            Word(nums).parseString(""ABC"")  # -> Exception: Expected W:(0123...) (at char 0), (line:1, col:1)&#xa;            Word(nums).setName(""integer"").parseString(""ABC"")  # -> Exception: Expected integer (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        self.name = name&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        if hasattr(self,""exception""):&#xa;            self.exception.msg = self.errmsg&#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        """"""&#xa;        Define name for referencing matching tokens as a nested attribute&#xa;        of the returned parse results.&#xa;        NOTE: this returns a *copy* of the original C{ParserElement} object;&#xa;        this is so that the client can define a basic element, such as an&#xa;        integer, and reference it in multiple places with different names.&#xa;&#xa;        You can also set results names using the abbreviated syntax,&#xa;        C{expr(""name"")} in place of C{expr.setResultsName(""name"")} - &#xa;        see L{I{__call__}<__call__>}.&#xa;&#xa;        Example::&#xa;            date_str = (integer.setResultsName(""year"") + '/' &#xa;                        + integer.setResultsName(""month"") + '/' &#xa;                        + integer.setResultsName(""day""))&#xa;&#xa;            # equivalent form:&#xa;            date_str = integer(""year"") + '/' + integer(""month"") + '/' + integer(""day"")&#xa;        """"""&#xa;        newself = self.copy()&#xa;        if name.endswith(""*""):&#xa;            name = name[:-1]&#xa;            listAllMatches=True&#xa;        newself.resultsName = name&#xa;        newself.modalResults = not listAllMatches&#xa;        return newself&#xa;&#xa;    def setBreak(self,breakFlag = True):&#xa;        """"""Method to invoke the Python pdb debugger when this element is&#xa;           about to be parsed. Set C{breakFlag} to True to enable, False to&#xa;           disable.&#xa;        """"""&#xa;        if breakFlag:&#xa;            _parseMethod = self._parse&#xa;            def breaker(instring, loc, doActions=True, callPreParse=True):&#xa;                import pdb&#xa;                pdb.set_trace()&#xa;                return _parseMethod( instring, loc, doActions, callPreParse )&#xa;            breaker._originalParseMethod = _parseMethod&#xa;            self._parse = breaker&#xa;        else:&#xa;            if hasattr(self._parse,""_originalParseMethod""):&#xa;                self._parse = self._parse._originalParseMethod&#xa;        return self&#xa;&#xa;    def setParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Define action to perform when successfully matching parse element definition.&#xa;        Parse action fn is a callable method with 0-3 arguments, called as C{fn(s,loc,toks)},&#xa;        C{fn(loc,toks)}, C{fn(toks)}, or just C{fn()}, where:&#xa;         - s   = the original string being parsed (see note below)&#xa;         - loc = the location of the matching substring&#xa;         - toks = a list of the matched tokens, packaged as a C{L{ParseResults}} object&#xa;        If the functions in fns modify the tokens, they can return them as the return&#xa;        value from fn, and the modified list of tokens will replace the original.&#xa;        Otherwise, fn does not need to return any value.&#xa;&#xa;        Optional keyword arguments:&#xa;         - callDuringTry = (default=C{False}) indicate if parse action should be run during lookaheads and alternate testing&#xa;&#xa;        Note: the default parsing behavior is to expand tabs in the input string&#xa;        before starting the parsing process.  See L{I{parseString}<parseString>} for more information&#xa;        on parsing strings containing C{<TAB>}s, and suggested methods to maintain a&#xa;        consistent view of the parsed string, the parse location, and line and column&#xa;        positions within the parsed string.&#xa;        &#xa;        Example::&#xa;            integer = Word(nums)&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            date_str.parseString(""1999/12/31"")  # -> ['1999', '/', '12', '/', '31']&#xa;&#xa;            # use parse action to convert to ints at parse time&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            date_str = integer + '/' + integer + '/' + integer&#xa;&#xa;            # note that integer fields are now ints, not strings&#xa;            date_str.parseString(""1999/12/31"")  # -> [1999, '/', 12, '/', 31]&#xa;        """"""&#xa;        self.parseAction = list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addParseAction( self, *fns, **kwargs ):&#xa;        """"""&#xa;        Add parse action to expression's list of parse actions. See L{I{setParseAction}<setParseAction>}.&#xa;        &#xa;        See examples in L{I{copy}<copy>}.&#xa;        """"""&#xa;        self.parseAction += list(map(_trim_arity, list(fns)))&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def addCondition(self, *fns, **kwargs):&#xa;        """"""Add a boolean predicate function to expression's list of parse actions. See &#xa;        L{I{setParseAction}<setParseAction>} for function call signatures. Unlike C{setParseAction}, &#xa;        functions passed to C{addCondition} need to return boolean success/fail of the condition.&#xa;&#xa;        Optional keyword arguments:&#xa;         - message = define a custom message to be used in the raised exception&#xa;         - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException&#xa;         &#xa;        Example::&#xa;            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;            year_int = integer.copy()&#xa;            year_int.addCondition(lambda toks: toks[0] >= 2000, message=""Only support years 2000 and later"")&#xa;            date_str = year_int + '/' + integer + '/' + integer&#xa;&#xa;            result = date_str.parseString(""1999/12/31"")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)&#xa;        """"""&#xa;        msg = kwargs.get(""message"", ""failed user-defined condition"")&#xa;        exc_type = ParseFatalException if kwargs.get(""fatal"", False) else ParseException&#xa;        for fn in fns:&#xa;            def pa(s,l,t):&#xa;                if not bool(_trim_arity(fn)(s,l,t)):&#xa;                    raise exc_type(s,l,msg)&#xa;            self.parseAction.append(pa)&#xa;        self.callDuringTry = self.callDuringTry or kwargs.get(""callDuringTry"", False)&#xa;        return self&#xa;&#xa;    def setFailAction( self, fn ):&#xa;        """"""Define action to perform if parsing fails at this expression.&#xa;           Fail acton fn is a callable function that takes the arguments&#xa;           C{fn(s,loc,expr,err)} where:&#xa;            - s = string being parsed&#xa;            - loc = location where expression match was attempted and failed&#xa;            - expr = the parse expression that failed&#xa;            - err = the exception thrown&#xa;           The function returns no value.  It may throw C{L{ParseFatalException}}&#xa;           if it is desired to stop parsing immediately.""""""&#xa;        self.failAction = fn&#xa;        return self&#xa;&#xa;    def _skipIgnorables( self, instring, loc ):&#xa;        exprsFound = True&#xa;        while exprsFound:&#xa;            exprsFound = False&#xa;            for e in self.ignoreExprs:&#xa;                try:&#xa;                    while 1:&#xa;                        loc,dummy = e._parse( instring, loc )&#xa;                        exprsFound = True&#xa;                except ParseException:&#xa;                    pass&#xa;        return loc&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if self.ignoreExprs:&#xa;            loc = self._skipIgnorables( instring, loc )&#xa;&#xa;        if self.skipWhitespace:&#xa;            wt = self.whiteChars&#xa;            instrlen = len(instring)&#xa;            while loc < instrlen and instring[loc] in wt:&#xa;                loc += 1&#xa;&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        return loc, []&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return tokenlist&#xa;&#xa;    #~ @profile&#xa;    def _parseNoCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        debugging = ( self.debug ) #and doActions )&#xa;&#xa;        if debugging or self.failAction:&#xa;            #~ print (""Match"",self,""at loc"",loc,""(%d,%d)"" % ( lineno(loc,instring), col(loc,instring) ))&#xa;            if (self.debugActions[0] ):&#xa;                self.debugActions[0]( instring, loc, self )&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            try:&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            except ParseBaseException as err:&#xa;                #~ print (""Exception raised:"", err)&#xa;                if self.debugActions[2]:&#xa;                    self.debugActions[2]( instring, tokensStart, self, err )&#xa;                if self.failAction:&#xa;                    self.failAction( instring, tokensStart, self, err )&#xa;                raise&#xa;        else:&#xa;            if callPreParse and self.callPreparse:&#xa;                preloc = self.preParse( instring, loc )&#xa;            else:&#xa;                preloc = loc&#xa;            tokensStart = preloc&#xa;            if self.mayIndexError or loc >= len(instring):&#xa;                try:&#xa;                    loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;                except IndexError:&#xa;                    raise ParseException( instring, len(instring), self.errmsg, self )&#xa;            else:&#xa;                loc,tokens = self.parseImpl( instring, preloc, doActions )&#xa;&#xa;        tokens = self.postParse( instring, loc, tokens )&#xa;&#xa;        retTokens = ParseResults( tokens, self.resultsName, asList=self.saveAsList, modal=self.modalResults )&#xa;        if self.parseAction and (doActions or self.callDuringTry):&#xa;            if debugging:&#xa;                try:&#xa;                    for fn in self.parseAction:&#xa;                        tokens = fn( instring, tokensStart, retTokens )&#xa;                        if tokens is not None:&#xa;                            retTokens = ParseResults( tokens,&#xa;                                                      self.resultsName,&#xa;                                                      asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                      modal=self.modalResults )&#xa;                except ParseBaseException as err:&#xa;                    #~ print ""Exception raised in user parse action:"", err&#xa;                    if (self.debugActions[2] ):&#xa;                        self.debugActions[2]( instring, tokensStart, self, err )&#xa;                    raise&#xa;            else:&#xa;                for fn in self.parseAction:&#xa;                    tokens = fn( instring, tokensStart, retTokens )&#xa;                    if tokens is not None:&#xa;                        retTokens = ParseResults( tokens,&#xa;                                                  self.resultsName,&#xa;                                                  asList=self.saveAsList and isinstance(tokens,(ParseResults,list)),&#xa;                                                  modal=self.modalResults )&#xa;&#xa;        if debugging:&#xa;            #~ print (""Matched"",self,""->"",retTokens.asList())&#xa;            if (self.debugActions[1] ):&#xa;                self.debugActions[1]( instring, tokensStart, loc, self, retTokens )&#xa;&#xa;        return loc, retTokens&#xa;&#xa;    def tryParse( self, instring, loc ):&#xa;        try:&#xa;            return self._parse( instring, loc, doActions=False )[0]&#xa;        except ParseFatalException:&#xa;            raise ParseException( instring, loc, self.errmsg, self)&#xa;    &#xa;    def canParseNext(self, instring, loc):&#xa;        try:&#xa;            self.tryParse(instring, loc)&#xa;        except (ParseException, IndexError):&#xa;            return False&#xa;        else:&#xa;            return True&#xa;&#xa;    class _UnboundedCache(object):&#xa;        def __init__(self):&#xa;            cache = {}&#xa;            self.not_in_cache = not_in_cache = object()&#xa;&#xa;            def get(self, key):&#xa;                return cache.get(key, not_in_cache)&#xa;&#xa;            def set(self, key, value):&#xa;                cache[key] = value&#xa;&#xa;            def clear(self):&#xa;                cache.clear()&#xa;&#xa;            self.get = types.MethodType(get, self)&#xa;            self.set = types.MethodType(set, self)&#xa;            self.clear = types.MethodType(clear, self)&#xa;&#xa;    if _OrderedDict is not None:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = _OrderedDict()&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.popitem(False)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    else:&#xa;        class _FifoCache(object):&#xa;            def __init__(self, size):&#xa;                self.not_in_cache = not_in_cache = object()&#xa;&#xa;                cache = {}&#xa;                key_fifo = collections.deque([], size)&#xa;&#xa;                def get(self, key):&#xa;                    return cache.get(key, not_in_cache)&#xa;&#xa;                def set(self, key, value):&#xa;                    cache[key] = value&#xa;                    if len(cache) > size:&#xa;                        cache.pop(key_fifo.popleft(), None)&#xa;                    key_fifo.append(key)&#xa;&#xa;                def clear(self):&#xa;                    cache.clear()&#xa;                    key_fifo.clear()&#xa;&#xa;                self.get = types.MethodType(get, self)&#xa;                self.set = types.MethodType(set, self)&#xa;                self.clear = types.MethodType(clear, self)&#xa;&#xa;    # argument cache for optimizing repeated calls when backtracking through recursive expressions&#xa;    packrat_cache = {} # this is set later by enabledPackrat(); this is here so that resetCache() doesn't fail&#xa;    packrat_cache_lock = RLock()&#xa;    packrat_cache_stats = [0, 0]&#xa;&#xa;    # this method gets repeatedly called during backtracking with the same arguments -&#xa;    # we can cache these arguments and save ourselves the trouble of re-parsing the contained expression&#xa;    def _parseCache( self, instring, loc, doActions=True, callPreParse=True ):&#xa;        HIT, MISS = 0, 1&#xa;        lookup = (self, instring, loc, callPreParse, doActions)&#xa;        with ParserElement.packrat_cache_lock:&#xa;            cache = ParserElement.packrat_cache&#xa;            value = cache.get(lookup)&#xa;            if value is cache.not_in_cache:&#xa;                ParserElement.packrat_cache_stats[MISS] += 1&#xa;                try:&#xa;                    value = self._parseNoCache(instring, loc, doActions, callPreParse)&#xa;                except ParseBaseException as pe:&#xa;                    # cache a copy of the exception, without the traceback&#xa;                    cache.set(lookup, pe.__class__(*pe.args))&#xa;                    raise&#xa;                else:&#xa;                    cache.set(lookup, (value[0], value[1].copy()))&#xa;                    return value&#xa;            else:&#xa;                ParserElement.packrat_cache_stats[HIT] += 1&#xa;                if isinstance(value, Exception):&#xa;                    raise value&#xa;                return (value[0], value[1].copy())&#xa;&#xa;    _parse = _parseNoCache&#xa;&#xa;    @staticmethod&#xa;    def resetCache():&#xa;        ParserElement.packrat_cache.clear()&#xa;        ParserElement.packrat_cache_stats[:] = [0] * len(ParserElement.packrat_cache_stats)&#xa;&#xa;    _packratEnabled = False&#xa;    @staticmethod&#xa;    def enablePackrat(cache_size_limit=128):&#xa;        """"""Enables ""packrat"" parsing, which adds memoizing to the parsing logic.&#xa;           Repeated parse attempts at the same string location (which happens&#xa;           often in many complex grammars) can immediately return a cached value,&#xa;           instead of re-executing parsing/validating code.  Memoizing is done of&#xa;           both valid results and parsing exceptions.&#xa;           &#xa;           Parameters:&#xa;            - cache_size_limit - (default=C{128}) - if an integer value is provided&#xa;              will limit the size of the packrat cache; if None is passed, then&#xa;              the cache size will be unbounded; if 0 is passed, the cache will&#xa;              be effectively disabled.&#xa;            &#xa;           This speedup may break existing programs that use parse actions that&#xa;           have side-effects.  For this reason, packrat parsing is disabled when&#xa;           you first import pyparsing.  To activate the packrat feature, your&#xa;           program must call the class method C{ParserElement.enablePackrat()}.  If&#xa;           your program uses C{psyco} to ""compile as you go"", you must call&#xa;           C{enablePackrat} before calling C{psyco.full()}.  If you do not do this,&#xa;           Python will crash.  For best results, call C{enablePackrat()} immediately&#xa;           after importing pyparsing.&#xa;           &#xa;           Example::&#xa;               import pyparsing&#xa;               pyparsing.ParserElement.enablePackrat()&#xa;        """"""&#xa;        if not ParserElement._packratEnabled:&#xa;            ParserElement._packratEnabled = True&#xa;            if cache_size_limit is None:&#xa;                ParserElement.packrat_cache = ParserElement._UnboundedCache()&#xa;            else:&#xa;                ParserElement.packrat_cache = ParserElement._FifoCache(cache_size_limit)&#xa;            ParserElement._parse = ParserElement._parseCache&#xa;&#xa;    def parseString( self, instring, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression with the given string.&#xa;        This is the main interface to the client code, once the complete&#xa;        expression has been built.&#xa;&#xa;        If you want the grammar to require that the entire input string be&#xa;        successfully parsed, then set C{parseAll} to True (equivalent to ending&#xa;        the grammar with C{L{StringEnd()}}).&#xa;&#xa;        Note: C{parseString} implicitly calls C{expandtabs()} on the input string,&#xa;        in order to report proper column numbers in parse actions.&#xa;        If the input string contains tabs and&#xa;        the grammar uses parse actions that use the C{loc} argument to index into the&#xa;        string being parsed, you can ensure you have a consistent view of the input&#xa;        string by:&#xa;         - calling C{parseWithTabs} on your grammar before calling C{parseString}&#xa;           (see L{I{parseWithTabs}<parseWithTabs>})&#xa;         - define your parse action using the full C{(s,loc,toks)} signature, and&#xa;           reference the input string using the parse action's C{s} argument&#xa;         - explictly expand the tabs in your input string before calling&#xa;           C{parseString}&#xa;        &#xa;        Example::&#xa;            Word('a').parseString('aaaaabaaa')  # -> ['aaaaa']&#xa;            Word('a').parseString('aaaaabaaa', parseAll=True)  # -> Exception: Expected end of text&#xa;        """"""&#xa;        ParserElement.resetCache()&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;            #~ self.saveAsList = True&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;        if not self.keepTabs:&#xa;            instring = instring.expandtabs()&#xa;        try:&#xa;            loc, tokens = self._parse( instring, 0 )&#xa;            if parseAll:&#xa;                loc = self.preParse( instring, loc )&#xa;                se = Empty() + StringEnd()&#xa;                se._parse( instring, loc )&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;        else:&#xa;            return tokens&#xa;&#xa;    def scanString( self, instring, maxMatches=_MAX_INT, overlap=False ):&#xa;        """"""&#xa;        Scan the input string for expression matches.  Each match will return the&#xa;        matching tokens, start location, and end location.  May be called with optional&#xa;        C{maxMatches} argument, to clip scanning after 'n' matches are found.  If&#xa;        C{overlap} is specified, then overlapping matches will be reported.&#xa;&#xa;        Note that the start and end locations are reported relative to the string&#xa;        being parsed.  See L{I{parseString}<parseString>} for more information on parsing&#xa;        strings with embedded tabs.&#xa;&#xa;        Example::&#xa;            source = ""sldjf123lsdjjkf345sldkjf879lkjsfd987""&#xa;            print(source)&#xa;            for tokens,start,end in Word(alphas).scanString(source):&#xa;                print(' '*start + '^'*(end-start))&#xa;                print(' '*start + tokens[0])&#xa;        &#xa;        prints::&#xa;        &#xa;            sldjf123lsdjjkf345sldkjf879lkjsfd987&#xa;            ^^^^^&#xa;            sldjf&#xa;                    ^^^^^^^&#xa;                    lsdjjkf&#xa;                              ^^^^^^&#xa;                              sldkjf&#xa;                                       ^^^^^^&#xa;                                       lkjsfd&#xa;        """"""&#xa;        if not self.streamlined:&#xa;            self.streamline()&#xa;        for e in self.ignoreExprs:&#xa;            e.streamline()&#xa;&#xa;        if not self.keepTabs:&#xa;            instring = _ustr(instring).expandtabs()&#xa;        instrlen = len(instring)&#xa;        loc = 0&#xa;        preparseFn = self.preParse&#xa;        parseFn = self._parse&#xa;        ParserElement.resetCache()&#xa;        matches = 0&#xa;        try:&#xa;            while loc <= instrlen and matches < maxMatches:&#xa;                try:&#xa;                    preloc = preparseFn( instring, loc )&#xa;                    nextLoc,tokens = parseFn( instring, preloc, callPreParse=False )&#xa;                except ParseException:&#xa;                    loc = preloc+1&#xa;                else:&#xa;                    if nextLoc > loc:&#xa;                        matches += 1&#xa;                        yield tokens, preloc, nextLoc&#xa;                        if overlap:&#xa;                            nextloc = preparseFn( instring, loc )&#xa;                            if nextloc > loc:&#xa;                                loc = nextLoc&#xa;                            else:&#xa;                                loc += 1&#xa;                        else:&#xa;                            loc = nextLoc&#xa;                    else:&#xa;                        loc = preloc+1&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def transformString( self, instring ):&#xa;        """"""&#xa;        Extension to C{L{scanString}}, to modify matching text with modified tokens that may&#xa;        be returned from a parse action.  To use C{transformString}, define a grammar and&#xa;        attach a parse action to it that modifies the returned token list.&#xa;        Invoking C{transformString()} on a target string will then scan for matches,&#xa;        and replace the matched text patterns according to the logic in the parse&#xa;        action.  C{transformString()} returns the resulting transformed string.&#xa;        &#xa;        Example::&#xa;            wd = Word(alphas)&#xa;            wd.setParseAction(lambda toks: toks[0].title())&#xa;            &#xa;            print(wd.transformString(""now is the winter of our discontent made glorious summer by this sun of york.""))&#xa;        Prints::&#xa;            Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York.&#xa;        """"""&#xa;        out = []&#xa;        lastE = 0&#xa;        # force preservation of <TAB>s, to minimize unwanted transformation of string, and to&#xa;        # keep string locs straight between transformString and scanString&#xa;        self.keepTabs = True&#xa;        try:&#xa;            for t,s,e in self.scanString( instring ):&#xa;                out.append( instring[lastE:s] )&#xa;                if t:&#xa;                    if isinstance(t,ParseResults):&#xa;                        out += t.asList()&#xa;                    elif isinstance(t,list):&#xa;                        out += t&#xa;                    else:&#xa;                        out.append(t)&#xa;                lastE = e&#xa;            out.append(instring[lastE:])&#xa;            out = [o for o in out if o]&#xa;            return """".join(map(_ustr,_flatten(out)))&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def searchString( self, instring, maxMatches=_MAX_INT ):&#xa;        """"""&#xa;        Another extension to C{L{scanString}}, simplifying the access to the tokens found&#xa;        to match the given parse expression.  May be called with optional&#xa;        C{maxMatches} argument, to clip searching after 'n' matches are found.&#xa;        &#xa;        Example::&#xa;            # a capitalized word starts with an uppercase letter, followed by zero or more lowercase letters&#xa;            cap_word = Word(alphas.upper(), alphas.lower())&#xa;            &#xa;            print(cap_word.searchString(""More than Iron, more than Lead, more than Gold I need Electricity""))&#xa;        prints::&#xa;            ['More', 'Iron', 'Lead', 'Gold', 'I']&#xa;        """"""&#xa;        try:&#xa;            return ParseResults([ t for t,s,e in self.scanString( instring, maxMatches ) ])&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def split(self, instring, maxsplit=_MAX_INT, includeSeparators=False):&#xa;        """"""&#xa;        Generator method to split a string using the given expression as a separator.&#xa;        May be called with optional C{maxsplit} argument, to limit the number of splits;&#xa;        and the optional C{includeSeparators} argument (default=C{False}), if the separating&#xa;        matching text should be included in the split results.&#xa;        &#xa;        Example::        &#xa;            punc = oneOf(list("".,;:/-!?""))&#xa;            print(list(punc.split(""This, this?, this sentence, is badly punctuated!"")))&#xa;        prints::&#xa;            ['This', ' this', '', ' this sentence', ' is badly punctuated', '']&#xa;        """"""&#xa;        splits = 0&#xa;        last = 0&#xa;        for t,s,e in self.scanString(instring, maxMatches=maxsplit):&#xa;            yield instring[last:s]&#xa;            if includeSeparators:&#xa;                yield t[0]&#xa;            last = e&#xa;        yield instring[last:]&#xa;&#xa;    def __add__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator - returns C{L{And}}. Adding strings to a ParserElement&#xa;        converts them to L{Literal}s by default.&#xa;        &#xa;        Example::&#xa;            greet = Word(alphas) + "","" + Word(alphas) + ""!""&#xa;            hello = ""Hello, World!""&#xa;            print (hello, ""->"", greet.parseString(hello))&#xa;        Prints::&#xa;            Hello, World! -> ['Hello', ',', 'World', '!']&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, other ] )&#xa;&#xa;    def __radd__(self, other ):&#xa;        """"""&#xa;        Implementation of + operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other + self&#xa;&#xa;    def __sub__(self, other):&#xa;        """"""&#xa;        Implementation of - operator, returns C{L{And}} with error stop&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return And( [ self, And._ErrorStop(), other ] )&#xa;&#xa;    def __rsub__(self, other ):&#xa;        """"""&#xa;        Implementation of - operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other - self&#xa;&#xa;    def __mul__(self,other):&#xa;        """"""&#xa;        Implementation of * operator, allows use of C{expr * 3} in place of&#xa;        C{expr + expr + expr}.  Expressions may also me multiplied by a 2-integer&#xa;        tuple, similar to C{{min,max}} multipliers in regular expressions.  Tuples&#xa;        may also include C{None} as in:&#xa;         - C{expr*(n,None)} or C{expr*(n,)} is equivalent&#xa;              to C{expr*n + L{ZeroOrMore}(expr)}&#xa;              (read as ""at least n instances of C{expr}"")&#xa;         - C{expr*(None,n)} is equivalent to C{expr*(0,n)}&#xa;              (read as ""0 to n instances of C{expr}"")&#xa;         - C{expr*(None,None)} is equivalent to C{L{ZeroOrMore}(expr)}&#xa;         - C{expr*(1,None)} is equivalent to C{L{OneOrMore}(expr)}&#xa;&#xa;        Note that C{expr*(None,n)} does not raise an exception if&#xa;        more than n exprs exist in the input stream; that is,&#xa;        C{expr*(None,n)} does not enforce a maximum number of expr&#xa;        occurrences.  If this behavior is desired, then write&#xa;        C{expr*(None,n) + ~expr}&#xa;        """"""&#xa;        if isinstance(other,int):&#xa;            minElements, optElements = other,0&#xa;        elif isinstance(other,tuple):&#xa;            other = (other + (None, None))[:2]&#xa;            if other[0] is None:&#xa;                other = (0, other[1])&#xa;            if isinstance(other[0],int) and other[1] is None:&#xa;                if other[0] == 0:&#xa;                    return ZeroOrMore(self)&#xa;                if other[0] == 1:&#xa;                    return OneOrMore(self)&#xa;                else:&#xa;                    return self*other[0] + ZeroOrMore(self)&#xa;            elif isinstance(other[0],int) and isinstance(other[1],int):&#xa;                minElements, optElements = other&#xa;                optElements -= minElements&#xa;            else:&#xa;                raise TypeError(""cannot multiply 'ParserElement' and ('%s','%s') objects"", type(other[0]),type(other[1]))&#xa;        else:&#xa;            raise TypeError(""cannot multiply 'ParserElement' and '%s' objects"", type(other))&#xa;&#xa;        if minElements < 0:&#xa;            raise ValueError(""cannot multiply ParserElement by negative value"")&#xa;        if optElements < 0:&#xa;            raise ValueError(""second tuple value must be greater or equal to first tuple value"")&#xa;        if minElements == optElements == 0:&#xa;            raise ValueError(""cannot multiply ParserElement by 0 or (0,0)"")&#xa;&#xa;        if (optElements):&#xa;            def makeOptionalList(n):&#xa;                if n>1:&#xa;                    return Optional(self + makeOptionalList(n-1))&#xa;                else:&#xa;                    return Optional(self)&#xa;            if minElements:&#xa;                if minElements == 1:&#xa;                    ret = self + makeOptionalList(optElements)&#xa;                else:&#xa;                    ret = And([self]*minElements) + makeOptionalList(optElements)&#xa;            else:&#xa;                ret = makeOptionalList(optElements)&#xa;        else:&#xa;            if minElements == 1:&#xa;                ret = self&#xa;            else:&#xa;                ret = And([self]*minElements)&#xa;        return ret&#xa;&#xa;    def __rmul__(self, other):&#xa;        return self.__mul__(other)&#xa;&#xa;    def __or__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator - returns C{L{MatchFirst}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return MatchFirst( [ self, other ] )&#xa;&#xa;    def __ror__(self, other ):&#xa;        """"""&#xa;        Implementation of | operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other | self&#xa;&#xa;    def __xor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator - returns C{L{Or}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Or( [ self, other ] )&#xa;&#xa;    def __rxor__(self, other ):&#xa;        """"""&#xa;        Implementation of ^ operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other ^ self&#xa;&#xa;    def __and__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator - returns C{L{Each}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return Each( [ self, other ] )&#xa;&#xa;    def __rand__(self, other ):&#xa;        """"""&#xa;        Implementation of & operator when left operand is not a C{L{ParserElement}}&#xa;        """"""&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        if not isinstance( other, ParserElement ):&#xa;            warnings.warn(""Cannot combine element of type %s with ParserElement"" % type(other),&#xa;                    SyntaxWarning, stacklevel=2)&#xa;            return None&#xa;        return other & self&#xa;&#xa;    def __invert__( self ):&#xa;        """"""&#xa;        Implementation of ~ operator - returns C{L{NotAny}}&#xa;        """"""&#xa;        return NotAny( self )&#xa;&#xa;    def __call__(self, name=None):&#xa;        """"""&#xa;        Shortcut for C{L{setResultsName}}, with C{listAllMatches=False}.&#xa;        &#xa;        If C{name} is given with a trailing C{'*'} character, then C{listAllMatches} will be&#xa;        passed as C{True}.&#xa;           &#xa;        If C{name} is omitted, same as calling C{L{copy}}.&#xa;&#xa;        Example::&#xa;            # these are equivalent&#xa;            userdata = Word(alphas).setResultsName(""name"") + Word(nums+""-"").setResultsName(""socsecno"")&#xa;            userdata = Word(alphas)(""name"") + Word(nums+""-"")(""socsecno"")             &#xa;        """"""&#xa;        if name is not None:&#xa;            return self.setResultsName(name)&#xa;        else:&#xa;            return self.copy()&#xa;&#xa;    def suppress( self ):&#xa;        """"""&#xa;        Suppresses the output of this C{ParserElement}; useful to keep punctuation from&#xa;        cluttering up returned output.&#xa;        """"""&#xa;        return Suppress( self )&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""&#xa;        Disables the skipping of whitespace before matching the characters in the&#xa;        C{ParserElement}'s defined pattern.  This is normally only used internally by&#xa;        the pyparsing module, but may be needed in some whitespace-sensitive grammars.&#xa;        """"""&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def setWhitespaceChars( self, chars ):&#xa;        """"""&#xa;        Overrides the default whitespace chars&#xa;        """"""&#xa;        self.skipWhitespace = True&#xa;        self.whiteChars = chars&#xa;        self.copyDefaultWhiteChars = False&#xa;        return self&#xa;&#xa;    def parseWithTabs( self ):&#xa;        """"""&#xa;        Overrides default behavior to expand C{<TAB>}s to spaces before parsing the input string.&#xa;        Must be called before C{parseString} when the input grammar contains elements that&#xa;        match C{<TAB>} characters.&#xa;        """"""&#xa;        self.keepTabs = True&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        """"""&#xa;        Define expression to be ignored (e.g., comments) while doing pattern&#xa;        matching; may be called repeatedly, to define multiple comment or other&#xa;        ignorable patterns.&#xa;        &#xa;        Example::&#xa;            patt = OneOrMore(Word(alphas))&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj']&#xa;            &#xa;            patt.ignore(cStyleComment)&#xa;            patt.parseString('ablaj /* comment */ lskjd') # -> ['ablaj', 'lskjd']&#xa;        """"""&#xa;        if isinstance(other, basestring):&#xa;            other = Suppress(other)&#xa;&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                self.ignoreExprs.append(other)&#xa;        else:&#xa;            self.ignoreExprs.append( Suppress( other.copy() ) )&#xa;        return self&#xa;&#xa;    def setDebugActions( self, startAction, successAction, exceptionAction ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        """"""&#xa;        self.debugActions = (startAction or _defaultStartDebugAction,&#xa;                             successAction or _defaultSuccessDebugAction,&#xa;                             exceptionAction or _defaultExceptionDebugAction)&#xa;        self.debug = True&#xa;        return self&#xa;&#xa;    def setDebug( self, flag=True ):&#xa;        """"""&#xa;        Enable display of debugging messages while doing pattern matching.&#xa;        Set C{flag} to True to enable, False to disable.&#xa;&#xa;        Example::&#xa;            wd = Word(alphas).setName(""alphaword"")&#xa;            integer = Word(nums).setName(""numword"")&#xa;            term = wd | integer&#xa;            &#xa;            # turn on debugging for wd&#xa;            wd.setDebug()&#xa;&#xa;            OneOrMore(term).parseString(""abc 123 xyz 890"")&#xa;        &#xa;        prints::&#xa;            Match alphaword at loc 0(1,1)&#xa;            Matched alphaword -> ['abc']&#xa;            Match alphaword at loc 3(1,4)&#xa;            Exception raised:Expected alphaword (at char 4), (line:1, col:5)&#xa;            Match alphaword at loc 7(1,8)&#xa;            Matched alphaword -> ['xyz']&#xa;            Match alphaword at loc 11(1,12)&#xa;            Exception raised:Expected alphaword (at char 12), (line:1, col:13)&#xa;            Match alphaword at loc 15(1,16)&#xa;            Exception raised:Expected alphaword (at char 15), (line:1, col:16)&#xa;&#xa;        The output shown is that produced by the default debug actions - custom debug actions can be&#xa;        specified using L{setDebugActions}. Prior to attempting&#xa;        to match the C{wd} expression, the debugging message C{""Match <exprname> at loc <n>(<line>,<col>)""}&#xa;        is shown. Then if the parse succeeds, a C{""Matched""} message is shown, or an C{""Exception raised""}&#xa;        message is shown. Also note the use of L{setName} to assign a human-readable name to the expression,&#xa;        which makes debugging and exception messages easier to understand - for instance, the default&#xa;        name created for the C{Word} expression without calling C{setName} is C{""W:(ABCD...)""}.&#xa;        """"""&#xa;        if flag:&#xa;            self.setDebugActions( _defaultStartDebugAction, _defaultSuccessDebugAction, _defaultExceptionDebugAction )&#xa;        else:&#xa;            self.debug = False&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        return self.name&#xa;&#xa;    def __repr__( self ):&#xa;        return _ustr(self)&#xa;&#xa;    def streamline( self ):&#xa;        self.streamlined = True&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        pass&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        """"""&#xa;        Check defined expressions for valid structure, check for infinite recursive definitions.&#xa;        """"""&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def parseFile( self, file_or_filename, parseAll=False ):&#xa;        """"""&#xa;        Execute the parse expression on the given file or filename.&#xa;        If a filename is specified (instead of a file object),&#xa;        the entire file is opened, read, and closed before parsing.&#xa;        """"""&#xa;        try:&#xa;            file_contents = file_or_filename.read()&#xa;        except AttributeError:&#xa;            with open(file_or_filename, ""r"") as f:&#xa;                file_contents = f.read()&#xa;        try:&#xa;            return self.parseString(file_contents, parseAll)&#xa;        except ParseBaseException as exc:&#xa;            if ParserElement.verbose_stacktrace:&#xa;                raise&#xa;            else:&#xa;                # catch and re-raise exception from here, clears out pyparsing internal stack trace&#xa;                raise exc&#xa;&#xa;    def __eq__(self,other):&#xa;        if isinstance(other, ParserElement):&#xa;            return self is other or vars(self) == vars(other)&#xa;        elif isinstance(other, basestring):&#xa;            return self.matches(other)&#xa;        else:&#xa;            return super(ParserElement,self)==other&#xa;&#xa;    def __ne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def __hash__(self):&#xa;        return hash(id(self))&#xa;&#xa;    def __req__(self,other):&#xa;        return self == other&#xa;&#xa;    def __rne__(self,other):&#xa;        return not (self == other)&#xa;&#xa;    def matches(self, testString, parseAll=True):&#xa;        """"""&#xa;        Method for quick testing of a parser against a test string. Good for simple &#xa;        inline microtests of sub expressions while building up larger parser.&#xa;           &#xa;        Parameters:&#xa;         - testString - to test against this expression for a match&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests&#xa;            &#xa;        Example::&#xa;            expr = Word(nums)&#xa;            assert expr.matches(""100"")&#xa;        """"""&#xa;        try:&#xa;            self.parseString(_ustr(testString), parseAll=parseAll)&#xa;            return True&#xa;        except ParseBaseException:&#xa;            return False&#xa;                &#xa;    def runTests(self, tests, parseAll=True, comment='#', fullDump=True, printResults=True, failureTests=False):&#xa;        """"""&#xa;        Execute the parse expression on a series of test strings, showing each&#xa;        test, the parsed results or where the parse failed. Quick and easy way to&#xa;        run a parse expression against a list of sample strings.&#xa;           &#xa;        Parameters:&#xa;         - tests - a list of separate test strings, or a multiline string of test strings&#xa;         - parseAll - (default=C{True}) - flag to pass to C{L{parseString}} when running tests           &#xa;         - comment - (default=C{'#'}) - expression for indicating embedded comments in the test &#xa;              string; pass None to disable comment filtering&#xa;         - fullDump - (default=C{True}) - dump results as list followed by results names in nested outline;&#xa;              if False, only dump nested list&#xa;         - printResults - (default=C{True}) prints test output to stdout&#xa;         - failureTests - (default=C{False}) indicates if these tests are expected to fail parsing&#xa;&#xa;        Returns: a (success, results) tuple, where success indicates that all tests succeeded&#xa;        (or failed if C{failureTests} is True), and the results contain a list of lines of each &#xa;        test's output&#xa;        &#xa;        Example::&#xa;            number_expr = pyparsing_common.number.copy()&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # unsigned integer&#xa;                100&#xa;                # negative integer&#xa;                -100&#xa;                # float with scientific notation&#xa;                6.02e23&#xa;                # integer with scientific notation&#xa;                1e-12&#xa;                ''')&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;&#xa;            result = number_expr.runTests('''&#xa;                # stray character&#xa;                100Z&#xa;                # missing leading digit before '.'&#xa;                -.100&#xa;                # too many '.'&#xa;                3.14.159&#xa;                ''', failureTests=True)&#xa;            print(""Success"" if result[0] else ""Failed!"")&#xa;        prints::&#xa;            # unsigned integer&#xa;            100&#xa;            [100]&#xa;&#xa;            # negative integer&#xa;            -100&#xa;            [-100]&#xa;&#xa;            # float with scientific notation&#xa;            6.02e23&#xa;            [6.02e+23]&#xa;&#xa;            # integer with scientific notation&#xa;            1e-12&#xa;            [1e-12]&#xa;&#xa;            Success&#xa;            &#xa;            # stray character&#xa;            100Z&#xa;               ^&#xa;            FAIL: Expected end of text (at char 3), (line:1, col:4)&#xa;&#xa;            # missing leading digit before '.'&#xa;            -.100&#xa;            ^&#xa;            FAIL: Expected {real number with scientific notation | real number | signed integer} (at char 0), (line:1, col:1)&#xa;&#xa;            # too many '.'&#xa;            3.14.159&#xa;                ^&#xa;            FAIL: Expected end of text (at char 4), (line:1, col:5)&#xa;&#xa;            Success&#xa;&#xa;        Each test string must be on a single line. If you want to test a string that spans multiple&#xa;        lines, create a test like this::&#xa;&#xa;            expr.runTest(r""this is a test\\n of strings that spans \\n 3 lines"")&#xa;        &#xa;        (Note that this is a raw string literal, you must include the leading 'r'.)&#xa;        """"""&#xa;        if isinstance(tests, basestring):&#xa;            tests = list(map(str.strip, tests.rstrip().splitlines()))&#xa;        if isinstance(comment, basestring):&#xa;            comment = Literal(comment)&#xa;        allResults = []&#xa;        comments = []&#xa;        success = True&#xa;        for t in tests:&#xa;            if comment is not None and comment.matches(t, False) or comments and not t:&#xa;                comments.append(t)&#xa;                continue&#xa;            if not t:&#xa;                continue&#xa;            out = ['\n'.join(comments), t]&#xa;            comments = []&#xa;            try:&#xa;                t = t.replace(r'\n','\n')&#xa;                result = self.parseString(t, parseAll=parseAll)&#xa;                out.append(result.dump(full=fullDump))&#xa;                success = success and not failureTests&#xa;            except ParseBaseException as pe:&#xa;                fatal = ""(FATAL)"" if isinstance(pe, ParseFatalException) else """"&#xa;                if '\n' in t:&#xa;                    out.append(line(pe.loc, t))&#xa;                    out.append(' '*(col(pe.loc,t)-1) + '^' + fatal)&#xa;                else:&#xa;                    out.append(' '*pe.loc + '^' + fatal)&#xa;                out.append(""FAIL: "" + str(pe))&#xa;                success = success and failureTests&#xa;                result = pe&#xa;            except Exception as exc:&#xa;                out.append(""FAIL-EXCEPTION: "" + str(exc))&#xa;                success = success and failureTests&#xa;                result = exc&#xa;&#xa;            if printResults:&#xa;                if fullDump:&#xa;                    out.append('')&#xa;                print('\n'.join(out))&#xa;&#xa;            allResults.append((t, result))&#xa;        &#xa;        return success, allResults&#xa;&#xa;        &#xa;class Token(ParserElement):&#xa;    """"""&#xa;    Abstract C{ParserElement} subclass, for defining atomic matching patterns.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Token,self).__init__( savelist=False )&#xa;&#xa;&#xa;class Empty(Token):&#xa;    """"""&#xa;    An empty token, will always match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(Empty,self).__init__()&#xa;        self.name = ""Empty""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;&#xa;class NoMatch(Token):&#xa;    """"""&#xa;    A token that will never match.&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(NoMatch,self).__init__()&#xa;        self.name = ""NoMatch""&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.errmsg = ""Unmatchable token""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Literal(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string.&#xa;    &#xa;    Example::&#xa;        Literal('blah').parseString('blah')  # -> ['blah']&#xa;        Literal('blah').parseString('blahfooblah')  # -> ['blah']&#xa;        Literal('blah').parseString('bla')  # -> Exception: Expected ""blah""&#xa;    &#xa;    For case-insensitive matching, use L{CaselessLiteral}.&#xa;    &#xa;    For keyword matching (force word break before and after the matched string),&#xa;    use L{Keyword} or L{CaselessKeyword}.&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(Literal,self).__init__()&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Literal; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;            self.__class__ = Empty&#xa;        self.name = '""%s""' % _ustr(self.match)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;&#xa;    # Performance tuning: this routine gets called a *lot*&#xa;    # if this is a single character match string  and the first character matches,&#xa;    # short-circuit as quickly as possible, and avoid calling startswith&#xa;    #~ @profile&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if (instring[loc] == self.firstMatchChar and&#xa;            (self.matchLen==1 or instring.startswith(self.match,loc)) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;_L = Literal&#xa;ParserElement._literalStringClass = Literal&#xa;&#xa;class Keyword(Token):&#xa;    """"""&#xa;    Token to exactly match a specified string as a keyword, that is, it must be&#xa;    immediately followed by a non-keyword character.  Compare with C{L{Literal}}:&#xa;     - C{Literal(""if"")} will match the leading C{'if'} in C{'ifAndOnlyIf'}.&#xa;     - C{Keyword(""if"")} will not; it will only match the leading C{'if'} in C{'if x=1'}, or C{'if(y==2)'}&#xa;    Accepts two optional constructor arguments in addition to the keyword string:&#xa;     - C{identChars} is a string of characters that would be valid identifier characters,&#xa;          defaulting to all alphanumerics + ""_"" and ""$""&#xa;     - C{caseless} allows case-insensitive matching, default is C{False}.&#xa;       &#xa;    Example::&#xa;        Keyword(""start"").parseString(""start"")  # -> ['start']&#xa;        Keyword(""start"").parseString(""starting"")  # -> Exception&#xa;&#xa;    For case-insensitive matching, use L{CaselessKeyword}.&#xa;    """"""&#xa;    DEFAULT_KEYWORD_CHARS = alphanums+""_$""&#xa;&#xa;    def __init__( self, matchString, identChars=None, caseless=False ):&#xa;        super(Keyword,self).__init__()&#xa;        if identChars is None:&#xa;            identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        self.match = matchString&#xa;        self.matchLen = len(matchString)&#xa;        try:&#xa;            self.firstMatchChar = matchString[0]&#xa;        except IndexError:&#xa;            warnings.warn(""null string passed to Keyword; use Empty() instead"",&#xa;                            SyntaxWarning, stacklevel=2)&#xa;        self.name = '""%s""' % self.match&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = False&#xa;        self.mayIndexError = False&#xa;        self.caseless = caseless&#xa;        if caseless:&#xa;            self.caselessmatch = matchString.upper()&#xa;            identChars = identChars.upper()&#xa;        self.identChars = set(identChars)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.caseless:&#xa;            if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;                 (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) and&#xa;                 (loc == 0 or instring[loc-1].upper() not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        else:&#xa;            if (instring[loc] == self.firstMatchChar and&#xa;                (self.matchLen==1 or instring.startswith(self.match,loc)) and&#xa;                (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen] not in self.identChars) and&#xa;                (loc == 0 or instring[loc-1] not in self.identChars) ):&#xa;                return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;    def copy(self):&#xa;        c = super(Keyword,self).copy()&#xa;        c.identChars = Keyword.DEFAULT_KEYWORD_CHARS&#xa;        return c&#xa;&#xa;    @staticmethod&#xa;    def setDefaultKeywordChars( chars ):&#xa;        """"""Overrides the default Keyword chars&#xa;        """"""&#xa;        Keyword.DEFAULT_KEYWORD_CHARS = chars&#xa;&#xa;class CaselessLiteral(Literal):&#xa;    """"""&#xa;    Token to match a specified string, ignoring case of letters.&#xa;    Note: the matched results will always be in the case of the given&#xa;    match string, NOT the case of the input text.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessLiteral(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessKeyword}.)&#xa;    """"""&#xa;    def __init__( self, matchString ):&#xa;        super(CaselessLiteral,self).__init__( matchString.upper() )&#xa;        # Preserve the defining literal.&#xa;        self.returnString = matchString&#xa;        self.name = ""'%s'"" % self.returnString&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[ loc:loc+self.matchLen ].upper() == self.match:&#xa;            return loc+self.matchLen, self.returnString&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CaselessKeyword(Keyword):&#xa;    """"""&#xa;    Caseless version of L{Keyword}.&#xa;&#xa;    Example::&#xa;        OneOrMore(CaselessKeyword(""CMD"")).parseString(""cmd CMD Cmd10"") # -> ['CMD', 'CMD']&#xa;        &#xa;    (Contrast with example for L{CaselessLiteral}.)&#xa;    """"""&#xa;    def __init__( self, matchString, identChars=None ):&#xa;        super(CaselessKeyword,self).__init__( matchString, identChars, caseless=True )&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if ( (instring[ loc:loc+self.matchLen ].upper() == self.caselessmatch) and&#xa;             (loc >= len(instring)-self.matchLen or instring[loc+self.matchLen].upper() not in self.identChars) ):&#xa;            return loc+self.matchLen, self.match&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class CloseMatch(Token):&#xa;    """"""&#xa;    A variation on L{Literal} which matches ""close"" matches, that is, &#xa;    strings with at most 'n' mismatching characters. C{CloseMatch} takes parameters:&#xa;     - C{match_string} - string to be matched&#xa;     - C{maxMismatches} - (C{default=1}) maximum number of mismatches allowed to count as a match&#xa;    &#xa;    The results from a successful parse will contain the matched text from the input string and the following named results:&#xa;     - C{mismatches} - a list of the positions within the match_string where mismatches were found&#xa;     - C{original} - the original match_string used to compare against the input string&#xa;    &#xa;    If C{mismatches} is an empty list, then the match was an exact match.&#xa;    &#xa;    Example::&#xa;        patt = CloseMatch(""ATCATCGAATGGA"")&#xa;        patt.parseString(""ATCATCGAAXGGA"") # -> (['ATCATCGAAXGGA'], {'mismatches': [[9]], 'original': ['ATCATCGAATGGA']})&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> Exception: Expected 'ATCATCGAATGGA' (with up to 1 mismatches) (at char 0), (line:1, col:1)&#xa;&#xa;        # exact match&#xa;        patt.parseString(""ATCATCGAATGGA"") # -> (['ATCATCGAATGGA'], {'mismatches': [[]], 'original': ['ATCATCGAATGGA']})&#xa;&#xa;        # close match allowing up to 2 mismatches&#xa;        patt = CloseMatch(""ATCATCGAATGGA"", maxMismatches=2)&#xa;        patt.parseString(""ATCAXCGAAXGGA"") # -> (['ATCAXCGAAXGGA'], {'mismatches': [[4, 9]], 'original': ['ATCATCGAATGGA']})&#xa;    """"""&#xa;    def __init__(self, match_string, maxMismatches=1):&#xa;        super(CloseMatch,self).__init__()&#xa;        self.name = match_string&#xa;        self.match_string = match_string&#xa;        self.maxMismatches = maxMismatches&#xa;        self.errmsg = ""Expected %r (with up to %d mismatches)"" % (self.match_string, self.maxMismatches)&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        start = loc&#xa;        instrlen = len(instring)&#xa;        maxloc = start + len(self.match_string)&#xa;&#xa;        if maxloc <= instrlen:&#xa;            match_string = self.match_string&#xa;            match_stringloc = 0&#xa;            mismatches = []&#xa;            maxMismatches = self.maxMismatches&#xa;&#xa;            for match_stringloc,s_m in enumerate(zip(instring[loc:maxloc], self.match_string)):&#xa;                src,mat = s_m&#xa;                if src != mat:&#xa;                    mismatches.append(match_stringloc)&#xa;                    if len(mismatches) > maxMismatches:&#xa;                        break&#xa;            else:&#xa;                loc = match_stringloc + 1&#xa;                results = ParseResults([instring[start:loc]])&#xa;                results['original'] = self.match_string&#xa;                results['mismatches'] = mismatches&#xa;                return loc, results&#xa;&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;&#xa;class Word(Token):&#xa;    """"""&#xa;    Token for matching words composed of allowed character sets.&#xa;    Defined with string containing all allowed initial characters,&#xa;    an optional string containing allowed body characters (if omitted,&#xa;    defaults to the initial character set), and an optional minimum,&#xa;    maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction. An optional&#xa;    C{excludeChars} parameter can list characters that might be found in &#xa;    the input C{bodyChars} string; useful to define a word of all printables&#xa;    except for one or two characters, for instance.&#xa;    &#xa;    L{srange} is useful for defining custom character set strings for defining &#xa;    C{Word} expressions, using range notation from regular expression character sets.&#xa;    &#xa;    A common mistake is to use C{Word} to match a specific literal string, as in &#xa;    C{Word(""Address"")}. Remember that C{Word} uses the string argument to define&#xa;    I{sets} of matchable characters. This expression would match ""Add"", ""AAA"",&#xa;    ""dAred"", or any other word made up of the characters 'A', 'd', 'r', 'e', and 's'.&#xa;    To match an exact literal string, use L{Literal} or L{Keyword}.&#xa;&#xa;    pyparsing includes helper strings for building Words:&#xa;     - L{alphas}&#xa;     - L{nums}&#xa;     - L{alphanums}&#xa;     - L{hexnums}&#xa;     - L{alphas8bit} (alphabetic characters in ASCII range 128-255 - accented, tilded, umlauted, etc.)&#xa;     - L{punc8bit} (non-alphabetic characters in ASCII range 128-255 - currency, symbols, superscripts, diacriticals, etc.)&#xa;     - L{printables} (any non-whitespace character)&#xa;&#xa;    Example::&#xa;        # a word composed of digits&#xa;        integer = Word(nums) # equivalent to Word(""0123456789"") or Word(srange(""0-9""))&#xa;        &#xa;        # a word with a leading capital, and zero or more lowercase&#xa;        capital_word = Word(alphas.upper(), alphas.lower())&#xa;&#xa;        # hostnames are alphanumeric, with leading alpha, and '-'&#xa;        hostname = Word(alphas, alphanums+'-')&#xa;        &#xa;        # roman numeral (not a strict parser, accepts invalid mix of characters)&#xa;        roman = Word(""IVXLCDM"")&#xa;        &#xa;        # any string of non-whitespace characters, except for ','&#xa;        csv_value = Word(printables, excludeChars="","")&#xa;    """"""&#xa;    def __init__( self, initChars, bodyChars=None, min=1, max=0, exact=0, asKeyword=False, excludeChars=None ):&#xa;        super(Word,self).__init__()&#xa;        if excludeChars:&#xa;            initChars = ''.join(c for c in initChars if c not in excludeChars)&#xa;            if bodyChars:&#xa;                bodyChars = ''.join(c for c in bodyChars if c not in excludeChars)&#xa;        self.initCharsOrig = initChars&#xa;        self.initChars = set(initChars)&#xa;        if bodyChars :&#xa;            self.bodyCharsOrig = bodyChars&#xa;            self.bodyChars = set(bodyChars)&#xa;        else:&#xa;            self.bodyCharsOrig = initChars&#xa;            self.bodyChars = set(initChars)&#xa;&#xa;        self.maxSpecified = max > 0&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(Word()) if zero-length word is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.asKeyword = asKeyword&#xa;&#xa;        if ' ' not in self.initCharsOrig+self.bodyCharsOrig and (min==1 and max==0 and exact==0):&#xa;            if self.bodyCharsOrig == self.initCharsOrig:&#xa;                self.reString = ""[%s]+"" % _escapeRegexRangeChars(self.initCharsOrig)&#xa;            elif len(self.initCharsOrig) == 1:&#xa;                self.reString = ""%s[%s]*"" % \&#xa;                                      (re.escape(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            else:&#xa;                self.reString = ""[%s][%s]*"" % \&#xa;                                      (_escapeRegexRangeChars(self.initCharsOrig),&#xa;                                      _escapeRegexRangeChars(self.bodyCharsOrig),)&#xa;            if self.asKeyword:&#xa;                self.reString = r""\b""+self.reString+r""\b""&#xa;            try:&#xa;                self.re = re.compile( self.reString )&#xa;            except Exception:&#xa;                self.re = None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.re:&#xa;            result = self.re.match(instring,loc)&#xa;            if not result:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;            loc = result.end()&#xa;            return loc, result.group()&#xa;&#xa;        if not(instring[ loc ] in self.initChars):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        instrlen = len(instring)&#xa;        bodychars = self.bodyChars&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, instrlen )&#xa;        while loc < maxloc and instring[loc] in bodychars:&#xa;            loc += 1&#xa;&#xa;        throwException = False&#xa;        if loc - start < self.minLen:&#xa;            throwException = True&#xa;        if self.maxSpecified and loc < instrlen and instring[loc] in bodychars:&#xa;            throwException = True&#xa;        if self.asKeyword:&#xa;            if (start>0 and instring[start-1] in bodychars) or (loc<instrlen and instring[loc] in bodychars):&#xa;                throwException = True&#xa;&#xa;        if throwException:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Word,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;&#xa;        if self.strRepr is None:&#xa;&#xa;            def charsAsStr(s):&#xa;                if len(s)>4:&#xa;                    return s[:4]+""...""&#xa;                else:&#xa;                    return s&#xa;&#xa;            if ( self.initCharsOrig != self.bodyCharsOrig ):&#xa;                self.strRepr = ""W:(%s,%s)"" % ( charsAsStr(self.initCharsOrig), charsAsStr(self.bodyCharsOrig) )&#xa;            else:&#xa;                self.strRepr = ""W:(%s)"" % charsAsStr(self.initCharsOrig)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Regex(Token):&#xa;    """"""&#xa;    Token for matching strings that match a given regular expression.&#xa;    Defined with string specifying the regular expression in a form recognized by the inbuilt Python re module.&#xa;    If the given regex contains named groups (defined using C{(?P<name>...)}), these will be preserved as &#xa;    named parse results.&#xa;&#xa;    Example::&#xa;        realnum = Regex(r""[+-]?\d+\.\d*"")&#xa;        date = Regex(r'(?P<year>\d{4})-(?P<month>\d\d?)-(?P<day>\d\d?)')&#xa;        # ref: http://stackoverflow.com/questions/267399/how-do-you-match-only-valid-roman-numerals-with-a-regular-expression&#xa;        roman = Regex(r""M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})"")&#xa;    """"""&#xa;    compiledREtype = type(re.compile(""[A-Z]""))&#xa;    def __init__( self, pattern, flags=0):&#xa;        """"""The parameters C{pattern} and C{flags} are passed to the C{re.compile()} function as-is. See the Python C{re} module for an explanation of the acceptable patterns and flags.""""""&#xa;        super(Regex,self).__init__()&#xa;&#xa;        if isinstance(pattern, basestring):&#xa;            if not pattern:&#xa;                warnings.warn(""null string passed to Regex; use Empty() instead"",&#xa;                        SyntaxWarning, stacklevel=2)&#xa;&#xa;            self.pattern = pattern&#xa;            self.flags = flags&#xa;&#xa;            try:&#xa;                self.re = re.compile(self.pattern, self.flags)&#xa;                self.reString = self.pattern&#xa;            except sre_constants.error:&#xa;                warnings.warn(""invalid pattern (%s) passed to Regex"" % pattern,&#xa;                    SyntaxWarning, stacklevel=2)&#xa;                raise&#xa;&#xa;        elif isinstance(pattern, Regex.compiledREtype):&#xa;            self.re = pattern&#xa;            self.pattern = \&#xa;            self.reString = str(pattern)&#xa;            self.flags = flags&#xa;            &#xa;        else:&#xa;            raise ValueError(""Regex may only be constructed with a string or a compiled RE object"")&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = self.re.match(instring,loc)&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        d = result.groupdict()&#xa;        ret = ParseResults(result.group())&#xa;        if d:&#xa;            for k in d:&#xa;                ret[k] = d[k]&#xa;        return loc,ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(Regex,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""Re:(%s)"" % repr(self.pattern)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class QuotedString(Token):&#xa;    r""""""&#xa;    Token for matching strings that are delimited by quoting characters.&#xa;    &#xa;    Defined with the following parameters:&#xa;        - quoteChar - string of one or more characters defining the quote delimiting string&#xa;        - escChar - character to escape quotes, typically backslash (default=C{None})&#xa;        - escQuote - special quote sequence to escape an embedded quote string (such as SQL's """" to escape an embedded "") (default=C{None})&#xa;        - multiline - boolean indicating whether quotes can span multiple lines (default=C{False})&#xa;        - unquoteResults - boolean indicating whether the matched text should be unquoted (default=C{True})&#xa;        - endQuoteChar - string of one or more characters defining the end of the quote delimited string (default=C{None} => same as quoteChar)&#xa;        - convertWhitespaceEscapes - convert escaped whitespace (C{'\t'}, C{'\n'}, etc.) to actual whitespace (default=C{True})&#xa;&#xa;    Example::&#xa;        qs = QuotedString('""')&#xa;        print(qs.searchString('lsjdf ""This is the quote"" sldjf'))&#xa;        complex_qs = QuotedString('{{', endQuoteChar='}}')&#xa;        print(complex_qs.searchString('lsjdf {{This is the ""quote""}} sldjf'))&#xa;        sql_qs = QuotedString('""', escQuote='""""')&#xa;        print(sql_qs.searchString('lsjdf ""This is the quote with """"embedded"""" quotes"" sldjf'))&#xa;    prints::&#xa;        [['This is the quote']]&#xa;        [['This is the ""quote""']]&#xa;        [['This is the quote with ""embedded"" quotes']]&#xa;    """"""&#xa;    def __init__( self, quoteChar, escChar=None, escQuote=None, multiline=False, unquoteResults=True, endQuoteChar=None, convertWhitespaceEscapes=True):&#xa;        super(QuotedString,self).__init__()&#xa;&#xa;        # remove white space from quote chars - wont work anyway&#xa;        quoteChar = quoteChar.strip()&#xa;        if not quoteChar:&#xa;            warnings.warn(""quoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;            raise SyntaxError()&#xa;&#xa;        if endQuoteChar is None:&#xa;            endQuoteChar = quoteChar&#xa;        else:&#xa;            endQuoteChar = endQuoteChar.strip()&#xa;            if not endQuoteChar:&#xa;                warnings.warn(""endQuoteChar cannot be the empty string"",SyntaxWarning,stacklevel=2)&#xa;                raise SyntaxError()&#xa;&#xa;        self.quoteChar = quoteChar&#xa;        self.quoteCharLen = len(quoteChar)&#xa;        self.firstQuoteChar = quoteChar[0]&#xa;        self.endQuoteChar = endQuoteChar&#xa;        self.endQuoteCharLen = len(endQuoteChar)&#xa;        self.escChar = escChar&#xa;        self.escQuote = escQuote&#xa;        self.unquoteResults = unquoteResults&#xa;        self.convertWhitespaceEscapes = convertWhitespaceEscapes&#xa;&#xa;        if multiline:&#xa;            self.flags = re.MULTILINE | re.DOTALL&#xa;            self.pattern = r'%s(?:[^%s%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        else:&#xa;            self.flags = 0&#xa;            self.pattern = r'%s(?:[^%s\n\r%s]' % \&#xa;                ( re.escape(self.quoteChar),&#xa;                  _escapeRegexRangeChars(self.endQuoteChar[0]),&#xa;                  (escChar is not None and _escapeRegexRangeChars(escChar) or '') )&#xa;        if len(self.endQuoteChar) > 1:&#xa;            self.pattern += (&#xa;                '|(?:' + ')|(?:'.join(""%s[^%s]"" % (re.escape(self.endQuoteChar[:i]),&#xa;                                               _escapeRegexRangeChars(self.endQuoteChar[i]))&#xa;                                    for i in range(len(self.endQuoteChar)-1,0,-1)) + ')'&#xa;                )&#xa;        if escQuote:&#xa;            self.pattern += (r'|(?:%s)' % re.escape(escQuote))&#xa;        if escChar:&#xa;            self.pattern += (r'|(?:%s.)' % re.escape(escChar))&#xa;            self.escCharReplacePattern = re.escape(self.escChar)+""(.)""&#xa;        self.pattern += (r')*%s' % re.escape(self.endQuoteChar))&#xa;&#xa;        try:&#xa;            self.re = re.compile(self.pattern, self.flags)&#xa;            self.reString = self.pattern&#xa;        except sre_constants.error:&#xa;            warnings.warn(""invalid pattern (%s) passed to Regex"" % self.pattern,&#xa;                SyntaxWarning, stacklevel=2)&#xa;            raise&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayIndexError = False&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        result = instring[loc] == self.firstQuoteChar and self.re.match(instring,loc) or None&#xa;        if not result:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        loc = result.end()&#xa;        ret = result.group()&#xa;&#xa;        if self.unquoteResults:&#xa;&#xa;            # strip off quotes&#xa;            ret = ret[self.quoteCharLen:-self.endQuoteCharLen]&#xa;&#xa;            if isinstance(ret,basestring):&#xa;                # replace escaped whitespace&#xa;                if '\\' in ret and self.convertWhitespaceEscapes:&#xa;                    ws_map = {&#xa;                        r'\t' : '\t',&#xa;                        r'\n' : '\n',&#xa;                        r'\f' : '\f',&#xa;                        r'\r' : '\r',&#xa;                    }&#xa;                    for wslit,wschar in ws_map.items():&#xa;                        ret = ret.replace(wslit, wschar)&#xa;&#xa;                # replace escaped characters&#xa;                if self.escChar:&#xa;                    ret = re.sub(self.escCharReplacePattern,""\g<1>"",ret)&#xa;&#xa;                # replace escaped quotes&#xa;                if self.escQuote:&#xa;                    ret = ret.replace(self.escQuote, self.endQuoteChar)&#xa;&#xa;        return loc, ret&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(QuotedString,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""quoted string, starting with %s ending with %s"" % (self.quoteChar, self.endQuoteChar)&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class CharsNotIn(Token):&#xa;    """"""&#xa;    Token for matching words composed of characters I{not} in a given set (will&#xa;    include whitespace in matched characters if not listed in the provided exclusion set - see example).&#xa;    Defined with string containing all disallowed characters, and an optional&#xa;    minimum, maximum, and/or exact length.  The default value for C{min} is 1 (a&#xa;    minimum value < 1 is not valid); the default values for C{max} and C{exact}&#xa;    are 0, meaning no maximum or exact length restriction.&#xa;&#xa;    Example::&#xa;        # define a comma-separated-value as anything that is not a ','&#xa;        csv_value = CharsNotIn(',')&#xa;        print(delimitedList(csv_value).parseString(""dkls,lsdkjf,s12 34,@!#,213""))&#xa;    prints::&#xa;        ['dkls', 'lsdkjf', 's12 34', '@!#', '213']&#xa;    """"""&#xa;    def __init__( self, notChars, min=1, max=0, exact=0 ):&#xa;        super(CharsNotIn,self).__init__()&#xa;        self.skipWhitespace = False&#xa;        self.notChars = notChars&#xa;&#xa;        if min < 1:&#xa;            raise ValueError(""cannot specify a minimum length < 1; use Optional(CharsNotIn()) if zero-length char group is permitted"")&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;        self.name = _ustr(self)&#xa;        self.errmsg = ""Expected "" + self.name&#xa;        self.mayReturnEmpty = ( self.minLen == 0 )&#xa;        self.mayIndexError = False&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if instring[loc] in self.notChars:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        start = loc&#xa;        loc += 1&#xa;        notchars = self.notChars&#xa;        maxlen = min( start+self.maxLen, len(instring) )&#xa;        while loc < maxlen and \&#xa;              (instring[loc] not in notchars):&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(CharsNotIn, self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            if len(self.notChars) > 4:&#xa;                self.strRepr = ""!W:(%s...)"" % self.notChars[:4]&#xa;            else:&#xa;                self.strRepr = ""!W:(%s)"" % self.notChars&#xa;&#xa;        return self.strRepr&#xa;&#xa;class White(Token):&#xa;    """"""&#xa;    Special matching class for matching whitespace.  Normally, whitespace is ignored&#xa;    by pyparsing grammars.  This class is included when some whitespace structures&#xa;    are significant.  Define with a string containing the whitespace characters to be&#xa;    matched; default is C{"" \\t\\r\\n""}.  Also takes optional C{min}, C{max}, and C{exact} arguments,&#xa;    as defined for the C{L{Word}} class.&#xa;    """"""&#xa;    whiteStrs = {&#xa;        "" "" : ""<SPC>"",&#xa;        ""\t"": ""<TAB>"",&#xa;        ""\n"": ""<LF>"",&#xa;        ""\r"": ""<CR>"",&#xa;        ""\f"": ""<FF>"",&#xa;        }&#xa;    def __init__(self, ws="" \t\r\n"", min=1, max=0, exact=0):&#xa;        super(White,self).__init__()&#xa;        self.matchWhite = ws&#xa;        self.setWhitespaceChars( """".join(c for c in self.whiteChars if c not in self.matchWhite) )&#xa;        #~ self.leaveWhitespace()&#xa;        self.name = ("""".join(White.whiteStrs[c] for c in self.matchWhite))&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Expected "" + self.name&#xa;&#xa;        self.minLen = min&#xa;&#xa;        if max > 0:&#xa;            self.maxLen = max&#xa;        else:&#xa;            self.maxLen = _MAX_INT&#xa;&#xa;        if exact > 0:&#xa;            self.maxLen = exact&#xa;            self.minLen = exact&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if not(instring[ loc ] in self.matchWhite):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        start = loc&#xa;        loc += 1&#xa;        maxloc = start + self.maxLen&#xa;        maxloc = min( maxloc, len(instring) )&#xa;        while loc < maxloc and instring[loc] in self.matchWhite:&#xa;            loc += 1&#xa;&#xa;        if loc - start < self.minLen:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        return loc, instring[start:loc]&#xa;&#xa;&#xa;class _PositionToken(Token):&#xa;    def __init__( self ):&#xa;        super(_PositionToken,self).__init__()&#xa;        self.name=self.__class__.__name__&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;&#xa;class GoToColumn(_PositionToken):&#xa;    """"""&#xa;    Token to advance to a specific column of input text; useful for tabular report scraping.&#xa;    """"""&#xa;    def __init__( self, colno ):&#xa;        super(GoToColumn,self).__init__()&#xa;        self.col = colno&#xa;&#xa;    def preParse( self, instring, loc ):&#xa;        if col(loc,instring) != self.col:&#xa;            instrlen = len(instring)&#xa;            if self.ignoreExprs:&#xa;                loc = self._skipIgnorables( instring, loc )&#xa;            while loc < instrlen and instring[loc].isspace() and col( loc, instring ) != self.col :&#xa;                loc += 1&#xa;        return loc&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        thiscol = col( loc, instring )&#xa;        if thiscol > self.col:&#xa;            raise ParseException( instring, loc, ""Text not in expected column"", self )&#xa;        newloc = loc + self.col - thiscol&#xa;        ret = instring[ loc: newloc ]&#xa;        return newloc, ret&#xa;&#xa;&#xa;class LineStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of a line within the parse string&#xa;    &#xa;    Example::&#xa;    &#xa;        test = '''\&#xa;        AAA this line&#xa;        AAA and this line&#xa;          AAA but not this one&#xa;        B AAA and definitely not this one&#xa;        '''&#xa;&#xa;        for t in (LineStart() + 'AAA' + restOfLine).searchString(test):&#xa;            print(t)&#xa;    &#xa;    Prints::&#xa;        ['AAA', ' this line']&#xa;        ['AAA', ' and this line']    &#xa;&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineStart,self).__init__()&#xa;        self.errmsg = ""Expected start of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if col(loc, instring) == 1:&#xa;            return loc, []&#xa;        raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class LineEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of a line within the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(LineEnd,self).__init__()&#xa;        self.setWhitespaceChars( ParserElement.DEFAULT_WHITE_CHARS.replace(""\n"","""") )&#xa;        self.errmsg = ""Expected end of line""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc<len(instring):&#xa;            if instring[loc] == ""\n"":&#xa;                return loc+1, ""\n""&#xa;            else:&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class StringStart(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the beginning of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringStart,self).__init__()&#xa;        self.errmsg = ""Expected start of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            # see if entire string up to here is just whitespace and ignoreables&#xa;            if loc != self.preParse( instring, 0 ):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class StringEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if current position is at the end of the parse string&#xa;    """"""&#xa;    def __init__( self ):&#xa;        super(StringEnd,self).__init__()&#xa;        self.errmsg = ""Expected end of text""&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if loc < len(instring):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        elif loc == len(instring):&#xa;            return loc+1, []&#xa;        elif loc > len(instring):&#xa;            return loc, []&#xa;        else:&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;class WordStart(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the beginning of a Word, and&#xa;    is not preceded by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordStart(alphanums)}. C{WordStart} will also match at the beginning of&#xa;    the string being parsed, or at the beginning of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordStart,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.errmsg = ""Not at the start of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        if loc != 0:&#xa;            if (instring[loc-1] in self.wordChars or&#xa;                instring[loc] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;class WordEnd(_PositionToken):&#xa;    """"""&#xa;    Matches if the current position is at the end of a Word, and&#xa;    is not followed by any character in a given set of C{wordChars}&#xa;    (default=C{printables}). To emulate the C{\b} behavior of regular expressions,&#xa;    use C{WordEnd(alphanums)}. C{WordEnd} will also match at the end of&#xa;    the string being parsed, or at the end of a line.&#xa;    """"""&#xa;    def __init__(self, wordChars = printables):&#xa;        super(WordEnd,self).__init__()&#xa;        self.wordChars = set(wordChars)&#xa;        self.skipWhitespace = False&#xa;        self.errmsg = ""Not at the end of a word""&#xa;&#xa;    def parseImpl(self, instring, loc, doActions=True ):&#xa;        instrlen = len(instring)&#xa;        if instrlen>0 and loc<instrlen:&#xa;            if (instring[loc] in self.wordChars or&#xa;                instring[loc-1] not in self.wordChars):&#xa;                raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;&#xa;class ParseExpression(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of ParserElement, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(ParseExpression,self).__init__(savelist)&#xa;        if isinstance( exprs, _generatorType ):&#xa;            exprs = list(exprs)&#xa;&#xa;        if isinstance( exprs, basestring ):&#xa;            self.exprs = [ ParserElement._literalStringClass( exprs ) ]&#xa;        elif isinstance( exprs, collections.Iterable ):&#xa;            exprs = list(exprs)&#xa;            # if sequence of strings provided, wrap with Literal&#xa;            if all(isinstance(expr, basestring) for expr in exprs):&#xa;                exprs = map(ParserElement._literalStringClass, exprs)&#xa;            self.exprs = list(exprs)&#xa;        else:&#xa;            try:&#xa;                self.exprs = list( exprs )&#xa;            except TypeError:&#xa;                self.exprs = [ exprs ]&#xa;        self.callPreparse = False&#xa;&#xa;    def __getitem__( self, i ):&#xa;        return self.exprs[i]&#xa;&#xa;    def append( self, other ):&#xa;        self.exprs.append( other )&#xa;        self.strRepr = None&#xa;        return self&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        """"""Extends C{leaveWhitespace} defined in base class, and also invokes C{leaveWhitespace} on&#xa;           all contained expressions.""""""&#xa;        self.skipWhitespace = False&#xa;        self.exprs = [ e.copy() for e in self.exprs ]&#xa;        for e in self.exprs:&#xa;            e.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseExpression, self).ignore( other )&#xa;                for e in self.exprs:&#xa;                    e.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseExpression, self).ignore( other )&#xa;            for e in self.exprs:&#xa;                e.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseExpression,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.exprs) )&#xa;        return self.strRepr&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseExpression,self).streamline()&#xa;&#xa;        for e in self.exprs:&#xa;            e.streamline()&#xa;&#xa;        # collapse nested And's of the form And( And( And( a,b), c), d) to And( a,b,c,d )&#xa;        # but only if there are no parse actions or resultsNames on the nested And's&#xa;        # (likewise for Or's and MatchFirst's)&#xa;        if ( len(self.exprs) == 2 ):&#xa;            other = self.exprs[0]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = other.exprs[:] + [ self.exprs[1] ]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;            other = self.exprs[-1]&#xa;            if ( isinstance( other, self.__class__ ) and&#xa;                  not(other.parseAction) and&#xa;                  other.resultsName is None and&#xa;                  not other.debug ):&#xa;                self.exprs = self.exprs[:-1] + other.exprs[:]&#xa;                self.strRepr = None&#xa;                self.mayReturnEmpty |= other.mayReturnEmpty&#xa;                self.mayIndexError  |= other.mayIndexError&#xa;&#xa;        self.errmsg = ""Expected "" + _ustr(self)&#xa;        &#xa;        return self&#xa;&#xa;    def setResultsName( self, name, listAllMatches=False ):&#xa;        ret = super(ParseExpression,self).setResultsName(name,listAllMatches)&#xa;        return ret&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        for e in self.exprs:&#xa;            e.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;        &#xa;    def copy(self):&#xa;        ret = super(ParseExpression,self).copy()&#xa;        ret.exprs = [e.copy() for e in self.exprs]&#xa;        return ret&#xa;&#xa;class And(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found in the given order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'+'} operator.&#xa;    May also be constructed using the C{'-'} operator, which will suppress backtracking.&#xa;&#xa;    Example::&#xa;        integer = Word(nums)&#xa;        name_expr = OneOrMore(Word(alphas))&#xa;&#xa;        expr = And([integer(""id""),name_expr(""name""),integer(""age"")])&#xa;        # more easily written as:&#xa;        expr = integer(""id"") + name_expr(""name"") + integer(""age"")&#xa;    """"""&#xa;&#xa;    class _ErrorStop(Empty):&#xa;        def __init__(self, *args, **kwargs):&#xa;            super(And._ErrorStop,self).__init__(*args, **kwargs)&#xa;            self.name = '-'&#xa;            self.leaveWhitespace()&#xa;&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(And,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.setWhitespaceChars( self.exprs[0].whiteChars )&#xa;        self.skipWhitespace = self.exprs[0].skipWhitespace&#xa;        self.callPreparse = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        # pass False as last arg to _parse for first element, since we already&#xa;        # pre-parsed the string as part of our And pre-parsing&#xa;        loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )&#xa;        errorStop = False&#xa;        for e in self.exprs[1:]:&#xa;            if isinstance(e, And._ErrorStop):&#xa;                errorStop = True&#xa;                continue&#xa;            if errorStop:&#xa;                try:&#xa;                    loc, exprtokens = e._parse( instring, loc, doActions )&#xa;                except ParseSyntaxException:&#xa;                    raise&#xa;                except ParseBaseException as pe:&#xa;                    pe.__traceback__ = None&#xa;                    raise ParseSyntaxException._from_exception(pe)&#xa;                except IndexError:&#xa;                    raise ParseSyntaxException(instring, len(instring), self.errmsg, self)&#xa;            else:&#xa;                loc, exprtokens = e._parse( instring, loc, doActions )&#xa;            if exprtokens or exprtokens.haskeys():&#xa;                resultlist += exprtokens&#xa;        return loc, resultlist&#xa;&#xa;    def __iadd__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #And( [ self, other ] )&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;            if not e.mayReturnEmpty:&#xa;                break&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;&#xa;class Or(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the expression that matches the longest string will be used.&#xa;    May be constructed using the C{'^'} operator.&#xa;&#xa;    Example::&#xa;        # construct Or using '^' operator&#xa;        &#xa;        number = Word(nums) ^ Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789""))&#xa;    prints::&#xa;        [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(Or,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        matches = []&#xa;        for e in self.exprs:&#xa;            try:&#xa;                loc2 = e.tryParse( instring, loc )&#xa;            except ParseException as err:&#xa;                err.__traceback__ = None&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;            else:&#xa;                # save match among all matches, to retry longest to shortest&#xa;                matches.append((loc2, e))&#xa;&#xa;        if matches:&#xa;            matches.sort(key=lambda x: -x[0])&#xa;            for _,e in matches:&#xa;                try:&#xa;                    return e._parse( instring, loc, doActions )&#xa;                except ParseException as err:&#xa;                    err.__traceback__ = None&#xa;                    if err.loc > maxExcLoc:&#xa;                        maxException = err&#xa;                        maxExcLoc = err.loc&#xa;&#xa;        if maxException is not None:&#xa;            maxException.msg = self.errmsg&#xa;            raise maxException&#xa;        else:&#xa;            raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;&#xa;    def __ixor__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #Or( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" ^ "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class MatchFirst(ParseExpression):&#xa;    """"""&#xa;    Requires that at least one C{ParseExpression} is found.&#xa;    If two expressions match, the first one listed is the one that will match.&#xa;    May be constructed using the C{'|'} operator.&#xa;&#xa;    Example::&#xa;        # construct MatchFirst using '|' operator&#xa;        &#xa;        # watch the order of expressions to match&#xa;        number = Word(nums) | Combine(Word(nums) + '.' + Word(nums))&#xa;        print(number.searchString(""123 3.1416 789"")) #  Fail! -> [['123'], ['3'], ['1416'], ['789']]&#xa;&#xa;        # put more selective expression first&#xa;        number = Combine(Word(nums) + '.' + Word(nums)) | Word(nums)&#xa;        print(number.searchString(""123 3.1416 789"")) #  Better -> [['123'], ['3.1416'], ['789']]&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = False ):&#xa;        super(MatchFirst,self).__init__(exprs, savelist)&#xa;        if self.exprs:&#xa;            self.mayReturnEmpty = any(e.mayReturnEmpty for e in self.exprs)&#xa;        else:&#xa;            self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        maxExcLoc = -1&#xa;        maxException = None&#xa;        for e in self.exprs:&#xa;            try:&#xa;                ret = e._parse( instring, loc, doActions )&#xa;                return ret&#xa;            except ParseException as err:&#xa;                if err.loc > maxExcLoc:&#xa;                    maxException = err&#xa;                    maxExcLoc = err.loc&#xa;            except IndexError:&#xa;                if len(instring) > maxExcLoc:&#xa;                    maxException = ParseException(instring,len(instring),e.errmsg,self)&#xa;                    maxExcLoc = len(instring)&#xa;&#xa;        # only got here if no expression matched, raise exception for match that made it the furthest&#xa;        else:&#xa;            if maxException is not None:&#xa;                maxException.msg = self.errmsg&#xa;                raise maxException&#xa;            else:&#xa;                raise ParseException(instring, loc, ""no defined alternatives to match"", self)&#xa;&#xa;    def __ior__(self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass( other )&#xa;        return self.append( other ) #MatchFirst( [ self, other ] )&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" | "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class Each(ParseExpression):&#xa;    """"""&#xa;    Requires all given C{ParseExpression}s to be found, but in any order.&#xa;    Expressions may be separated by whitespace.&#xa;    May be constructed using the C{'&'} operator.&#xa;&#xa;    Example::&#xa;        color = oneOf(""RED ORANGE YELLOW GREEN BLUE PURPLE BLACK WHITE BROWN"")&#xa;        shape_type = oneOf(""SQUARE CIRCLE TRIANGLE STAR HEXAGON OCTAGON"")&#xa;        integer = Word(nums)&#xa;        shape_attr = ""shape:"" + shape_type(""shape"")&#xa;        posn_attr = ""posn:"" + Group(integer(""x"") + ',' + integer(""y""))(""posn"")&#xa;        color_attr = ""color:"" + color(""color"")&#xa;        size_attr = ""size:"" + integer(""size"")&#xa;&#xa;        # use Each (using operator '&') to accept attributes in any order &#xa;        # (shape and posn are required, color and size are optional)&#xa;        shape_spec = shape_attr & posn_attr & Optional(color_attr) & Optional(size_attr)&#xa;&#xa;        shape_spec.runTests('''&#xa;            shape: SQUARE color: BLACK posn: 100, 120&#xa;            shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;            color:GREEN size:20 shape:TRIANGLE posn:20,40&#xa;            '''&#xa;            )&#xa;    prints::&#xa;        shape: SQUARE color: BLACK posn: 100, 120&#xa;        ['shape:', 'SQUARE', 'color:', 'BLACK', 'posn:', ['100', ',', '120']]&#xa;        - color: BLACK&#xa;        - posn: ['100', ',', '120']&#xa;          - x: 100&#xa;          - y: 120&#xa;        - shape: SQUARE&#xa;&#xa;&#xa;        shape: CIRCLE size: 50 color: BLUE posn: 50,80&#xa;        ['shape:', 'CIRCLE', 'size:', '50', 'color:', 'BLUE', 'posn:', ['50', ',', '80']]&#xa;        - color: BLUE&#xa;        - posn: ['50', ',', '80']&#xa;          - x: 50&#xa;          - y: 80&#xa;        - shape: CIRCLE&#xa;        - size: 50&#xa;&#xa;&#xa;        color: GREEN size: 20 shape: TRIANGLE posn: 20,40&#xa;        ['color:', 'GREEN', 'size:', '20', 'shape:', 'TRIANGLE', 'posn:', ['20', ',', '40']]&#xa;        - color: GREEN&#xa;        - posn: ['20', ',', '40']&#xa;          - x: 20&#xa;          - y: 40&#xa;        - shape: TRIANGLE&#xa;        - size: 20&#xa;    """"""&#xa;    def __init__( self, exprs, savelist = True ):&#xa;        super(Each,self).__init__(exprs, savelist)&#xa;        self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)&#xa;        self.skipWhitespace = True&#xa;        self.initExprGroups = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.initExprGroups:&#xa;            self.opt1map = dict((id(e.expr),e) for e in self.exprs if isinstance(e,Optional))&#xa;            opt1 = [ e.expr for e in self.exprs if isinstance(e,Optional) ]&#xa;            opt2 = [ e for e in self.exprs if e.mayReturnEmpty and not isinstance(e,Optional)]&#xa;            self.optionals = opt1 + opt2&#xa;            self.multioptionals = [ e.expr for e in self.exprs if isinstance(e,ZeroOrMore) ]&#xa;            self.multirequired = [ e.expr for e in self.exprs if isinstance(e,OneOrMore) ]&#xa;            self.required = [ e for e in self.exprs if not isinstance(e,(Optional,ZeroOrMore,OneOrMore)) ]&#xa;            self.required += self.multirequired&#xa;            self.initExprGroups = False&#xa;        tmpLoc = loc&#xa;        tmpReqd = self.required[:]&#xa;        tmpOpt  = self.optionals[:]&#xa;        matchOrder = []&#xa;&#xa;        keepMatching = True&#xa;        while keepMatching:&#xa;            tmpExprs = tmpReqd + tmpOpt + self.multioptionals + self.multirequired&#xa;            failed = []&#xa;            for e in tmpExprs:&#xa;                try:&#xa;                    tmpLoc = e.tryParse( instring, tmpLoc )&#xa;                except ParseException:&#xa;                    failed.append(e)&#xa;                else:&#xa;                    matchOrder.append(self.opt1map.get(id(e),e))&#xa;                    if e in tmpReqd:&#xa;                        tmpReqd.remove(e)&#xa;                    elif e in tmpOpt:&#xa;                        tmpOpt.remove(e)&#xa;            if len(failed) == len(tmpExprs):&#xa;                keepMatching = False&#xa;&#xa;        if tmpReqd:&#xa;            missing = "", "".join(_ustr(e) for e in tmpReqd)&#xa;            raise ParseException(instring,loc,""Missing one or more required elements (%s)"" % missing )&#xa;&#xa;        # add any unmatched Optionals, in case they have default values defined&#xa;        matchOrder += [e for e in self.exprs if isinstance(e,Optional) and e.expr in tmpOpt]&#xa;&#xa;        resultlist = []&#xa;        for e in matchOrder:&#xa;            loc,results = e._parse(instring,loc,doActions)&#xa;            resultlist.append(results)&#xa;&#xa;        finalResults = sum(resultlist, ParseResults([]))&#xa;        return loc, finalResults&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + "" & "".join(_ustr(e) for e in self.exprs) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        for e in self.exprs:&#xa;            e.checkRecursion( subRecCheckList )&#xa;&#xa;&#xa;class ParseElementEnhance(ParserElement):&#xa;    """"""&#xa;    Abstract subclass of C{ParserElement}, for combining and post-processing parsed tokens.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(ParseElementEnhance,self).__init__(savelist)&#xa;        if isinstance( expr, basestring ):&#xa;            if issubclass(ParserElement._literalStringClass, Token):&#xa;                expr = ParserElement._literalStringClass(expr)&#xa;            else:&#xa;                expr = ParserElement._literalStringClass(Literal(expr))&#xa;        self.expr = expr&#xa;        self.strRepr = None&#xa;        if expr is not None:&#xa;            self.mayIndexError = expr.mayIndexError&#xa;            self.mayReturnEmpty = expr.mayReturnEmpty&#xa;            self.setWhitespaceChars( expr.whiteChars )&#xa;            self.skipWhitespace = expr.skipWhitespace&#xa;            self.saveAsList = expr.saveAsList&#xa;            self.callPreparse = expr.callPreparse&#xa;            self.ignoreExprs.extend(expr.ignoreExprs)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr is not None:&#xa;            return self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        else:&#xa;            raise ParseException("""",loc,self.errmsg,self)&#xa;&#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        self.expr = self.expr.copy()&#xa;        if self.expr is not None:&#xa;            self.expr.leaveWhitespace()&#xa;        return self&#xa;&#xa;    def ignore( self, other ):&#xa;        if isinstance( other, Suppress ):&#xa;            if other not in self.ignoreExprs:&#xa;                super( ParseElementEnhance, self).ignore( other )&#xa;                if self.expr is not None:&#xa;                    self.expr.ignore( self.ignoreExprs[-1] )&#xa;        else:&#xa;            super( ParseElementEnhance, self).ignore( other )&#xa;            if self.expr is not None:&#xa;                self.expr.ignore( self.ignoreExprs[-1] )&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        super(ParseElementEnhance,self).streamline()&#xa;        if self.expr is not None:&#xa;            self.expr.streamline()&#xa;        return self&#xa;&#xa;    def checkRecursion( self, parseElementList ):&#xa;        if self in parseElementList:&#xa;            raise RecursiveGrammarException( parseElementList+[self] )&#xa;        subRecCheckList = parseElementList[:] + [ self ]&#xa;        if self.expr is not None:&#xa;            self.expr.checkRecursion( subRecCheckList )&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        tmp = validateTrace[:]+[self]&#xa;        if self.expr is not None:&#xa;            self.expr.validate(tmp)&#xa;        self.checkRecursion( [] )&#xa;&#xa;    def __str__( self ):&#xa;        try:&#xa;            return super(ParseElementEnhance,self).__str__()&#xa;        except Exception:&#xa;            pass&#xa;&#xa;        if self.strRepr is None and self.expr is not None:&#xa;            self.strRepr = ""%s:(%s)"" % ( self.__class__.__name__, _ustr(self.expr) )&#xa;        return self.strRepr&#xa;&#xa;&#xa;class FollowedBy(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead matching of the given parse expression.  C{FollowedBy}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression matches at the current&#xa;    position.  C{FollowedBy} always returns a null token list.&#xa;&#xa;    Example::&#xa;        # use FollowedBy to match a label only if it is followed by a ':'&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        OneOrMore(attr_expr).parseString(""shape: SQUARE color: BLACK posn: upper left"").pprint()&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['color', 'BLACK'], ['posn', 'upper left']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(FollowedBy,self).__init__(expr)&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self.expr.tryParse( instring, loc )&#xa;        return loc, []&#xa;&#xa;&#xa;class NotAny(ParseElementEnhance):&#xa;    """"""&#xa;    Lookahead to disallow matching with the given parse expression.  C{NotAny}&#xa;    does I{not} advance the parsing position within the input string, it only&#xa;    verifies that the specified parse expression does I{not} match at the current&#xa;    position.  Also, C{NotAny} does I{not} skip over leading whitespace. C{NotAny}&#xa;    always returns a null token list.  May be constructed using the '~' operator.&#xa;&#xa;    Example::&#xa;        &#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(NotAny,self).__init__(expr)&#xa;        #~ self.leaveWhitespace()&#xa;        self.skipWhitespace = False  # do NOT use self.leaveWhitespace(), don't want to propagate to exprs&#xa;        self.mayReturnEmpty = True&#xa;        self.errmsg = ""Found unwanted token, ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        if self.expr.canParseNext(instring, loc):&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;        return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""~{"" + _ustr(self.expr) + ""}""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _MultipleMatch(ParseElementEnhance):&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(_MultipleMatch, self).__init__(expr)&#xa;        self.saveAsList = True&#xa;        ender = stopOn&#xa;        if isinstance(ender, basestring):&#xa;            ender = ParserElement._literalStringClass(ender)&#xa;        self.not_ender = ~ender if ender is not None else None&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        self_expr_parse = self.expr._parse&#xa;        self_skip_ignorables = self._skipIgnorables&#xa;        check_ender = self.not_ender is not None&#xa;        if check_ender:&#xa;            try_not_ender = self.not_ender.tryParse&#xa;        &#xa;        # must be at least one (but first see if we are the stopOn sentinel;&#xa;        # if so, fail)&#xa;        if check_ender:&#xa;            try_not_ender(instring, loc)&#xa;        loc, tokens = self_expr_parse( instring, loc, doActions, callPreParse=False )&#xa;        try:&#xa;            hasIgnoreExprs = (not not self.ignoreExprs)&#xa;            while 1:&#xa;                if check_ender:&#xa;                    try_not_ender(instring, loc)&#xa;                if hasIgnoreExprs:&#xa;                    preloc = self_skip_ignorables( instring, loc )&#xa;                else:&#xa;                    preloc = loc&#xa;                loc, tmptokens = self_expr_parse( instring, preloc, doActions )&#xa;                if tmptokens or tmptokens.haskeys():&#xa;                    tokens += tmptokens&#xa;        except (ParseException,IndexError):&#xa;            pass&#xa;&#xa;        return loc, tokens&#xa;        &#xa;class OneOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Repetition of one or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match one or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: BLACK""&#xa;        OneOrMore(attr_expr).parseString(text).pprint()  # Fail! read 'color' as data instead of next label -> [['shape', 'SQUARE color']]&#xa;&#xa;        # use stopOn attribute for OneOrMore to avoid reading label string as part of the data&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        OneOrMore(attr_expr).parseString(text).pprint() # Better -> [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'BLACK']]&#xa;        &#xa;        # could also be written as&#xa;        (attr_expr * (1,)).parseString(text).pprint()&#xa;    """"""&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""{"" + _ustr(self.expr) + ""}...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class ZeroOrMore(_MultipleMatch):&#xa;    """"""&#xa;    Optional repetition of zero or more of the given expression.&#xa;    &#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - stopOn - (default=C{None}) - expression for a terminating sentinel&#xa;          (only required if the sentinel would ordinarily match the repetition &#xa;          expression)          &#xa;&#xa;    Example: similar to L{OneOrMore}&#xa;    """"""&#xa;    def __init__( self, expr, stopOn=None):&#xa;        super(ZeroOrMore,self).__init__(expr, stopOn=stopOn)&#xa;        self.mayReturnEmpty = True&#xa;        &#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            return super(ZeroOrMore, self).parseImpl(instring, loc, doActions)&#xa;        except (ParseException,IndexError):&#xa;            return loc, []&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]...""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class _NullToken(object):&#xa;    def __bool__(self):&#xa;        return False&#xa;    __nonzero__ = __bool__&#xa;    def __str__(self):&#xa;        return """"&#xa;&#xa;_optionalNotMatched = _NullToken()&#xa;class Optional(ParseElementEnhance):&#xa;    """"""&#xa;    Optional matching of the given expression.&#xa;&#xa;    Parameters:&#xa;     - expr - expression that must match zero or more times&#xa;     - default (optional) - value to be returned if the optional expression is not found.&#xa;&#xa;    Example::&#xa;        # US postal code can be a 5-digit zip, plus optional 4-digit qualifier&#xa;        zip = Combine(Word(nums, exact=5) + Optional('-' + Word(nums, exact=4)))&#xa;        zip.runTests('''&#xa;            # traditional ZIP code&#xa;            12345&#xa;            &#xa;            # ZIP+4 form&#xa;            12101-0001&#xa;            &#xa;            # invalid ZIP&#xa;            98765-&#xa;            ''')&#xa;    prints::&#xa;        # traditional ZIP code&#xa;        12345&#xa;        ['12345']&#xa;&#xa;        # ZIP+4 form&#xa;        12101-0001&#xa;        ['12101-0001']&#xa;&#xa;        # invalid ZIP&#xa;        98765-&#xa;             ^&#xa;        FAIL: Expected end of text (at char 5), (line:1, col:6)&#xa;    """"""&#xa;    def __init__( self, expr, default=_optionalNotMatched ):&#xa;        super(Optional,self).__init__( expr, savelist=False )&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.defaultValue = default&#xa;        self.mayReturnEmpty = True&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        try:&#xa;            loc, tokens = self.expr._parse( instring, loc, doActions, callPreParse=False )&#xa;        except (ParseException,IndexError):&#xa;            if self.defaultValue is not _optionalNotMatched:&#xa;                if self.expr.resultsName:&#xa;                    tokens = ParseResults([ self.defaultValue ])&#xa;                    tokens[self.expr.resultsName] = self.defaultValue&#xa;                else:&#xa;                    tokens = [ self.defaultValue ]&#xa;            else:&#xa;                tokens = []&#xa;        return loc, tokens&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;&#xa;        if self.strRepr is None:&#xa;            self.strRepr = ""["" + _ustr(self.expr) + ""]""&#xa;&#xa;        return self.strRepr&#xa;&#xa;class SkipTo(ParseElementEnhance):&#xa;    """"""&#xa;    Token for skipping over all undefined text until the matched expression is found.&#xa;&#xa;    Parameters:&#xa;     - expr - target expression marking the end of the data to be skipped&#xa;     - include - (default=C{False}) if True, the target expression is also parsed &#xa;          (the skipped text and target expression are returned as a 2-element list).&#xa;     - ignore - (default=C{None}) used to define grammars (typically quoted strings and &#xa;          comments) that might contain false matches to the target expression&#xa;     - failOn - (default=C{None}) define expressions that are not allowed to be &#xa;          included in the skipped test; if found before the target expression is found, &#xa;          the SkipTo is not a match&#xa;&#xa;    Example::&#xa;        report = '''&#xa;            Outstanding Issues Report - 1 Jan 2000&#xa;&#xa;               # | Severity | Description                               |  Days Open&#xa;            -----+----------+-------------------------------------------+-----------&#xa;             101 | Critical | Intermittent system crash                 |          6&#xa;              94 | Cosmetic | Spelling error on Login ('log|n')         |         14&#xa;              79 | Minor    | System slow when running too many reports |         47&#xa;            '''&#xa;        integer = Word(nums)&#xa;        SEP = Suppress('|')&#xa;        # use SkipTo to simply match everything up until the next SEP&#xa;        # - ignore quoted strings, so that a '|' character inside a quoted string does not match&#xa;        # - parse action will call token.strip() for each matched token, i.e., the description body&#xa;        string_data = SkipTo(SEP, ignore=quotedString)&#xa;        string_data.setParseAction(tokenMap(str.strip))&#xa;        ticket_expr = (integer(""issue_num"") + SEP &#xa;                      + string_data(""sev"") + SEP &#xa;                      + string_data(""desc"") + SEP &#xa;                      + integer(""days_open""))&#xa;        &#xa;        for tkt in ticket_expr.searchString(report):&#xa;            print tkt.dump()&#xa;    prints::&#xa;        ['101', 'Critical', 'Intermittent system crash', '6']&#xa;        - days_open: 6&#xa;        - desc: Intermittent system crash&#xa;        - issue_num: 101&#xa;        - sev: Critical&#xa;        ['94', 'Cosmetic', ""Spelling error on Login ('log|n')"", '14']&#xa;        - days_open: 14&#xa;        - desc: Spelling error on Login ('log|n')&#xa;        - issue_num: 94&#xa;        - sev: Cosmetic&#xa;        ['79', 'Minor', 'System slow when running too many reports', '47']&#xa;        - days_open: 47&#xa;        - desc: System slow when running too many reports&#xa;        - issue_num: 79&#xa;        - sev: Minor&#xa;    """"""&#xa;    def __init__( self, other, include=False, ignore=None, failOn=None ):&#xa;        super( SkipTo, self ).__init__( other )&#xa;        self.ignoreExpr = ignore&#xa;        self.mayReturnEmpty = True&#xa;        self.mayIndexError = False&#xa;        self.includeMatch = include&#xa;        self.asList = False&#xa;        if isinstance(failOn, basestring):&#xa;            self.failOn = ParserElement._literalStringClass(failOn)&#xa;        else:&#xa;            self.failOn = failOn&#xa;        self.errmsg = ""No match found for ""+_ustr(self.expr)&#xa;&#xa;    def parseImpl( self, instring, loc, doActions=True ):&#xa;        startloc = loc&#xa;        instrlen = len(instring)&#xa;        expr = self.expr&#xa;        expr_parse = self.expr._parse&#xa;        self_failOn_canParseNext = self.failOn.canParseNext if self.failOn is not None else None&#xa;        self_ignoreExpr_tryParse = self.ignoreExpr.tryParse if self.ignoreExpr is not None else None&#xa;        &#xa;        tmploc = loc&#xa;        while tmploc <= instrlen:&#xa;            if self_failOn_canParseNext is not None:&#xa;                # break if failOn expression matches&#xa;                if self_failOn_canParseNext(instring, tmploc):&#xa;                    break&#xa;                    &#xa;            if self_ignoreExpr_tryParse is not None:&#xa;                # advance past ignore expressions&#xa;                while 1:&#xa;                    try:&#xa;                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)&#xa;                    except ParseBaseException:&#xa;                        break&#xa;            &#xa;            try:&#xa;                expr_parse(instring, tmploc, doActions=False, callPreParse=False)&#xa;            except (ParseException, IndexError):&#xa;                # no match, advance loc in string&#xa;                tmploc += 1&#xa;            else:&#xa;                # matched skipto expr, done&#xa;                break&#xa;&#xa;        else:&#xa;            # ran off the end of the input string without matching skipto expr, fail&#xa;            raise ParseException(instring, loc, self.errmsg, self)&#xa;&#xa;        # build up return values&#xa;        loc = tmploc&#xa;        skiptext = instring[startloc:loc]&#xa;        skipresult = ParseResults(skiptext)&#xa;        &#xa;        if self.includeMatch:&#xa;            loc, mat = expr_parse(instring,loc,doActions,callPreParse=False)&#xa;            skipresult += mat&#xa;&#xa;        return loc, skipresult&#xa;&#xa;class Forward(ParseElementEnhance):&#xa;    """"""&#xa;    Forward declaration of an expression to be defined later -&#xa;    used for recursive grammars, such as algebraic infix notation.&#xa;    When the expression is known, it is assigned to the C{Forward} variable using the '<<' operator.&#xa;&#xa;    Note: take care when assigning to C{Forward} not to overlook precedence of operators.&#xa;    Specifically, '|' has a lower precedence than '<<', so that::&#xa;        fwdExpr << a | b | c&#xa;    will actually be evaluated as::&#xa;        (fwdExpr << a) | b | c&#xa;    thereby leaving b and c out as parseable alternatives.  It is recommended that you&#xa;    explicitly group the values inserted into the C{Forward}::&#xa;        fwdExpr << (a | b | c)&#xa;    Converting to use the '<<=' operator instead will avoid this problem.&#xa;&#xa;    See L{ParseResults.pprint} for an example of a recursive parser created using&#xa;    C{Forward}.&#xa;    """"""&#xa;    def __init__( self, other=None ):&#xa;        super(Forward,self).__init__( other, savelist=False )&#xa;&#xa;    def __lshift__( self, other ):&#xa;        if isinstance( other, basestring ):&#xa;            other = ParserElement._literalStringClass(other)&#xa;        self.expr = other&#xa;        self.strRepr = None&#xa;        self.mayIndexError = self.expr.mayIndexError&#xa;        self.mayReturnEmpty = self.expr.mayReturnEmpty&#xa;        self.setWhitespaceChars( self.expr.whiteChars )&#xa;        self.skipWhitespace = self.expr.skipWhitespace&#xa;        self.saveAsList = self.expr.saveAsList&#xa;        self.ignoreExprs.extend(self.expr.ignoreExprs)&#xa;        return self&#xa;        &#xa;    def __ilshift__(self, other):&#xa;        return self << other&#xa;    &#xa;    def leaveWhitespace( self ):&#xa;        self.skipWhitespace = False&#xa;        return self&#xa;&#xa;    def streamline( self ):&#xa;        if not self.streamlined:&#xa;            self.streamlined = True&#xa;            if self.expr is not None:&#xa;                self.expr.streamline()&#xa;        return self&#xa;&#xa;    def validate( self, validateTrace=[] ):&#xa;        if self not in validateTrace:&#xa;            tmp = validateTrace[:]+[self]&#xa;            if self.expr is not None:&#xa;                self.expr.validate(tmp)&#xa;        self.checkRecursion([])&#xa;&#xa;    def __str__( self ):&#xa;        if hasattr(self,""name""):&#xa;            return self.name&#xa;        return self.__class__.__name__ + "": ...""&#xa;&#xa;        # stubbed out for now - creates awful memory and perf issues&#xa;        self._revertClass = self.__class__&#xa;        self.__class__ = _ForwardNoRecurse&#xa;        try:&#xa;            if self.expr is not None:&#xa;                retString = _ustr(self.expr)&#xa;            else:&#xa;                retString = ""None""&#xa;        finally:&#xa;            self.__class__ = self._revertClass&#xa;        return self.__class__.__name__ + "": "" + retString&#xa;&#xa;    def copy(self):&#xa;        if self.expr is not None:&#xa;            return super(Forward,self).copy()&#xa;        else:&#xa;            ret = Forward()&#xa;            ret <<= self&#xa;            return ret&#xa;&#xa;class _ForwardNoRecurse(Forward):&#xa;    def __str__( self ):&#xa;        return ""...""&#xa;&#xa;class TokenConverter(ParseElementEnhance):&#xa;    """"""&#xa;    Abstract subclass of C{ParseExpression}, for converting parsed results.&#xa;    """"""&#xa;    def __init__( self, expr, savelist=False ):&#xa;        super(TokenConverter,self).__init__( expr )#, savelist )&#xa;        self.saveAsList = False&#xa;&#xa;class Combine(TokenConverter):&#xa;    """"""&#xa;    Converter to concatenate all matching tokens to a single string.&#xa;    By default, the matching patterns must also be contiguous in the input string;&#xa;    this can be disabled by specifying C{'adjacent=False'} in the constructor.&#xa;&#xa;    Example::&#xa;        real = Word(nums) + '.' + Word(nums)&#xa;        print(real.parseString('3.1416')) # -> ['3', '.', '1416']&#xa;        # will also erroneously match the following&#xa;        print(real.parseString('3. 1416')) # -> ['3', '.', '1416']&#xa;&#xa;        real = Combine(Word(nums) + '.' + Word(nums))&#xa;        print(real.parseString('3.1416')) # -> ['3.1416']&#xa;        # no match when there are internal spaces&#xa;        print(real.parseString('3. 1416')) # -> Exception: Expected W:(0123...)&#xa;    """"""&#xa;    def __init__( self, expr, joinString="""", adjacent=True ):&#xa;        super(Combine,self).__init__( expr )&#xa;        # suppress whitespace-stripping in contained parse expressions, but re-enable it on the Combine itself&#xa;        if adjacent:&#xa;            self.leaveWhitespace()&#xa;        self.adjacent = adjacent&#xa;        self.skipWhitespace = True&#xa;        self.joinString = joinString&#xa;        self.callPreparse = True&#xa;&#xa;    def ignore( self, other ):&#xa;        if self.adjacent:&#xa;            ParserElement.ignore(self, other)&#xa;        else:&#xa;            super( Combine, self).ignore( other )&#xa;        return self&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        retToks = tokenlist.copy()&#xa;        del retToks[:]&#xa;        retToks += ParseResults([ """".join(tokenlist._asStringList(self.joinString)) ], modal=self.modalResults)&#xa;&#xa;        if self.resultsName and retToks.haskeys():&#xa;            return [ retToks ]&#xa;        else:&#xa;            return retToks&#xa;&#xa;class Group(TokenConverter):&#xa;    """"""&#xa;    Converter to return the matched tokens as a list - useful for returning tokens of C{L{ZeroOrMore}} and C{L{OneOrMore}} expressions.&#xa;&#xa;    Example::&#xa;        ident = Word(alphas)&#xa;        num = Word(nums)&#xa;        term = ident | num&#xa;        func = ident + Optional(delimitedList(term))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', 'a', 'b', '100']&#xa;&#xa;        func = ident + Group(Optional(delimitedList(term)))&#xa;        print(func.parseString(""fn a,b,100""))  # -> ['fn', ['a', 'b', '100']]&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Group,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return [ tokenlist ]&#xa;&#xa;class Dict(TokenConverter):&#xa;    """"""&#xa;    Converter to return a repetitive expression as a list, but also as a dictionary.&#xa;    Each element can also be referenced using the first token in the expression as its key.&#xa;    Useful for tabular report scraping when the first column can be used as a item key.&#xa;&#xa;    Example::&#xa;        data_word = Word(alphas)&#xa;        label = data_word + FollowedBy(':')&#xa;        attr_expr = Group(label + Suppress(':') + OneOrMore(data_word).setParseAction(' '.join))&#xa;&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        &#xa;        # print attributes as plain groups&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        # instead of OneOrMore(expr), parse using Dict(OneOrMore(Group(expr))) - Dict will auto-assign names&#xa;        result = Dict(OneOrMore(Group(attr_expr))).parseString(text)&#xa;        print(result.dump())&#xa;        &#xa;        # access named fields as dict entries, or output as dict&#xa;        print(result['shape'])        &#xa;        print(result.asDict())&#xa;    prints::&#xa;        ['shape', 'SQUARE', 'posn', 'upper left', 'color', 'light blue', 'texture', 'burlap']&#xa;&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        {'color': 'light blue', 'posn': 'upper left', 'texture': 'burlap', 'shape': 'SQUARE'}&#xa;    See more examples at L{ParseResults} of accessing fields by results name.&#xa;    """"""&#xa;    def __init__( self, expr ):&#xa;        super(Dict,self).__init__( expr )&#xa;        self.saveAsList = True&#xa;&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        for i,tok in enumerate(tokenlist):&#xa;            if len(tok) == 0:&#xa;                continue&#xa;            ikey = tok[0]&#xa;            if isinstance(ikey,int):&#xa;                ikey = _ustr(tok[0]).strip()&#xa;            if len(tok)==1:&#xa;                tokenlist[ikey] = _ParseResultsWithOffset("""",i)&#xa;            elif len(tok)==2 and not isinstance(tok[1],ParseResults):&#xa;                tokenlist[ikey] = _ParseResultsWithOffset(tok[1],i)&#xa;            else:&#xa;                dictvalue = tok.copy() #ParseResults(i)&#xa;                del dictvalue[0]&#xa;                if len(dictvalue)!= 1 or (isinstance(dictvalue,ParseResults) and dictvalue.haskeys()):&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue,i)&#xa;                else:&#xa;                    tokenlist[ikey] = _ParseResultsWithOffset(dictvalue[0],i)&#xa;&#xa;        if self.resultsName:&#xa;            return [ tokenlist ]&#xa;        else:&#xa;            return tokenlist&#xa;&#xa;&#xa;class Suppress(TokenConverter):&#xa;    """"""&#xa;    Converter for ignoring the results of a parsed expression.&#xa;&#xa;    Example::&#xa;        source = ""a, b, c,d""&#xa;        wd = Word(alphas)&#xa;        wd_list1 = wd + ZeroOrMore(',' + wd)&#xa;        print(wd_list1.parseString(source))&#xa;&#xa;        # often, delimiters that are useful during parsing are just in the&#xa;        # way afterward - use Suppress to keep them out of the parsed output&#xa;        wd_list2 = wd + ZeroOrMore(Suppress(',') + wd)&#xa;        print(wd_list2.parseString(source))&#xa;    prints::&#xa;        ['a', ',', 'b', ',', 'c', ',', 'd']&#xa;        ['a', 'b', 'c', 'd']&#xa;    (See also L{delimitedList}.)&#xa;    """"""&#xa;    def postParse( self, instring, loc, tokenlist ):&#xa;        return []&#xa;&#xa;    def suppress( self ):&#xa;        return self&#xa;&#xa;&#xa;class OnlyOnce(object):&#xa;    """"""&#xa;    Wrapper for parse actions, to ensure they are only called once.&#xa;    """"""&#xa;    def __init__(self, methodCall):&#xa;        self.callable = _trim_arity(methodCall)&#xa;        self.called = False&#xa;    def __call__(self,s,l,t):&#xa;        if not self.called:&#xa;            results = self.callable(s,l,t)&#xa;            self.called = True&#xa;            return results&#xa;        raise ParseException(s,l,"""")&#xa;    def reset(self):&#xa;        self.called = False&#xa;&#xa;def traceParseAction(f):&#xa;    """"""&#xa;    Decorator for debugging parse actions. &#xa;    &#xa;    When the parse action is called, this decorator will print C{"">> entering I{method-name}(line:I{current_source_line}, I{parse_location}, I{matched_tokens})"".}&#xa;    When the parse action completes, the decorator will print C{""<<""} followed by the returned value, or any exception that the parse action raised.&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;&#xa;        @traceParseAction&#xa;        def remove_duplicate_chars(tokens):&#xa;            return ''.join(sorted(set(''.join(tokens)))&#xa;&#xa;        wds = OneOrMore(wd).setParseAction(remove_duplicate_chars)&#xa;        print(wds.parseString(""slkdjs sld sldd sdlf sdljf""))&#xa;    prints::&#xa;        >>entering remove_duplicate_chars(line: 'slkdjs sld sldd sdlf sdljf', 0, (['slkdjs', 'sld', 'sldd', 'sdlf', 'sdljf'], {}))&#xa;        <<leaving remove_duplicate_chars (ret: 'dfjkls')&#xa;        ['dfjkls']&#xa;    """"""&#xa;    f = _trim_arity(f)&#xa;    def z(*paArgs):&#xa;        thisFunc = f.__name__&#xa;        s,l,t = paArgs[-3:]&#xa;        if len(paArgs)>3:&#xa;            thisFunc = paArgs[0].__class__.__name__ + '.' + thisFunc&#xa;        sys.stderr.write( "">>entering %s(line: '%s', %d, %r)\n"" % (thisFunc,line(l,s),l,t) )&#xa;        try:&#xa;            ret = f(*paArgs)&#xa;        except Exception as exc:&#xa;            sys.stderr.write( ""<<leaving %s (exception: %s)\n"" % (thisFunc,exc) )&#xa;            raise&#xa;        sys.stderr.write( ""<<leaving %s (ret: %r)\n"" % (thisFunc,ret) )&#xa;        return ret&#xa;    try:&#xa;        z.__name__ = f.__name__&#xa;    except AttributeError:&#xa;        pass&#xa;    return z&#xa;&#xa;#&#xa;# global helpers&#xa;#&#xa;def delimitedList( expr, delim="","", combine=False ):&#xa;    """"""&#xa;    Helper to define a delimited list of expressions - the delimiter defaults to ','.&#xa;    By default, the list elements and delimiters can have intervening whitespace, and&#xa;    comments, but this can be overridden by passing C{combine=True} in the constructor.&#xa;    If C{combine} is set to C{True}, the matching tokens are returned as a single token&#xa;    string, with the delimiters included; otherwise, the matching tokens are returned&#xa;    as a list of tokens, with the delimiters suppressed.&#xa;&#xa;    Example::&#xa;        delimitedList(Word(alphas)).parseString(""aa,bb,cc"") # -> ['aa', 'bb', 'cc']&#xa;        delimitedList(Word(hexnums), delim=':', combine=True).parseString(""AA:BB:CC:DD:EE"") # -> ['AA:BB:CC:DD:EE']&#xa;    """"""&#xa;    dlName = _ustr(expr)+"" [""+_ustr(delim)+"" ""+_ustr(expr)+""]...""&#xa;    if combine:&#xa;        return Combine( expr + ZeroOrMore( delim + expr ) ).setName(dlName)&#xa;    else:&#xa;        return ( expr + ZeroOrMore( Suppress( delim ) + expr ) ).setName(dlName)&#xa;&#xa;def countedArray( expr, intExpr=None ):&#xa;    """"""&#xa;    Helper to define a counted list of expressions.&#xa;    This helper defines a pattern of the form::&#xa;        integer expr expr expr...&#xa;    where the leading integer tells how many expr expressions follow.&#xa;    The matched tokens returns the array of expr tokens as a list - the leading count token is suppressed.&#xa;    &#xa;    If C{intExpr} is specified, it should be a pyparsing expression that produces an integer value.&#xa;&#xa;    Example::&#xa;        countedArray(Word(alphas)).parseString('2 ab cd ef')  # -> ['ab', 'cd']&#xa;&#xa;        # in this parser, the leading integer value is given in binary,&#xa;        # '10' indicating that 2 values are in the array&#xa;        binaryConstant = Word('01').setParseAction(lambda t: int(t[0], 2))&#xa;        countedArray(Word(alphas), intExpr=binaryConstant).parseString('10 ab cd ef')  # -> ['ab', 'cd']&#xa;    """"""&#xa;    arrayExpr = Forward()&#xa;    def countFieldParseAction(s,l,t):&#xa;        n = t[0]&#xa;        arrayExpr << (n and Group(And([expr]*n)) or Group(empty))&#xa;        return []&#xa;    if intExpr is None:&#xa;        intExpr = Word(nums).setParseAction(lambda t:int(t[0]))&#xa;    else:&#xa;        intExpr = intExpr.copy()&#xa;    intExpr.setName(""arrayLen"")&#xa;    intExpr.addParseAction(countFieldParseAction, callDuringTry=True)&#xa;    return ( intExpr + arrayExpr ).setName('(len) ' + _ustr(expr) + '...')&#xa;&#xa;def _flatten(L):&#xa;    ret = []&#xa;    for i in L:&#xa;        if isinstance(i,list):&#xa;            ret.extend(_flatten(i))&#xa;        else:&#xa;            ret.append(i)&#xa;    return ret&#xa;&#xa;def matchPreviousLiteral(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousLiteral(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches a&#xa;    previous literal, will also match the leading C{""1:1""} in C{""1:10""}.&#xa;    If this is not desired, use C{matchPreviousExpr}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        if t:&#xa;            if len(t) == 1:&#xa;                rep << t[0]&#xa;            else:&#xa;                # flatten t tokens&#xa;                tflat = _flatten(t.asList())&#xa;                rep << And(Literal(tt) for tt in tflat)&#xa;        else:&#xa;            rep << Empty()&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def matchPreviousExpr(expr):&#xa;    """"""&#xa;    Helper to define an expression that is indirectly defined from&#xa;    the tokens matched in a previous expression, that is, it looks&#xa;    for a 'repeat' of a previous expression.  For example::&#xa;        first = Word(nums)&#xa;        second = matchPreviousExpr(first)&#xa;        matchExpr = first + "":"" + second&#xa;    will match C{""1:1""}, but not C{""1:2""}.  Because this matches by&#xa;    expressions, will I{not} match the leading C{""1:1""} in C{""1:10""};&#xa;    the expressions are evaluated first, and then compared, so&#xa;    C{""1""} is compared with C{""10""}.&#xa;    Do I{not} use with packrat parsing enabled.&#xa;    """"""&#xa;    rep = Forward()&#xa;    e2 = expr.copy()&#xa;    rep <<= e2&#xa;    def copyTokenToRepeater(s,l,t):&#xa;        matchTokens = _flatten(t.asList())&#xa;        def mustMatchTheseTokens(s,l,t):&#xa;            theseTokens = _flatten(t.asList())&#xa;            if  theseTokens != matchTokens:&#xa;                raise ParseException("""",0,"""")&#xa;        rep.setParseAction( mustMatchTheseTokens, callDuringTry=True )&#xa;    expr.addParseAction(copyTokenToRepeater, callDuringTry=True)&#xa;    rep.setName('(prev) ' + _ustr(expr))&#xa;    return rep&#xa;&#xa;def _escapeRegexRangeChars(s):&#xa;    #~  escape these chars: ^-]&#xa;    for c in r""\^-]"":&#xa;        s = s.replace(c,_bslash+c)&#xa;    s = s.replace(""\n"",r""\n"")&#xa;    s = s.replace(""\t"",r""\t"")&#xa;    return _ustr(s)&#xa;&#xa;def oneOf( strs, caseless=False, useRegex=True ):&#xa;    """"""&#xa;    Helper to quickly define a set of alternative Literals, and makes sure to do&#xa;    longest-first testing when there is a conflict, regardless of the input order,&#xa;    but returns a C{L{MatchFirst}} for best performance.&#xa;&#xa;    Parameters:&#xa;     - strs - a string of space-delimited literals, or a collection of string literals&#xa;     - caseless - (default=C{False}) - treat all literals as caseless&#xa;     - useRegex - (default=C{True}) - as an optimization, will generate a Regex&#xa;          object; otherwise, will generate a C{MatchFirst} object (if C{caseless=True}, or&#xa;          if creating a C{Regex} raises an exception)&#xa;&#xa;    Example::&#xa;        comp_oper = oneOf(""< = > <= >= !="")&#xa;        var = Word(alphas)&#xa;        number = Word(nums)&#xa;        term = var | number&#xa;        comparison_expr = term + comp_oper + term&#xa;        print(comparison_expr.searchString(""B = 12  AA=23 B<=AA AA>12""))&#xa;    prints::&#xa;        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]&#xa;    """"""&#xa;    if caseless:&#xa;        isequal = ( lambda a,b: a.upper() == b.upper() )&#xa;        masks = ( lambda a,b: b.upper().startswith(a.upper()) )&#xa;        parseElementClass = CaselessLiteral&#xa;    else:&#xa;        isequal = ( lambda a,b: a == b )&#xa;        masks = ( lambda a,b: b.startswith(a) )&#xa;        parseElementClass = Literal&#xa;&#xa;    symbols = []&#xa;    if isinstance(strs,basestring):&#xa;        symbols = strs.split()&#xa;    elif isinstance(strs, collections.Iterable):&#xa;        symbols = list(strs)&#xa;    else:&#xa;        warnings.warn(""Invalid argument to oneOf, expected string or iterable"",&#xa;                SyntaxWarning, stacklevel=2)&#xa;    if not symbols:&#xa;        return NoMatch()&#xa;&#xa;    i = 0&#xa;    while i < len(symbols)-1:&#xa;        cur = symbols[i]&#xa;        for j,other in enumerate(symbols[i+1:]):&#xa;            if ( isequal(other, cur) ):&#xa;                del symbols[i+j+1]&#xa;                break&#xa;            elif ( masks(cur, other) ):&#xa;                del symbols[i+j+1]&#xa;                symbols.insert(i,other)&#xa;                cur = other&#xa;                break&#xa;        else:&#xa;            i += 1&#xa;&#xa;    if not caseless and useRegex:&#xa;        #~ print (strs,""->"", ""|"".join( [ _escapeRegexChars(sym) for sym in symbols] ))&#xa;        try:&#xa;            if len(symbols)==len("""".join(symbols)):&#xa;                return Regex( ""[%s]"" % """".join(_escapeRegexRangeChars(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;            else:&#xa;                return Regex( ""|"".join(re.escape(sym) for sym in symbols) ).setName(' | '.join(symbols))&#xa;        except Exception:&#xa;            warnings.warn(""Exception creating Regex for oneOf, building MatchFirst"",&#xa;                    SyntaxWarning, stacklevel=2)&#xa;&#xa;&#xa;    # last resort, just use MatchFirst&#xa;    return MatchFirst(parseElementClass(sym) for sym in symbols).setName(' | '.join(symbols))&#xa;&#xa;def dictOf( key, value ):&#xa;    """"""&#xa;    Helper to easily and clearly define a dictionary by specifying the respective patterns&#xa;    for the key and value.  Takes care of defining the C{L{Dict}}, C{L{ZeroOrMore}}, and C{L{Group}} tokens&#xa;    in the proper order.  The key pattern can include delimiting markers or punctuation,&#xa;    as long as they are suppressed, thereby leaving the significant key text.  The value&#xa;    pattern can include named results, so that the C{Dict} results can include named token&#xa;    fields.&#xa;&#xa;    Example::&#xa;        text = ""shape: SQUARE posn: upper left color: light blue texture: burlap""&#xa;        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join))&#xa;        print(OneOrMore(attr_expr).parseString(text).dump())&#xa;        &#xa;        attr_label = label&#xa;        attr_value = Suppress(':') + OneOrMore(data_word, stopOn=label).setParseAction(' '.join)&#xa;&#xa;        # similar to Dict, but simpler call format&#xa;        result = dictOf(attr_label, attr_value).parseString(text)&#xa;        print(result.dump())&#xa;        print(result['shape'])&#xa;        print(result.shape)  # object attribute access works too&#xa;        print(result.asDict())&#xa;    prints::&#xa;        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]&#xa;        - color: light blue&#xa;        - posn: upper left&#xa;        - shape: SQUARE&#xa;        - texture: burlap&#xa;        SQUARE&#xa;        SQUARE&#xa;        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}&#xa;    """"""&#xa;    return Dict( ZeroOrMore( Group ( key + value ) ) )&#xa;&#xa;def originalTextFor(expr, asString=True):&#xa;    """"""&#xa;    Helper to return the original, untokenized text for a given expression.  Useful to&#xa;    restore the parsed fields of an HTML start tag into the raw tag text itself, or to&#xa;    revert separate tokens with intervening whitespace back to the original matching&#xa;    input text. By default, returns astring containing the original parsed text.  &#xa;       &#xa;    If the optional C{asString} argument is passed as C{False}, then the return value is a &#xa;    C{L{ParseResults}} containing any results names that were originally matched, and a &#xa;    single token containing the original matched text from the input string.  So if &#xa;    the expression passed to C{L{originalTextFor}} contains expressions with defined&#xa;    results names, you must set C{asString} to C{False} if you want to preserve those&#xa;    results name values.&#xa;&#xa;    Example::&#xa;        src = ""this is test <b> bold <i>text</i> </b> normal text ""&#xa;        for tag in (""b"",""i""):&#xa;            opener,closer = makeHTMLTags(tag)&#xa;            patt = originalTextFor(opener + SkipTo(closer) + closer)&#xa;            print(patt.searchString(src)[0])&#xa;    prints::&#xa;        ['<b> bold <i>text</i> </b>']&#xa;        ['<i>text</i>']&#xa;    """"""&#xa;    locMarker = Empty().setParseAction(lambda s,loc,t: loc)&#xa;    endlocMarker = locMarker.copy()&#xa;    endlocMarker.callPreparse = False&#xa;    matchExpr = locMarker(""_original_start"") + expr + endlocMarker(""_original_end"")&#xa;    if asString:&#xa;        extractText = lambda s,l,t: s[t._original_start:t._original_end]&#xa;    else:&#xa;        def extractText(s,l,t):&#xa;            t[:] = [s[t.pop('_original_start'):t.pop('_original_end')]]&#xa;    matchExpr.setParseAction(extractText)&#xa;    matchExpr.ignoreExprs = expr.ignoreExprs&#xa;    return matchExpr&#xa;&#xa;def ungroup(expr): &#xa;    """"""&#xa;    Helper to undo pyparsing's default grouping of And expressions, even&#xa;    if all but one are non-empty.&#xa;    """"""&#xa;    return TokenConverter(expr).setParseAction(lambda t:t[0])&#xa;&#xa;def locatedExpr(expr):&#xa;    """"""&#xa;    Helper to decorate a returned token with its starting and ending locations in the input string.&#xa;    This helper adds the following results names:&#xa;     - locn_start = location where matched expression begins&#xa;     - locn_end = location where matched expression ends&#xa;     - value = the actual parsed results&#xa;&#xa;    Be careful if the input text contains C{<TAB>} characters, you may want to call&#xa;    C{L{ParserElement.parseWithTabs}}&#xa;&#xa;    Example::&#xa;        wd = Word(alphas)&#xa;        for match in locatedExpr(wd).searchString(""ljsdf123lksdjjf123lkkjj1222""):&#xa;            print(match)&#xa;    prints::&#xa;        [[0, 'ljsdf', 5]]&#xa;        [[8, 'lksdjjf', 15]]&#xa;        [[18, 'lkkjj', 23]]&#xa;    """"""&#xa;    locator = Empty().setParseAction(lambda s,l,t: l)&#xa;    return Group(locator(""locn_start"") + expr(""value"") + locator.copy().leaveWhitespace()(""locn_end""))&#xa;&#xa;&#xa;# convenience constants for positional expressions&#xa;empty       = Empty().setName(""empty"")&#xa;lineStart   = LineStart().setName(""lineStart"")&#xa;lineEnd     = LineEnd().setName(""lineEnd"")&#xa;stringStart = StringStart().setName(""stringStart"")&#xa;stringEnd   = StringEnd().setName(""stringEnd"")&#xa;&#xa;_escapedPunc = Word( _bslash, r""\[]-*.$+^?()~ "", exact=2 ).setParseAction(lambda s,l,t:t[0][1])&#xa;_escapedHexChar = Regex(r""\\0?[xX][0-9a-fA-F]+"").setParseAction(lambda s,l,t:unichr(int(t[0].lstrip(r'\0x'),16)))&#xa;_escapedOctChar = Regex(r""\\0[0-7]+"").setParseAction(lambda s,l,t:unichr(int(t[0][1:],8)))&#xa;_singleChar = _escapedPunc | _escapedHexChar | _escapedOctChar | Word(printables, excludeChars=r'\]', exact=1) | Regex(r""\w"", re.UNICODE)&#xa;_charRange = Group(_singleChar + Suppress(""-"") + _singleChar)&#xa;_reBracketExpr = Literal(""["") + Optional(""^"").setResultsName(""negate"") + Group( OneOrMore( _charRange | _singleChar ) ).setResultsName(""body"") + ""]""&#xa;&#xa;def srange(s):&#xa;    r""""""&#xa;    Helper to easily define string ranges for use in Word construction.  Borrows&#xa;    syntax from regexp '[]' string range definitions::&#xa;        srange(""[0-9]"")   -> ""0123456789""&#xa;        srange(""[a-z]"")   -> ""abcdefghijklmnopqrstuvwxyz""&#xa;        srange(""[a-z$_]"") -> ""abcdefghijklmnopqrstuvwxyz$_""&#xa;    The input string must be enclosed in []'s, and the returned string is the expanded&#xa;    character set joined into a single string.&#xa;    The values enclosed in the []'s may be:&#xa;     - a single character&#xa;     - an escaped character with a leading backslash (such as C{\-} or C{\]})&#xa;     - an escaped hex character with a leading C{'\x'} (C{\x21}, which is a C{'!'} character) &#xa;         (C{\0x##} is also supported for backwards compatibility) &#xa;     - an escaped octal character with a leading C{'\0'} (C{\041}, which is a C{'!'} character)&#xa;     - a range of any of the above, separated by a dash (C{'a-z'}, etc.)&#xa;     - any combination of the above (C{'aeiouy'}, C{'a-zA-Z0-9_$'}, etc.)&#xa;    """"""&#xa;    _expanded = lambda p: p if not isinstance(p,ParseResults) else ''.join(unichr(c) for c in range(ord(p[0]),ord(p[1])+1))&#xa;    try:&#xa;        return """".join(_expanded(part) for part in _reBracketExpr.parseString(s).body)&#xa;    except Exception:&#xa;        return """"&#xa;&#xa;def matchOnlyAtCol(n):&#xa;    """"""&#xa;    Helper method for defining parse actions that require matching at a specific&#xa;    column in the input text.&#xa;    """"""&#xa;    def verifyCol(strg,locn,toks):&#xa;        if col(locn,strg) != n:&#xa;            raise ParseException(strg,locn,""matched token not at column %d"" % n)&#xa;    return verifyCol&#xa;&#xa;def replaceWith(replStr):&#xa;    """"""&#xa;    Helper method for common parse actions that simply return a literal value.  Especially&#xa;    useful when used with C{L{transformString<ParserElement.transformString>}()}.&#xa;&#xa;    Example::&#xa;        num = Word(nums).setParseAction(lambda toks: int(toks[0]))&#xa;        na = oneOf(""N/A NA"").setParseAction(replaceWith(math.nan))&#xa;        term = na | num&#xa;        &#xa;        OneOrMore(term).parseString(""324 234 N/A 234"") # -> [324, 234, nan, 234]&#xa;    """"""&#xa;    return lambda s,l,t: [replStr]&#xa;&#xa;def removeQuotes(s,l,t):&#xa;    """"""&#xa;    Helper parse action for removing quotation marks from parsed quoted strings.&#xa;&#xa;    Example::&#xa;        # by default, quotation marks are included in parsed results&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""'Now is the Winter of our Discontent'""]&#xa;&#xa;        # use removeQuotes to strip quotation marks from parsed results&#xa;        quotedString.setParseAction(removeQuotes)&#xa;        quotedString.parseString(""'Now is the Winter of our Discontent'"") # -> [""Now is the Winter of our Discontent""]&#xa;    """"""&#xa;    return t[0][1:-1]&#xa;&#xa;def tokenMap(func, *args):&#xa;    """"""&#xa;    Helper to define a parse action by mapping a function to all elements of a ParseResults list.If any additional &#xa;    args are passed, they are forwarded to the given function as additional arguments after&#xa;    the token, as in C{hex_integer = Word(hexnums).setParseAction(tokenMap(int, 16))}, which will convert the&#xa;    parsed data to an integer using base 16.&#xa;&#xa;    Example (compare the last to example in L{ParserElement.transformString}::&#xa;        hex_ints = OneOrMore(Word(hexnums)).setParseAction(tokenMap(int, 16))&#xa;        hex_ints.runTests('''&#xa;            00 11 22 aa FF 0a 0d 1a&#xa;            ''')&#xa;        &#xa;        upperword = Word(alphas).setParseAction(tokenMap(str.upper))&#xa;        OneOrMore(upperword).runTests('''&#xa;            my kingdom for a horse&#xa;            ''')&#xa;&#xa;        wd = Word(alphas).setParseAction(tokenMap(str.title))&#xa;        OneOrMore(wd).setParseAction(' '.join).runTests('''&#xa;            now is the winter of our discontent made glorious summer by this sun of york&#xa;            ''')&#xa;    prints::&#xa;        00 11 22 aa FF 0a 0d 1a&#xa;        [0, 17, 34, 170, 255, 10, 13, 26]&#xa;&#xa;        my kingdom for a horse&#xa;        ['MY', 'KINGDOM', 'FOR', 'A', 'HORSE']&#xa;&#xa;        now is the winter of our discontent made glorious summer by this sun of york&#xa;        ['Now Is The Winter Of Our Discontent Made Glorious Summer By This Sun Of York']&#xa;    """"""&#xa;    def pa(s,l,t):&#xa;        return [func(tokn, *args) for tokn in t]&#xa;&#xa;    try:&#xa;        func_name = getattr(func, '__name__', &#xa;                            getattr(func, '__class__').__name__)&#xa;    except Exception:&#xa;        func_name = str(func)&#xa;    pa.__name__ = func_name&#xa;&#xa;    return pa&#xa;&#xa;upcaseTokens = tokenMap(lambda t: _ustr(t).upper())&#xa;""""""(Deprecated) Helper parse action to convert tokens to upper case. Deprecated in favor of L{pyparsing_common.upcaseTokens}""""""&#xa;&#xa;downcaseTokens = tokenMap(lambda t: _ustr(t).lower())&#xa;""""""(Deprecated) Helper parse action to convert tokens to lower case. Deprecated in favor of L{pyparsing_common.downcaseTokens}""""""&#xa;    &#xa;def _makeTags(tagStr, xml):&#xa;    """"""Internal helper to construct opening and closing tag expressions, given a tag name""""""&#xa;    if isinstance(tagStr,basestring):&#xa;        resname = tagStr&#xa;        tagStr = Keyword(tagStr, caseless=not xml)&#xa;    else:&#xa;        resname = tagStr.name&#xa;&#xa;    tagAttrName = Word(alphas,alphanums+""_-:"")&#xa;    if (xml):&#xa;        tagAttrValue = dblQuotedString.copy().setParseAction( removeQuotes )&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName + Suppress(""="") + tagAttrValue ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    else:&#xa;        printablesLessRAbrack = """".join(c for c in printables if c not in "">"")&#xa;        tagAttrValue = quotedString.copy().setParseAction( removeQuotes ) | Word(printablesLessRAbrack)&#xa;        openTag = Suppress(""<"") + tagStr(""tag"") + \&#xa;                Dict(ZeroOrMore(Group( tagAttrName.setParseAction(downcaseTokens) + \&#xa;                Optional( Suppress(""="") + tagAttrValue ) ))) + \&#xa;                Optional(""/"",default=[False]).setResultsName(""empty"").setParseAction(lambda s,l,t:t[0]=='/') + Suppress("">"")&#xa;    closeTag = Combine(_L(""</"") + tagStr + "">"")&#xa;&#xa;    openTag = openTag.setResultsName(""start""+"""".join(resname.replace("":"","" "").title().split())).setName(""<%s>"" % resname)&#xa;    closeTag = closeTag.setResultsName(""end""+"""".join(resname.replace("":"","" "").title().split())).setName(""</%s>"" % resname)&#xa;    openTag.tag = resname&#xa;    closeTag.tag = resname&#xa;    return openTag, closeTag&#xa;&#xa;def makeHTMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for HTML, given a tag name. Matches&#xa;    tags in either upper or lower case, attributes with namespaces and with quoted or unquoted values.&#xa;&#xa;    Example::&#xa;        text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;        # makeHTMLTags returns pyparsing expressions for the opening and closing tags as a 2-tuple&#xa;        a,a_end = makeHTMLTags(""A"")&#xa;        link_expr = a + SkipTo(a_end)(""link_text"") + a_end&#xa;        &#xa;        for link in link_expr.searchString(text):&#xa;            # attributes in the <A> tag (like ""href"" shown here) are also accessible as named results&#xa;            print(link.link_text, '->', link.href)&#xa;    prints::&#xa;        pyparsing -> http://pyparsing.wikispaces.com&#xa;    """"""&#xa;    return _makeTags( tagStr, False )&#xa;&#xa;def makeXMLTags(tagStr):&#xa;    """"""&#xa;    Helper to construct opening and closing tag expressions for XML, given a tag name. Matches&#xa;    tags only in the given upper/lower case.&#xa;&#xa;    Example: similar to L{makeHTMLTags}&#xa;    """"""&#xa;    return _makeTags( tagStr, True )&#xa;&#xa;def withAttribute(*args,**attrDict):&#xa;    """"""&#xa;    Helper to create a validating parse action to be used with start tags created&#xa;    with C{L{makeXMLTags}} or C{L{makeHTMLTags}}. Use C{withAttribute} to qualify a starting tag&#xa;    with a required attribute value, to avoid false matches on common tags such as&#xa;    C{<TD>} or C{<DIV>}.&#xa;&#xa;    Call C{withAttribute} with a series of attribute names and values. Specify the list&#xa;    of filter attributes names and values as:&#xa;     - keyword arguments, as in C{(align=""right"")}, or&#xa;     - as an explicit dict with C{**} operator, when an attribute name is also a Python&#xa;          reserved word, as in C{**{""class"":""Customer"", ""align"":""right""}}&#xa;     - a list of name-value tuples, as in ( (""ns1:class"", ""Customer""), (""ns2:align"",""right"") )&#xa;    For attribute names with a namespace prefix, you must use the second form.  Attribute&#xa;    names are matched insensitive to upper/lower case.&#xa;       &#xa;    If just testing for C{class} (with or without a namespace), use C{L{withClass}}.&#xa;&#xa;    To verify that the attribute exists, but without specifying a value, pass&#xa;    C{withAttribute.ANY_VALUE} as the value.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div type=""grid"">1 4 0 1 0</div>&#xa;            <div type=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this has no type</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;&#xa;        # only match div tag having a type attribute with value ""grid""&#xa;        div_grid = div().setParseAction(withAttribute(type=""grid""))&#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        # construct a match with any div tag having a type attribute, regardless of the value&#xa;        div_any_type = div().setParseAction(withAttribute(type=withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    if args:&#xa;        attrs = args[:]&#xa;    else:&#xa;        attrs = attrDict.items()&#xa;    attrs = [(k,v) for k,v in attrs]&#xa;    def pa(s,l,tokens):&#xa;        for attrName,attrValue in attrs:&#xa;            if attrName not in tokens:&#xa;                raise ParseException(s,l,""no matching attribute "" + attrName)&#xa;            if attrValue != withAttribute.ANY_VALUE and tokens[attrName] != attrValue:&#xa;                raise ParseException(s,l,""attribute '%s' has value '%s', must be '%s'"" %&#xa;                                            (attrName, tokens[attrName], attrValue))&#xa;    return pa&#xa;withAttribute.ANY_VALUE = object()&#xa;&#xa;def withClass(classname, namespace=''):&#xa;    """"""&#xa;    Simplified version of C{L{withAttribute}} when matching on a div class - made&#xa;    difficult because C{class} is a reserved word in Python.&#xa;&#xa;    Example::&#xa;        html = '''&#xa;            <div>&#xa;            Some text&#xa;            <div class=""grid"">1 4 0 1 0</div>&#xa;            <div class=""graph"">1,3 2,3 1,1</div>&#xa;            <div>this &lt;div&gt; has no class</div>&#xa;            </div>&#xa;                &#xa;        '''&#xa;        div,div_end = makeHTMLTags(""div"")&#xa;        div_grid = div().setParseAction(withClass(""grid""))&#xa;        &#xa;        grid_expr = div_grid + SkipTo(div | div_end)(""body"")&#xa;        for grid_header in grid_expr.searchString(html):&#xa;            print(grid_header.body)&#xa;        &#xa;        div_any_type = div().setParseAction(withClass(withAttribute.ANY_VALUE))&#xa;        div_expr = div_any_type + SkipTo(div | div_end)(""body"")&#xa;        for div_header in div_expr.searchString(html):&#xa;            print(div_header.body)&#xa;    prints::&#xa;        1 4 0 1 0&#xa;&#xa;        1 4 0 1 0&#xa;        1,3 2,3 1,1&#xa;    """"""&#xa;    classattr = ""%s:class"" % namespace if namespace else ""class""&#xa;    return withAttribute(**{classattr : classname})        &#xa;&#xa;opAssoc = _Constants()&#xa;opAssoc.LEFT = object()&#xa;opAssoc.RIGHT = object()&#xa;&#xa;def infixNotation( baseExpr, opList, lpar=Suppress('('), rpar=Suppress(')') ):&#xa;    """"""&#xa;    Helper method for constructing grammars of expressions made up of&#xa;    operators working in a precedence hierarchy.  Operators may be unary or&#xa;    binary, left- or right-associative.  Parse actions can also be attached&#xa;    to operator expressions. The generated parser will also recognize the use &#xa;    of parentheses to override operator precedences (see example below).&#xa;    &#xa;    Note: if you define a deep operator list, you may see performance issues&#xa;    when using infixNotation. See L{ParserElement.enablePackrat} for a&#xa;    mechanism to potentially improve your parser performance.&#xa;&#xa;    Parameters:&#xa;     - baseExpr - expression representing the most basic element for the nested&#xa;     - opList - list of tuples, one for each operator precedence level in the&#xa;      expression grammar; each tuple is of the form&#xa;      (opExpr, numTerms, rightLeftAssoc, parseAction), where:&#xa;       - opExpr is the pyparsing expression for the operator;&#xa;          may also be a string, which will be converted to a Literal;&#xa;          if numTerms is 3, opExpr is a tuple of two expressions, for the&#xa;          two operators separating the 3 terms&#xa;       - numTerms is the number of terms for this operator (must&#xa;          be 1, 2, or 3)&#xa;       - rightLeftAssoc is the indicator whether the operator is&#xa;          right or left associative, using the pyparsing-defined&#xa;          constants C{opAssoc.RIGHT} and C{opAssoc.LEFT}.&#xa;       - parseAction is the parse action to be associated with&#xa;          expressions matching this operator expression (the&#xa;          parse action tuple member may be omitted)&#xa;     - lpar - expression for matching left-parentheses (default=C{Suppress('(')})&#xa;     - rpar - expression for matching right-parentheses (default=C{Suppress(')')})&#xa;&#xa;    Example::&#xa;        # simple example of four-function arithmetic with ints and variable names&#xa;        integer = pyparsing_common.signed_integer&#xa;        varname = pyparsing_common.identifier &#xa;        &#xa;        arith_expr = infixNotation(integer | varname,&#xa;            [&#xa;            ('-', 1, opAssoc.RIGHT),&#xa;            (oneOf('* /'), 2, opAssoc.LEFT),&#xa;            (oneOf('+ -'), 2, opAssoc.LEFT),&#xa;            ])&#xa;        &#xa;        arith_expr.runTests('''&#xa;            5+3*6&#xa;            (5+3)*6&#xa;            -2--11&#xa;            ''', fullDump=False)&#xa;    prints::&#xa;        5+3*6&#xa;        [[5, '+', [3, '*', 6]]]&#xa;&#xa;        (5+3)*6&#xa;        [[[5, '+', 3], '*', 6]]&#xa;&#xa;        -2--11&#xa;        [[['-', 2], '-', ['-', 11]]]&#xa;    """"""&#xa;    ret = Forward()&#xa;    lastExpr = baseExpr | ( lpar + ret + rpar )&#xa;    for i,operDef in enumerate(opList):&#xa;        opExpr,arity,rightLeftAssoc,pa = (operDef + (None,))[:4]&#xa;        termName = ""%s term"" % opExpr if arity < 3 else ""%s%s term"" % opExpr&#xa;        if arity == 3:&#xa;            if opExpr is None or len(opExpr) != 2:&#xa;                raise ValueError(""if numterms=3, opExpr must be a tuple or list of two expressions"")&#xa;            opExpr1, opExpr2 = opExpr&#xa;        thisExpr = Forward().setName(termName)&#xa;        if rightLeftAssoc == opAssoc.LEFT:&#xa;            if arity == 1:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr) + Group( lastExpr + OneOrMore( opExpr ) )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + lastExpr) + Group( lastExpr + OneOrMore( opExpr + lastExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr+lastExpr) + Group( lastExpr + OneOrMore(lastExpr) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr) + \&#xa;                            Group( lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        elif rightLeftAssoc == opAssoc.RIGHT:&#xa;            if arity == 1:&#xa;                # try to avoid LR with this extra test&#xa;                if not isinstance(opExpr, Optional):&#xa;                    opExpr = Optional(opExpr)&#xa;                matchExpr = FollowedBy(opExpr.expr + thisExpr) + Group( opExpr + thisExpr )&#xa;            elif arity == 2:&#xa;                if opExpr is not None:&#xa;                    matchExpr = FollowedBy(lastExpr + opExpr + thisExpr) + Group( lastExpr + OneOrMore( opExpr + thisExpr ) )&#xa;                else:&#xa;                    matchExpr = FollowedBy(lastExpr + thisExpr) + Group( lastExpr + OneOrMore( thisExpr ) )&#xa;            elif arity == 3:&#xa;                matchExpr = FollowedBy(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr) + \&#xa;                            Group( lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr )&#xa;            else:&#xa;                raise ValueError(""operator must be unary (1), binary (2), or ternary (3)"")&#xa;        else:&#xa;            raise ValueError(""operator must indicate right or left associativity"")&#xa;        if pa:&#xa;            matchExpr.setParseAction( pa )&#xa;        thisExpr <<= ( matchExpr.setName(termName) | lastExpr )&#xa;        lastExpr = thisExpr&#xa;    ret <<= lastExpr&#xa;    return ret&#xa;&#xa;operatorPrecedence = infixNotation&#xa;""""""(Deprecated) Former name of C{L{infixNotation}}, will be dropped in a future release.""""""&#xa;&#xa;dblQuotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""').setName(""string enclosed in double quotes"")&#xa;sglQuotedString = Combine(Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""string enclosed in single quotes"")&#xa;quotedString = Combine(Regex(r'""(?:[^""\n\r\\]|(?:"""")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*')+'""'|&#xa;                       Regex(r""'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*"")+""'"").setName(""quotedString using single or double quotes"")&#xa;unicodeString = Combine(_L('u') + quotedString.copy()).setName(""unicode string literal"")&#xa;&#xa;def nestedExpr(opener=""("", closer="")"", content=None, ignoreExpr=quotedString.copy()):&#xa;    """"""&#xa;    Helper method for defining nested lists enclosed in opening and closing&#xa;    delimiters (""("" and "")"" are the default).&#xa;&#xa;    Parameters:&#xa;     - opener - opening character for a nested list (default=C{""(""}); can also be a pyparsing expression&#xa;     - closer - closing character for a nested list (default=C{"")""}); can also be a pyparsing expression&#xa;     - content - expression for items within the nested lists (default=C{None})&#xa;     - ignoreExpr - expression for ignoring opening and closing delimiters (default=C{quotedString})&#xa;&#xa;    If an expression is not provided for the content argument, the nested&#xa;    expression will capture all whitespace-delimited content between delimiters&#xa;    as a list of separate values.&#xa;&#xa;    Use the C{ignoreExpr} argument to define expressions that may contain&#xa;    opening or closing characters that should not be treated as opening&#xa;    or closing characters for nesting, such as quotedString or a comment&#xa;    expression.  Specify multiple expressions using an C{L{Or}} or C{L{MatchFirst}}.&#xa;    The default is L{quotedString}, but if no expressions are to be ignored,&#xa;    then pass C{None} for this argument.&#xa;&#xa;    Example::&#xa;        data_type = oneOf(""void int short long char float double"")&#xa;        decl_data_type = Combine(data_type + Optional(Word('*')))&#xa;        ident = Word(alphas+'_', alphanums+'_')&#xa;        number = pyparsing_common.number&#xa;        arg = Group(decl_data_type + ident)&#xa;        LPAR,RPAR = map(Suppress, ""()"")&#xa;&#xa;        code_body = nestedExpr('{', '}', ignoreExpr=(quotedString | cStyleComment))&#xa;&#xa;        c_function = (decl_data_type(""type"") &#xa;                      + ident(""name"")&#xa;                      + LPAR + Optional(delimitedList(arg), [])(""args"") + RPAR &#xa;                      + code_body(""body""))&#xa;        c_function.ignore(cStyleComment)&#xa;        &#xa;        source_code = '''&#xa;            int is_odd(int x) { &#xa;                return (x%2); &#xa;            }&#xa;                &#xa;            int dec_to_hex(char hchar) { &#xa;                if (hchar >= '0' && hchar <= '9') { &#xa;                    return (ord(hchar)-ord('0')); &#xa;                } else { &#xa;                    return (10+ord(hchar)-ord('A'));&#xa;                } &#xa;            }&#xa;        '''&#xa;        for func in c_function.searchString(source_code):&#xa;            print(""%(name)s (%(type)s) args: %(args)s"" % func)&#xa;&#xa;    prints::&#xa;        is_odd (int) args: [['int', 'x']]&#xa;        dec_to_hex (int) args: [['char', 'hchar']]&#xa;    """"""&#xa;    if opener == closer:&#xa;        raise ValueError(""opening and closing strings cannot be the same"")&#xa;    if content is None:&#xa;        if isinstance(opener,basestring) and isinstance(closer,basestring):&#xa;            if len(opener) == 1 and len(closer)==1:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr +&#xa;                                    CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (empty.copy()+CharsNotIn(opener+closer+ParserElement.DEFAULT_WHITE_CHARS&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;            else:&#xa;                if ignoreExpr is not None:&#xa;                    content = (Combine(OneOrMore(~ignoreExpr + &#xa;                                    ~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;                else:&#xa;                    content = (Combine(OneOrMore(~Literal(opener) + ~Literal(closer) +&#xa;                                    CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS,exact=1))&#xa;                                ).setParseAction(lambda t:t[0].strip()))&#xa;        else:&#xa;            raise ValueError(""opening and closing arguments must be strings if no content expression is given"")&#xa;    ret = Forward()&#xa;    if ignoreExpr is not None:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ignoreExpr | ret | content ) + Suppress(closer) )&#xa;    else:&#xa;        ret <<= Group( Suppress(opener) + ZeroOrMore( ret | content )  + Suppress(closer) )&#xa;    ret.setName('nested %s%s expression' % (opener,closer))&#xa;    return ret&#xa;&#xa;def indentedBlock(blockStatementExpr, indentStack, indent=True):&#xa;    """"""&#xa;    Helper method for defining space-delimited indentation blocks, such as&#xa;    those used to define block statements in Python source code.&#xa;&#xa;    Parameters:&#xa;     - blockStatementExpr - expression defining syntax of statement that&#xa;            is repeated within the indented block&#xa;     - indentStack - list created by caller to manage indentation stack&#xa;            (multiple statementWithIndentedBlock expressions within a single grammar&#xa;            should share a common indentStack)&#xa;     - indent - boolean indicating whether block must be indented beyond the&#xa;            the current level; set to False for block of left-most statements&#xa;            (default=C{True})&#xa;&#xa;    A valid block must contain at least one C{blockStatement}.&#xa;&#xa;    Example::&#xa;        data = '''&#xa;        def A(z):&#xa;          A1&#xa;          B = 100&#xa;          G = A2&#xa;          A2&#xa;          A3&#xa;        B&#xa;        def BB(a,b,c):&#xa;          BB1&#xa;          def BBA():&#xa;            bba1&#xa;            bba2&#xa;            bba3&#xa;        C&#xa;        D&#xa;        def spam(x,y):&#xa;             def eggs(z):&#xa;                 pass&#xa;        '''&#xa;&#xa;&#xa;        indentStack = [1]&#xa;        stmt = Forward()&#xa;&#xa;        identifier = Word(alphas, alphanums)&#xa;        funcDecl = (""def"" + identifier + Group( ""("" + Optional( delimitedList(identifier) ) + "")"" ) + "":"")&#xa;        func_body = indentedBlock(stmt, indentStack)&#xa;        funcDef = Group( funcDecl + func_body )&#xa;&#xa;        rvalue = Forward()&#xa;        funcCall = Group(identifier + ""("" + Optional(delimitedList(rvalue)) + "")"")&#xa;        rvalue << (funcCall | identifier | Word(nums))&#xa;        assignment = Group(identifier + ""="" + rvalue)&#xa;        stmt << ( funcDef | assignment | identifier )&#xa;&#xa;        module_body = OneOrMore(stmt)&#xa;&#xa;        parseTree = module_body.parseString(data)&#xa;        parseTree.pprint()&#xa;    prints::&#xa;        [['def',&#xa;          'A',&#xa;          ['(', 'z', ')'],&#xa;          ':',&#xa;          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],&#xa;         'B',&#xa;         ['def',&#xa;          'BB',&#xa;          ['(', 'a', 'b', 'c', ')'],&#xa;          ':',&#xa;          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],&#xa;         'C',&#xa;         'D',&#xa;         ['def',&#xa;          'spam',&#xa;          ['(', 'x', 'y', ')'],&#xa;          ':',&#xa;          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]] &#xa;    """"""&#xa;    def checkPeerIndent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if curCol != indentStack[-1]:&#xa;            if curCol > indentStack[-1]:&#xa;                raise ParseFatalException(s,l,""illegal nesting"")&#xa;            raise ParseException(s,l,""not a peer entry"")&#xa;&#xa;    def checkSubIndent(s,l,t):&#xa;        curCol = col(l,s)&#xa;        if curCol > indentStack[-1]:&#xa;            indentStack.append( curCol )&#xa;        else:&#xa;            raise ParseException(s,l,""not a subentry"")&#xa;&#xa;    def checkUnindent(s,l,t):&#xa;        if l >= len(s): return&#xa;        curCol = col(l,s)&#xa;        if not(indentStack and curCol < indentStack[-1] and curCol <= indentStack[-2]):&#xa;            raise ParseException(s,l,""not an unindent"")&#xa;        indentStack.pop()&#xa;&#xa;    NL = OneOrMore(LineEnd().setWhitespaceChars(""\t "").suppress())&#xa;    INDENT = (Empty() + Empty().setParseAction(checkSubIndent)).setName('INDENT')&#xa;    PEER   = Empty().setParseAction(checkPeerIndent).setName('')&#xa;    UNDENT = Empty().setParseAction(checkUnindent).setName('UNINDENT')&#xa;    if indent:&#xa;        smExpr = Group( Optional(NL) +&#xa;            #~ FollowedBy(blockStatementExpr) +&#xa;            INDENT + (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) + UNDENT)&#xa;    else:&#xa;        smExpr = Group( Optional(NL) +&#xa;            (OneOrMore( PEER + Group(blockStatementExpr) + Optional(NL) )) )&#xa;    blockStatementExpr.ignore(_bslash + LineEnd())&#xa;    return smExpr.setName('indented block')&#xa;&#xa;alphas8bit = srange(r""[\0xc0-\0xd6\0xd8-\0xf6\0xf8-\0xff]"")&#xa;punc8bit = srange(r""[\0xa1-\0xbf\0xd7\0xf7]"")&#xa;&#xa;anyOpenTag,anyCloseTag = makeHTMLTags(Word(alphas,alphanums+""_:"").setName('any tag'))&#xa;_htmlEntityMap = dict(zip(""gt lt amp nbsp quot apos"".split(),'><& ""\''))&#xa;commonHTMLEntity = Regex('&(?P<entity>' + '|'.join(_htmlEntityMap.keys()) +"");"").setName(""common HTML entity"")&#xa;def replaceHTMLEntity(t):&#xa;    """"""Helper parser action to replace common HTML entities with their special characters""""""&#xa;    return _htmlEntityMap.get(t.entity)&#xa;&#xa;# it's easy to get these comment structures wrong - they're very common, so may as well make them available&#xa;cStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/').setName(""C style comment"")&#xa;""Comment of the form C{/* ... */}""&#xa;&#xa;htmlComment = Regex(r""<!--[\s\S]*?-->"").setName(""HTML comment"")&#xa;""Comment of the form C{<!-- ... -->}""&#xa;&#xa;restOfLine = Regex(r"".*"").leaveWhitespace().setName(""rest of line"")&#xa;dblSlashComment = Regex(r""//(?:\\\n|[^\n])*"").setName(""// comment"")&#xa;""Comment of the form C{// ... (to end of line)}""&#xa;&#xa;cppStyleComment = Combine(Regex(r""/\*(?:[^*]|\*(?!/))*"") + '*/'| dblSlashComment).setName(""C++ style comment"")&#xa;""Comment of either form C{L{cStyleComment}} or C{L{dblSlashComment}}""&#xa;&#xa;javaStyleComment = cppStyleComment&#xa;""Same as C{L{cppStyleComment}}""&#xa;&#xa;pythonStyleComment = Regex(r""#.*"").setName(""Python style comment"")&#xa;""Comment of the form C{# ... (to end of line)}""&#xa;&#xa;_commasepitem = Combine(OneOrMore(Word(printables, excludeChars=',') +&#xa;                                  Optional( Word("" \t"") +&#xa;                                            ~Literal("","") + ~LineEnd() ) ) ).streamline().setName(""commaItem"")&#xa;commaSeparatedList = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""commaSeparatedList"")&#xa;""""""(Deprecated) Predefined expression of 1 or more printable words or quoted strings, separated by commas.&#xa;   This expression is deprecated in favor of L{pyparsing_common.comma_separated_list}.""""""&#xa;&#xa;# some other useful expressions - using lower-case class name since we are really using this as a namespace&#xa;class pyparsing_common:&#xa;    """"""&#xa;    Here are some common low-level expressions that may be useful in jump-starting parser development:&#xa;     - numeric forms (L{integers<integer>}, L{reals<real>}, L{scientific notation<sci_real>})&#xa;     - common L{programming identifiers<identifier>}&#xa;     - network addresses (L{MAC<mac_address>}, L{IPv4<ipv4_address>}, L{IPv6<ipv6_address>})&#xa;     - ISO8601 L{dates<iso8601_date>} and L{datetime<iso8601_datetime>}&#xa;     - L{UUID<uuid>}&#xa;     - L{comma-separated list<comma_separated_list>}&#xa;    Parse actions:&#xa;     - C{L{convertToInteger}}&#xa;     - C{L{convertToFloat}}&#xa;     - C{L{convertToDate}}&#xa;     - C{L{convertToDatetime}}&#xa;     - C{L{stripHTMLTags}}&#xa;     - C{L{upcaseTokens}}&#xa;     - C{L{downcaseTokens}}&#xa;&#xa;    Example::&#xa;        pyparsing_common.number.runTests('''&#xa;            # any int or real number, returned as the appropriate type&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.fnumber.runTests('''&#xa;            # any int or real number, returned as float&#xa;            100&#xa;            -100&#xa;            +100&#xa;            3.14159&#xa;            6.02e23&#xa;            1e-12&#xa;            ''')&#xa;&#xa;        pyparsing_common.hex_integer.runTests('''&#xa;            # hex numbers&#xa;            100&#xa;            FF&#xa;            ''')&#xa;&#xa;        pyparsing_common.fraction.runTests('''&#xa;            # fractions&#xa;            1/2&#xa;            -3/4&#xa;            ''')&#xa;&#xa;        pyparsing_common.mixed_integer.runTests('''&#xa;            # mixed fractions&#xa;            1&#xa;            1/2&#xa;            -3/4&#xa;            1-3/4&#xa;            ''')&#xa;&#xa;        import uuid&#xa;        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;        pyparsing_common.uuid.runTests('''&#xa;            # uuid&#xa;            12345678-1234-5678-1234-567812345678&#xa;            ''')&#xa;    prints::&#xa;        # any int or real number, returned as the appropriate type&#xa;        100&#xa;        [100]&#xa;&#xa;        -100&#xa;        [-100]&#xa;&#xa;        +100&#xa;        [100]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # any int or real number, returned as float&#xa;        100&#xa;        [100.0]&#xa;&#xa;        -100&#xa;        [-100.0]&#xa;&#xa;        +100&#xa;        [100.0]&#xa;&#xa;        3.14159&#xa;        [3.14159]&#xa;&#xa;        6.02e23&#xa;        [6.02e+23]&#xa;&#xa;        1e-12&#xa;        [1e-12]&#xa;&#xa;        # hex numbers&#xa;        100&#xa;        [256]&#xa;&#xa;        FF&#xa;        [255]&#xa;&#xa;        # fractions&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        # mixed fractions&#xa;        1&#xa;        [1]&#xa;&#xa;        1/2&#xa;        [0.5]&#xa;&#xa;        -3/4&#xa;        [-0.75]&#xa;&#xa;        1-3/4&#xa;        [1.75]&#xa;&#xa;        # uuid&#xa;        12345678-1234-5678-1234-567812345678&#xa;        [UUID('12345678-1234-5678-1234-567812345678')]&#xa;    """"""&#xa;&#xa;    convertToInteger = tokenMap(int)&#xa;    """"""&#xa;    Parse action for converting parsed integers to Python int&#xa;    """"""&#xa;&#xa;    convertToFloat = tokenMap(float)&#xa;    """"""&#xa;    Parse action for converting parsed numbers to Python float&#xa;    """"""&#xa;&#xa;    integer = Word(nums).setName(""integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an unsigned integer, returns an int""""""&#xa;&#xa;    hex_integer = Word(hexnums).setName(""hex integer"").setParseAction(tokenMap(int,16))&#xa;    """"""expression that parses a hexadecimal integer, returns an int""""""&#xa;&#xa;    signed_integer = Regex(r'[+-]?\d+').setName(""signed integer"").setParseAction(convertToInteger)&#xa;    """"""expression that parses an integer with optional leading sign, returns an int""""""&#xa;&#xa;    fraction = (signed_integer().setParseAction(convertToFloat) + '/' + signed_integer().setParseAction(convertToFloat)).setName(""fraction"")&#xa;    """"""fractional expression of an integer divided by an integer, returns a float""""""&#xa;    fraction.addParseAction(lambda t: t[0]/t[-1])&#xa;&#xa;    mixed_integer = (fraction | signed_integer + Optional(Optional('-').suppress() + fraction)).setName(""fraction or mixed integer-fraction"")&#xa;    """"""mixed integer of the form 'integer - fraction', with optional leading integer, returns float""""""&#xa;    mixed_integer.addParseAction(sum)&#xa;&#xa;    real = Regex(r'[+-]?\d+\.\d*').setName(""real number"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number and returns a float""""""&#xa;&#xa;    sci_real = Regex(r'[+-]?\d+([eE][+-]?\d+|\.\d*([eE][+-]?\d+)?)').setName(""real number with scientific notation"").setParseAction(convertToFloat)&#xa;    """"""expression that parses a floating point number with optional scientific notation and returns a float""""""&#xa;&#xa;    # streamlining this expression makes the docs nicer-looking&#xa;    number = (sci_real | real | signed_integer).streamline()&#xa;    """"""any numeric expression, returns the corresponding Python type""""""&#xa;&#xa;    fnumber = Regex(r'[+-]?\d+\.?\d*([eE][+-]?\d+)?').setName(""fnumber"").setParseAction(convertToFloat)&#xa;    """"""any int or real number, returned as float""""""&#xa;    &#xa;    identifier = Word(alphas+'_', alphanums+'_').setName(""identifier"")&#xa;    """"""typical code identifier (leading alpha or '_', followed by 0 or more alphas, nums, or '_')""""""&#xa;    &#xa;    ipv4_address = Regex(r'(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})(\.(25[0-5]|2[0-4][0-9]|1?[0-9]{1,2})){3}').setName(""IPv4 address"")&#xa;    ""IPv4 address (C{0.0.0.0 - 255.255.255.255})""&#xa;&#xa;    _ipv6_part = Regex(r'[0-9a-fA-F]{1,4}').setName(""hex_integer"")&#xa;    _full_ipv6_address = (_ipv6_part + (':' + _ipv6_part)*7).setName(""full IPv6 address"")&#xa;    _short_ipv6_address = (Optional(_ipv6_part + (':' + _ipv6_part)*(0,6)) + ""::"" + Optional(_ipv6_part + (':' + _ipv6_part)*(0,6))).setName(""short IPv6 address"")&#xa;    _short_ipv6_address.addCondition(lambda t: sum(1 for tt in t if pyparsing_common._ipv6_part.matches(tt)) < 8)&#xa;    _mixed_ipv6_address = (""::ffff:"" + ipv4_address).setName(""mixed IPv6 address"")&#xa;    ipv6_address = Combine((_full_ipv6_address | _mixed_ipv6_address | _short_ipv6_address).setName(""IPv6 address"")).setName(""IPv6 address"")&#xa;    ""IPv6 address (long, short, or mixed form)""&#xa;    &#xa;    mac_address = Regex(r'[0-9a-fA-F]{2}([:.-])[0-9a-fA-F]{2}(?:\1[0-9a-fA-F]{2}){4}').setName(""MAC address"")&#xa;    ""MAC address xx:xx:xx:xx:xx (may also have '-' or '.' delimiters)""&#xa;&#xa;    @staticmethod&#xa;    def convertToDate(fmt=""%Y-%m-%d""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed date string to Python datetime.date&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%d""})&#xa;&#xa;        Example::&#xa;            date_expr = pyparsing_common.iso8601_date.copy()&#xa;            date_expr.setParseAction(pyparsing_common.convertToDate())&#xa;            print(date_expr.parseString(""1999-12-31""))&#xa;        prints::&#xa;            [datetime.date(1999, 12, 31)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt).date()&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    @staticmethod&#xa;    def convertToDatetime(fmt=""%Y-%m-%dT%H:%M:%S.%f""):&#xa;        """"""&#xa;        Helper to create a parse action for converting parsed datetime string to Python datetime.datetime&#xa;&#xa;        Params -&#xa;         - fmt - format to be passed to datetime.strptime (default=C{""%Y-%m-%dT%H:%M:%S.%f""})&#xa;&#xa;        Example::&#xa;            dt_expr = pyparsing_common.iso8601_datetime.copy()&#xa;            dt_expr.setParseAction(pyparsing_common.convertToDatetime())&#xa;            print(dt_expr.parseString(""1999-12-31T23:59:59.999""))&#xa;        prints::&#xa;            [datetime.datetime(1999, 12, 31, 23, 59, 59, 999000)]&#xa;        """"""&#xa;        def cvt_fn(s,l,t):&#xa;            try:&#xa;                return datetime.strptime(t[0], fmt)&#xa;            except ValueError as ve:&#xa;                raise ParseException(s, l, str(ve))&#xa;        return cvt_fn&#xa;&#xa;    iso8601_date = Regex(r'(?P<year>\d{4})(?:-(?P<month>\d\d)(?:-(?P<day>\d\d))?)?').setName(""ISO8601 date"")&#xa;    ""ISO8601 date (C{yyyy-mm-dd})""&#xa;&#xa;    iso8601_datetime = Regex(r'(?P<year>\d{4})-(?P<month>\d\d)-(?P<day>\d\d)[T ](?P<hour>\d\d):(?P<minute>\d\d)(:(?P<second>\d\d(\.\d*)?)?)?(?P<tz>Z|[+-]\d\d:?\d\d)?').setName(""ISO8601 datetime"")&#xa;    ""ISO8601 datetime (C{yyyy-mm-ddThh:mm:ss.s(Z|+-00:00)}) - trailing seconds, milliseconds, and timezone optional; accepts separating C{'T'} or C{' '}""&#xa;&#xa;    uuid = Regex(r'[0-9a-fA-F]{8}(-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}').setName(""UUID"")&#xa;    ""UUID (C{xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx})""&#xa;&#xa;    _html_stripper = anyOpenTag.suppress() | anyCloseTag.suppress()&#xa;    @staticmethod&#xa;    def stripHTMLTags(s, l, tokens):&#xa;        """"""&#xa;        Parse action to remove HTML tags from web page HTML source&#xa;&#xa;        Example::&#xa;            # strip HTML links from normal text &#xa;            text = '<td>More info at the <a href=""http://pyparsing.wikispaces.com"">pyparsing</a> wiki page</td>'&#xa;            td,td_end = makeHTMLTags(""TD"")&#xa;            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)(""body"") + td_end&#xa;            &#xa;            print(table_text.parseString(text).body) # -> 'More info at the pyparsing wiki page'&#xa;        """"""&#xa;        return pyparsing_common._html_stripper.transformString(tokens[0])&#xa;&#xa;    _commasepitem = Combine(OneOrMore(~Literal("","") + ~LineEnd() + Word(printables, excludeChars=',') &#xa;                                        + Optional( White("" \t"") ) ) ).streamline().setName(""commaItem"")&#xa;    comma_separated_list = delimitedList( Optional( quotedString.copy() | _commasepitem, default="""") ).setName(""comma separated list"")&#xa;    """"""Predefined expression of 1 or more printable words or quoted strings, separated by commas.""""""&#xa;&#xa;    upcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).upper()))&#xa;    """"""Parse action to convert tokens to upper case.""""""&#xa;&#xa;    downcaseTokens = staticmethod(tokenMap(lambda t: _ustr(t).lower()))&#xa;    """"""Parse action to convert tokens to lower case.""""""&#xa;&#xa;&#xa;if __name__ == ""__main__"":&#xa;&#xa;    selectToken    = CaselessLiteral(""select"")&#xa;    fromToken      = CaselessLiteral(""from"")&#xa;&#xa;    ident          = Word(alphas, alphanums + ""_$"")&#xa;&#xa;    columnName     = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    columnNameList = Group(delimitedList(columnName)).setName(""columns"")&#xa;    columnSpec     = ('*' | columnNameList)&#xa;&#xa;    tableName      = delimitedList(ident, ""."", combine=True).setParseAction(upcaseTokens)&#xa;    tableNameList  = Group(delimitedList(tableName)).setName(""tables"")&#xa;    &#xa;    simpleSQL      = selectToken(""command"") + columnSpec(""columns"") + fromToken + tableNameList(""tables"")&#xa;&#xa;    # demo runTests method, including embedded comments in test string&#xa;    simpleSQL.runTests(""""""&#xa;        # '*' as column list and dotted table name&#xa;        select * from SYS.XYZZY&#xa;&#xa;        # caseless match on ""SELECT"", and casts back to ""select""&#xa;        SELECT * from XYZZY, ABC&#xa;&#xa;        # list of column names, and mixed case SELECT keyword&#xa;        Select AA,BB,CC from Sys.dual&#xa;&#xa;        # multiple tables&#xa;        Select A, B, C from Sys.dual, Table2&#xa;&#xa;        # invalid SELECT keyword - should fail&#xa;        Xelect A, B, C from Sys.dual&#xa;&#xa;        # incomplete command - should fail&#xa;        Select&#xa;&#xa;        # invalid column name - should fail&#xa;        Select ^^^ frox Sys.dual&#xa;&#xa;        """""")&#xa;&#xa;    pyparsing_common.number.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    # any int or real number, returned as float&#xa;    pyparsing_common.fnumber.runTests(""""""&#xa;        100&#xa;        -100&#xa;        +100&#xa;        3.14159&#xa;        6.02e23&#xa;        1e-12&#xa;        """""")&#xa;&#xa;    pyparsing_common.hex_integer.runTests(""""""&#xa;        100&#xa;        FF&#xa;        """""")&#xa;&#xa;    import uuid&#xa;    pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))&#xa;    pyparsing_common.uuid.runTests(""""""&#xa;        12345678-1234-5678-1234-567812345678&#xa;        """""")&#xa;"
31791476|"import matplotlib.pyplot as plt&#xa;import sys&#xa;import pandas as pd&#xa;&#xa;&#xa;def get_parm():&#xa;    """"""retrieves mandatory parameter to program&#xa;&#xa;    @param: none&#xa;    @type: n/a&#xa;&#xa;    """"""&#xa;    try:&#xa;        return sys.argv[1]&#xa;    except:&#xa;        print ('Must enter file name as parameter')&#xa;        exit()&#xa;&#xa;&#xa;def read_file(filename):&#xa;    """"""reads a file into a pandas dataframe&#xa;&#xa;    @param: filename The name of the file to read&#xa;    @type: string&#xa;&#xa;    """"""&#xa;    try:&#xa;        return pd.read_csv(filename)&#xa;    except:&#xa;        print ('Error retrieving file')&#xa;        exit()&#xa;&#xa;def select_data(benchmark_df, mongo_version, config_replicas, mongos_instances, shard_replicas, shards_per_replica):&#xa;    benchmark_df = benchmark_df[benchmark_df.cloud == ""chameleon""]&#xa;    benchmark_df = benchmark_df[benchmark_df.test_size == ""large""]&#xa;&#xa;    if mongo_version != 'X':&#xa;        benchmark_df = benchmark_df[benchmark_df.mongo_version == mongo_version]&#xa;&#xa;    if config_replicas != 'X':&#xa;        benchmark_df = benchmark_df[benchmark_df.config_replicas == config_replicas]&#xa;&#xa;    if mongos_instances != 'X':&#xa;        benchmark_df = benchmark_df[benchmark_df.mongos_instances == mongos_instances]&#xa;&#xa;    if shard_replicas != 'X':&#xa;        benchmark_df = benchmark_df[benchmark_df.shard_replicas == shard_replicas]&#xa;&#xa;    if shards_per_replica != 'X':&#xa;        benchmark_df = benchmark_df[benchmark_df.shards_per_replica == shards_per_replica]&#xa;&#xa;    #benchmark_df1 = benchmark_df.groupby(['cloud', 'config_replicas', 'mongos_instances', 'shard_replicas', 'shards_per_replica']).mean()&#xa;&#xa;    #http://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe&#xa;    benchmark_df = benchmark_df.groupby(['cloud', 'config_replicas', 'mongos_instances', 'shard_replicas', 'shards_per_replica'], as_index=False).mean()&#xa;    #http://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe&#xa;&#xa;    #print benchmark_df1['shard_replicas']&#xa;    #print benchmark_df1&#xa;    #print benchmark_df&#xa;&#xa;    benchmark_df = benchmark_df.sort_values(by='shard_replicas', ascending=1)&#xa;&#xa;    return benchmark_df&#xa;&#xa;def make_figure(mapreduce_seconds_32, shards_32, mapreduce_seconds_34, shards_34):&#xa;    """"""formats and creates a line chart&#xa;&#xa;    @param1: find_seconds_kilo Array with find_seconds from kilo&#xa;    @type: numpy array&#xa;    @param2: shards_kilo Array with shards from kilo&#xa;    @type: numpy array&#xa;    @param3: find_seconds_chameleon Array with find_seconds from chameleon&#xa;    @type: numpy array&#xa;    @param4: shards_chameleon Array with shards from chameleon&#xa;    @type: numpy array&#xa;    """"""&#xa;    fig = plt.figure()&#xa;    #plt.title('Average MongoImport Runtime with Various Numbers of Shards')&#xa;    plt.ylabel('Runtime in Seconds')&#xa;    plt.xlabel('Number of Shards')&#xa;&#xa;    # Make the chart&#xa;    plt.plot(shards_32, mapreduce_seconds_32, label='Version 3.2')&#xa;    plt.plot(shards_34, mapreduce_seconds_34, label='Version 3.4')&#xa;&#xa;&#xa;    #http://stackoverflow.com/questions/11744990/how-to-set-auto-for-upper-limit-but-keep-a-fixed-lower-limit-with-matplotlib&#xa;    plt.ylim(ymin=0)&#xa;&#xa;    plt.legend(loc='best')&#xa;&#xa;    # Show the chart (for testing)&#xa;    # plt.show()&#xa;    # Save the chart&#xa;    fig.savefig('../report/version_mapreduce.png')&#xa;&#xa;&#xa;# Run the program by calling the functions&#xa;if __name__ == ""__main__"":&#xa;&#xa;    filename = get_parm()&#xa;    benchmark_df = read_file(filename)&#xa;&#xa;    mongo_version = 32&#xa;    config_replicas = 1&#xa;    mongos_instances = 1&#xa;    shard_replicas = 'X'&#xa;    shards_per_replica = 1&#xa;    select_df = select_data(benchmark_df, mongo_version, config_replicas, mongos_instances, shard_replicas, shards_per_replica)&#xa;    #http://stackoverflow.com/questions/31791476/pandas-dataframe-to-numpy-array-valueerror&#xa;    #percentage death=\&#xa;    mapreduce_seconds_32=select_df.as_matrix(columns=[select_df.columns[8]])&#xa;    shards_32 = select_df.as_matrix(columns=[select_df.columns[3]])&#xa;    #http://stackoverflow.com/questions/31791476/pandas-dataframe-to-numpy-array-valueerror&#xa;&#xa;    mongo_version = 34&#xa;    config_replicas = 1&#xa;    mongos_instances = 1&#xa;    shard_replicas = 'X'&#xa;    shards_per_replica = 1&#xa;    select_df = select_data(benchmark_df, mongo_version, config_replicas, mongos_instances, shard_replicas, shards_per_replica)&#xa;    #http://stackoverflow.com/questions/31791476/pandas-dataframe-to-numpy-array-valueerror&#xa;    #percentage death=\&#xa;    mapreduce_seconds_34=select_df.as_matrix(columns=[select_df.columns[8]])&#xa;    shards_34 = select_df.as_matrix(columns=[select_df.columns[3]])&#xa;    #http://stackoverflow.com/questions/31791476/pandas-dataframe-to-numpy-array-valueerror&#xa;&#xa;    make_figure(mapreduce_seconds_32, shards_32, mapreduce_seconds_34, shards_34)&#xa;"
1189781|"# Copyright (c) 2013 Google Inc. All rights reserved.&#xa;# Use of this source code is governed by a BSD-style license that can be&#xa;# found in the LICENSE file.&#xa;&#xa;# Notes:&#xa;#&#xa;# This is all roughly based on the Makefile system used by the Linux&#xa;# kernel, but is a non-recursive make -- we put the entire dependency&#xa;# graph in front of make and let it figure it out.&#xa;#&#xa;# The code below generates a separate .mk file for each target, but&#xa;# all are sourced by the top-level Makefile.  This means that all&#xa;# variables in .mk-files clobber one another.  Be careful to use :=&#xa;# where appropriate for immediate evaluation, and similarly to watch&#xa;# that you're not relying on a variable value to last beween different&#xa;# .mk files.&#xa;#&#xa;# TODOs:&#xa;#&#xa;# Global settings and utility functions are currently stuffed in the&#xa;# toplevel Makefile.  It may make sense to generate some .mk files on&#xa;# the side to keep the the files readable.&#xa;&#xa;import os&#xa;import re&#xa;import sys&#xa;import subprocess&#xa;import gyp&#xa;import gyp.common&#xa;import gyp.xcode_emulation&#xa;from gyp.common import GetEnvironFallback&#xa;from gyp.common import GypError&#xa;&#xa;generator_default_variables = {&#xa;  'EXECUTABLE_PREFIX': '',&#xa;  'EXECUTABLE_SUFFIX': '',&#xa;  'STATIC_LIB_PREFIX': 'lib',&#xa;  'SHARED_LIB_PREFIX': 'lib',&#xa;  'STATIC_LIB_SUFFIX': '.a',&#xa;  'INTERMEDIATE_DIR': '$(obj).$(TOOLSET)/$(TARGET)/geni',&#xa;  'SHARED_INTERMEDIATE_DIR': '$(obj)/gen',&#xa;  'PRODUCT_DIR': '$(builddir)',&#xa;  'RULE_INPUT_ROOT': '%(INPUT_ROOT)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_DIRNAME': '%(INPUT_DIRNAME)s',  # This gets expanded by Python.&#xa;  'RULE_INPUT_PATH': '$(abspath $<)',&#xa;  'RULE_INPUT_EXT': '$(suffix $<)',&#xa;  'RULE_INPUT_NAME': '$(notdir $<)',&#xa;  'CONFIGURATION_NAME': '$(BUILDTYPE)',&#xa;}&#xa;&#xa;# Make supports multiple toolsets&#xa;generator_supports_multiple_toolsets = True&#xa;&#xa;# Request sorted dependencies in the order from dependents to dependencies.&#xa;generator_wants_sorted_dependencies = False&#xa;&#xa;# Placates pylint.&#xa;generator_additional_non_configuration_keys = []&#xa;generator_additional_path_sections = []&#xa;generator_extra_sources_for_rules = []&#xa;generator_filelist_paths = None&#xa;&#xa;&#xa;def CalculateVariables(default_variables, params):&#xa;  """"""Calculate additional variables for use in the build (called by gyp).""""""&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  if flavor == 'mac':&#xa;    default_variables.setdefault('OS', 'mac')&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.dylib')&#xa;    default_variables.setdefault('SHARED_LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;    default_variables.setdefault('LIB_DIR',&#xa;                                 generator_default_variables['PRODUCT_DIR'])&#xa;&#xa;    # Copy additional generator configuration data from Xcode, which is shared&#xa;    # by the Mac Make generator.&#xa;    import gyp.generator.xcode as xcode_generator&#xa;    global generator_additional_non_configuration_keys&#xa;    generator_additional_non_configuration_keys = getattr(xcode_generator,&#xa;        'generator_additional_non_configuration_keys', [])&#xa;    global generator_additional_path_sections&#xa;    generator_additional_path_sections = getattr(xcode_generator,&#xa;        'generator_additional_path_sections', [])&#xa;    global generator_extra_sources_for_rules&#xa;    generator_extra_sources_for_rules = getattr(xcode_generator,&#xa;        'generator_extra_sources_for_rules', [])&#xa;    COMPILABLE_EXTENSIONS.update({'.m': 'objc', '.mm' : 'objcxx'})&#xa;  else:&#xa;    operating_system = flavor&#xa;    if flavor == 'android':&#xa;      operating_system = 'linux'  # Keep this legacy behavior for now.&#xa;    default_variables.setdefault('OS', operating_system)&#xa;    default_variables.setdefault('SHARED_LIB_SUFFIX', '.so')&#xa;    default_variables.setdefault('SHARED_LIB_DIR','$(builddir)/lib.$(TOOLSET)')&#xa;    default_variables.setdefault('LIB_DIR', '$(obj).$(TOOLSET)')&#xa;&#xa;&#xa;def CalculateGeneratorInputInfo(params):&#xa;  """"""Calculate the generator specific info that gets fed to input (called by&#xa;  gyp).""""""&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  # Android NDK requires a strict link order.&#xa;  if android_ndk_version:&#xa;    global generator_wants_sorted_dependencies&#xa;    generator_wants_sorted_dependencies = True&#xa;&#xa;  output_dir = params['options'].generator_output or \&#xa;               params['options'].toplevel_dir&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  qualified_out_dir = os.path.normpath(os.path.join(&#xa;    output_dir, builddir_name, 'gypfiles'))&#xa;&#xa;  global generator_filelist_paths&#xa;  generator_filelist_paths = {&#xa;    'toplevel': params['options'].toplevel_dir,&#xa;    'qualified_out_dir': qualified_out_dir,&#xa;  }&#xa;&#xa;&#xa;# The .d checking code below uses these functions:&#xa;# wildcard, sort, foreach, shell, wordlist&#xa;# wildcard can handle spaces, the rest can't.&#xa;# Since I could find no way to make foreach work with spaces in filenames&#xa;# correctly, the .d files have spaces replaced with another character. The .d&#xa;# file for&#xa;#     Chromium\ Framework.framework/foo&#xa;# is for example&#xa;#     out/Release/.deps/out/Release/Chromium?Framework.framework/foo&#xa;# This is the replacement character.&#xa;SPACE_REPLACEMENT = '?'&#xa;&#xa;&#xa;LINK_COMMANDS_LINUX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;&#xa;# We support two kinds of shared objects (.so):&#xa;# 1) shared_library, which is just bundling together many dependent libraries&#xa;# into a link line.&#xa;# 2) loadable_module, which is generating a module intended for dlopen().&#xa;#&#xa;# They differ only slightly:&#xa;# In the former case, we want to package all dependent code into the .so.&#xa;# In the latter case, we want to package just the API exposed by the&#xa;# outermost module.&#xa;# This means shared_library uses --whole-archive, while loadable_module doesn't.&#xa;# (Note that --whole-archive is incompatible with the --start-group used in&#xa;# normal linking.)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_MAC = """"""\&#xa;quiet_cmd_alink = LIBTOOL-STATIC $@&#xa;cmd_alink = rm -f $@ && ./gyp-mac-tool filter-libtool libtool $(GYP_LIBTOOLFLAGS) -static -o $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o ""$@"" $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -bundle $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;LINK_COMMANDS_ANDROID = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) crsT $@ $(filter %.o,$^)&#xa;&#xa;# Due to circular dependencies between libraries :(, we wrap the&#xa;# special ""figure out circular dependencies"" flags around the entire&#xa;# input list during linking.&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;quiet_cmd_link_host = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ -Wl,--start-group $(LD_INPUTS) -Wl,--end-group $(LIBS)&#xa;cmd_link_host = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;# Other shared-object link notes:&#xa;# - Set SONAME to the library filename so our binaries don't reference&#xa;# the local, absolute paths used on the link command-line.&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--whole-archive $(LD_INPUTS) -Wl,--no-whole-archive $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ -Wl,--start-group $(filter-out FORCE_DO_CMD, $^) -Wl,--end-group $(LIBS)&#xa;quiet_cmd_solink_module_host = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module_host = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -Wl,-soname=$(@F) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;LINK_COMMANDS_AIX = """"""\&#xa;quiet_cmd_alink = AR($(TOOLSET)) $@&#xa;cmd_alink = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_alink_thin = AR($(TOOLSET)) $@&#xa;cmd_alink_thin = rm -f $@ && $(AR.$(TOOLSET)) -X32_64 crs $@ $(filter %.o,$^)&#xa;&#xa;quiet_cmd_link = LINK($(TOOLSET)) $@&#xa;cmd_link = $(LINK.$(TOOLSET)) $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink = SOLINK($(TOOLSET)) $@&#xa;cmd_solink = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(LD_INPUTS) $(LIBS)&#xa;&#xa;quiet_cmd_solink_module = SOLINK_MODULE($(TOOLSET)) $@&#xa;cmd_solink_module = $(LINK.$(TOOLSET)) -shared $(GYP_LDFLAGS) $(LDFLAGS.$(TOOLSET)) -o $@ $(filter-out FORCE_DO_CMD, $^) $(LIBS)&#xa;""""""&#xa;&#xa;&#xa;# Header of toplevel Makefile.&#xa;# This should go into the build tree, but it's easier to keep it here for now.&#xa;SHARED_HEADER = (""""""\&#xa;# We borrow heavily from the kernel build setup, though we are simpler since&#xa;# we don't have Kconfig tweaking settings on us.&#xa;&#xa;# The implicit make rules have it looking for RCS files, among other things.&#xa;# We instead explicitly write all the rules we care about.&#xa;# It's even quicker (saves ~200ms) to pass -r on the command line.&#xa;MAKEFLAGS=-r&#xa;&#xa;# The source directory tree.&#xa;srcdir := %(srcdir)s&#xa;abs_srcdir := $(abspath $(srcdir))&#xa;&#xa;# The name of the builddir.&#xa;builddir_name ?= %(builddir)s&#xa;&#xa;# The V=1 flag on command line makes us verbosely print command lines.&#xa;ifdef V&#xa;  quiet=&#xa;else&#xa;  quiet=quiet_&#xa;endif&#xa;&#xa;# Specify BUILDTYPE=Release on the command line for a release build.&#xa;BUILDTYPE ?= %(default_configuration)s&#xa;&#xa;# Directory all our build output goes into.&#xa;# Note that this must be two directories beneath src/ for unit tests to pass,&#xa;# as they reach into the src/ directory for data with relative paths.&#xa;builddir ?= $(builddir_name)/$(BUILDTYPE)&#xa;abs_builddir := $(abspath $(builddir))&#xa;depsdir := $(builddir)/.deps&#xa;&#xa;# Object output directory.&#xa;obj := $(builddir)/obj&#xa;abs_obj := $(abspath $(obj))&#xa;&#xa;# We build up a list of every single one of the targets so we can slurp in the&#xa;# generated dependency rule Makefiles in one pass.&#xa;all_deps :=&#xa;&#xa;%(make_global_settings)s&#xa;&#xa;CC.target ?= %(CC.target)s&#xa;CFLAGS.target ?= $(CPPFLAGS) $(CFLAGS)&#xa;CXX.target ?= %(CXX.target)s&#xa;CXXFLAGS.target ?= $(CPPFLAGS) $(CXXFLAGS)&#xa;LINK.target ?= %(LINK.target)s&#xa;LDFLAGS.target ?= $(LDFLAGS)&#xa;AR.target ?= $(AR)&#xa;&#xa;# C++ apps need to be linked with g++.&#xa;LINK ?= $(CXX.target)&#xa;&#xa;# TODO(evan): move all cross-compilation logic to gyp-time so we don't need&#xa;# to replicate this environment fallback in make as well.&#xa;CC.host ?= %(CC.host)s&#xa;CFLAGS.host ?= $(CPPFLAGS_host) $(CFLAGS_host)&#xa;CXX.host ?= %(CXX.host)s&#xa;CXXFLAGS.host ?= $(CPPFLAGS_host) $(CXXFLAGS_host)&#xa;LINK.host ?= %(LINK.host)s&#xa;LDFLAGS.host ?=&#xa;AR.host ?= %(AR.host)s&#xa;&#xa;# Define a dir function that can handle spaces.&#xa;# http://www.gnu.org/software/make/manual/make.html#Syntax-of-Functions&#xa;# ""leading spaces cannot appear in the text of the first argument as written.&#xa;# These characters can be put into the argument value by variable substitution.""&#xa;empty :=&#xa;space := $(empty) $(empty)&#xa;&#xa;# http://stackoverflow.com/questions/1189781/using-make-dir-or-notdir-on-a-path-with-spaces&#xa;replace_spaces = $(subst $(space),"""""" + SPACE_REPLACEMENT + """""",$1)&#xa;unreplace_spaces = $(subst """""" + SPACE_REPLACEMENT + """""",$(space),$1)&#xa;dirx = $(call unreplace_spaces,$(dir $(call replace_spaces,$1)))&#xa;&#xa;# Flags to make gcc output dependency info.  Note that you need to be&#xa;# careful here to use the flags that ccache and distcc can understand.&#xa;# We write to a dep file on the side first and then rename at the end&#xa;# so we can't end up with a broken dep file.&#xa;depfile = $(depsdir)/$(call replace_spaces,$@).d&#xa;DEPFLAGS = -MMD -MF $(depfile).raw&#xa;&#xa;# We have to fixup the deps output in a few ways.&#xa;# (1) the file output should mention the proper .o file.&#xa;# ccache or distcc lose the path to the target, so we convert a rule of&#xa;# the form:&#xa;#   foobar.o: DEP1 DEP2&#xa;# into&#xa;#   path/to/foobar.o: DEP1 DEP2&#xa;# (2) we want missing files not to cause us to fail to build.&#xa;# We want to rewrite&#xa;#   foobar.o: DEP1 DEP2 \\&#xa;#               DEP3&#xa;# to&#xa;#   DEP1:&#xa;#   DEP2:&#xa;#   DEP3:&#xa;# so if the files are missing, they're just considered phony rules.&#xa;# We have to do some pretty insane escaping to get those backslashes&#xa;# and dollar signs past make, the shell, and sed at the same time.&#xa;# Doesn't work with spaces, but that's fine: .d files have spaces in&#xa;# their names replaced with other characters.""""""&#xa;r""""""&#xa;define fixup_dep&#xa;# The depfile may not exist if the input file didn't have any #includes.&#xa;touch $(depfile).raw&#xa;# Fixup path as in (1).&#xa;sed -e ""s|^$(notdir $@)|$@|"" $(depfile).raw >> $(depfile)&#xa;# Add extra rules as in (2).&#xa;# We remove slashes and replace spaces with new lines;&#xa;# remove blank lines;&#xa;# delete the first line and append a colon to the remaining lines.&#xa;sed -e 's|\\||' -e 'y| |\n|' $(depfile).raw |\&#xa;  grep -v '^$$'                             |\&#xa;  sed -e 1d -e 's|$$|:|'                     \&#xa;    >> $(depfile)&#xa;rm $(depfile).raw&#xa;endef&#xa;""""""&#xa;""""""&#xa;# Command definitions:&#xa;# - cmd_foo is the actual command to run;&#xa;# - quiet_cmd_foo is the brief-output summary of the command.&#xa;&#xa;quiet_cmd_cc = CC($(TOOLSET)) $@&#xa;cmd_cc = $(CC.$(TOOLSET)) $(GYP_CFLAGS) $(DEPFLAGS) $(CFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;&#xa;quiet_cmd_cxx = CXX($(TOOLSET)) $@&#xa;cmd_cxx = $(CXX.$(TOOLSET)) $(GYP_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;%(extra_commands)s&#xa;quiet_cmd_touch = TOUCH $@&#xa;cmd_touch = touch $@&#xa;&#xa;quiet_cmd_copy = COPY $@&#xa;# send stderr to /dev/null to ignore messages when linking directories.&#xa;cmd_copy = rm -rf ""$@"" && cp %(copy_archive_args)s ""$<"" ""$@""&#xa;&#xa;%(link_commands)s&#xa;""""""&#xa;&#xa;r""""""&#xa;# Define an escape_quotes function to escape single quotes.&#xa;# This allows us to handle quotes properly as long as we always use&#xa;# use single quotes and escape_quotes.&#xa;escape_quotes = $(subst ','\'',$(1))&#xa;# This comment is here just to include a ' to unconfuse syntax highlighting.&#xa;# Define an escape_vars function to escape '$' variable syntax.&#xa;# This allows us to read/write command lines with shell variables (e.g.&#xa;# $LD_LIBRARY_PATH), without triggering make substitution.&#xa;escape_vars = $(subst $$,$$$$,$(1))&#xa;# Helper that expands to a shell command to echo a string exactly as it is in&#xa;# make. This uses printf instead of echo because printf's behaviour with respect&#xa;# to escape sequences is more portable than echo's across different shells&#xa;# (e.g., dash, bash).&#xa;exact_echo = printf '%%s\n' '$(call escape_quotes,$(1))'&#xa;""""""&#xa;""""""&#xa;# Helper to compare the command we're about to run against the command&#xa;# we logged the last time we ran the command.  Produces an empty&#xa;# string (false) when the commands match.&#xa;# Tricky point: Make has no string-equality test function.&#xa;# The kernel uses the following, but it seems like it would have false&#xa;# positives, where one string reordered its arguments.&#xa;#   arg_check = $(strip $(filter-out $(cmd_$(1)), $(cmd_$@)) \\&#xa;#                       $(filter-out $(cmd_$@), $(cmd_$(1))))&#xa;# We instead substitute each for the empty string into the other, and&#xa;# say they're equal if both substitutions produce the empty string.&#xa;# .d files contain """""" + SPACE_REPLACEMENT + \&#xa;                   """""" instead of spaces, take that into account.&#xa;command_changed = $(or $(subst $(cmd_$(1)),,$(cmd_$(call replace_spaces,$@))),\\&#xa;                       $(subst $(cmd_$(call replace_spaces,$@)),,$(cmd_$(1))))&#xa;&#xa;# Helper that is non-empty when a prerequisite changes.&#xa;# Normally make does this implicitly, but we force rules to always run&#xa;# so we can check their command lines.&#xa;#   $? -- new prerequisites&#xa;#   $| -- order-only dependencies&#xa;prereq_changed = $(filter-out FORCE_DO_CMD,$(filter-out $|,$?))&#xa;&#xa;# Helper that executes all postbuilds until one fails.&#xa;define do_postbuilds&#xa;  @E=0;\\&#xa;  for p in $(POSTBUILDS); do\\&#xa;    eval $$p;\\&#xa;    E=$$?;\\&#xa;    if [ $$E -ne 0 ]; then\\&#xa;      break;\\&#xa;    fi;\\&#xa;  done;\\&#xa;  if [ $$E -ne 0 ]; then\\&#xa;    rm -rf ""$@"";\\&#xa;    exit $$E;\\&#xa;  fi&#xa;endef&#xa;&#xa;# do_cmd: run a command via the above cmd_foo names, if necessary.&#xa;# Should always run for a given target to handle command-line changes.&#xa;# Second argument, if non-zero, makes it do asm/C/C++ dependency munging.&#xa;# Third argument, if non-zero, makes it do POSTBUILDS processing.&#xa;# Note: We intentionally do NOT call dirx for depfile, since it contains """""" + \&#xa;                                                     SPACE_REPLACEMENT + """""" for&#xa;# spaces already and dirx strips the """""" + SPACE_REPLACEMENT + \&#xa;                                     """""" characters.&#xa;define do_cmd&#xa;$(if $(or $(command_changed),$(prereq_changed)),&#xa;  @$(call exact_echo,  $($(quiet)cmd_$(1)))&#xa;  @mkdir -p ""$(call dirx,$@)"" ""$(dir $(depfile))""&#xa;  $(if $(findstring flock,$(word %(flock_index)d,$(cmd_$1))),&#xa;    @$(cmd_$(1))&#xa;    @echo ""  $(quiet_cmd_$(1)): Finished"",&#xa;    @$(cmd_$(1))&#xa;  )&#xa;  @$(call exact_echo,$(call escape_vars,cmd_$(call replace_spaces,$@) := $(cmd_$(1)))) > $(depfile)&#xa;  @$(if $(2),$(fixup_dep))&#xa;  $(if $(and $(3), $(POSTBUILDS)),&#xa;    $(call do_postbuilds)&#xa;  )&#xa;)&#xa;endef&#xa;&#xa;# Declare the ""%(default_target)s"" target first so it is the default,&#xa;# even though we don't have the deps yet.&#xa;.PHONY: %(default_target)s&#xa;%(default_target)s:&#xa;&#xa;# make looks for ways to re-generate included makefiles, but in our case, we&#xa;# don't have a direct way. Explicitly telling make that it has nothing to do&#xa;# for them makes it go faster.&#xa;%%.d: ;&#xa;&#xa;# Use FORCE_DO_CMD to force a target to run.  Should be coupled with&#xa;# do_cmd.&#xa;.PHONY: FORCE_DO_CMD&#xa;FORCE_DO_CMD:&#xa;&#xa;"""""")&#xa;&#xa;SHARED_HEADER_MAC_COMMANDS = """"""&#xa;quiet_cmd_objc = CXX($(TOOLSET)) $@&#xa;cmd_objc = $(CC.$(TOOLSET)) $(GYP_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;quiet_cmd_objcxx = CXX($(TOOLSET)) $@&#xa;cmd_objcxx = $(CXX.$(TOOLSET)) $(GYP_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# Commands for precompiled header files.&#xa;quiet_cmd_pch_c = CXX($(TOOLSET)) $@&#xa;cmd_pch_c = $(CC.$(TOOLSET)) $(GYP_PCH_CFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_cc = CXX($(TOOLSET)) $@&#xa;cmd_pch_cc = $(CC.$(TOOLSET)) $(GYP_PCH_CXXFLAGS) $(DEPFLAGS) $(CXXFLAGS.$(TOOLSET)) -c -o $@ $<&#xa;quiet_cmd_pch_m = CXX($(TOOLSET)) $@&#xa;cmd_pch_m = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;quiet_cmd_pch_mm = CXX($(TOOLSET)) $@&#xa;cmd_pch_mm = $(CC.$(TOOLSET)) $(GYP_PCH_OBJCXXFLAGS) $(DEPFLAGS) -c -o $@ $<&#xa;&#xa;# gyp-mac-tool is written next to the root Makefile by gyp.&#xa;# Use $(4) for the command, since $(2) and $(3) are used as flag by do_cmd&#xa;# already.&#xa;quiet_cmd_mac_tool = MACTOOL $(4) $<&#xa;cmd_mac_tool = ./gyp-mac-tool $(4) $< ""$@""&#xa;&#xa;quiet_cmd_mac_package_framework = PACKAGE FRAMEWORK $@&#xa;cmd_mac_package_framework = ./gyp-mac-tool package-framework ""$@"" $(4)&#xa;&#xa;quiet_cmd_infoplist = INFOPLIST $@&#xa;cmd_infoplist = $(CC.$(TOOLSET)) -E -P -Wno-trigraphs -x c $(INFOPLIST_DEFINES) ""$<"" -o ""$@""&#xa;""""""&#xa;&#xa;&#xa;def WriteRootHeaderSuffixRules(writer):&#xa;  extensions = sorted(COMPILABLE_EXTENSIONS.keys(), key=str.lower)&#xa;&#xa;  writer.write('# Suffix rules, putting all outputs into $(obj).\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;&#xa;  writer.write('\n# Try building from generated source, too.\n')&#xa;  for ext in extensions:&#xa;    writer.write(&#xa;        '$(obj).$(TOOLSET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;  for ext in extensions:&#xa;    writer.write('$(obj).$(TOOLSET)/%%.o: $(obj)/%%%s FORCE_DO_CMD\n' % ext)&#xa;    writer.write('\t@$(call do_cmd,%s,1)\n' % COMPILABLE_EXTENSIONS[ext])&#xa;  writer.write('\n')&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT1 = (""""""\&#xa;# Suffix rules, putting all outputs into $(obj).&#xa;"""""")&#xa;&#xa;&#xa;SHARED_HEADER_SUFFIX_RULES_COMMENT2 = (""""""\&#xa;# Try building from generated source, too.&#xa;"""""")&#xa;&#xa;&#xa;SHARED_FOOTER = """"""\&#xa;# ""all"" is a concatenation of the ""all"" targets from all the included&#xa;# sub-makefiles. This is just here to clarify.&#xa;all:&#xa;&#xa;# Add in dependency-tracking rules.  $(all_deps) is the list of every single&#xa;# target in our tree. Only consider the ones with .d (dependency) info:&#xa;d_files := $(wildcard $(foreach f,$(all_deps),$(depsdir)/$(f).d))&#xa;ifneq ($(d_files),)&#xa;  include $(d_files)&#xa;endif&#xa;""""""&#xa;&#xa;header = """"""\&#xa;# This file is generated by gyp; do not edit.&#xa;&#xa;""""""&#xa;&#xa;# Maps every compilable file extension to the do_cmd that compiles it.&#xa;COMPILABLE_EXTENSIONS = {&#xa;  '.c': 'cc',&#xa;  '.cc': 'cxx',&#xa;  '.cpp': 'cxx',&#xa;  '.cxx': 'cxx',&#xa;  '.s': 'cc',&#xa;  '.S': 'cc',&#xa;}&#xa;&#xa;def Compilable(filename):&#xa;  """"""Return true if the file is compilable (should be in OBJS).""""""&#xa;  for res in (filename.endswith(e) for e in COMPILABLE_EXTENSIONS):&#xa;    if res:&#xa;      return True&#xa;  return False&#xa;&#xa;&#xa;def Linkable(filename):&#xa;  """"""Return true if the file is linkable (should be on the link line).""""""&#xa;  return filename.endswith('.o')&#xa;&#xa;&#xa;def Target(filename):&#xa;  """"""Translate a compilable filename to its .o target.""""""&#xa;  return os.path.splitext(filename)[0] + '.o'&#xa;&#xa;&#xa;def EscapeShellArgument(s):&#xa;  """"""Quotes an argument so that it will be interpreted literally by a POSIX&#xa;     shell. Taken from&#xa;     http://stackoverflow.com/questions/35817/whats-the-best-way-to-escape-ossystem-calls-in-python&#xa;     """"""&#xa;  return ""'"" + s.replace(""'"", ""'\\''"") + ""'""&#xa;&#xa;&#xa;def EscapeMakeVariableExpansion(s):&#xa;  """"""Make has its own variable expansion syntax using $. We must escape it for&#xa;     string to be interpreted literally.""""""&#xa;  return s.replace('$', '$$')&#xa;&#xa;&#xa;def EscapeCppDefine(s):&#xa;  """"""Escapes a CPP define so that it will reach the compiler unaltered.""""""&#xa;  s = EscapeShellArgument(s)&#xa;  s = EscapeMakeVariableExpansion(s)&#xa;  # '#' characters must be escaped even embedded in a string, else Make will&#xa;  # treat it as the start of a comment.&#xa;  return s.replace('#', r'\#')&#xa;&#xa;&#xa;def QuoteIfNecessary(string):&#xa;  """"""TODO: Should this ideally be replaced with one or more of the above&#xa;     functions?""""""&#xa;  if '""' in string:&#xa;    string = '""' + string.replace('""', '\\""') + '""'&#xa;  return string&#xa;&#xa;&#xa;def StringToMakefileVariable(string):&#xa;  """"""Convert a string to a value that is acceptable as a make variable name.""""""&#xa;  return re.sub('[^a-zA-Z0-9_]', '_', string)&#xa;&#xa;&#xa;srcdir_prefix = ''&#xa;def Sourceify(path):&#xa;  """"""Convert a path to its source directory form.""""""&#xa;  if '$(' in path:&#xa;    return path&#xa;  if os.path.isabs(path):&#xa;    return path&#xa;  return srcdir_prefix + path&#xa;&#xa;&#xa;def QuoteSpaces(s, quote=r'\ '):&#xa;  return s.replace(' ', quote)&#xa;&#xa;&#xa;# TODO: Avoid code duplication with _ValidateSourcesForMSVSProject in msvs.py.&#xa;def _ValidateSourcesForOSX(spec, all_sources):&#xa;  """"""Makes sure if duplicate basenames are not specified in the source list.&#xa;&#xa;  Arguments:&#xa;    spec: The target dictionary containing the properties of the target.&#xa;  """"""&#xa;  if spec.get('type', None) != 'static_library':&#xa;    return&#xa;&#xa;  basenames = {}&#xa;  for source in all_sources:&#xa;    name, ext = os.path.splitext(source)&#xa;    is_compiled_file = ext in [&#xa;        '.c', '.cc', '.cpp', '.cxx', '.m', '.mm', '.s', '.S']&#xa;    if not is_compiled_file:&#xa;      continue&#xa;    basename = os.path.basename(name)  # Don't include extension.&#xa;    basenames.setdefault(basename, []).append(source)&#xa;&#xa;  error = ''&#xa;  for basename, files in basenames.iteritems():&#xa;    if len(files) > 1:&#xa;      error += '  %s: %s\n' % (basename, ' '.join(files))&#xa;&#xa;  if error:&#xa;    print('static library %s has several files with the same basename:\n' %&#xa;          spec['target_name'] + error + 'libtool on OS X will generate' +&#xa;          ' warnings for them.')&#xa;    raise GypError('Duplicate basenames in sources section, see list above')&#xa;&#xa;&#xa;# Map from qualified target to path to output.&#xa;target_outputs = {}&#xa;# Map from qualified target to any linkable output.  A subset&#xa;# of target_outputs.  E.g. when mybinary depends on liba, we want to&#xa;# include liba in the linker line; when otherbinary depends on&#xa;# mybinary, we just want to build mybinary first.&#xa;target_link_deps = {}&#xa;&#xa;&#xa;class MakefileWriter(object):&#xa;  """"""MakefileWriter packages up the writing of one target-specific foobar.mk.&#xa;&#xa;  Its only real entry point is Write(), and is mostly used for namespacing.&#xa;  """"""&#xa;&#xa;  def __init__(self, generator_flags, flavor):&#xa;    self.generator_flags = generator_flags&#xa;    self.flavor = flavor&#xa;&#xa;    self.suffix_rules_srcdir = {}&#xa;    self.suffix_rules_objdir1 = {}&#xa;    self.suffix_rules_objdir2 = {}&#xa;&#xa;    # Generate suffix rules for all compilable extensions.&#xa;    for ext in COMPILABLE_EXTENSIONS.keys():&#xa;      # Suffix rules for source folder.&#xa;      self.suffix_rules_srcdir.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(srcdir)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;      # Suffix rules for generated source files.&#xa;      self.suffix_rules_objdir1.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj).$(TOOLSET)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;      self.suffix_rules_objdir2.update({ext: (""""""\&#xa;$(obj).$(TOOLSET)/$(TARGET)/%%.o: $(obj)/%%%s FORCE_DO_CMD&#xa;	@$(call do_cmd,%s,1)&#xa;"""""" % (ext, COMPILABLE_EXTENSIONS[ext]))})&#xa;&#xa;&#xa;  def Write(self, qualified_target, base_path, output_filename, spec, configs,&#xa;            part_of_all):&#xa;    """"""The main entry point: writes a .mk file for a single target.&#xa;&#xa;    Arguments:&#xa;      qualified_target: target we're generating&#xa;      base_path: path relative to source root we're building in, used to resolve&#xa;                 target-relative paths&#xa;      output_filename: output .mk file name to write&#xa;      spec, configs: gyp info&#xa;      part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;&#xa;    self.fp = open(output_filename, 'w')&#xa;&#xa;    self.fp.write(header)&#xa;&#xa;    self.qualified_target = qualified_target&#xa;    self.path = base_path&#xa;    self.target = spec['target_name']&#xa;    self.type = spec['type']&#xa;    self.toolset = spec['toolset']&#xa;&#xa;    self.is_mac_bundle = gyp.xcode_emulation.IsMacBundle(self.flavor, spec)&#xa;    if self.flavor == 'mac':&#xa;      self.xcode_settings = gyp.xcode_emulation.XcodeSettings(spec)&#xa;    else:&#xa;      self.xcode_settings = None&#xa;&#xa;    deps, link_deps = self.ComputeDeps(spec)&#xa;&#xa;    # Some of the generation below can add extra output, sources, or&#xa;    # link dependencies.  All of the out params of the functions that&#xa;    # follow use names like extra_foo.&#xa;    extra_outputs = []&#xa;    extra_sources = []&#xa;    extra_link_deps = []&#xa;    extra_mac_bundle_resources = []&#xa;    mac_bundle_deps = []&#xa;&#xa;    if self.is_mac_bundle:&#xa;      self.output = self.ComputeMacBundleOutput(spec)&#xa;      self.output_binary = self.ComputeMacBundleBinaryOutput(spec)&#xa;    else:&#xa;      self.output = self.output_binary = self.ComputeOutput(spec)&#xa;&#xa;    self.is_standalone_static_library = bool(&#xa;        spec.get('standalone_static_library', 0))&#xa;    self._INSTALLABLE_TARGETS = ('executable', 'loadable_module',&#xa;                                 'shared_library')&#xa;    if (self.is_standalone_static_library or&#xa;        self.type in self._INSTALLABLE_TARGETS):&#xa;      self.alias = os.path.basename(self.output)&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;    else:&#xa;      self.alias = self.output&#xa;      install_path = self.output&#xa;&#xa;    self.WriteLn(""TOOLSET := "" + self.toolset)&#xa;    self.WriteLn(""TARGET := "" + self.target)&#xa;&#xa;    # Actions must come first, since they can generate more OBJs for use below.&#xa;    if 'actions' in spec:&#xa;      self.WriteActions(spec['actions'], extra_sources, extra_outputs,&#xa;                        extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    # Rules must be early like actions.&#xa;    if 'rules' in spec:&#xa;      self.WriteRules(spec['rules'], extra_sources, extra_outputs,&#xa;                      extra_mac_bundle_resources, part_of_all)&#xa;&#xa;    if 'copies' in spec:&#xa;      self.WriteCopies(spec['copies'], extra_outputs, part_of_all)&#xa;&#xa;    # Bundle resources.&#xa;    if self.is_mac_bundle:&#xa;      all_mac_bundle_resources = (&#xa;          spec.get('mac_bundle_resources', []) + extra_mac_bundle_resources)&#xa;      self.WriteMacBundleResources(all_mac_bundle_resources, mac_bundle_deps)&#xa;      self.WriteMacInfoPlist(mac_bundle_deps)&#xa;&#xa;    # Sources.&#xa;    all_sources = spec.get('sources', []) + extra_sources&#xa;    if all_sources:&#xa;      if self.flavor == 'mac':&#xa;        # libtool on OS X generates warnings for duplicate basenames in the same&#xa;        # target.&#xa;        _ValidateSourcesForOSX(spec, all_sources)&#xa;      self.WriteSources(&#xa;          configs, deps, all_sources, extra_outputs,&#xa;          extra_link_deps, part_of_all,&#xa;          gyp.xcode_emulation.MacPrefixHeader(&#xa;              self.xcode_settings, lambda p: Sourceify(self.Absolutify(p)),&#xa;              self.Pchify))&#xa;      sources = filter(Compilable, all_sources)&#xa;      if sources:&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT1)&#xa;        extensions = set([os.path.splitext(s)[1] for s in sources])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_srcdir:&#xa;            self.WriteLn(self.suffix_rules_srcdir[ext])&#xa;        self.WriteLn(SHARED_HEADER_SUFFIX_RULES_COMMENT2)&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir1:&#xa;            self.WriteLn(self.suffix_rules_objdir1[ext])&#xa;        for ext in extensions:&#xa;          if ext in self.suffix_rules_objdir2:&#xa;            self.WriteLn(self.suffix_rules_objdir2[ext])&#xa;        self.WriteLn('# End of this set of suffix rules')&#xa;&#xa;        # Add dependency from bundle to bundle binary.&#xa;        if self.is_mac_bundle:&#xa;          mac_bundle_deps.append(self.output_binary)&#xa;&#xa;    self.WriteTarget(spec, configs, deps, extra_link_deps + link_deps,&#xa;                     mac_bundle_deps, extra_outputs, part_of_all)&#xa;&#xa;    # Update global list of target outputs, used in dependency tracking.&#xa;    target_outputs[qualified_target] = install_path&#xa;&#xa;    # Update global list of link dependencies.&#xa;    if self.type in ('static_library', 'shared_library'):&#xa;      target_link_deps[qualified_target] = self.output_binary&#xa;&#xa;    # Currently any versions have the same effect, but in future the behavior&#xa;    # could be different.&#xa;    if self.generator_flags.get('android_ndk_version', None):&#xa;      self.WriteAndroidNdkModuleRule(self.target, all_sources, link_deps)&#xa;&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteSubMake(self, output_filename, makefile_path, targets, build_dir):&#xa;    """"""Write a ""sub-project"" Makefile.&#xa;&#xa;    This is a small, wrapper Makefile that calls the top-level Makefile to build&#xa;    the targets from a single gyp file (i.e. a sub-project).&#xa;&#xa;    Arguments:&#xa;      output_filename: sub-project Makefile name to write&#xa;      makefile_path: path to the top-level Makefile&#xa;      targets: list of ""all"" targets for this sub-project&#xa;      build_dir: build output directory, relative to the sub-project&#xa;    """"""&#xa;    gyp.common.EnsureDirExists(output_filename)&#xa;    self.fp = open(output_filename, 'w')&#xa;    self.fp.write(header)&#xa;    # For consistency with other builders, put sub-project build output in the&#xa;    # sub-project dir (see test/subdirectory/gyptest-subdir-all.py).&#xa;    self.WriteLn('export builddir_name ?= %s' %&#xa;                 os.path.join(os.path.dirname(output_filename), build_dir))&#xa;    self.WriteLn('.PHONY: all')&#xa;    self.WriteLn('all:')&#xa;    if makefile_path:&#xa;      makefile_path = ' -C ' + makefile_path&#xa;    self.WriteLn('\t$(MAKE)%s %s' % (makefile_path, ' '.join(targets)))&#xa;    self.fp.close()&#xa;&#xa;&#xa;  def WriteActions(self, actions, extra_sources, extra_outputs,&#xa;                   extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'actions' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   actions (used to make other pieces dependent on these&#xa;                   actions)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for action in actions:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 action['action_name']))&#xa;      self.WriteLn('### Rules for action ""%s"":' % action['action_name'])&#xa;      inputs = action['inputs']&#xa;      outputs = action['outputs']&#xa;&#xa;      # Build up a list of outputs.&#xa;      # Collect the output dirs we'll need.&#xa;      dirs = set()&#xa;      for out in outputs:&#xa;        dir = os.path.split(out)[0]&#xa;        if dir:&#xa;          dirs.add(dir)&#xa;      if int(action.get('process_outputs_as_sources', False)):&#xa;        extra_sources += outputs&#xa;      if int(action.get('process_outputs_as_mac_bundle_resources', False)):&#xa;        extra_mac_bundle_resources += outputs&#xa;&#xa;      # Write the actual command.&#xa;      action_commands = action['action']&#xa;      if self.flavor == 'mac':&#xa;        action_commands = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                          for command in action_commands]&#xa;      command = gyp.common.EncodePOSIXShellList(action_commands)&#xa;      if 'message' in action:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, action['message']))&#xa;      else:&#xa;        self.WriteLn('quiet_cmd_%s = ACTION %s $@' % (name, name))&#xa;      if len(dirs) > 0:&#xa;        command = 'mkdir -p %s' % ' '.join(dirs) + '; ' + command&#xa;&#xa;      cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;      # command and cd_action get written to a toplevel variable called&#xa;      # cmd_foo. Toplevel variables can't handle things that change per&#xa;      # makefile like $(TARGET), so hardcode the target.&#xa;      command = command.replace('$(TARGET)', self.target)&#xa;      cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;&#xa;      # Set LD_LIBRARY_PATH in case the action runs an executable from this&#xa;      # build which links to shared libs from this build.&#xa;      # actions run on the host, so they should in theory only use host&#xa;      # libraries, but until everything is made cross-compile safe, also use&#xa;      # target libraries.&#xa;      # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;      self.WriteLn('cmd_%s = LD_LIBRARY_PATH=$(builddir)/lib.host:'&#xa;                   '$(builddir)/lib.target:$$LD_LIBRARY_PATH; '&#xa;                   'export LD_LIBRARY_PATH; '&#xa;                   '%s%s'&#xa;                   % (name, cd_action, command))&#xa;      self.WriteLn()&#xa;      outputs = map(self.Absolutify, outputs)&#xa;      # The makefile rules are all relative to the top dir, but the gyp actions&#xa;      # are defined relative to their containing dir.  This replaces the obj&#xa;      # variable for the action rule with an absolute version so that the output&#xa;      # goes in the right place.&#xa;      # Only write the 'obj' and 'builddir' rules for the ""primary"" output (:1);&#xa;      # it's superfluous for the ""extra outputs"", and this avoids accidentally&#xa;      # writing duplicate dummy rules for those outputs.&#xa;      # Same for environment.&#xa;      self.WriteLn(""%s: obj := $(abs_obj)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteLn(""%s: builddir := $(abs_builddir)"" % QuoteSpaces(outputs[0]))&#xa;      self.WriteSortedXcodeEnv(outputs[0], self.GetSortedXcodeEnv())&#xa;&#xa;      for input in inputs:&#xa;        assert ' ' not in input, (&#xa;            ""Spaces in action input filenames not supported (%s)""  % input)&#xa;      for output in outputs:&#xa;        assert ' ' not in output, (&#xa;            ""Spaces in action output filenames not supported (%s)""  % output)&#xa;&#xa;      # See the comment in WriteCopies about expanding env vars.&#xa;      outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;      inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;      self.WriteDoCmd(outputs, map(Sourceify, map(self.Absolutify, inputs)),&#xa;                      part_of_all=part_of_all, command=name)&#xa;&#xa;      # Stuff the outputs in a variable so we can refer to them later.&#xa;      outputs_variable = 'action_%s_outputs' % name&#xa;      self.WriteLn('%s := %s' % (outputs_variable, ' '.join(outputs)))&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;      self.WriteLn()&#xa;&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteRules(self, rules, extra_sources, extra_outputs,&#xa;                 extra_mac_bundle_resources, part_of_all):&#xa;    """"""Write Makefile code for any 'rules' from the gyp input.&#xa;&#xa;    extra_sources: a list that will be filled in with newly generated source&#xa;                   files, if any&#xa;    extra_outputs: a list that will be filled in with any outputs of these&#xa;                   rules (used to make other pieces dependent on these rules)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    env = self.GetSortedXcodeEnv()&#xa;    for rule in rules:&#xa;      name = StringToMakefileVariable('%s_%s' % (self.qualified_target,&#xa;                                                 rule['rule_name']))&#xa;      count = 0&#xa;      self.WriteLn('### Generated for rule %s:' % name)&#xa;&#xa;      all_outputs = []&#xa;&#xa;      for rule_source in rule.get('rule_sources', []):&#xa;        dirs = set()&#xa;        (rule_source_dirname, rule_source_basename) = os.path.split(rule_source)&#xa;        (rule_source_root, rule_source_ext) = \&#xa;            os.path.splitext(rule_source_basename)&#xa;&#xa;        outputs = [self.ExpandInputRoot(out, rule_source_root,&#xa;                                        rule_source_dirname)&#xa;                   for out in rule['outputs']]&#xa;&#xa;        for out in outputs:&#xa;          dir = os.path.dirname(out)&#xa;          if dir:&#xa;            dirs.add(dir)&#xa;        if int(rule.get('process_outputs_as_sources', False)):&#xa;          extra_sources += outputs&#xa;        if int(rule.get('process_outputs_as_mac_bundle_resources', False)):&#xa;          extra_mac_bundle_resources += outputs&#xa;        inputs = map(Sourceify, map(self.Absolutify, [rule_source] +&#xa;                                    rule.get('inputs', [])))&#xa;        actions = ['$(call do_cmd,%s_%d)' % (name, count)]&#xa;&#xa;        if name == 'resources_grit':&#xa;          # HACK: This is ugly.  Grit intentionally doesn't touch the&#xa;          # timestamp of its output file when the file doesn't change,&#xa;          # which is fine in hash-based dependency systems like scons&#xa;          # and forge, but not kosher in the make world.  After some&#xa;          # discussion, hacking around it here seems like the least&#xa;          # amount of pain.&#xa;          actions += ['@touch --no-create $@']&#xa;&#xa;        # See the comment in WriteCopies about expanding env vars.&#xa;        outputs = [gyp.xcode_emulation.ExpandEnvVars(o, env) for o in outputs]&#xa;        inputs = [gyp.xcode_emulation.ExpandEnvVars(i, env) for i in inputs]&#xa;&#xa;        outputs = map(self.Absolutify, outputs)&#xa;        all_outputs += outputs&#xa;        # Only write the 'obj' and 'builddir' rules for the ""primary"" output&#xa;        # (:1); it's superfluous for the ""extra outputs"", and this avoids&#xa;        # accidentally writing duplicate dummy rules for those outputs.&#xa;        self.WriteLn('%s: obj := $(abs_obj)' % outputs[0])&#xa;        self.WriteLn('%s: builddir := $(abs_builddir)' % outputs[0])&#xa;        self.WriteMakeRule(outputs, inputs, actions,&#xa;                           command=""%s_%d"" % (name, count))&#xa;        # Spaces in rule filenames are not supported, but rule variables have&#xa;        # spaces in them (e.g. RULE_INPUT_PATH expands to '$(abspath $<)').&#xa;        # The spaces within the variables are valid, so remove the variables&#xa;        # before checking.&#xa;        variables_with_spaces = re.compile(r'\$\([^ ]* \$<\)')&#xa;        for output in outputs:&#xa;          output = re.sub(variables_with_spaces, '', output)&#xa;          assert ' ' not in output, (&#xa;              ""Spaces in rule filenames not yet supported (%s)""  % output)&#xa;        self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;        action = [self.ExpandInputRoot(ac, rule_source_root,&#xa;                                       rule_source_dirname)&#xa;                  for ac in rule['action']]&#xa;        mkdirs = ''&#xa;        if len(dirs) > 0:&#xa;          mkdirs = 'mkdir -p %s; ' % ' '.join(dirs)&#xa;        cd_action = 'cd %s; ' % Sourceify(self.path or '.')&#xa;&#xa;        # action, cd_action, and mkdirs get written to a toplevel variable&#xa;        # called cmd_foo. Toplevel variables can't handle things that change&#xa;        # per makefile like $(TARGET), so hardcode the target.&#xa;        if self.flavor == 'mac':&#xa;          action = [gyp.xcode_emulation.ExpandEnvVars(command, env)&#xa;                    for command in action]&#xa;        action = gyp.common.EncodePOSIXShellList(action)&#xa;        action = action.replace('$(TARGET)', self.target)&#xa;        cd_action = cd_action.replace('$(TARGET)', self.target)&#xa;        mkdirs = mkdirs.replace('$(TARGET)', self.target)&#xa;&#xa;        # Set LD_LIBRARY_PATH in case the rule runs an executable from this&#xa;        # build which links to shared libs from this build.&#xa;        # rules run on the host, so they should in theory only use host&#xa;        # libraries, but until everything is made cross-compile safe, also use&#xa;        # target libraries.&#xa;        # TODO(piman): when everything is cross-compile safe, remove lib.target&#xa;        self.WriteLn(&#xa;            ""cmd_%(name)s_%(count)d = LD_LIBRARY_PATH=""&#xa;              ""$(builddir)/lib.host:$(builddir)/lib.target:$$LD_LIBRARY_PATH; ""&#xa;              ""export LD_LIBRARY_PATH; ""&#xa;              ""%(cd_action)s%(mkdirs)s%(action)s"" % {&#xa;          'action': action,&#xa;          'cd_action': cd_action,&#xa;          'count': count,&#xa;          'mkdirs': mkdirs,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn(&#xa;            'quiet_cmd_%(name)s_%(count)d = RULE %(name)s_%(count)d $@' % {&#xa;          'count': count,&#xa;          'name': name,&#xa;        })&#xa;        self.WriteLn()&#xa;        count += 1&#xa;&#xa;      outputs_variable = 'rule_%s_outputs' % name&#xa;      self.WriteList(all_outputs, outputs_variable)&#xa;      extra_outputs.append('$(%s)' % outputs_variable)&#xa;&#xa;      self.WriteLn('### Finished generating for rule: %s' % name)&#xa;      self.WriteLn()&#xa;    self.WriteLn('### Finished generating for all rules')&#xa;    self.WriteLn('')&#xa;&#xa;&#xa;  def WriteCopies(self, copies, extra_outputs, part_of_all):&#xa;    """"""Write Makefile code for any 'copies' from the gyp input.&#xa;&#xa;    extra_outputs: a list that will be filled in with any outputs of this action&#xa;                   (used to make other pieces dependent on this action)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;    self.WriteLn('### Generated for copy rule.')&#xa;&#xa;    variable = StringToMakefileVariable(self.qualified_target + '_copies')&#xa;    outputs = []&#xa;    for copy in copies:&#xa;      for path in copy['files']:&#xa;        # Absolutify() may call normpath, and will strip trailing slashes.&#xa;        path = Sourceify(self.Absolutify(path))&#xa;        filename = os.path.split(path)[1]&#xa;        output = Sourceify(self.Absolutify(os.path.join(copy['destination'],&#xa;                                                        filename)))&#xa;&#xa;        # If the output path has variables in it, which happens in practice for&#xa;        # 'copies', writing the environment as target-local doesn't work,&#xa;        # because the variables are already needed for the target name.&#xa;        # Copying the environment variables into global make variables doesn't&#xa;        # work either, because then the .d files will potentially contain spaces&#xa;        # after variable expansion, and .d file handling cannot handle spaces.&#xa;        # As a workaround, manually expand variables at gyp time. Since 'copies'&#xa;        # can't run scripts, there's no need to write the env then.&#xa;        # WriteDoCmd() will escape spaces for .d files.&#xa;        env = self.GetSortedXcodeEnv()&#xa;        output = gyp.xcode_emulation.ExpandEnvVars(output, env)&#xa;        path = gyp.xcode_emulation.ExpandEnvVars(path, env)&#xa;        self.WriteDoCmd([output], [path], 'copy', part_of_all)&#xa;        outputs.append(output)&#xa;    self.WriteLn('%s = %s' % (variable, ' '.join(map(QuoteSpaces, outputs))))&#xa;    extra_outputs.append('$(%s)' % variable)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteMacBundleResources(self, resources, bundle_deps):&#xa;    """"""Writes Makefile code for 'mac_bundle_resources'.""""""&#xa;    self.WriteLn('### Generated for mac_bundle_resources')&#xa;&#xa;    for output, res in gyp.xcode_emulation.GetMacBundleResources(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        map(Sourceify, map(self.Absolutify, resources))):&#xa;      _, ext = os.path.splitext(output)&#xa;      if ext != '.xcassets':&#xa;        # Make does not supports '.xcassets' emulation.&#xa;        self.WriteDoCmd([output], [res], 'mac_tool,,,copy-bundle-resource',&#xa;                        part_of_all=True)&#xa;        bundle_deps.append(output)&#xa;&#xa;&#xa;  def WriteMacInfoPlist(self, bundle_deps):&#xa;    """"""Write Makefile code for bundle Info.plist files.""""""&#xa;    info_plist, out, defines, extra_env = gyp.xcode_emulation.GetMacInfoPlist(&#xa;        generator_default_variables['PRODUCT_DIR'], self.xcode_settings,&#xa;        lambda p: Sourceify(self.Absolutify(p)))&#xa;    if not info_plist:&#xa;      return&#xa;    if defines:&#xa;      # Create an intermediate file to store preprocessed results.&#xa;      intermediate_plist = ('$(obj).$(TOOLSET)/$(TARGET)/' +&#xa;          os.path.basename(info_plist))&#xa;      self.WriteList(defines, intermediate_plist + ': INFOPLIST_DEFINES', '-D',&#xa;          quoter=EscapeCppDefine)&#xa;      self.WriteMakeRule([intermediate_plist], [info_plist],&#xa;          ['$(call do_cmd,infoplist)',&#xa;           # ""Convert"" the plist so that any weird whitespace changes from the&#xa;           # preprocessor do not affect the XML parser in mac_tool.&#xa;           '@plutil -convert xml1 $@ $@'])&#xa;      info_plist = intermediate_plist&#xa;    # plists can contain envvars and substitute them into the file.&#xa;    self.WriteSortedXcodeEnv(&#xa;        out, self.GetSortedXcodeEnv(additional_settings=extra_env))&#xa;    self.WriteDoCmd([out], [info_plist], 'mac_tool,,,copy-info-plist',&#xa;                    part_of_all=True)&#xa;    bundle_deps.append(out)&#xa;&#xa;&#xa;  def WriteSources(self, configs, deps, sources,&#xa;                   extra_outputs, extra_link_deps,&#xa;                   part_of_all, precompiled_header):&#xa;    """"""Write Makefile code for any 'sources' from the gyp input.&#xa;    These are source files necessary to build the current target.&#xa;&#xa;    configs, deps, sources: input from gyp.&#xa;    extra_outputs: a list of extra outputs this action should be dependent on;&#xa;                   used to serialize action/rules before compilation&#xa;    extra_link_deps: a list that will be filled in with any outputs of&#xa;                     compilation (to be used in link lines)&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    # Write configuration-specific variables for CFLAGS, etc.&#xa;    for configname in sorted(configs.keys()):&#xa;      config = configs[configname]&#xa;      self.WriteList(config.get('defines'), 'DEFS_%s' % configname, prefix='-D',&#xa;          quoter=EscapeCppDefine)&#xa;&#xa;      if self.flavor == 'mac':&#xa;        cflags = self.xcode_settings.GetCflags(configname)&#xa;        cflags_c = self.xcode_settings.GetCflagsC(configname)&#xa;        cflags_cc = self.xcode_settings.GetCflagsCC(configname)&#xa;        cflags_objc = self.xcode_settings.GetCflagsObjC(configname)&#xa;        cflags_objcc = self.xcode_settings.GetCflagsObjCC(configname)&#xa;      else:&#xa;        cflags = config.get('cflags')&#xa;        cflags_c = config.get('cflags_c')&#xa;        cflags_cc = config.get('cflags_cc')&#xa;&#xa;      self.WriteLn(""# Flags passed to all source files."");&#xa;      self.WriteList(cflags, 'CFLAGS_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C files."");&#xa;      self.WriteList(cflags_c, 'CFLAGS_C_%s' % configname)&#xa;      self.WriteLn(""# Flags passed to only C++ files."");&#xa;      self.WriteList(cflags_cc, 'CFLAGS_CC_%s' % configname)&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""# Flags passed to only ObjC files."");&#xa;        self.WriteList(cflags_objc, 'CFLAGS_OBJC_%s' % configname)&#xa;        self.WriteLn(""# Flags passed to only ObjC++ files."");&#xa;        self.WriteList(cflags_objcc, 'CFLAGS_OBJCC_%s' % configname)&#xa;      includes = config.get('include_dirs')&#xa;      if includes:&#xa;        includes = map(Sourceify, map(self.Absolutify, includes))&#xa;      self.WriteList(includes, 'INCS_%s' % configname, prefix='-I')&#xa;&#xa;    compilable = filter(Compilable, sources)&#xa;    objs = map(self.Objectify, map(self.Absolutify, map(Target, compilable)))&#xa;    self.WriteList(objs, 'OBJS')&#xa;&#xa;    for obj in objs:&#xa;      assert ' ' not in obj, (&#xa;          ""Spaces in object filenames not supported (%s)""  % obj)&#xa;    self.WriteLn('# Add to the list of files we specially track '&#xa;                 'dependencies for.')&#xa;    self.WriteLn('all_deps += $(OBJS)')&#xa;    self.WriteLn()&#xa;&#xa;    # Make sure our dependencies are built first.&#xa;    if deps:&#xa;      self.WriteMakeRule(['$(OBJS)'], deps,&#xa;                         comment = 'Make sure our dependencies are built '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    # Make sure the actions and rules run first.&#xa;    # If they generate any extra headers etc., the per-.o file dep tracking&#xa;    # will catch the proper rebuilds, so order only is still ok here.&#xa;    if extra_outputs:&#xa;      self.WriteMakeRule(['$(OBJS)'], extra_outputs,&#xa;                         comment = 'Make sure our actions/rules run '&#xa;                                   'before any of us.',&#xa;                         order_only = True)&#xa;&#xa;    pchdeps = precompiled_header.GetObjDependencies(compilable, objs )&#xa;    if pchdeps:&#xa;      self.WriteLn('# Dependencies from obj files to their precompiled headers')&#xa;      for source, obj, gch in pchdeps:&#xa;        self.WriteLn('%s: %s' % (obj, gch))&#xa;      self.WriteLn('# End precompiled header dependencies')&#xa;&#xa;    if objs:&#xa;      extra_link_deps.append('$(OBJS)')&#xa;      self.WriteLn(""""""\&#xa;# CFLAGS et al overrides must be target-local.&#xa;# See ""Target-specific Variable Values"" in the GNU Make manual."""""")&#xa;      self.WriteLn(""$(OBJS): TOOLSET := $(TOOLSET)"")&#xa;      self.WriteLn(""$(OBJS): GYP_CFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('c') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_C_$(BUILDTYPE))"")&#xa;      self.WriteLn(""$(OBJS): GYP_CXXFLAGS := ""&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""%s "" % precompiled_header.GetInclude('cc') +&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_CC_$(BUILDTYPE))"")&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('m') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_C_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJC_$(BUILDTYPE))"")&#xa;        self.WriteLn(""$(OBJS): GYP_OBJCXXFLAGS := ""&#xa;                     ""$(DEFS_$(BUILDTYPE)) ""&#xa;                     ""$(INCS_$(BUILDTYPE)) ""&#xa;                     ""%s "" % precompiled_header.GetInclude('mm') +&#xa;                     ""$(CFLAGS_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_CC_$(BUILDTYPE)) ""&#xa;                     ""$(CFLAGS_OBJCC_$(BUILDTYPE))"")&#xa;&#xa;    self.WritePchTargets(precompiled_header.GetPchBuildCommands())&#xa;&#xa;    # If there are any object files in our input file list, link them into our&#xa;    # output.&#xa;    extra_link_deps += filter(Linkable, sources)&#xa;&#xa;    self.WriteLn()&#xa;&#xa;  def WritePchTargets(self, pch_commands):&#xa;    """"""Writes make rules to compile prefix headers.""""""&#xa;    if not pch_commands:&#xa;      return&#xa;&#xa;    for gch, lang_flag, lang, input in pch_commands:&#xa;      extra_flags = {&#xa;        'c': '$(CFLAGS_C_$(BUILDTYPE))',&#xa;        'cc': '$(CFLAGS_CC_$(BUILDTYPE))',&#xa;        'm': '$(CFLAGS_C_$(BUILDTYPE)) $(CFLAGS_OBJC_$(BUILDTYPE))',&#xa;        'mm': '$(CFLAGS_CC_$(BUILDTYPE)) $(CFLAGS_OBJCC_$(BUILDTYPE))',&#xa;      }[lang]&#xa;      var_name = {&#xa;        'c': 'GYP_PCH_CFLAGS',&#xa;        'cc': 'GYP_PCH_CXXFLAGS',&#xa;        'm': 'GYP_PCH_OBJCFLAGS',&#xa;        'mm': 'GYP_PCH_OBJCXXFLAGS',&#xa;      }[lang]&#xa;      self.WriteLn(""%s: %s := %s "" % (gch, var_name, lang_flag) +&#xa;                   ""$(DEFS_$(BUILDTYPE)) ""&#xa;                   ""$(INCS_$(BUILDTYPE)) ""&#xa;                   ""$(CFLAGS_$(BUILDTYPE)) "" +&#xa;                   extra_flags)&#xa;&#xa;      self.WriteLn('%s: %s FORCE_DO_CMD' % (gch, input))&#xa;      self.WriteLn('\t@$(call do_cmd,pch_%s,1)' % lang)&#xa;      self.WriteLn('')&#xa;      assert ' ' not in gch, (&#xa;          ""Spaces in gch filenames not supported (%s)""  % gch)&#xa;      self.WriteLn('all_deps += %s' % gch)&#xa;      self.WriteLn('')&#xa;&#xa;&#xa;  def ComputeOutputBasename(self, spec):&#xa;    """"""Return the 'output basename' of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      'libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    if self.flavor == 'mac' and self.type in (&#xa;        'static_library', 'executable', 'shared_library', 'loadable_module'):&#xa;      return self.xcode_settings.GetExecutablePath()&#xa;&#xa;    target = spec['target_name']&#xa;    target_prefix = ''&#xa;    target_ext = ''&#xa;    if self.type == 'static_library':&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.a'&#xa;    elif self.type in ('loadable_module', 'shared_library'):&#xa;      if target[:3] == 'lib':&#xa;        target = target[3:]&#xa;      target_prefix = 'lib'&#xa;      target_ext = '.so'&#xa;    elif self.type == 'none':&#xa;      target = '%s.stamp' % target&#xa;    elif self.type != 'executable':&#xa;      print (""ERROR: What output file should be generated?"",&#xa;             ""type"", self.type, ""target"", target)&#xa;&#xa;    target_prefix = spec.get('product_prefix', target_prefix)&#xa;    target = spec.get('product_name', target)&#xa;    product_ext = spec.get('product_extension')&#xa;    if product_ext:&#xa;      target_ext = '.' + product_ext&#xa;&#xa;    return target_prefix + target + target_ext&#xa;&#xa;&#xa;  def _InstallImmediately(self):&#xa;    return self.toolset == 'target' and self.flavor == 'mac' and self.type in (&#xa;          'static_library', 'executable', 'shared_library', 'loadable_module')&#xa;&#xa;&#xa;  def ComputeOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) of a gyp spec.&#xa;&#xa;    E.g., the loadable module 'foobar' in directory 'baz' will produce&#xa;      '$(obj)/baz/libfoobar.so'&#xa;    """"""&#xa;    assert not self.is_mac_bundle&#xa;&#xa;    path = os.path.join('$(obj).' + self.toolset, self.path)&#xa;    if self.type == 'executable' or self._InstallImmediately():&#xa;      path = '$(builddir)'&#xa;    path = spec.get('product_dir', path)&#xa;    return os.path.join(path, self.ComputeOutputBasename(spec))&#xa;&#xa;&#xa;  def ComputeMacBundleOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to a bundle output directory.""""""&#xa;    assert self.is_mac_bundle&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetWrapperName())&#xa;&#xa;&#xa;  def ComputeMacBundleBinaryOutput(self, spec):&#xa;    """"""Return the 'output' (full output path) to the binary in a bundle.""""""&#xa;    path = generator_default_variables['PRODUCT_DIR']&#xa;    return os.path.join(path, self.xcode_settings.GetExecutablePath())&#xa;&#xa;&#xa;  def ComputeDeps(self, spec):&#xa;    """"""Compute the dependencies of a gyp spec.&#xa;&#xa;    Returns a tuple (deps, link_deps), where each is a list of&#xa;    filenames that will need to be put in front of make for either&#xa;    building (deps) or linking (link_deps).&#xa;    """"""&#xa;    deps = []&#xa;    link_deps = []&#xa;    if 'dependencies' in spec:&#xa;      deps.extend([target_outputs[dep] for dep in spec['dependencies']&#xa;                   if target_outputs[dep]])&#xa;      for dep in spec['dependencies']:&#xa;        if dep in target_link_deps:&#xa;          link_deps.append(target_link_deps[dep])&#xa;      deps.extend(link_deps)&#xa;      # TODO: It seems we need to transitively link in libraries (e.g. -lfoo)?&#xa;      # This hack makes it work:&#xa;      # link_deps.extend(spec.get('libraries', []))&#xa;    return (gyp.common.uniquer(deps), gyp.common.uniquer(link_deps))&#xa;&#xa;&#xa;  def WriteDependencyOnExtraOutputs(self, target, extra_outputs):&#xa;    self.WriteMakeRule([self.output_binary], extra_outputs,&#xa;                       comment = 'Build our special outputs first.',&#xa;                       order_only = True)&#xa;&#xa;&#xa;  def WriteTarget(self, spec, configs, deps, link_deps, bundle_deps,&#xa;                  extra_outputs, part_of_all):&#xa;    """"""Write Makefile code to produce the final target of the gyp spec.&#xa;&#xa;    spec, configs: input from gyp.&#xa;    deps, link_deps: dependency lists; see ComputeDeps()&#xa;    extra_outputs: any extra outputs that our target should depend on&#xa;    part_of_all: flag indicating this target is part of 'all'&#xa;    """"""&#xa;&#xa;    self.WriteLn('### Rules for final target.')&#xa;&#xa;    if extra_outputs:&#xa;      self.WriteDependencyOnExtraOutputs(self.output_binary, extra_outputs)&#xa;      self.WriteMakeRule(extra_outputs, deps,&#xa;                         comment=('Preserve order dependency of '&#xa;                                  'special output on deps.'),&#xa;                         order_only = True)&#xa;&#xa;    target_postbuilds = {}&#xa;    if self.type != 'none':&#xa;      for configname in sorted(configs.keys()):&#xa;        config = configs[configname]&#xa;        if self.flavor == 'mac':&#xa;          ldflags = self.xcode_settings.GetLdflags(configname,&#xa;              generator_default_variables['PRODUCT_DIR'],&#xa;              lambda p: Sourceify(self.Absolutify(p)))&#xa;&#xa;          # TARGET_POSTBUILDS_$(BUILDTYPE) is added to postbuilds later on.&#xa;          gyp_to_build = gyp.common.InvertRelativePath(self.path)&#xa;          target_postbuild = self.xcode_settings.AddImplicitPostbuilds(&#xa;              configname,&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output))),&#xa;              QuoteSpaces(os.path.normpath(os.path.join(gyp_to_build,&#xa;                                                        self.output_binary))))&#xa;          if target_postbuild:&#xa;            target_postbuilds[configname] = target_postbuild&#xa;        else:&#xa;          ldflags = config.get('ldflags', [])&#xa;          # Compute an rpath for this output if needed.&#xa;          if any(dep.endswith('.so') or '.so.' in dep for dep in deps):&#xa;            # We want to get the literal string ""$ORIGIN"" into the link command,&#xa;            # so we need lots of escaping.&#xa;            ldflags.append(r'-Wl,-rpath=\$$ORIGIN/lib.%s/' % self.toolset)&#xa;            ldflags.append(r'-Wl,-rpath-link=\$(builddir)/lib.%s/' %&#xa;                           self.toolset)&#xa;        library_dirs = config.get('library_dirs', [])&#xa;        ldflags += [('-L%s' % library_dir) for library_dir in library_dirs]&#xa;        self.WriteList(ldflags, 'LDFLAGS_%s' % configname)&#xa;        if self.flavor == 'mac':&#xa;          self.WriteList(self.xcode_settings.GetLibtoolflags(configname),&#xa;                         'LIBTOOLFLAGS_%s' % configname)&#xa;      libraries = spec.get('libraries')&#xa;      if libraries:&#xa;        # Remove duplicate entries&#xa;        libraries = gyp.common.uniquer(libraries)&#xa;        if self.flavor == 'mac':&#xa;          libraries = self.xcode_settings.AdjustLibraries(libraries)&#xa;      self.WriteList(libraries, 'LIBS')&#xa;      self.WriteLn('%s: GYP_LDFLAGS := $(LDFLAGS_$(BUILDTYPE))' %&#xa;          QuoteSpaces(self.output_binary))&#xa;      self.WriteLn('%s: LIBS := $(LIBS)' % QuoteSpaces(self.output_binary))&#xa;&#xa;      if self.flavor == 'mac':&#xa;        self.WriteLn('%s: GYP_LIBTOOLFLAGS := $(LIBTOOLFLAGS_$(BUILDTYPE))' %&#xa;            QuoteSpaces(self.output_binary))&#xa;&#xa;    # Postbuild actions. Like actions, but implicitly depend on the target's&#xa;    # output.&#xa;    postbuilds = []&#xa;    if self.flavor == 'mac':&#xa;      if target_postbuilds:&#xa;        postbuilds.append('$(TARGET_POSTBUILDS_$(BUILDTYPE))')&#xa;      postbuilds.extend(&#xa;          gyp.xcode_emulation.GetSpecPostbuildCommands(spec))&#xa;&#xa;    if postbuilds:&#xa;      # Envvars may be referenced by TARGET_POSTBUILDS_$(BUILDTYPE),&#xa;      # so we must output its definition first, since we declare variables&#xa;      # using "":="".&#xa;      self.WriteSortedXcodeEnv(self.output, self.GetSortedXcodePostbuildEnv())&#xa;&#xa;      for configname in target_postbuilds:&#xa;        self.WriteLn('%s: TARGET_POSTBUILDS_%s := %s' %&#xa;            (QuoteSpaces(self.output),&#xa;             configname,&#xa;             gyp.common.EncodePOSIXShellList(target_postbuilds[configname])))&#xa;&#xa;      # Postbuilds expect to be run in the gyp file's directory, so insert an&#xa;      # implicit postbuild to cd to there.&#xa;      postbuilds.insert(0, gyp.common.EncodePOSIXShellList(['cd', self.path]))&#xa;      for i in xrange(len(postbuilds)):&#xa;        if not postbuilds[i].startswith('$'):&#xa;          postbuilds[i] = EscapeShellArgument(postbuilds[i])&#xa;      self.WriteLn('%s: builddir := $(abs_builddir)' % QuoteSpaces(self.output))&#xa;      self.WriteLn('%s: POSTBUILDS := %s' % (&#xa;          QuoteSpaces(self.output), ' '.join(postbuilds)))&#xa;&#xa;    # A bundle directory depends on its dependencies such as bundle resources&#xa;    # and bundle binary. When all dependencies have been built, the bundle&#xa;    # needs to be packaged.&#xa;    if self.is_mac_bundle:&#xa;      # If the framework doesn't contain a binary, then nothing depends&#xa;      # on the actions -- make the framework depend on them directly too.&#xa;      self.WriteDependencyOnExtraOutputs(self.output, extra_outputs)&#xa;&#xa;      # Bundle dependencies. Note that the code below adds actions to this&#xa;      # target, so if you move these two lines, move the lines below as well.&#xa;      self.WriteList(map(QuoteSpaces, bundle_deps), 'BUNDLE_DEPS')&#xa;      self.WriteLn('%s: $(BUNDLE_DEPS)' % QuoteSpaces(self.output))&#xa;&#xa;      # After the framework is built, package it. Needs to happen before&#xa;      # postbuilds, since postbuilds depend on this.&#xa;      if self.type in ('shared_library', 'loadable_module'):&#xa;        self.WriteLn('\t@$(call do_cmd,mac_package_framework,,,%s)' %&#xa;            self.xcode_settings.GetFrameworkVersion())&#xa;&#xa;      # Bundle postbuilds can depend on the whole bundle, so run them after&#xa;      # the bundle is packaged, not already after the bundle binary is done.&#xa;      if postbuilds:&#xa;        self.WriteLn('\t@$(call do_postbuilds)')&#xa;      postbuilds = []  # Don't write postbuilds for target's output.&#xa;&#xa;      # Needed by test/mac/gyptest-rebuild.py.&#xa;      self.WriteLn('\t@true  # No-op, used by tests')&#xa;&#xa;      # Since this target depends on binary and resources which are in&#xa;      # nested subfolders, the framework directory will be older than&#xa;      # its dependencies usually. To prevent this rule from executing&#xa;      # on every build (expensive, especially with postbuilds), expliclity&#xa;      # update the time on the framework directory.&#xa;      self.WriteLn('\t@touch -c %s' % QuoteSpaces(self.output))&#xa;&#xa;    if postbuilds:&#xa;      assert not self.is_mac_bundle, ('Postbuilds for bundles should be done '&#xa;          'on the bundle, not the binary (target \'%s\')' % self.target)&#xa;      assert 'product_dir' not in spec, ('Postbuilds do not work with '&#xa;          'custom product_dir')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;          QuoteSpaces(self.output_binary),&#xa;          ' '.join(map(QuoteSpaces, link_deps))))&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'link', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;&#xa;    elif self.type == 'static_library':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in alink input filenames not supported (%s)""  % link_dep)&#xa;      if (self.flavor not in ('mac', 'openbsd', 'netbsd', 'win') and not&#xa;          self.is_standalone_static_library):&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink_thin',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'alink', part_of_all,&#xa;                        postbuilds=postbuilds)&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('%s: LD_INPUTS := %s' % (&#xa;            QuoteSpaces(self.output_binary),&#xa;            ' '.join(map(QuoteSpaces, link_deps))))&#xa;      self.WriteDoCmd([self.output_binary], link_deps, 'solink', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    elif self.type == 'loadable_module':&#xa;      for link_dep in link_deps:&#xa;        assert ' ' not in link_dep, (&#xa;            ""Spaces in module input filenames not supported (%s)""  % link_dep)&#xa;      if self.toolset == 'host' and self.flavor == 'android':&#xa;        self.WriteDoCmd([self.output_binary], link_deps, 'solink_module_host',&#xa;                        part_of_all, postbuilds=postbuilds)&#xa;      else:&#xa;        self.WriteDoCmd(&#xa;            [self.output_binary], link_deps, 'solink_module', part_of_all,&#xa;            postbuilds=postbuilds)&#xa;    elif self.type == 'none':&#xa;      # Write a stamp line.&#xa;      self.WriteDoCmd([self.output_binary], deps, 'touch', part_of_all,&#xa;                      postbuilds=postbuilds)&#xa;    else:&#xa;      print ""WARNING: no output for"", self.type, target&#xa;&#xa;    # Add an alias for each target (if there are any outputs).&#xa;    # Installable target aliases are created below.&#xa;    if ((self.output and self.output != self.target) and&#xa;        (self.type not in self._INSTALLABLE_TARGETS)):&#xa;      self.WriteMakeRule([self.target], [self.output],&#xa;                         comment='Add target alias', phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [self.target],&#xa;                           comment = 'Add target alias to ""all"" target.',&#xa;                           phony = True)&#xa;&#xa;    # Add special-case rules for our installable targets.&#xa;    # 1) They need to install to the build dir or ""product"" dir.&#xa;    # 2) They get shortcuts for building (e.g. ""make chrome"").&#xa;    # 3) They are part of ""make all"".&#xa;    if (self.type in self._INSTALLABLE_TARGETS or&#xa;        self.is_standalone_static_library):&#xa;      if self.type == 'shared_library':&#xa;        file_desc = 'shared library'&#xa;      elif self.type == 'static_library':&#xa;        file_desc = 'static library'&#xa;      else:&#xa;        file_desc = 'executable'&#xa;      install_path = self._InstallableTargetInstallPath()&#xa;      installable_deps = [self.output]&#xa;      if (self.flavor == 'mac' and not 'product_dir' in spec and&#xa;          self.toolset == 'target'):&#xa;        # On mac, products are created in install_path immediately.&#xa;        assert install_path == self.output, '%s != %s' % (&#xa;            install_path, self.output)&#xa;&#xa;      # Point the target alias to the final binary output.&#xa;      self.WriteMakeRule([self.target], [install_path],&#xa;                         comment='Add target alias', phony = True)&#xa;      if install_path != self.output:&#xa;        assert not self.is_mac_bundle  # See comment a few lines above.&#xa;        self.WriteDoCmd([install_path], [self.output], 'copy',&#xa;                        comment = 'Copy this to the %s output path.' %&#xa;                        file_desc, part_of_all=part_of_all)&#xa;        installable_deps.append(install_path)&#xa;      if self.output != self.alias and self.alias != self.target:&#xa;        self.WriteMakeRule([self.alias], installable_deps,&#xa;                           comment = 'Short alias for building this %s.' %&#xa;                           file_desc, phony = True)&#xa;      if part_of_all:&#xa;        self.WriteMakeRule(['all'], [install_path],&#xa;                           comment = 'Add %s to ""all"" target.' % file_desc,&#xa;                           phony = True)&#xa;&#xa;&#xa;  def WriteList(self, value_list, variable=None, prefix='',&#xa;                quoter=QuoteIfNecessary):&#xa;    """"""Write a variable definition that is a list of values.&#xa;&#xa;    E.g. WriteList(['a','b'], 'foo', prefix='blah') writes out&#xa;         foo = blaha blahb&#xa;    but in a pretty-printed style.&#xa;    """"""&#xa;    values = ''&#xa;    if value_list:&#xa;      value_list = [quoter(prefix + l) for l in value_list]&#xa;      values = ' \\\n\t' + ' \\\n\t'.join(value_list)&#xa;    self.fp.write('%s :=%s\n\n' % (variable, values))&#xa;&#xa;&#xa;  def WriteDoCmd(self, outputs, inputs, command, part_of_all, comment=None,&#xa;                 postbuilds=False):&#xa;    """"""Write a Makefile rule that uses do_cmd.&#xa;&#xa;    This makes the outputs dependent on the command line that was run,&#xa;    as well as support the V= make command line flag.&#xa;    """"""&#xa;    suffix = ''&#xa;    if postbuilds:&#xa;      assert ',' not in command&#xa;      suffix = ',,1'  # Tell do_cmd to honor $POSTBUILDS&#xa;    self.WriteMakeRule(outputs, inputs,&#xa;                       actions = ['$(call do_cmd,%s%s)' % (command, suffix)],&#xa;                       comment = comment,&#xa;                       command = command,&#xa;                       force = True)&#xa;    # Add our outputs to the list of targets we read depfiles from.&#xa;    # all_deps is only used for deps file reading, and for deps files we replace&#xa;    # spaces with ? because escaping doesn't work with make's $(sort) and&#xa;    # other functions.&#xa;    outputs = [QuoteSpaces(o, SPACE_REPLACEMENT) for o in outputs]&#xa;    self.WriteLn('all_deps += %s' % ' '.join(outputs))&#xa;&#xa;&#xa;  def WriteMakeRule(self, outputs, inputs, actions=None, comment=None,&#xa;                    order_only=False, force=False, phony=False, command=None):&#xa;    """"""Write a Makefile rule, with some extra tricks.&#xa;&#xa;    outputs: a list of outputs for the rule (note: this is not directly&#xa;             supported by make; see comments below)&#xa;    inputs: a list of inputs for the rule&#xa;    actions: a list of shell commands to run for the rule&#xa;    comment: a comment to put in the Makefile above the rule (also useful&#xa;             for making this Python script's code self-documenting)&#xa;    order_only: if true, makes the dependency order-only&#xa;    force: if true, include FORCE_DO_CMD as an order-only dep&#xa;    phony: if true, the rule does not actually generate the named output, the&#xa;           output is just a name to run the rule&#xa;    command: (optional) command name to generate unambiguous labels&#xa;    """"""&#xa;    outputs = map(QuoteSpaces, outputs)&#xa;    inputs = map(QuoteSpaces, inputs)&#xa;&#xa;    if comment:&#xa;      self.WriteLn('# ' + comment)&#xa;    if phony:&#xa;      self.WriteLn('.PHONY: ' + ' '.join(outputs))&#xa;    if actions:&#xa;      self.WriteLn(""%s: TOOLSET := $(TOOLSET)"" % outputs[0])&#xa;    force_append = ' FORCE_DO_CMD' if force else ''&#xa;&#xa;    if order_only:&#xa;      # Order only rule: Just write a simple rule.&#xa;      # TODO(evanm): just make order_only a list of deps instead of this hack.&#xa;      self.WriteLn('%s: | %s%s' %&#xa;                   (' '.join(outputs), ' '.join(inputs), force_append))&#xa;    elif len(outputs) == 1:&#xa;      # Regular rule, one output: Just write a simple rule.&#xa;      self.WriteLn('%s: %s%s' % (outputs[0], ' '.join(inputs), force_append))&#xa;    else:&#xa;      # Regular rule, more than one output: Multiple outputs are tricky in&#xa;      # make. We will write three rules:&#xa;      # - All outputs depend on an intermediate file.&#xa;      # - Make .INTERMEDIATE depend on the intermediate.&#xa;      # - The intermediate file depends on the inputs and executes the&#xa;      #   actual command.&#xa;      # - The intermediate recipe will 'touch' the intermediate file.&#xa;      # - The multi-output rule will have an do-nothing recipe.&#xa;      intermediate = ""%s.intermediate"" % (command if command else self.target)&#xa;      self.WriteLn('%s: %s' % (' '.join(outputs), intermediate))&#xa;      self.WriteLn('\t%s' % '@:');&#xa;      self.WriteLn('%s: %s' % ('.INTERMEDIATE', intermediate))&#xa;      self.WriteLn('%s: %s%s' %&#xa;                   (intermediate, ' '.join(inputs), force_append))&#xa;      actions.insert(0, '$(call do_cmd,touch)')&#xa;&#xa;    if actions:&#xa;      for action in actions:&#xa;        self.WriteLn('\t%s' % action)&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteAndroidNdkModuleRule(self, module_name, all_sources, link_deps):&#xa;    """"""Write a set of LOCAL_XXX definitions for Android NDK.&#xa;&#xa;    These variable definitions will be used by Android NDK but do nothing for&#xa;    non-Android applications.&#xa;&#xa;    Arguments:&#xa;      module_name: Android NDK module name, which must be unique among all&#xa;          module names.&#xa;      all_sources: A list of source files (will be filtered by Compilable).&#xa;      link_deps: A list of link dependencies, which must be sorted in&#xa;          the order from dependencies to dependents.&#xa;    """"""&#xa;    if self.type not in ('executable', 'shared_library', 'static_library'):&#xa;      return&#xa;&#xa;    self.WriteLn('# Variable definitions for Android applications')&#xa;    self.WriteLn('include $(CLEAR_VARS)')&#xa;    self.WriteLn('LOCAL_MODULE := ' + module_name)&#xa;    self.WriteLn('LOCAL_CFLAGS := $(CFLAGS_$(BUILDTYPE)) '&#xa;                 '$(DEFS_$(BUILDTYPE)) '&#xa;                 # LOCAL_CFLAGS is applied to both of C and C++.  There is&#xa;                 # no way to specify $(CFLAGS_C_$(BUILDTYPE)) only for C&#xa;                 # sources.&#xa;                 '$(CFLAGS_C_$(BUILDTYPE)) '&#xa;                 # $(INCS_$(BUILDTYPE)) includes the prefix '-I' while&#xa;                 # LOCAL_C_INCLUDES does not expect it.  So put it in&#xa;                 # LOCAL_CFLAGS.&#xa;                 '$(INCS_$(BUILDTYPE))')&#xa;    # LOCAL_CXXFLAGS is obsolete and LOCAL_CPPFLAGS is preferred.&#xa;    self.WriteLn('LOCAL_CPPFLAGS := $(CFLAGS_CC_$(BUILDTYPE))')&#xa;    self.WriteLn('LOCAL_C_INCLUDES :=')&#xa;    self.WriteLn('LOCAL_LDLIBS := $(LDFLAGS_$(BUILDTYPE)) $(LIBS)')&#xa;&#xa;    # Detect the C++ extension.&#xa;    cpp_ext = {'.cc': 0, '.cpp': 0, '.cxx': 0}&#xa;    default_cpp_ext = '.cpp'&#xa;    for filename in all_sources:&#xa;      ext = os.path.splitext(filename)[1]&#xa;      if ext in cpp_ext:&#xa;        cpp_ext[ext] += 1&#xa;        if cpp_ext[ext] > cpp_ext[default_cpp_ext]:&#xa;          default_cpp_ext = ext&#xa;    self.WriteLn('LOCAL_CPP_EXTENSION := ' + default_cpp_ext)&#xa;&#xa;    self.WriteList(map(self.Absolutify, filter(Compilable, all_sources)),&#xa;                   'LOCAL_SRC_FILES')&#xa;&#xa;    # Filter out those which do not match prefix and suffix and produce&#xa;    # the resulting list without prefix and suffix.&#xa;    def DepsToModules(deps, prefix, suffix):&#xa;      modules = []&#xa;      for filepath in deps:&#xa;        filename = os.path.basename(filepath)&#xa;        if filename.startswith(prefix) and filename.endswith(suffix):&#xa;          modules.append(filename[len(prefix):-len(suffix)])&#xa;      return modules&#xa;&#xa;    # Retrieve the default value of 'SHARED_LIB_SUFFIX'&#xa;    params = {'flavor': 'linux'}&#xa;    default_variables = {}&#xa;    CalculateVariables(default_variables, params)&#xa;&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['SHARED_LIB_PREFIX'],&#xa;                      default_variables['SHARED_LIB_SUFFIX']),&#xa;        'LOCAL_SHARED_LIBRARIES')&#xa;    self.WriteList(&#xa;        DepsToModules(link_deps,&#xa;                      generator_default_variables['STATIC_LIB_PREFIX'],&#xa;                      generator_default_variables['STATIC_LIB_SUFFIX']),&#xa;        'LOCAL_STATIC_LIBRARIES')&#xa;&#xa;    if self.type == 'executable':&#xa;      self.WriteLn('include $(BUILD_EXECUTABLE)')&#xa;    elif self.type == 'shared_library':&#xa;      self.WriteLn('include $(BUILD_SHARED_LIBRARY)')&#xa;    elif self.type == 'static_library':&#xa;      self.WriteLn('include $(BUILD_STATIC_LIBRARY)')&#xa;    self.WriteLn()&#xa;&#xa;&#xa;  def WriteLn(self, text=''):&#xa;    self.fp.write(text + '\n')&#xa;&#xa;&#xa;  def GetSortedXcodeEnv(self, additional_settings=None):&#xa;    return gyp.xcode_emulation.GetSortedXcodeEnv(&#xa;        self.xcode_settings, ""$(abs_builddir)"",&#xa;        os.path.join(""$(abs_srcdir)"", self.path), ""$(BUILDTYPE)"",&#xa;        additional_settings)&#xa;&#xa;&#xa;  def GetSortedXcodePostbuildEnv(self):&#xa;    # CHROMIUM_STRIP_SAVE_FILE is a chromium-specific hack.&#xa;    # TODO(thakis): It would be nice to have some general mechanism instead.&#xa;    strip_save_file = self.xcode_settings.GetPerTargetSetting(&#xa;        'CHROMIUM_STRIP_SAVE_FILE', '')&#xa;    # Even if strip_save_file is empty, explicitly write it. Else a postbuild&#xa;    # might pick up an export from an earlier target.&#xa;    return self.GetSortedXcodeEnv(&#xa;        additional_settings={'CHROMIUM_STRIP_SAVE_FILE': strip_save_file})&#xa;&#xa;&#xa;  def WriteSortedXcodeEnv(self, target, env):&#xa;    for k, v in env:&#xa;      # For&#xa;      #  foo := a\ b&#xa;      # the escaped space does the right thing. For&#xa;      #  export foo := a\ b&#xa;      # it does not -- the backslash is written to the env as literal character.&#xa;      # So don't escape spaces in |env[k]|.&#xa;      self.WriteLn('%s: export %s := %s' % (QuoteSpaces(target), k, v))&#xa;&#xa;&#xa;  def Objectify(self, path):&#xa;    """"""Convert a path to its output directory form.""""""&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/' % self.toolset)&#xa;    if not '$(obj)' in path:&#xa;      path = '$(obj).%s/$(TARGET)/%s' % (self.toolset, path)&#xa;    return path&#xa;&#xa;&#xa;  def Pchify(self, path, lang):&#xa;    """"""Convert a prefix header path to its output directory form.""""""&#xa;    path = self.Absolutify(path)&#xa;    if '$(' in path:&#xa;      path = path.replace('$(obj)/', '$(obj).%s/$(TARGET)/pch-%s' %&#xa;                          (self.toolset, lang))&#xa;      return path&#xa;    return '$(obj).%s/$(TARGET)/pch-%s/%s' % (self.toolset, lang, path)&#xa;&#xa;&#xa;  def Absolutify(self, path):&#xa;    """"""Convert a subdirectory-relative path into a base-relative path.&#xa;    Skips over paths that contain variables.""""""&#xa;    if '$(' in path:&#xa;      # Don't call normpath in this case, as it might collapse the&#xa;      # path too aggressively if it features '..'. However it's still&#xa;      # important to strip trailing slashes.&#xa;      return path.rstrip('/')&#xa;    return os.path.normpath(os.path.join(self.path, path))&#xa;&#xa;&#xa;  def ExpandInputRoot(self, template, expansion, dirname):&#xa;    if '%(INPUT_ROOT)s' not in template and '%(INPUT_DIRNAME)s' not in template:&#xa;      return template&#xa;    path = template % {&#xa;        'INPUT_ROOT': expansion,&#xa;        'INPUT_DIRNAME': dirname,&#xa;        }&#xa;    return path&#xa;&#xa;&#xa;  def _InstallableTargetInstallPath(self):&#xa;    """"""Returns the location of the final output for an installable target.""""""&#xa;    # Xcode puts shared_library results into PRODUCT_DIR, and some gyp files&#xa;    # rely on this. Emulate this behavior for mac.&#xa;&#xa;    # XXX(TooTallNate): disabling this code since we don't want this behavior...&#xa;    #if (self.type == 'shared_library' and&#xa;    #    (self.flavor != 'mac' or self.toolset != 'target')):&#xa;    #  # Install all shared libs into a common directory (per toolset) for&#xa;    #  # convenient access with LD_LIBRARY_PATH.&#xa;    #  return '$(builddir)/lib.%s/%s' % (self.toolset, self.alias)&#xa;    return '$(builddir)/' + self.alias&#xa;&#xa;&#xa;def WriteAutoRegenerationRule(params, root_makefile, makefile_name,&#xa;                              build_files):&#xa;  """"""Write the target to regenerate the Makefile.""""""&#xa;  options = params['options']&#xa;  build_files_args = [gyp.common.RelativePath(filename, options.toplevel_dir)&#xa;                      for filename in params['build_files_arg']]&#xa;&#xa;  gyp_binary = gyp.common.FixIfRelativePath(params['gyp_binary'],&#xa;                                            options.toplevel_dir)&#xa;  if not gyp_binary.startswith(os.sep):&#xa;    gyp_binary = os.path.join('.', gyp_binary)&#xa;&#xa;  root_makefile.write(&#xa;      ""quiet_cmd_regen_makefile = ACTION Regenerating $@\n""&#xa;      ""cmd_regen_makefile = cd $(srcdir); %(cmd)s\n""&#xa;      ""%(makefile_name)s: %(deps)s\n""&#xa;      ""\t$(call do_cmd,regen_makefile)\n\n"" % {&#xa;          'makefile_name': makefile_name,&#xa;          'deps': ' '.join(map(Sourceify, build_files)),&#xa;          'cmd': gyp.common.EncodePOSIXShellList(&#xa;                     [gyp_binary, '-fmake'] +&#xa;                     gyp.RegenerateFlags(options) +&#xa;                     build_files_args)})&#xa;&#xa;&#xa;def PerformBuild(data, configurations, params):&#xa;  options = params['options']&#xa;  for config in configurations:&#xa;    arguments = ['make']&#xa;    if options.toplevel_dir and options.toplevel_dir != '.':&#xa;      arguments += '-C', options.toplevel_dir&#xa;    arguments.append('BUILDTYPE=' + config)&#xa;    print 'Building [%s]: %s' % (config, arguments)&#xa;    subprocess.check_call(arguments)&#xa;&#xa;&#xa;def GenerateOutput(target_list, target_dicts, data, params):&#xa;  options = params['options']&#xa;  flavor = gyp.common.GetFlavor(params)&#xa;  generator_flags = params.get('generator_flags', {})&#xa;  builddir_name = generator_flags.get('output_dir', 'out')&#xa;  android_ndk_version = generator_flags.get('android_ndk_version', None)&#xa;  default_target = generator_flags.get('default_target', 'all')&#xa;&#xa;  def CalculateMakefilePath(build_file, base_name):&#xa;    """"""Determine where to write a Makefile for a given gyp file.""""""&#xa;    # Paths in gyp files are relative to the .gyp file, but we want&#xa;    # paths relative to the source root for the master makefile.  Grab&#xa;    # the path of the .gyp file as the base to relativize against.&#xa;    # E.g. ""foo/bar"" when we're constructing targets for ""foo/bar/baz.gyp"".&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.depth)&#xa;    # We write the file in the base_path directory.&#xa;    output_file = os.path.join(options.depth, base_path, base_name)&#xa;    if options.generator_output:&#xa;      output_file = os.path.join(&#xa;          options.depth, options.generator_output, base_path, base_name)&#xa;    base_path = gyp.common.RelativePath(os.path.dirname(build_file),&#xa;                                        options.toplevel_dir)&#xa;    return base_path, output_file&#xa;&#xa;  # TODO:  search for the first non-'Default' target.  This can go&#xa;  # away when we add verification that all targets have the&#xa;  # necessary configurations.&#xa;  default_configuration = None&#xa;  toolsets = set([target_dicts[target]['toolset'] for target in target_list])&#xa;  for target in target_list:&#xa;    spec = target_dicts[target]&#xa;    if spec['default_configuration'] != 'Default':&#xa;      default_configuration = spec['default_configuration']&#xa;      break&#xa;  if not default_configuration:&#xa;    default_configuration = 'Default'&#xa;&#xa;  srcdir = '.'&#xa;  makefile_name = 'Makefile' + options.suffix&#xa;  makefile_path = os.path.join(options.toplevel_dir, makefile_name)&#xa;  if options.generator_output:&#xa;    global srcdir_prefix&#xa;    makefile_path = os.path.join(&#xa;        options.toplevel_dir, options.generator_output, makefile_name)&#xa;    srcdir = gyp.common.RelativePath(srcdir, options.generator_output)&#xa;    srcdir_prefix = '$(srcdir)/'&#xa;&#xa;  flock_command= 'flock'&#xa;  copy_archive_arguments = '-af'&#xa;  header_params = {&#xa;      'default_target': default_target,&#xa;      'builddir': builddir_name,&#xa;      'default_configuration': default_configuration,&#xa;      'flock': flock_command,&#xa;      'flock_index': 1,&#xa;      'link_commands': LINK_COMMANDS_LINUX,&#xa;      'extra_commands': '',&#xa;      'srcdir': srcdir,&#xa;      'copy_archive_args': copy_archive_arguments,&#xa;    }&#xa;  if flavor == 'mac':&#xa;    flock_command = './gyp-mac-tool flock'&#xa;    header_params.update({&#xa;        'flock': flock_command,&#xa;        'flock_index': 2,&#xa;        'link_commands': LINK_COMMANDS_MAC,&#xa;        'extra_commands': SHARED_HEADER_MAC_COMMANDS,&#xa;    })&#xa;  elif flavor == 'android':&#xa;    header_params.update({&#xa;        'link_commands': LINK_COMMANDS_ANDROID,&#xa;    })&#xa;  elif flavor == 'solaris':&#xa;    header_params.update({&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;  elif flavor == 'freebsd':&#xa;    # Note: OpenBSD has sysutils/flock. lockf seems to be FreeBSD specific.&#xa;    header_params.update({&#xa;        'flock': 'lockf',&#xa;    })&#xa;  elif flavor == 'openbsd':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;    })&#xa;  elif flavor == 'aix':&#xa;    copy_archive_arguments = '-pPRf'&#xa;    header_params.update({&#xa;        'copy_archive_args': copy_archive_arguments,&#xa;        'link_commands': LINK_COMMANDS_AIX,&#xa;        'flock': './gyp-flock-tool flock',&#xa;        'flock_index': 2,&#xa;    })&#xa;&#xa;  header_params.update({&#xa;    'CC.target':   GetEnvironFallback(('CC_target', 'CC'), '$(CC)'),&#xa;    'AR.target':   GetEnvironFallback(('AR_target', 'AR'), '$(AR)'),&#xa;    'CXX.target':  GetEnvironFallback(('CXX_target', 'CXX'), '$(CXX)'),&#xa;    'LINK.target': GetEnvironFallback(('LINK_target', 'LINK'), '$(LINK)'),&#xa;    'CC.host':     GetEnvironFallback(('CC_host', 'CC'), 'gcc'),&#xa;    'AR.host':     GetEnvironFallback(('AR_host', 'AR'), 'ar'),&#xa;    'CXX.host':    GetEnvironFallback(('CXX_host', 'CXX'), 'g++'),&#xa;    'LINK.host':   GetEnvironFallback(('LINK_host', 'LINK'), '$(CXX.host)'),&#xa;  })&#xa;&#xa;  build_file, _, _ = gyp.common.ParseQualifiedTarget(target_list[0])&#xa;  make_global_settings_array = data[build_file].get('make_global_settings', [])&#xa;  wrappers = {}&#xa;  for key, value in make_global_settings_array:&#xa;    if key.endswith('_wrapper'):&#xa;      wrappers[key[:-len('_wrapper')]] = '$(abspath %s)' % value&#xa;  make_global_settings = ''&#xa;  for key, value in make_global_settings_array:&#xa;    if re.match('.*_wrapper', key):&#xa;      continue&#xa;    if value[0] != '$':&#xa;      value = '$(abspath %s)' % value&#xa;    wrapper = wrappers.get(key)&#xa;    if wrapper:&#xa;      value = '%s %s' % (wrapper, value)&#xa;      del wrappers[key]&#xa;    if key in ('CC', 'CC.host', 'CXX', 'CXX.host'):&#xa;      make_global_settings += (&#xa;          'ifneq (,$(filter $(origin %s), undefined default))\n' % key)&#xa;      # Let gyp-time envvars win over global settings.&#xa;      env_key = key.replace('.', '_')  # CC.host -> CC_host&#xa;      if env_key in os.environ:&#xa;        value = os.environ[env_key]&#xa;      make_global_settings += '  %s = %s\n' % (key, value)&#xa;      make_global_settings += 'endif\n'&#xa;    else:&#xa;      make_global_settings += '%s ?= %s\n' % (key, value)&#xa;  # TODO(ukai): define cmd when only wrapper is specified in&#xa;  # make_global_settings.&#xa;&#xa;  header_params['make_global_settings'] = make_global_settings&#xa;&#xa;  gyp.common.EnsureDirExists(makefile_path)&#xa;  root_makefile = open(makefile_path, 'w')&#xa;  root_makefile.write(SHARED_HEADER % header_params)&#xa;  # Currently any versions have the same effect, but in future the behavior&#xa;  # could be different.&#xa;  if android_ndk_version:&#xa;    root_makefile.write(&#xa;        '# Define LOCAL_PATH for build of Android applications.\n'&#xa;        'LOCAL_PATH := $(call my-dir)\n'&#xa;        '\n')&#xa;  for toolset in toolsets:&#xa;    root_makefile.write('TOOLSET := %s\n' % toolset)&#xa;    WriteRootHeaderSuffixRules(root_makefile)&#xa;&#xa;  # Put build-time support tools next to the root Makefile.&#xa;  dest_path = os.path.dirname(makefile_path)&#xa;  gyp.common.CopyTool(flavor, dest_path)&#xa;&#xa;  # Find the list of targets that derive from the gyp file(s) being built.&#xa;  needed_targets = set()&#xa;  for build_file in params['build_files']:&#xa;    for target in gyp.common.AllTargets(target_list, target_dicts, build_file):&#xa;      needed_targets.add(target)&#xa;&#xa;  build_files = set()&#xa;  include_list = set()&#xa;  for qualified_target in target_list:&#xa;    build_file, target, toolset = gyp.common.ParseQualifiedTarget(&#xa;        qualified_target)&#xa;&#xa;    this_make_global_settings = data[build_file].get('make_global_settings', [])&#xa;    assert make_global_settings_array == this_make_global_settings, (&#xa;        ""make_global_settings needs to be the same for all targets. %s vs. %s"" %&#xa;        (this_make_global_settings, make_global_settings))&#xa;&#xa;    build_files.add(gyp.common.RelativePath(build_file, options.toplevel_dir))&#xa;    included_files = data[build_file]['included_files']&#xa;    for included_file in included_files:&#xa;      # The included_files entries are relative to the dir of the build file&#xa;      # that included them, so we have to undo that and then make them relative&#xa;      # to the root dir.&#xa;      relative_include_file = gyp.common.RelativePath(&#xa;          gyp.common.UnrelativePath(included_file, build_file),&#xa;          options.toplevel_dir)&#xa;      abs_include_file = os.path.abspath(relative_include_file)&#xa;      # If the include file is from the ~/.gyp dir, we should use absolute path&#xa;      # so that relocating the src dir doesn't break the path.&#xa;      if (params['home_dot_gyp'] and&#xa;          abs_include_file.startswith(params['home_dot_gyp'])):&#xa;        build_files.add(abs_include_file)&#xa;      else:&#xa;        build_files.add(relative_include_file)&#xa;&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        target + '.' + toolset + options.suffix + '.mk')&#xa;&#xa;    spec = target_dicts[qualified_target]&#xa;    configs = spec['configurations']&#xa;&#xa;    if flavor == 'mac':&#xa;      gyp.xcode_emulation.MergeGlobalXcodeSettingsToSpec(data[build_file], spec)&#xa;&#xa;    writer = MakefileWriter(generator_flags, flavor)&#xa;    writer.Write(qualified_target, base_path, output_file, spec, configs,&#xa;                 part_of_all=qualified_target in needed_targets)&#xa;&#xa;    # Our root_makefile lives at the source root.  Compute the relative path&#xa;    # from there to the output_file for including.&#xa;    mkfile_rel_path = gyp.common.RelativePath(output_file,&#xa;                                              os.path.dirname(makefile_path))&#xa;    include_list.add(mkfile_rel_path)&#xa;&#xa;  # Write out per-gyp (sub-project) Makefiles.&#xa;  depth_rel_path = gyp.common.RelativePath(options.depth, os.getcwd())&#xa;  for build_file in build_files:&#xa;    # The paths in build_files were relativized above, so undo that before&#xa;    # testing against the non-relativized items in target_list and before&#xa;    # calculating the Makefile path.&#xa;    build_file = os.path.join(depth_rel_path, build_file)&#xa;    gyp_targets = [target_dicts[target]['target_name'] for target in target_list&#xa;                   if target.startswith(build_file) and&#xa;                   target in needed_targets]&#xa;    # Only generate Makefiles for gyp files with targets.&#xa;    if not gyp_targets:&#xa;      continue&#xa;    base_path, output_file = CalculateMakefilePath(build_file,&#xa;        os.path.splitext(os.path.basename(build_file))[0] + '.Makefile')&#xa;    makefile_rel_path = gyp.common.RelativePath(os.path.dirname(makefile_path),&#xa;                                                os.path.dirname(output_file))&#xa;    writer.WriteSubMake(output_file, makefile_rel_path, gyp_targets,&#xa;                        builddir_name)&#xa;&#xa;&#xa;  # Write out the sorted list of includes.&#xa;  root_makefile.write('\n')&#xa;  for include_file in sorted(include_list):&#xa;    # We wrap each .mk include in an if statement so users can tell make to&#xa;    # not load a file by setting NO_LOAD.  The below make code says, only&#xa;    # load the .mk file if the .mk filename doesn't start with a token in&#xa;    # NO_LOAD.&#xa;    root_makefile.write(&#xa;        ""ifeq ($(strip $(foreach prefix,$(NO_LOAD),\\\n""&#xa;        ""    $(findstring $(join ^,$(prefix)),\\\n""&#xa;        ""                 $(join ^,"" + include_file + "")))),)\n"")&#xa;    root_makefile.write(""  include "" + include_file + ""\n"")&#xa;    root_makefile.write(""endif\n"")&#xa;  root_makefile.write('\n')&#xa;&#xa;  if (not generator_flags.get('standalone')&#xa;      and generator_flags.get('auto_regeneration', True)):&#xa;    WriteAutoRegenerationRule(params, root_makefile, makefile_name, build_files)&#xa;&#xa;  root_makefile.write(SHARED_FOOTER)&#xa;&#xa;  root_makefile.close()&#xa;"
1838699|"""""""Miscellaneous utility functions and classes.&#xa;&#xa;This module is used internally by Tornado.  It is not necessarily expected&#xa;that the functions and classes defined here will be useful to other&#xa;applications, but they are documented here in case they are.&#xa;&#xa;The one public-facing part of this module is the `Configurable` class&#xa;and its `~Configurable.configure` method, which becomes a part of the&#xa;interface of its subclasses, including `.AsyncHTTPClient`, `.IOLoop`,&#xa;and `.Resolver`.&#xa;""""""&#xa;&#xa;from __future__ import absolute_import, division, print_function, with_statement&#xa;&#xa;import array&#xa;import os&#xa;import re&#xa;import sys&#xa;import zlib&#xa;&#xa;PY3 = sys.version_info >= (3,)&#xa;&#xa;if PY3:&#xa;    xrange = range&#xa;&#xa;# inspect.getargspec() raises DeprecationWarnings in Python 3.5.&#xa;# The two functions have compatible interfaces for the parts we need.&#xa;if PY3:&#xa;    from inspect import getfullargspec as getargspec&#xa;else:&#xa;    from inspect import getargspec&#xa;&#xa;# Aliases for types that are spelled differently in different Python&#xa;# versions. bytes_type is deprecated and no longer used in Tornado&#xa;# itself but is left in case anyone outside Tornado is using it.&#xa;bytes_type = bytes&#xa;if PY3:&#xa;    unicode_type = str&#xa;    basestring_type = str&#xa;else:&#xa;    # The names unicode and basestring don't exist in py3 so silence flake8.&#xa;    unicode_type = unicode  # noqa&#xa;    basestring_type = basestring  # noqa&#xa;&#xa;&#xa;try:&#xa;    import typing  # noqa&#xa;    from typing import cast&#xa;&#xa;    _ObjectDictBase = typing.Dict[str, typing.Any]&#xa;except ImportError:&#xa;    _ObjectDictBase = dict&#xa;&#xa;    def cast(typ, x):&#xa;        return x&#xa;else:&#xa;    # More imports that are only needed in type comments.&#xa;    import datetime  # noqa&#xa;    import types  # noqa&#xa;    from typing import Any, AnyStr, Union, Optional, Dict, Mapping  # noqa&#xa;    from typing import Tuple, Match, Callable  # noqa&#xa;&#xa;    if PY3:&#xa;        _BaseString = str&#xa;    else:&#xa;        _BaseString = Union[bytes, unicode_type]&#xa;&#xa;&#xa;class ObjectDict(_ObjectDictBase):&#xa;    """"""Makes a dictionary behave like an object, with attribute-style access.&#xa;    """"""&#xa;    def __getattr__(self, name):&#xa;        # type: (str) -> Any&#xa;        try:&#xa;            return self[name]&#xa;        except KeyError:&#xa;            raise AttributeError(name)&#xa;&#xa;    def __setattr__(self, name, value):&#xa;        # type: (str, Any) -> None&#xa;        self[name] = value&#xa;&#xa;&#xa;class GzipDecompressor(object):&#xa;    """"""Streaming gzip decompressor.&#xa;&#xa;    The interface is like that of `zlib.decompressobj` (without some of the&#xa;    optional arguments, but it understands gzip headers and checksums.&#xa;    """"""&#xa;    def __init__(self):&#xa;        # Magic parameter makes zlib module understand gzip header&#xa;        # http://stackoverflow.com/questions/1838699/how-can-i-decompress-a-gzip-stream-with-zlib&#xa;        # This works on cpython and pypy, but not jython.&#xa;        self.decompressobj = zlib.decompressobj(16 + zlib.MAX_WBITS)&#xa;&#xa;    def decompress(self, value, max_length=None):&#xa;        # type: (bytes, Optional[int]) -> bytes&#xa;        """"""Decompress a chunk, returning newly-available data.&#xa;&#xa;        Some data may be buffered for later processing; `flush` must&#xa;        be called when there is no more input data to ensure that&#xa;        all data was processed.&#xa;&#xa;        If ``max_length`` is given, some input data may be left over&#xa;        in ``unconsumed_tail``; you must retrieve this value and pass&#xa;        it back to a future call to `decompress` if it is not empty.&#xa;        """"""&#xa;        return self.decompressobj.decompress(value, max_length)&#xa;&#xa;    @property&#xa;    def unconsumed_tail(self):&#xa;        # type: () -> bytes&#xa;        """"""Returns the unconsumed portion left over&#xa;        """"""&#xa;        return self.decompressobj.unconsumed_tail&#xa;&#xa;    def flush(self):&#xa;        # type: () -> bytes&#xa;        """"""Return any remaining buffered data not yet returned by decompress.&#xa;&#xa;        Also checks for errors such as truncated input.&#xa;        No other methods may be called on this object after `flush`.&#xa;        """"""&#xa;        return self.decompressobj.flush()&#xa;&#xa;&#xa;def import_object(name):&#xa;    # type: (_BaseString) -> Any&#xa;    """"""Imports an object by name.&#xa;&#xa;    import_object('x') is equivalent to 'import x'.&#xa;    import_object('x.y.z') is equivalent to 'from x.y import z'.&#xa;&#xa;    >>> import tornado.escape&#xa;    >>> import_object('tornado.escape') is tornado.escape&#xa;    True&#xa;    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8&#xa;    True&#xa;    >>> import_object('tornado') is tornado&#xa;    True&#xa;    >>> import_object('tornado.missing_module')&#xa;    Traceback (most recent call last):&#xa;        ...&#xa;    ImportError: No module named missing_module&#xa;    """"""&#xa;    if not isinstance(name, str):&#xa;        # on python 2 a byte string is required.&#xa;        name = name.encode('utf-8')&#xa;    if name.count('.') == 0:&#xa;        return __import__(name, None, None)&#xa;&#xa;    parts = name.split('.')&#xa;    obj = __import__('.'.join(parts[:-1]), None, None, [parts[-1]], 0)&#xa;    try:&#xa;        return getattr(obj, parts[-1])&#xa;    except AttributeError:&#xa;        raise ImportError(""No module named %s"" % parts[-1])&#xa;&#xa;&#xa;# Stubs to make mypy happy (and later for actual type-checking).&#xa;def raise_exc_info(exc_info):&#xa;    # type: (Tuple[type, BaseException, types.TracebackType]) -> None&#xa;    pass&#xa;&#xa;&#xa;def exec_in(code, glob, loc=None):&#xa;    # type: (Any, Dict[str, Any], Optional[Mapping[str, Any]]) -> Any&#xa;    pass&#xa;&#xa;&#xa;if PY3:&#xa;    exec(""""""&#xa;def raise_exc_info(exc_info):&#xa;    raise exc_info[1].with_traceback(exc_info[2])&#xa;&#xa;def exec_in(code, glob, loc=None):&#xa;    if isinstance(code, str):&#xa;        code = compile(code, '<string>', 'exec', dont_inherit=True)&#xa;    exec(code, glob, loc)&#xa;"""""")&#xa;else:&#xa;    exec(""""""&#xa;def raise_exc_info(exc_info):&#xa;    raise exc_info[0], exc_info[1], exc_info[2]&#xa;&#xa;def exec_in(code, glob, loc=None):&#xa;    if isinstance(code, basestring):&#xa;        # exec(string) inherits the caller's future imports; compile&#xa;        # the string first to prevent that.&#xa;        code = compile(code, '<string>', 'exec', dont_inherit=True)&#xa;    exec code in glob, loc&#xa;"""""")&#xa;&#xa;&#xa;def errno_from_exception(e):&#xa;    # type: (BaseException) -> Optional[int]&#xa;    """"""Provides the errno from an Exception object.&#xa;&#xa;    There are cases that the errno attribute was not set so we pull&#xa;    the errno out of the args but if someone instantiates an Exception&#xa;    without any args you will get a tuple error. So this function&#xa;    abstracts all that behavior to give you a safe way to get the&#xa;    errno.&#xa;    """"""&#xa;&#xa;    if hasattr(e, 'errno'):&#xa;        return e.errno  # type: ignore&#xa;    elif e.args:&#xa;        return e.args[0]&#xa;    else:&#xa;        return None&#xa;&#xa;&#xa;_alphanum = frozenset(&#xa;    ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"")&#xa;&#xa;&#xa;def _re_unescape_replacement(match):&#xa;    # type: (Match[str]) -> str&#xa;    group = match.group(1)&#xa;    if group[0] in _alphanum:&#xa;        raise ValueError(""cannot unescape '\\\\%s'"" % group[0])&#xa;    return group&#xa;&#xa;_re_unescape_pattern = re.compile(r'\\(.)', re.DOTALL)&#xa;&#xa;&#xa;def re_unescape(s):&#xa;    # type: (str) -> str&#xa;    """"""Unescape a string escaped by `re.escape`.&#xa;&#xa;    May raise ``ValueError`` for regular expressions which could not&#xa;    have been produced by `re.escape` (for example, strings containing&#xa;    ``\d`` cannot be unescaped).&#xa;&#xa;    .. versionadded:: 4.4&#xa;    """"""&#xa;    return _re_unescape_pattern.sub(_re_unescape_replacement, s)&#xa;&#xa;&#xa;class Configurable(object):&#xa;    """"""Base class for configurable interfaces.&#xa;&#xa;    A configurable interface is an (abstract) class whose constructor&#xa;    acts as a factory function for one of its implementation subclasses.&#xa;    The implementation subclass as well as optional keyword arguments to&#xa;    its initializer can be set globally at runtime with `configure`.&#xa;&#xa;    By using the constructor as the factory method, the interface&#xa;    looks like a normal class, `isinstance` works as usual, etc.  This&#xa;    pattern is most useful when the choice of implementation is likely&#xa;    to be a global decision (e.g. when `~select.epoll` is available,&#xa;    always use it instead of `~select.select`), or when a&#xa;    previously-monolithic class has been split into specialized&#xa;    subclasses.&#xa;&#xa;    Configurable subclasses must define the class methods&#xa;    `configurable_base` and `configurable_default`, and use the instance&#xa;    method `initialize` instead of ``__init__``.&#xa;    """"""&#xa;    __impl_class = None  # type: type&#xa;    __impl_kwargs = None  # type: Dict[str, Any]&#xa;&#xa;    def __new__(cls, *args, **kwargs):&#xa;        base = cls.configurable_base()&#xa;        init_kwargs = {}&#xa;        if cls is base:&#xa;            impl = cls.configured_class()&#xa;            if base.__impl_kwargs:&#xa;                init_kwargs.update(base.__impl_kwargs)&#xa;        else:&#xa;            impl = cls&#xa;        init_kwargs.update(kwargs)&#xa;        instance = super(Configurable, cls).__new__(impl)&#xa;        # initialize vs __init__ chosen for compatibility with AsyncHTTPClient&#xa;        # singleton magic.  If we get rid of that we can switch to __init__&#xa;        # here too.&#xa;        instance.initialize(*args, **init_kwargs)&#xa;        return instance&#xa;&#xa;    @classmethod&#xa;    def configurable_base(cls):&#xa;        # type: () -> Any&#xa;        # TODO: This class needs https://github.com/python/typing/issues/107&#xa;        # to be fully typeable.&#xa;        """"""Returns the base class of a configurable hierarchy.&#xa;&#xa;        This will normally return the class in which it is defined.&#xa;        (which is *not* necessarily the same as the cls classmethod parameter).&#xa;        """"""&#xa;        raise NotImplementedError()&#xa;&#xa;    @classmethod&#xa;    def configurable_default(cls):&#xa;        # type: () -> type&#xa;        """"""Returns the implementation class to be used if none is configured.""""""&#xa;        raise NotImplementedError()&#xa;&#xa;    def initialize(self):&#xa;        # type: () -> None&#xa;        """"""Initialize a `Configurable` subclass instance.&#xa;&#xa;        Configurable classes should use `initialize` instead of ``__init__``.&#xa;&#xa;        .. versionchanged:: 4.2&#xa;           Now accepts positional arguments in addition to keyword arguments.&#xa;        """"""&#xa;&#xa;    @classmethod&#xa;    def configure(cls, impl, **kwargs):&#xa;        # type: (Any, **Any) -> None&#xa;        """"""Sets the class to use when the base class is instantiated.&#xa;&#xa;        Keyword arguments will be saved and added to the arguments passed&#xa;        to the constructor.  This can be used to set global defaults for&#xa;        some parameters.&#xa;        """"""&#xa;        base = cls.configurable_base()&#xa;        if isinstance(impl, (str, unicode_type)):&#xa;            impl = import_object(impl)&#xa;        if impl is not None and not issubclass(impl, cls):&#xa;            raise ValueError(""Invalid subclass of %s"" % cls)&#xa;        base.__impl_class = impl&#xa;        base.__impl_kwargs = kwargs&#xa;&#xa;    @classmethod&#xa;    def configured_class(cls):&#xa;        # type: () -> type&#xa;        """"""Returns the currently configured class.""""""&#xa;        base = cls.configurable_base()&#xa;        if cls.__impl_class is None:&#xa;            base.__impl_class = cls.configurable_default()&#xa;        return base.__impl_class&#xa;&#xa;    @classmethod&#xa;    def _save_configuration(cls):&#xa;        # type: () -> Tuple[type, Dict[str, Any]]&#xa;        base = cls.configurable_base()&#xa;        return (base.__impl_class, base.__impl_kwargs)&#xa;&#xa;    @classmethod&#xa;    def _restore_configuration(cls, saved):&#xa;        # type: (Tuple[type, Dict[str, Any]]) -> None&#xa;        base = cls.configurable_base()&#xa;        base.__impl_class = saved[0]&#xa;        base.__impl_kwargs = saved[1]&#xa;&#xa;&#xa;class ArgReplacer(object):&#xa;    """"""Replaces one value in an ``args, kwargs`` pair.&#xa;&#xa;    Inspects the function signature to find an argument by name&#xa;    whether it is passed by position or keyword.  For use in decorators&#xa;    and similar wrappers.&#xa;    """"""&#xa;    def __init__(self, func, name):&#xa;        # type: (Callable, str) -> None&#xa;        self.name = name&#xa;        try:&#xa;            self.arg_pos = self._getargnames(func).index(name)&#xa;        except ValueError:&#xa;            # Not a positional parameter&#xa;            self.arg_pos = None&#xa;&#xa;    def _getargnames(self, func):&#xa;        # type: (Callable) -> List[str]&#xa;        try:&#xa;            return getargspec(func).args&#xa;        except TypeError:&#xa;            if hasattr(func, 'func_code'):&#xa;                # Cython-generated code has all the attributes needed&#xa;                # by inspect.getargspec, but the inspect module only&#xa;                # works with ordinary functions. Inline the portion of&#xa;                # getargspec that we need here. Note that for static&#xa;                # functions the @cython.binding(True) decorator must&#xa;                # be used (for methods it works out of the box).&#xa;                code = func.func_code  # type: ignore&#xa;                return code.co_varnames[:code.co_argcount]&#xa;            raise&#xa;&#xa;    def get_old_value(self, args, kwargs, default=None):&#xa;        # type: (List[Any], Dict[str, Any], Any) -> Any&#xa;        """"""Returns the old value of the named argument without replacing it.&#xa;&#xa;        Returns ``default`` if the argument is not present.&#xa;        """"""&#xa;        if self.arg_pos is not None and len(args) > self.arg_pos:&#xa;            return args[self.arg_pos]&#xa;        else:&#xa;            return kwargs.get(self.name, default)&#xa;&#xa;    def replace(self, new_value, args, kwargs):&#xa;        # type: (Any, List[Any], Dict[str, Any]) -> Tuple[Any, List[Any], Dict[str, Any]]&#xa;        """"""Replace the named argument in ``args, kwargs`` with ``new_value``.&#xa;&#xa;        Returns ``(old_value, args, kwargs)``.  The returned ``args`` and&#xa;        ``kwargs`` objects may not be the same as the input objects, or&#xa;        the input objects may be mutated.&#xa;&#xa;        If the named argument was not found, ``new_value`` will be added&#xa;        to ``kwargs`` and None will be returned as ``old_value``.&#xa;        """"""&#xa;        if self.arg_pos is not None and len(args) > self.arg_pos:&#xa;            # The arg to replace is passed positionally&#xa;            old_value = args[self.arg_pos]&#xa;            args = list(args)  # *args is normally a tuple&#xa;            args[self.arg_pos] = new_value&#xa;        else:&#xa;            # The arg to replace is either omitted or passed by keyword.&#xa;            old_value = kwargs.get(self.name)&#xa;            kwargs[self.name] = new_value&#xa;        return old_value, args, kwargs&#xa;&#xa;&#xa;def timedelta_to_seconds(td):&#xa;    # type: (datetime.timedelta) -> float&#xa;    """"""Equivalent to td.total_seconds() (introduced in python 2.7).""""""&#xa;    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / float(10 ** 6)&#xa;&#xa;&#xa;def _websocket_mask_python(mask, data):&#xa;    # type: (bytes, bytes) -> bytes&#xa;    """"""Websocket masking function.&#xa;&#xa;    `mask` is a `bytes` object of length 4; `data` is a `bytes` object of any length.&#xa;    Returns a `bytes` object of the same length as `data` with the mask applied&#xa;    as specified in section 5.3 of RFC 6455.&#xa;&#xa;    This pure-python implementation may be replaced by an optimized version when available.&#xa;    """"""&#xa;    mask_arr = array.array(""B"", mask)&#xa;    unmasked_arr = array.array(""B"", data)&#xa;    for i in xrange(len(data)):&#xa;        unmasked_arr[i] = unmasked_arr[i] ^ mask_arr[i % 4]&#xa;    if PY3:&#xa;        # tostring was deprecated in py32.  It hasn't been removed,&#xa;        # but since we turn on deprecation warnings in our tests&#xa;        # we need to use the right one.&#xa;        return unmasked_arr.tobytes()&#xa;    else:&#xa;        return unmasked_arr.tostring()&#xa;&#xa;if (os.environ.get('TORNADO_NO_EXTENSION') or&#xa;        os.environ.get('TORNADO_EXTENSION') == '0'):&#xa;    # These environment variables exist to make it easier to do performance&#xa;    # comparisons; they are not guaranteed to remain supported in the future.&#xa;    _websocket_mask = _websocket_mask_python&#xa;else:&#xa;    try:&#xa;        from tornado.speedups import websocket_mask as _websocket_mask&#xa;    except ImportError:&#xa;        if os.environ.get('TORNADO_EXTENSION') == '1':&#xa;            raise&#xa;        _websocket_mask = _websocket_mask_python&#xa;&#xa;&#xa;def doctests():&#xa;    import doctest&#xa;    return doctest.DocTestSuite()&#xa;"
850749|"""""""&#xa;InaSAFE Disaster risk assessment tool developed by AusAid and World Bank&#xa;- **Ftp Client for Retrieving ftp data.**&#xa;&#xa;Contact : ole.moller.nielsen@gmail.com&#xa;&#xa;.. note:: This program is free software; you can redistribute it and/or modify&#xa;     it under the terms of the GNU General Public License as published by&#xa;     the Free Software Foundation; either version 2 of the License, or&#xa;     (at your option) any later version.&#xa;&#xa;""""""&#xa;__author__ = 'imajimatika@gmail.com'&#xa;__version__ = '0.5.0'&#xa;__date__ = '10/01/2013'&#xa;__copyright__ = ('Copyright 2012, Australia Indonesia Facility for '&#xa;                 'Disaster Reduction')&#xa;&#xa;import sys&#xa;import paramiko&#xa;import ntpath&#xa;from stat import S_ISDIR&#xa;from errno import ENOENT&#xa;import os&#xa;import logging&#xa;from utils import mkDir&#xa;&#xa;# The logger is intialised in utils.py by init&#xa;LOGGER = logging.getLogger('InaSAFE')&#xa;&#xa;my_host = '118.97.83.243'&#xa;my_username = 'geospasial'&#xa;try:&#xa;    my_password = os.environ['QUAKE_SERVER_PASSWORD']&#xa;except KeyError:&#xa;    LOGGER.exception('QUAKE_SERVER_PASSWORD not set!')&#xa;    sys.exit()&#xa;&#xa;my_remote_path = 'shakemaps'&#xa;&#xa;&#xa;class SFtpClient:&#xa;    """"""A utility class that contains methods to fetch a listings and files&#xa;    from an SSH protocol""""""&#xa;    def __init__(self, the_host=my_host, the_username=my_username,&#xa;                 the_password=my_password, the_working_dir=my_remote_path):&#xa;&#xa;        self.host = the_host&#xa;        self.username = the_username&#xa;        self.password = the_password&#xa;        self.working_dir = the_working_dir&#xa;&#xa;        # create transport object&#xa;        self.transport = paramiko.Transport(self.host)&#xa;        self.transport.connect(username=self.username, password=self.password)&#xa;&#xa;        # create sftp object&#xa;        self.sftp = paramiko.SFTPClient.from_transport(self.transport)&#xa;&#xa;        # go to remote_path folder, this is the default folder&#xa;        if not self.working_dir is None:&#xa;            self.sftp.chdir(self.working_dir)&#xa;        self.workdir_path = self.sftp.getcwd()&#xa;&#xa;    def download_path(self, remote_path, local_path):&#xa;        """""" Download remote_dir to local_dir.&#xa;        for example : remote_path = '20130111133900' will be download to&#xa;        local_dir/remote_path&#xa;        Must be in the parent directory of remote dir.&#xa;        """"""&#xa;        # Check if remote_dir is exist&#xa;        if not self.is_path_exist(remote_path):&#xa;            print 'remote path is not exist %s' % remote_path&#xa;            return False&#xa;        if self.is_dir(remote_path):&#xa;            # get directory name&#xa;            dir_name = get_path_tail(remote_path)&#xa;            # create directory in local machine&#xa;            local_dir_path = os.path.join(local_path, dir_name)&#xa;            mkDir(local_dir_path)&#xa;            # list all directory in remote path&#xa;            list_dir = self.sftp.listdir(remote_path)&#xa;            # iterate recursive&#xa;            for my_dir in list_dir:&#xa;                new_remote_path = os.path.join(remote_path, my_dir)&#xa;                self.download_path(new_remote_path, local_dir_path)&#xa;        else:&#xa;            # download file to local_path&#xa;            file_name = get_path_tail(remote_path)&#xa;            local_file_path = os.path.join(local_path, file_name)&#xa;&#xa;            LOGGER.info('file %s will be downloaded to %s' % (remote_path,&#xa;                                                        local_file_path))&#xa;            self.sftp.get(remote_path, local_file_path)&#xa;&#xa;    def is_dir(self, path):&#xa;        """"""Check if a path is a directory or not in sftp&#xa;        Reference: http://stackoverflow.com/a/8307575/1198772&#xa;        """"""&#xa;        try:&#xa;            return S_ISDIR(self.sftp.stat(path).st_mode)&#xa;        except IOError:&#xa;            #Path does not exist, so by definition not a directory&#xa;            return False&#xa;&#xa;    def is_path_exist(self, path):&#xa;        """"""os.path.exists for paramiko's SCP object&#xa;        Reference: http://stackoverflow.com/q/850749/1198772&#xa;        """"""&#xa;        try:&#xa;            self.sftp.stat(path)&#xa;        except IOError, e:&#xa;            if e.errno == ENOENT:&#xa;                return False&#xa;            raise&#xa;        else:&#xa;            return True&#xa;&#xa;    def getListing(self, remote_dir=None, my_func=None):&#xa;        """"""Return list of files and directories name under a remote_dir&#xa;        and return true when it is input to my_func&#xa;        """"""&#xa;        if remote_dir is None:&#xa;            remote_dir = self.workdir_path&#xa;        if self.is_path_exist(remote_dir):&#xa;            temp_list = self.sftp.listdir(remote_dir)&#xa;        else:&#xa;            LOGGER.debug('Directory %s is not exist, return None' % remote_dir)&#xa;            return None&#xa;        retval = []&#xa;        for my_temp in temp_list:&#xa;            if my_func(my_temp):&#xa;                retval.append(my_temp)&#xa;        return retval&#xa;&#xa;&#xa;def get_path_tail(path):&#xa;    '''Return tail of a path&#xa;    Reference : http://stackoverflow.com/a/8384788/1198772&#xa;    '''&#xa;    head, tail = ntpath.split(path)&#xa;    return tail or ntpath.basename(head)&#xa;"
2407126|"# -*- coding: utf-'8' ""-*-""&#xa;&#xa;import base64&#xa;try:&#xa;    import simplejson as json&#xa;except ImportError:&#xa;    import json&#xa;import logging&#xa;import urlparse&#xa;import werkzeug.urls&#xa;import urllib2&#xa;&#xa;from openerp.addons.payment.models.payment_acquirer import ValidationError&#xa;from openerp.addons.payment_mercadopago.controllers.main import MercadoPagoController&#xa;from openerp.osv import osv, fields&#xa;from openerp.tools.float_utils import float_compare&#xa;from openerp import SUPERUSER_ID&#xa;&#xa;_logger = logging.getLogger(__name__)&#xa;&#xa;&#xa;from openerp.addons.payment_mercadopago.mercadopago import mercadopago&#xa;&#xa;class AcquirerMercadopago(osv.Model):&#xa;    _inherit = 'payment.acquirer'&#xa;&#xa;    def _get_mercadopago_urls(self, cr, uid, environment, context=None):&#xa;        """""" MercadoPago URLS """"""&#xa;        if environment == 'prod':&#xa;            return {&#xa;                #https://www.mercadopago.com/mla/checkout/pay?pref_id=153438434-6eb25e49-1bb8-4553-95b2-36033be216ad&#xa;                #'mercadopago_form_url': 'https://www.paypal.com/cgi-bin/webscr',&#xa;                'mercadopago_form_url': 'https://www.mercadopago.com/mla/checkout/pay',&#xa;                'mercadopago_rest_url': 'https://api.mercadolibre.com/oauth/token',&#xa;            }&#xa;        else:&#xa;            return {&#xa;                #'mercadopago_form_url': 'https://www.sandbox.paypal.com/cgi-bin/webscr',&#xa;                #https://api.mercadolibre.com/oauth/token&#xa;                'mercadopago_form_url': 'https://sandbox.mercadopago.com/mla/checkout/pay',&#xa;                'mercadopago_rest_url': 'https://api.sandbox.mercadolibre.com/oauth/token',&#xa;            }&#xa;&#xa;    def _get_providers(self, cr, uid, context=None):&#xa;&#xa;        providers = super(AcquirerMercadopago, self)._get_providers(cr, uid, context=context)&#xa;        providers.append(['mercadopago', 'MercadoPago'])&#xa;&#xa;        print ""_get_providers: "", providers&#xa;&#xa;        return providers&#xa;&#xa;    _columns = {&#xa;        'mercadopago_client_id': fields.char('MercadoPago Client Id',256,required_if_provider='mercadopago'),&#xa;        'mercadopago_secret_key': fields.char('MercadoPago Secret Key',256,required_if_provider='mercadopago'),&#xa;&#xa;        'mercadopago_email_account': fields.char('MercadoPago Email ID', required_if_provider='mercadopago'),&#xa;&#xa;        'mercadopago_seller_account': fields.char(&#xa;            'MercadoPago Merchant ID',&#xa;            help='The Merchant ID is used to ensure communications coming from MercadoPago are valid and secured.'),&#xa;&#xa;        'mercadopago_use_ipn': fields.boolean('Use IPN', help='MercadoPago Instant Payment Notification'),&#xa;&#xa;        # Server 2 server&#xa;        'mercadopago_api_enabled': fields.boolean('Use Rest API'),&#xa;        'mercadopago_api_username': fields.char('Rest API Username'),&#xa;        'mercadopago_api_password': fields.char('Rest API Password'),&#xa;        'mercadopago_api_access_token': fields.char('Access Token'),&#xa;        'mercadopago_api_access_token_validity': fields.datetime('Access Token Validity'),&#xa;    }&#xa;&#xa;    _defaults = {&#xa;        'mercadopago_use_ipn': True,&#xa;        'fees_active': False,&#xa;        'fees_dom_fixed': 0.35,&#xa;        'fees_dom_var': 3.4,&#xa;        'fees_int_fixed': 0.35,&#xa;        'fees_int_var': 3.9,&#xa;        'mercadopago_api_enabled': False,&#xa;    }&#xa;&#xa;    def _migrate_mercadopago_account(self, cr, uid, context=None):&#xa;        """""" COMPLETE ME """"""&#xa;&#xa;        #cr.execute('SELECT id, mercadopago_account FROM res_company')&#xa;        #res = cr.fetchall()&#xa;        company_ids = self.pool.get( ""res.company"" ).search(cr,uid,[])&#xa;        for company in self.pool.get('res.company').browse(cr,uid,company_ids):&#xa;            company_id = company.id&#xa;            company_mercadopago_account = company.mercadopago_account&#xa;        #for (company_id, company_mercadopago_account) in res:&#xa;            if company_mercadopago_account:&#xa;                company_mercadopago_ids = self.search(cr, uid, [('company_id', '=', company_id), ('provider', '=', 'mercadopago')], limit=1, context=context)&#xa;                if company_mercadopago_ids:&#xa;                    self.write(cr, uid, company_mercadopago_ids, {'mercadopago_email_account': company_mercadopago_account}, context=context)&#xa;                else:&#xa;                    mercadopago_view = self.pool['ir.model.data'].get_object(cr, uid, 'payment_mercadopago', 'mercadopago_acquirer_button')&#xa;                    self.create(cr, uid, {&#xa;                        'name': 'MercadoPago',&#xa;                        'provider': 'mercadopago',&#xa;                        'mercadopago_email_account': company_mercadopago_account,&#xa;                        'view_template_id': mercadopago_view.id,&#xa;                    }, context=context)&#xa;        return True&#xa;&#xa;    def mercadopago_compute_fees(self, cr, uid, id, amount, currency_id, country_id, context=None):&#xa;        """""" Compute mercadopago fees.&#xa;&#xa;            :param float amount: the amount to pay&#xa;            :param integer country_id: an ID of a res.country, or None. This is&#xa;                                       the customer's country, to be compared to&#xa;                                       the acquirer company country.&#xa;            :return float fees: computed fees&#xa;        """"""&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;        if not acquirer.fees_active:&#xa;            return 0.0&#xa;        country = self.pool['res.country'].browse(cr, uid, country_id, context=context)&#xa;        if country and acquirer.company_id.country_id.id == country.id:&#xa;            percentage = acquirer.fees_dom_var&#xa;            fixed = acquirer.fees_dom_fixed&#xa;        else:&#xa;            percentage = acquirer.fees_int_var&#xa;            fixed = acquirer.fees_int_fixed&#xa;        fees = (percentage / 100.0 * amount + fixed ) / (1 - percentage / 100.0)&#xa;        return fees&#xa;&#xa;    def mercadopago_form_generate_values(self, cr, uid, id, partner_values, tx_values, context=None):&#xa;        base_url = self.pool['ir.config_parameter'].get_param(cr, SUPERUSER_ID, 'web.base.url')&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;&#xa;        print ""mercadopago_form_generate_values: tx_values: "", tx_values&#xa;        print ""partner_values:"", partner_values&#xa;&#xa;        MPago = False&#xa;        MPagoPrefId = False&#xa;&#xa;        if acquirer.mercadopago_client_id and acquirer.mercadopago_secret_key:&#xa;            MPago = mercadopago.MP( acquirer.mercadopago_client_id, acquirer.mercadopago_secret_key )&#xa;            print ""MPago: "", MPago&#xa;        else:&#xa;            error_msg = 'YOU MUST COMPLETE acquirer.mercadopago_client_id and acquirer.mercadopago_secret_key'&#xa;            _logger.error(error_msg)&#xa;            raise ValidationError(error_msg)&#xa;&#xa;        jsondump = """"&#xa;    &#xa;        if MPago:&#xa;&#xa;            if acquirer.environment==""prod"":&#xa;                MPago.sandbox_mode(False)&#xa;            else:&#xa;                MPago.sandbox_mode(True)&#xa;&#xa;            MPagoToken = MPago.get_access_token()&#xa;&#xa;            preference = {&#xa;                ""items"": [&#xa;                {&#xa;                    ""title"": ""Orden Ecommerce ""+ tx_values[""reference""] ,&#xa;                    #""picture_url"": ""https://www.mercadopago.com/org-img/MP3/home/logomp3.gif"",&#xa;                    ""quantity"": 1,&#xa;                    ""currency_id"":  tx_values['currency'] and tx_values['currency'].name or '',&#xa;                    ""unit_price"": tx_values[""amount""],&#xa;                    #""category_id"": ""Categora"",&#xa;                }&#xa;                ]&#xa;                ,&#xa;                ""payer"": {&#xa;		            ""name"": partner_values[""name""],&#xa;		            ""surname"": partner_values[""first_name""],&#xa;		            ""email"": partner_values[""email""],&#xa;#		            ""date_created"": ""2015-01-29T11:51:49.570-04:00"",&#xa;#		            ""phone"": {&#xa;#			            ""area_code"": ""+5411"",&#xa;#			            ""number"": partner_values[""phone""]&#xa;#		            },&#xa;#		            ""identification"": {&#xa;#			            ""type"": ""DNI"",&#xa;#			            ""number"": ""12345678""&#xa;#		            },&#xa;#		            ""address"": {&#xa;#			            ""street_name"": partner_values[""address""],&#xa;#			            ""street_number"": """",&#xa;#			            ""zip_code"": partner_values[""zip""]&#xa;#		            } contni&#xa;	            },&#xa;	            ""back_urls"": {&#xa;		            ""success"": '%s' % urlparse.urljoin( base_url, MercadoPagoController._return_url),&#xa;		            ""failure"": '%s' % urlparse.urljoin( base_url, MercadoPagoController._cancel_url),&#xa;		            ""pending"": '%s' % urlparse.urljoin( base_url, MercadoPagoController._return_url)&#xa;	            },&#xa;	            ""auto_return"": ""approved"",&#xa;#	            ""payment_methods"": {&#xa;#		            ""excluded_payment_methods"": [&#xa;#			            {&#xa;#				            ""id"": ""amex""&#xa;#			            }&#xa;#		            ],&#xa;#		            ""excluded_payment_types"": [&#xa;#			            {&#xa;#				            ""id"": ""ticket""&#xa;#			            }&#xa;#		            ],&#xa;#		            ""installments"": 24,&#xa;#		            ""default_payment_method_id"": '',&#xa;#		            ""default_installments"": '',&#xa;#	            },&#xa;#	            ""shipments"": {&#xa;#		            ""receiver_address"":&#xa;#		             {&#xa;#			            ""zip_code"": ""1430"",&#xa;#			            ""street_number"": 123,&#xa;#			            ""street_name"": ""Calle Trece"",&#xa;#			            ""floor"": 4,&#xa;#			            ""apartment"": ""C""&#xa;#		            }&#xa;#	            },&#xa;	            ""notification_url"": '%s' % urlparse.urljoin( base_url, MercadoPagoController._notify_url),&#xa;	            ""external_reference"": tx_values[""reference""],&#xa;	            ""expires"": True,&#xa;	            ""expiration_date_from"": ""2015-01-29T11:51:49.570-04:00"",&#xa;	            ""expiration_date_to"": ""2015-02-28T11:51:49.570-04:00""&#xa;                }&#xa;&#xa;            print ""preference:"", preference&#xa;&#xa;            preferenceResult = MPago.create_preference(preference)&#xa;        &#xa;            print ""preferenceResult: "", preferenceResult&#xa;            if 'response' in preferenceResult:&#xa;                if 'id' in preferenceResult['response']:&#xa;                    MPagoPrefId = preferenceResult['response']['id']&#xa;            else:&#xa;                error_msg = 'Returning response is:'&#xa;                error_msg+= json.dumps(preferenceResult, indent=2)&#xa;                _logger.error(error_msg)&#xa;                raise ValidationError(error_msg)&#xa;                &#xa;            &#xa;            if acquirer.environment==""prod"":&#xa;                linkpay = preferenceResult['response']['init_point']&#xa;            else:&#xa;                linkpay = preferenceResult['response']['sandbox_init_point']&#xa;&#xa;            jsondump = json.dumps( preferenceResult, indent=2 )&#xa;&#xa;            print ""linkpay:"", linkpay&#xa;            print ""jsondump:"", jsondump&#xa;            print ""MPagoPrefId: "", MPagoPrefId&#xa;            print ""MPagoToken: "", MPagoToken&#xa;            &#xa;&#xa;        mercadopago_tx_values = dict(tx_values)&#xa;        if MPagoPrefId:&#xa;            mercadopago_tx_values.update({&#xa;            'pref_id': MPagoPrefId,&#xa;#            'cmd': '_xclick',&#xa;#            'business': acquirer.mercadopago_email_account,&#xa;#            'item_name': tx_values['reference'],&#xa;#            'item_number': tx_values['reference'],&#xa;#            'amount': tx_values['amount'],&#xa;#            'currency_code': tx_values['currency'] and tx_values['currency'].name or '',&#xa;#            'address1': partner_values['address'],&#xa;#            'city': partner_values['city'],&#xa;#            'country': partner_values['country'] and partner_values['country'].name or '',&#xa;#            'state': partner_values['state'] and partner_values['state'].name or '',&#xa;#            'email': partner_values['email'],&#xa;#            'zip': partner_values['zip'],&#xa;#            'first_name': partner_values['first_name'],&#xa;#            'last_name': partner_values['last_name'],&#xa;#            'return': '%s' % urlparse.urljoin(base_url, MercadoPagoController._return_url),&#xa;#            'notify_url': '%s' % urlparse.urljoin(base_url, MercadoPagoController._notify_url),&#xa;#            'cancel_return': '%s' % urlparse.urljoin(base_url, MercadoPagoController._cancel_url),&#xa;            })&#xa;&#xa;#        if acquirer.fees_active:&#xa;#            mercadopago_tx_values['handling'] = '%.2f' % mercadopago_tx_values.pop('fees', 0.0)&#xa;#        if mercadopago_tx_values.get('return_url'):&#xa;#            mercadopago_tx_values['custom'] = json.dumps({'return_url': '%s' % mercadopago_tx_values.pop('return_url')})&#xa;        return partner_values, mercadopago_tx_values&#xa;&#xa;    def mercadopago_get_form_action_url(self, cr, uid, id, context=None):&#xa;        acquirer = self.browse(cr, uid, id, context=context)&#xa;        mercadopago_urls = self._get_mercadopago_urls(cr, uid, acquirer.environment, context=context)['mercadopago_form_url']&#xa;#        mercadopago_urls = mercadopago_urls + ""?pref_id="" + &#xa;        print ""mercadopago_get_form_action_url: "", mercadopago_urls&#xa;        return mercadopago_urls&#xa;&#xa;    def _mercadopago_s2s_get_access_token(self, cr, uid, ids, context=None):&#xa;        """"""&#xa;        Note: see # see http://stackoverflow.com/questions/2407126/python-urllib2-basic-auth-problem&#xa;        for explanation why we use Authorization header instead of urllib2&#xa;        password manager&#xa;        """"""&#xa;        res = dict.fromkeys(ids, False)&#xa;        parameters = werkzeug.url_encode({'grant_type': 'client_credentials'})&#xa;&#xa;        for acquirer in self.browse(cr, uid, ids, context=context):&#xa;            tx_url = self._get_mercadopago_urls(cr, uid, acquirer.environment)['mercadopago_rest_url']&#xa;            request = urllib2.Request(tx_url, parameters)&#xa;&#xa;            # add other headers (https://developer.paypal.com/webapps/developer/docs/integration/direct/make-your-first-call/)&#xa;            request.add_header('Accept', 'application/json')&#xa;            request.add_header('Accept-Language', 'en_US')&#xa;&#xa;            # add authorization header&#xa;            base64string = base64.encodestring('%s:%s' % (&#xa;                acquirer.mercadopago_api_username,&#xa;                acquirer.mercadopago_api_password)&#xa;            ).replace('\n', '')&#xa;            request.add_header(""Authorization"", ""Basic %s"" % base64string)&#xa;&#xa;            request = urllib2.urlopen(request)&#xa;            result = request.read()&#xa;            res[acquirer.id] = json.loads(result).get('access_token')&#xa;            request.close()&#xa;        return res&#xa;&#xa;&#xa;class TxMercadoPago(osv.Model):&#xa;    _inherit = 'payment.transaction'&#xa;&#xa;    _columns = {&#xa;        'mercadopago_txn_id': fields.char('Transaction ID'),&#xa;        'mercadopago_txn_type': fields.char('Transaction type'),&#xa;    }&#xa;&#xa;    # --------------------------------------------------&#xa;    # FORM RELATED METHODS&#xa;    # --------------------------------------------------&#xa;&#xa;    def _mercadopago_form_get_tx_from_data(self, cr, uid, data, context=None):&#xa;#        reference, txn_id = data.get('external_reference'), data.get('txn_id')&#xa;        reference, collection_id = data.get('external_reference'), data.get('collection_id')&#xa;        if not reference or not collection_id:&#xa;            error_msg = 'MercadoPago: received data with missing reference (%s) or collection_id (%s)' % (reference,collection_id)&#xa;            _logger.error(error_msg)&#xa;            raise ValidationError(error_msg)&#xa;&#xa;        # find tx -> @TDENOTE use txn_id ?&#xa;        tx_ids = self.pool['payment.transaction'].search(cr, uid, [('reference', '=', reference)], context=context)&#xa;        if not tx_ids or len(tx_ids) > 1:&#xa;            error_msg = 'MercadoPago: received data for reference %s' % (reference)&#xa;            if not tx_ids:&#xa;                error_msg += '; no order found'&#xa;            else:&#xa;                error_msg += '; multiple order found'&#xa;            _logger.error(error_msg)&#xa;            raise ValidationError(error_msg)&#xa;        return self.browse(cr, uid, tx_ids[0], context=context)&#xa;&#xa;    def _mercadopago_form_get_invalid_parameters(self, cr, uid, tx, data, context=None):&#xa;        invalid_parameters = []&#xa;        _logger.warning('Received a notification from MercadoLibre.')&#xa;&#xa;        # TODO: txn_id: shoudl be false at draft, set afterwards, and verified with txn details&#xa;#        if tx.acquirer_reference and data.get('txn_id') != tx.acquirer_reference:&#xa;#            invalid_parameters.append(('txn_id', data.get('txn_id'), tx.acquirer_reference))&#xa;        # check what is buyed&#xa;#        if float_compare(float(data.get('mc_gross', '0.0')), (tx.amount + tx.fees), 2) != 0:&#xa;#            invalid_parameters.append(('mc_gross', data.get('mc_gross'), '%.2f' % tx.amount))  # mc_gross is amount + fees&#xa;#        if data.get('mc_currency') != tx.currency_id.name:&#xa;#            invalid_parameters.append(('mc_currency', data.get('mc_currency'), tx.currency_id.name))&#xa;#        if 'handling_amount' in data and float_compare(float(data.get('handling_amount')), tx.fees, 2) != 0:&#xa;#            invalid_parameters.append(('handling_amount', data.get('handling_amount'), tx.fees))&#xa;        # check buyer&#xa;#        if tx.partner_reference and data.get('payer_id') != tx.partner_reference:&#xa;#            invalid_parameters.append(('payer_id', data.get('payer_id'), tx.partner_reference))&#xa;        # check seller&#xa;#        if data.get('receiver_email') != tx.acquirer_id.mercadopago_email_account:&#xa;#            invalid_parameters.append(('receiver_email', data.get('receiver_email'), tx.acquirer_id.mercadopago_email_account))&#xa;#        if data.get('receiver_id') and tx.acquirer_id.mercadopago_seller_account and data['receiver_id'] != tx.acquirer_id.mercadopago_seller_account:&#xa;#            invalid_parameters.append(('receiver_id', data.get('receiver_id'), tx.acquirer_id.mercadopago_seller_account))&#xa;&#xa;        return invalid_parameters&#xa;&#xa;#From https://developers.mercadopago.com/documentacion/notificaciones-de-pago&#xa;#&#xa;#approved 	El pago fue aprobado y acreditado.&#xa;#pending 	El usuario no complet el proceso de pago.&#xa;#in_process	El pago est siendo revisado.&#xa;#rejected 	El pago fue rechazado. El usuario puede intentar nuevamente.&#xa;#refunded (estado terminal) 	El pago fue devuelto al usuario.&#xa;#cancelled (estado terminal) 	El pago fue cancelado por superar el tiempo necesario para realizar el pago o por una de las partes.&#xa;#in_mediation 	Se inici una disputa para el pago.&#xa;#charged_back (estado terminal) 	Se realiz un contracargo en la tarjeta de crdito.&#xa;    #called by Trans.form_feedback(...) > %s_form_validate(...)&#xa;    def _mercadopago_form_validate(self, cr, uid, tx, data, context=None):&#xa;        status = data.get('collection_status')&#xa;        data = {&#xa;            'acquirer_reference': data.get('external_reference'),&#xa;            'mercadopago_txn_type': data.get('payment_type')            &#xa;        }&#xa;        if status in ['approved', 'processed']:&#xa;            _logger.info('Validated MercadoPago payment for tx %s: set as done' % (tx.reference))&#xa;            data.update(state='done', date_validate=data.get('payment_date', fields.datetime.now()))&#xa;            return tx.write(data)&#xa;        elif status in ['pending', 'in_process','in_mediation']:&#xa;            _logger.info('Received notification for MercadoPago payment %s: set as pending' % (tx.reference))&#xa;            data.update(state='pending', state_message=data.get('pending_reason', ''))&#xa;            return tx.write(data)&#xa;        elif status in ['cancelled','refunded','charged_back','rejected']:&#xa;            _logger.info('Received notification for MercadoPago payment %s: set as cancelled' % (tx.reference))&#xa;            data.update(state='cancel', state_message=data.get('cancel_reason', ''))&#xa;            return tx.write(data)            &#xa;        else:&#xa;            error = 'Received unrecognized status for MercadoPago payment %s: %s, set as error' % (tx.reference, status)&#xa;            _logger.info(error)&#xa;            data.update(state='error', state_message=error)&#xa;            return tx.write(data)&#xa;&#xa;    # --------------------------------------------------&#xa;    # SERVER2SERVER RELATED METHODS&#xa;    # --------------------------------------------------&#xa;&#xa;    def _mercadopago_try_url(self, request, tries=3, context=None):&#xa;        """""" Try to contact MercadoPago. Due to some issues, internal service errors&#xa;        seem to be quite frequent. Several tries are done before considering&#xa;        the communication as failed.&#xa;&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        done, res = False, None&#xa;        while (not done and tries):&#xa;            try:&#xa;                res = urllib2.urlopen(request)&#xa;                done = True&#xa;            except urllib2.HTTPError as e:&#xa;                res = e.read()&#xa;                e.close()&#xa;                if tries and res and json.loads(res)['name'] == 'INTERNAL_SERVICE_ERROR':&#xa;                    _logger.warning('Failed contacting MercadoPago, retrying (%s remaining)' % tries)&#xa;            tries = tries - 1&#xa;        if not res:&#xa;            pass&#xa;            # raise openerp.exceptions.&#xa;        result = res.read()&#xa;        res.close()&#xa;        return result&#xa;&#xa;    def _mercadopago_s2s_send(self, cr, uid, values, cc_values, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        tx_id = self.create(cr, uid, values, context=context)&#xa;        tx = self.browse(cr, uid, tx_id, context=context)&#xa;&#xa;        headers = {&#xa;            'Content-Type': 'application/json',&#xa;            'Authorization': 'Bearer %s' % tx.acquirer_id._mercadopago_s2s_get_access_token()[tx.acquirer_id.id],&#xa;        }&#xa;        data = {&#xa;            'intent': 'sale',&#xa;            'transactions': [{&#xa;                'amount': {&#xa;                    'total': '%.2f' % tx.amount,&#xa;                    'currency': tx.currency_id.name,&#xa;                },&#xa;                'description': tx.reference,&#xa;            }]&#xa;        }&#xa;        if cc_values:&#xa;            data['payer'] = {&#xa;                'payment_method': 'credit_card',&#xa;                'funding_instruments': [{&#xa;                    'credit_card': {&#xa;                        'number': cc_values['number'],&#xa;                        'type': cc_values['brand'],&#xa;                        'expire_month': cc_values['expiry_mm'],&#xa;                        'expire_year': cc_values['expiry_yy'],&#xa;                        'cvv2': cc_values['cvc'],&#xa;                        'first_name': tx.partner_name,&#xa;                        'last_name': tx.partner_name,&#xa;                        'billing_address': {&#xa;                            'line1': tx.partner_address,&#xa;                            'city': tx.partner_city,&#xa;                            'country_code': tx.partner_country_id.code,&#xa;                            'postal_code': tx.partner_zip,&#xa;                        }&#xa;                    }&#xa;                }]&#xa;            }&#xa;        else:&#xa;            # TODO: complete redirect URLs&#xa;            data['redirect_urls'] = {&#xa;                # 'return_url': 'http://example.com/your_redirect_url/',&#xa;                # 'cancel_url': 'http://example.com/your_cancel_url/',&#xa;            },&#xa;            data['payer'] = {&#xa;                'payment_method': 'mercadopago',&#xa;            }&#xa;        data = json.dumps(data)&#xa;&#xa;        request = urllib2.Request('https://api.sandbox.paypal.com/v1/payments/payment', data, headers)&#xa;        result = self._mercadopago_try_url(request, tries=3, context=context)&#xa;        return (tx_id, result)&#xa;&#xa;    def _mercadopago_s2s_get_invalid_parameters(self, cr, uid, tx, data, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        invalid_parameters = []&#xa;        return invalid_parameters&#xa;&#xa;    def _mercadopago_s2s_validate(self, cr, uid, tx, data, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        values = json.loads(data)&#xa;        status = values.get('state')&#xa;        if status in ['approved']:&#xa;            _logger.info('Validated Mercadopago s2s payment for tx %s: set as done' % (tx.reference))&#xa;            tx.write({&#xa;                'state': 'done',&#xa;                'date_validate': values.get('udpate_time', fields.datetime.now()),&#xa;                'mercadopago_txn_id': values['id'],&#xa;            })&#xa;            return True&#xa;        elif status in ['pending', 'expired']:&#xa;            _logger.info('Received notification for MercadoPago s2s payment %s: set as pending' % (tx.reference))&#xa;            tx.write({&#xa;                'state': 'pending',&#xa;                # 'state_message': data.get('pending_reason', ''),&#xa;                'mercadopago_txn_id': values['id'],&#xa;            })&#xa;            return True&#xa;        else:&#xa;            error = 'Received unrecognized status for MercadoPago s2s payment %s: %s, set as error' % (tx.reference, status)&#xa;            _logger.info(error)&#xa;            tx.write({&#xa;                'state': 'error',&#xa;                # 'state_message': error,&#xa;                'mercadopago_txn_id': values['id'],&#xa;            })&#xa;            return False&#xa;&#xa;    def _mercadopago_s2s_get_tx_status(self, cr, uid, tx, context=None):&#xa;        """"""&#xa;         .. versionadded:: pre-v8 saas-3&#xa;         .. warning::&#xa;&#xa;            Experimental code. You should not use it before OpenERP v8 official&#xa;            release.&#xa;        """"""&#xa;        # TDETODO: check tx.mercadopago_txn_id is set&#xa;        headers = {&#xa;            'Content-Type': 'application/json',&#xa;            'Authorization': 'Bearer %s' % tx.acquirer_id._mercadopago_s2s_get_access_token()[tx.acquirer_id.id],&#xa;        }&#xa;        url = 'https://api.sandbox.paypal.com/v1/payments/payment/%s' % (tx.mercadopago_txn_id)&#xa;        request = urllib2.Request(url, headers=headers)&#xa;        data = self._mercadopago_try_url(request, tries=3, context=context)&#xa;        return self.s2s_feedback(cr, uid, tx.id, data, context=context)&#xa;"
